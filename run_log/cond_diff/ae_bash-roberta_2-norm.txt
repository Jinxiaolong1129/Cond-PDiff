stty: 'standard input': Inappropriate ioctl for device
2024-11-05 00:24:07,373 - INFO - CUDA_VISIBLE_DEVICES: 0,1,2,7
2024-11-05 00:24:07,373 - INFO - layer_num: [[11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]
2024-11-05 00:24:07,373 - INFO - datasets_para: [['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']]
2024-11-05 00:24:07,373 - INFO - gpu_id: 0
2024-11-05 00:24:07,373 - INFO - batch_size: 256
2024-11-05 00:24:07,373 - INFO - epochs: 10000
2024-11-05 00:24:07,373 - INFO - ae_test: True
2024-11-05 00:24:07,373 - INFO - load_ae_checkpoint: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.999_end_epoch.10000/model/auto_encoder_model_9999.pth
2024-11-05 00:24:07,373 - INFO - lr: 0.001
2024-11-05 00:24:07,373 - INFO - patience: 5000
transformers.__file__: /home/jin509/para_diff/lora-cond-p-diff/src/transformers/__init__.py 
 peft.__file_: /home/jin509/para_diff/lora-cond-p-diff/src/peft/__init__.py
=====================================
layer number: [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
datasets: ['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']
=====================================
wandb: Tracking run with wandb version 0.12.21
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dataset_data = torch.tensor(dataset_matrix, dtype=torch.float32)
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return (torch.tensor(data_row, dtype=torch.float32) - mean) / std
2024-11-05 00:24:59,121 - INFO - Load model: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.999_end_epoch.10000/model/auto_encoder_model_9999.pth
2024-11-05 00:24:59,121 - INFO - Start epoch: 9999
2024-11-05 00:24:59,273 - INFO - Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.999_end_epoch.10000/model/auto_encoder_model_9999.pth with best metric -inf
2024-11-05 00:24:59,276 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.9999_end_epoch.10000/model
2024-11-05 00:24:59,277 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.9999_end_epoch.10000/log
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 00:24:59,681 - INFO - =========== ddpm diffusion model | num_conditions: 6 ===========
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
2024-11-05 00:25:02,994 - INFO - [diffusion][Epoch 0] Load model: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_7999.pth
2024-11-05 00:25:03,374 - INFO - [diffusion][Epoch 0] Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_7999.pth
2024-11-05 00:25:03,376 - INFO - [diffusion][Epoch 0] ========================================
2024-11-05 00:25:03,413 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.12000/clip_pdiff_model
2024-11-05 00:25:03,414 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.12000/log
2024-11-05 00:25:03,414 - INFO - [diffusion][Epoch 0] Training begin
2024-11-05 00:25:03,416 - INFO - [diffusion][Epoch 7999] Epoch 8000/12000
=====================================
Test the auto_encoder_model
training_parameters:
  ddpm_eval_epoch: 10
  ddpm_save_epoch: 1000
  ddpm_start_epoch: 1
  ddpm_end_epoch: 12000
  batch_size: 256
  lr: 0.001
  patience: 5000
  optimizer: Adam
  loss_function: MSE
  metrics:
  - accuracy
  weight_decay: 2.0e-06
  ddpm_test: false
  test_similarity: false
model_mean_type: eps
model_var_type: fixedlarge
loss_type: mse
model_parameters:
  in_channel: 1
  in_dim: 12
  num_conditions: 2
  cond_emb_size: 512
beta_schedule:
  start: 0.0001
  end: 0.02
  schedule: linear
  n_timestep: 1000
glue_test_parameters:
  config_path: config/multiple/glue.json

2024-11-05 00:25:08,661 - INFO - [diffusion][Epoch 7999] diffusion training Loss: 0.14231262914836407
2024-11-05 00:25:08,663 - INFO - [diffusion][Epoch 7999] diffusion learning rate: 0.001
2024-11-05 00:25:08,778 - INFO - [diffusion][Epoch 7999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:08,780 - INFO - [diffusion][Epoch 8000] Epoch 8001/12000
2024-11-05 00:25:12,936 - INFO - [diffusion][Epoch 8000] diffusion training Loss: 0.13793648406863213
2024-11-05 00:25:12,938 - INFO - [diffusion][Epoch 8000] diffusion learning rate: 0.001
2024-11-05 00:25:12,941 - INFO - [diffusion][Epoch 8000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:12,943 - INFO - [diffusion][Epoch 8001] Epoch 8002/12000
2024-11-05 00:25:17,127 - INFO - [diffusion][Epoch 8001] diffusion training Loss: 0.11858933977782726
2024-11-05 00:25:17,130 - INFO - [diffusion][Epoch 8001] diffusion learning rate: 0.001
2024-11-05 00:25:17,132 - INFO - [diffusion][Epoch 8001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:17,133 - INFO - [diffusion][Epoch 8002] Epoch 8003/12000
2024-11-05 00:25:21,186 - INFO - [diffusion][Epoch 8002] diffusion training Loss: 0.10969053581357002
2024-11-05 00:25:21,188 - INFO - [diffusion][Epoch 8002] diffusion learning rate: 0.001
2024-11-05 00:25:21,190 - INFO - [diffusion][Epoch 8002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:21,191 - INFO - [diffusion][Epoch 8003] Epoch 8004/12000
2024-11-05 00:25:25,366 - INFO - [diffusion][Epoch 8003] diffusion training Loss: 0.08485151454806328
2024-11-05 00:25:25,368 - INFO - [diffusion][Epoch 8003] diffusion learning rate: 0.001
2024-11-05 00:25:25,370 - INFO - [diffusion][Epoch 8003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:25,371 - INFO - [diffusion][Epoch 8004] Epoch 8005/12000
2024-11-05 00:25:29,640 - INFO - [diffusion][Epoch 8004] diffusion training Loss: 0.07703153230249882
2024-11-05 00:25:29,642 - INFO - [diffusion][Epoch 8004] diffusion learning rate: 0.001
2024-11-05 00:25:29,644 - INFO - [diffusion][Epoch 8004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:29,645 - INFO - [diffusion][Epoch 8005] Epoch 8006/12000
2024-11-05 00:25:33,742 - INFO - [diffusion][Epoch 8005] diffusion training Loss: 0.08363919332623482
2024-11-05 00:25:33,744 - INFO - [diffusion][Epoch 8005] diffusion learning rate: 0.001
2024-11-05 00:25:33,745 - INFO - [diffusion][Epoch 8005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:33,746 - INFO - [diffusion][Epoch 8006] Epoch 8007/12000
2024-11-05 00:25:38,132 - INFO - [diffusion][Epoch 8006] diffusion training Loss: 0.07116414979100227
2024-11-05 00:25:38,134 - INFO - [diffusion][Epoch 8006] diffusion learning rate: 0.001
2024-11-05 00:25:38,136 - INFO - [diffusion][Epoch 8006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:38,137 - INFO - [diffusion][Epoch 8007] Epoch 8008/12000
2024-11-05 00:25:42,406 - INFO - [diffusion][Epoch 8007] diffusion training Loss: 0.0745549164712429
2024-11-05 00:25:42,410 - INFO - [diffusion][Epoch 8007] diffusion learning rate: 0.001
2024-11-05 00:25:42,413 - INFO - [diffusion][Epoch 8007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:42,415 - INFO - [diffusion][Epoch 8008] Epoch 8009/12000
2024-11-05 00:25:46,786 - INFO - [diffusion][Epoch 8008] diffusion training Loss: 0.06695553101599216
2024-11-05 00:25:46,788 - INFO - [diffusion][Epoch 8008] diffusion learning rate: 0.001
2024-11-05 00:25:46,790 - INFO - [diffusion][Epoch 8008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:46,792 - INFO - [diffusion][Epoch 8009] Epoch 8010/12000
2024-11-05 00:25:50,845 - INFO - [diffusion][Epoch 8009] diffusion training Loss: 0.07197562418878078
2024-11-05 00:25:50,847 - INFO - [diffusion][Epoch 8009] diffusion learning rate: 0.001
2024-11-05 00:25:50,849 - INFO - [diffusion][Epoch 8009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:50,851 - INFO - [diffusion][Epoch 8010] Epoch 8011/12000
2024-11-05 00:25:55,096 - INFO - [diffusion][Epoch 8010] diffusion training Loss: 0.06558541394770145
2024-11-05 00:25:55,098 - INFO - [diffusion][Epoch 8010] diffusion learning rate: 0.001
2024-11-05 00:25:55,100 - INFO - [diffusion][Epoch 8010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:55,101 - INFO - [diffusion][Epoch 8011] Epoch 8012/12000
2024-11-05 00:25:59,121 - INFO - [diffusion][Epoch 8011] diffusion training Loss: 0.07043180242180824
2024-11-05 00:25:59,123 - INFO - [diffusion][Epoch 8011] diffusion learning rate: 0.001
2024-11-05 00:25:59,125 - INFO - [diffusion][Epoch 8011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:59,126 - INFO - [diffusion][Epoch 8012] Epoch 8013/12000
2024-11-05 00:26:03,157 - INFO - [diffusion][Epoch 8012] diffusion training Loss: 0.07012730464339256
2024-11-05 00:26:03,159 - INFO - [diffusion][Epoch 8012] diffusion learning rate: 0.001
2024-11-05 00:26:03,160 - INFO - [diffusion][Epoch 8012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:03,162 - INFO - [diffusion][Epoch 8013] Epoch 8014/12000
2024-11-05 00:26:07,326 - INFO - [diffusion][Epoch 8013] diffusion training Loss: 0.07203609123826027
2024-11-05 00:26:07,328 - INFO - [diffusion][Epoch 8013] diffusion learning rate: 0.001
2024-11-05 00:26:07,330 - INFO - [diffusion][Epoch 8013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:07,331 - INFO - [diffusion][Epoch 8014] Epoch 8015/12000
2024-11-05 00:26:11,515 - INFO - [diffusion][Epoch 8014] diffusion training Loss: 0.07082694955170155
2024-11-05 00:26:11,518 - INFO - [diffusion][Epoch 8014] diffusion learning rate: 0.001
2024-11-05 00:26:11,520 - INFO - [diffusion][Epoch 8014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:11,521 - INFO - [diffusion][Epoch 8015] Epoch 8016/12000
2024-11-05 00:26:15,538 - INFO - [diffusion][Epoch 8015] diffusion training Loss: 0.0668029896914959
2024-11-05 00:26:15,540 - INFO - [diffusion][Epoch 8015] diffusion learning rate: 0.001
2024-11-05 00:26:15,542 - INFO - [diffusion][Epoch 8015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:15,543 - INFO - [diffusion][Epoch 8016] Epoch 8017/12000
2024-11-05 00:26:19,643 - INFO - [diffusion][Epoch 8016] diffusion training Loss: 0.0658696535974741
2024-11-05 00:26:19,645 - INFO - [diffusion][Epoch 8016] diffusion learning rate: 0.001
2024-11-05 00:26:19,647 - INFO - [diffusion][Epoch 8016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:19,648 - INFO - [diffusion][Epoch 8017] Epoch 8018/12000
2024-11-05 00:26:23,753 - INFO - [diffusion][Epoch 8017] diffusion training Loss: 0.06815523561090231
2024-11-05 00:26:23,756 - INFO - [diffusion][Epoch 8017] diffusion learning rate: 0.001
2024-11-05 00:26:23,758 - INFO - [diffusion][Epoch 8017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:23,759 - INFO - [diffusion][Epoch 8018] Epoch 8019/12000
2024-11-05 00:26:27,894 - INFO - [diffusion][Epoch 8018] diffusion training Loss: 0.0716421827673912
2024-11-05 00:26:27,895 - INFO - [diffusion][Epoch 8018] diffusion learning rate: 0.001
2024-11-05 00:26:27,897 - INFO - [diffusion][Epoch 8018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:27,898 - INFO - [diffusion][Epoch 8019] Epoch 8020/12000
2024-11-05 00:26:32,021 - INFO - [diffusion][Epoch 8019] diffusion training Loss: 0.06839096173644066
2024-11-05 00:26:32,023 - INFO - [diffusion][Epoch 8019] diffusion learning rate: 0.001
2024-11-05 00:26:32,025 - INFO - [diffusion][Epoch 8019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:32,027 - INFO - [diffusion][Epoch 8020] Epoch 8021/12000
2024-11-05 00:26:36,669 - INFO - [diffusion][Epoch 8020] diffusion training Loss: 0.06854395754635334
2024-11-05 00:26:36,671 - INFO - [diffusion][Epoch 8020] diffusion learning rate: 0.001
2024-11-05 00:26:36,673 - INFO - [diffusion][Epoch 8020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:36,674 - INFO - [diffusion][Epoch 8021] Epoch 8022/12000
2024-11-05 00:26:40,833 - INFO - [diffusion][Epoch 8021] diffusion training Loss: 0.0721704326570034
2024-11-05 00:26:40,835 - INFO - [diffusion][Epoch 8021] diffusion learning rate: 0.001
2024-11-05 00:26:40,837 - INFO - [diffusion][Epoch 8021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:40,838 - INFO - [diffusion][Epoch 8022] Epoch 8023/12000
2024-11-05 00:26:44,923 - INFO - [diffusion][Epoch 8022] diffusion training Loss: 0.07043645530939102
2024-11-05 00:26:44,925 - INFO - [diffusion][Epoch 8022] diffusion learning rate: 0.001
2024-11-05 00:26:44,927 - INFO - [diffusion][Epoch 8022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:44,929 - INFO - [diffusion][Epoch 8023] Epoch 8024/12000
2024-11-05 00:26:49,265 - INFO - [diffusion][Epoch 8023] diffusion training Loss: 0.07218026742339134
2024-11-05 00:26:49,267 - INFO - [diffusion][Epoch 8023] diffusion learning rate: 0.001
2024-11-05 00:26:49,269 - INFO - [diffusion][Epoch 8023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:49,271 - INFO - [diffusion][Epoch 8024] Epoch 8025/12000
2024-11-05 00:26:53,483 - INFO - [diffusion][Epoch 8024] diffusion training Loss: 0.07191682234406471
2024-11-05 00:26:53,485 - INFO - [diffusion][Epoch 8024] diffusion learning rate: 0.001
2024-11-05 00:26:53,487 - INFO - [diffusion][Epoch 8024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:53,488 - INFO - [diffusion][Epoch 8025] Epoch 8026/12000
2024-11-05 00:26:57,790 - INFO - [diffusion][Epoch 8025] diffusion training Loss: 0.06892373785376549
2024-11-05 00:26:57,793 - INFO - [diffusion][Epoch 8025] diffusion learning rate: 0.001
2024-11-05 00:26:57,796 - INFO - [diffusion][Epoch 8025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:57,797 - INFO - [diffusion][Epoch 8026] Epoch 8027/12000
2024-11-05 00:27:02,092 - INFO - [diffusion][Epoch 8026] diffusion training Loss: 0.06853546388447285
2024-11-05 00:27:02,095 - INFO - [diffusion][Epoch 8026] diffusion learning rate: 0.001
2024-11-05 00:27:02,096 - INFO - [diffusion][Epoch 8026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:02,098 - INFO - [diffusion][Epoch 8027] Epoch 8028/12000
2024-11-05 00:27:06,324 - INFO - [diffusion][Epoch 8027] diffusion training Loss: 0.06987757794559002
2024-11-05 00:27:06,326 - INFO - [diffusion][Epoch 8027] diffusion learning rate: 0.001
2024-11-05 00:27:06,328 - INFO - [diffusion][Epoch 8027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:06,329 - INFO - [diffusion][Epoch 8028] Epoch 8029/12000
2024-11-05 00:27:10,513 - INFO - [diffusion][Epoch 8028] diffusion training Loss: 0.06805068999528885
2024-11-05 00:27:10,516 - INFO - [diffusion][Epoch 8028] diffusion learning rate: 0.001
2024-11-05 00:27:10,517 - INFO - [diffusion][Epoch 8028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:10,519 - INFO - [diffusion][Epoch 8029] Epoch 8030/12000
2024-11-05 00:27:14,607 - INFO - [diffusion][Epoch 8029] diffusion training Loss: 0.07129927724599838
2024-11-05 00:27:14,609 - INFO - [diffusion][Epoch 8029] diffusion learning rate: 0.001
2024-11-05 00:27:14,611 - INFO - [diffusion][Epoch 8029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:14,612 - INFO - [diffusion][Epoch 8030] Epoch 8031/12000
2024-11-05 00:27:18,791 - INFO - [diffusion][Epoch 8030] diffusion training Loss: 0.06724320352077484
2024-11-05 00:27:18,793 - INFO - [diffusion][Epoch 8030] diffusion learning rate: 0.001
2024-11-05 00:27:18,794 - INFO - [diffusion][Epoch 8030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:18,796 - INFO - [diffusion][Epoch 8031] Epoch 8032/12000
2024-11-05 00:27:23,027 - INFO - [diffusion][Epoch 8031] diffusion training Loss: 0.07767931558191776
2024-11-05 00:27:23,029 - INFO - [diffusion][Epoch 8031] diffusion learning rate: 0.001
2024-11-05 00:27:23,031 - INFO - [diffusion][Epoch 8031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:23,033 - INFO - [diffusion][Epoch 8032] Epoch 8033/12000
2024-11-05 00:27:26,917 - INFO - [diffusion][Epoch 8032] diffusion training Loss: 0.06503150332719088
2024-11-05 00:27:26,919 - INFO - [diffusion][Epoch 8032] diffusion learning rate: 0.001
2024-11-05 00:27:26,921 - INFO - [diffusion][Epoch 8032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:26,922 - INFO - [diffusion][Epoch 8033] Epoch 8034/12000
2024-11-05 00:27:31,126 - INFO - [diffusion][Epoch 8033] diffusion training Loss: 0.06830235943198204
2024-11-05 00:27:31,129 - INFO - [diffusion][Epoch 8033] diffusion learning rate: 0.001
2024-11-05 00:27:31,131 - INFO - [diffusion][Epoch 8033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:31,132 - INFO - [diffusion][Epoch 8034] Epoch 8035/12000
2024-11-05 00:27:35,273 - INFO - [diffusion][Epoch 8034] diffusion training Loss: 0.06656755320727825
2024-11-05 00:27:35,275 - INFO - [diffusion][Epoch 8034] diffusion learning rate: 0.001
2024-11-05 00:27:35,277 - INFO - [diffusion][Epoch 8034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:35,278 - INFO - [diffusion][Epoch 8035] Epoch 8036/12000
2024-11-05 00:27:39,495 - INFO - [diffusion][Epoch 8035] diffusion training Loss: 0.06647652201354504
2024-11-05 00:27:39,497 - INFO - [diffusion][Epoch 8035] diffusion learning rate: 0.001
2024-11-05 00:27:39,499 - INFO - [diffusion][Epoch 8035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:39,500 - INFO - [diffusion][Epoch 8036] Epoch 8037/12000
2024-11-05 00:27:43,715 - INFO - [diffusion][Epoch 8036] diffusion training Loss: 0.06987616419792175
2024-11-05 00:27:43,717 - INFO - [diffusion][Epoch 8036] diffusion learning rate: 0.001
2024-11-05 00:27:43,719 - INFO - [diffusion][Epoch 8036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:43,720 - INFO - [diffusion][Epoch 8037] Epoch 8038/12000
2024-11-05 00:27:48,039 - INFO - [diffusion][Epoch 8037] diffusion training Loss: 0.06349251978099346
2024-11-05 00:27:48,041 - INFO - [diffusion][Epoch 8037] diffusion learning rate: 0.001
2024-11-05 00:27:48,043 - INFO - [diffusion][Epoch 8037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:48,044 - INFO - [diffusion][Epoch 8038] Epoch 8039/12000
2024-11-05 00:27:52,228 - INFO - [diffusion][Epoch 8038] diffusion training Loss: 0.07029664516448975
2024-11-05 00:27:52,230 - INFO - [diffusion][Epoch 8038] diffusion learning rate: 0.001
2024-11-05 00:27:52,232 - INFO - [diffusion][Epoch 8038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:52,234 - INFO - [diffusion][Epoch 8039] Epoch 8040/12000
2024-11-05 00:27:56,377 - INFO - [diffusion][Epoch 8039] diffusion training Loss: 0.0703751053661108
2024-11-05 00:27:56,379 - INFO - [diffusion][Epoch 8039] diffusion learning rate: 0.001
2024-11-05 00:27:56,381 - INFO - [diffusion][Epoch 8039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:56,382 - INFO - [diffusion][Epoch 8040] Epoch 8041/12000
2024-11-05 00:28:00,638 - INFO - [diffusion][Epoch 8040] diffusion training Loss: 0.06747433356940746
2024-11-05 00:28:00,640 - INFO - [diffusion][Epoch 8040] diffusion learning rate: 0.001
2024-11-05 00:28:00,642 - INFO - [diffusion][Epoch 8040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:00,643 - INFO - [diffusion][Epoch 8041] Epoch 8042/12000
2024-11-05 00:28:04,918 - INFO - [diffusion][Epoch 8041] diffusion training Loss: 0.06915621738880873
2024-11-05 00:28:04,921 - INFO - [diffusion][Epoch 8041] diffusion learning rate: 0.001
2024-11-05 00:28:04,922 - INFO - [diffusion][Epoch 8041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:04,924 - INFO - [diffusion][Epoch 8042] Epoch 8043/12000
2024-11-05 00:28:09,064 - INFO - [diffusion][Epoch 8042] diffusion training Loss: 0.06795488949865103
2024-11-05 00:28:09,067 - INFO - [diffusion][Epoch 8042] diffusion learning rate: 0.001
2024-11-05 00:28:09,069 - INFO - [diffusion][Epoch 8042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:09,070 - INFO - [diffusion][Epoch 8043] Epoch 8044/12000
2024-11-05 00:28:13,222 - INFO - [diffusion][Epoch 8043] diffusion training Loss: 0.06911987997591496
2024-11-05 00:28:13,224 - INFO - [diffusion][Epoch 8043] diffusion learning rate: 0.001
2024-11-05 00:28:13,226 - INFO - [diffusion][Epoch 8043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:13,227 - INFO - [diffusion][Epoch 8044] Epoch 8045/12000
2024-11-05 00:28:17,301 - INFO - [diffusion][Epoch 8044] diffusion training Loss: 0.06301216129213572
2024-11-05 00:28:17,303 - INFO - [diffusion][Epoch 8044] diffusion learning rate: 0.001
2024-11-05 00:28:17,305 - INFO - [diffusion][Epoch 8044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:17,306 - INFO - [diffusion][Epoch 8045] Epoch 8046/12000
2024-11-05 00:28:21,403 - INFO - [diffusion][Epoch 8045] diffusion training Loss: 0.07281030528247356
2024-11-05 00:28:21,405 - INFO - [diffusion][Epoch 8045] diffusion learning rate: 0.001
2024-11-05 00:28:21,407 - INFO - [diffusion][Epoch 8045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:21,408 - INFO - [diffusion][Epoch 8046] Epoch 8047/12000
2024-11-05 00:28:25,446 - INFO - [diffusion][Epoch 8046] diffusion training Loss: 0.06987064145505428
2024-11-05 00:28:25,448 - INFO - [diffusion][Epoch 8046] diffusion learning rate: 0.001
2024-11-05 00:28:25,450 - INFO - [diffusion][Epoch 8046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:25,451 - INFO - [diffusion][Epoch 8047] Epoch 8048/12000
2024-11-05 00:28:29,422 - INFO - [diffusion][Epoch 8047] diffusion training Loss: 0.06980249285697937
2024-11-05 00:28:29,424 - INFO - [diffusion][Epoch 8047] diffusion learning rate: 0.001
2024-11-05 00:28:29,426 - INFO - [diffusion][Epoch 8047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:29,427 - INFO - [diffusion][Epoch 8048] Epoch 8049/12000
2024-11-05 00:28:33,556 - INFO - [diffusion][Epoch 8048] diffusion training Loss: 0.06635215412825346
2024-11-05 00:28:33,559 - INFO - [diffusion][Epoch 8048] diffusion learning rate: 0.001
2024-11-05 00:28:33,561 - INFO - [diffusion][Epoch 8048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:33,562 - INFO - [diffusion][Epoch 8049] Epoch 8050/12000
2024-11-05 00:28:37,733 - INFO - [diffusion][Epoch 8049] diffusion training Loss: 0.06602630019187927
2024-11-05 00:28:37,735 - INFO - [diffusion][Epoch 8049] diffusion learning rate: 0.001
2024-11-05 00:28:37,737 - INFO - [diffusion][Epoch 8049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:37,738 - INFO - [diffusion][Epoch 8050] Epoch 8051/12000
2024-11-05 00:28:41,826 - INFO - [diffusion][Epoch 8050] diffusion training Loss: 0.06835793145000935
2024-11-05 00:28:41,829 - INFO - [diffusion][Epoch 8050] diffusion learning rate: 0.001
2024-11-05 00:28:41,830 - INFO - [diffusion][Epoch 8050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:41,832 - INFO - [diffusion][Epoch 8051] Epoch 8052/12000
2024-11-05 00:28:45,990 - INFO - [diffusion][Epoch 8051] diffusion training Loss: 0.06197866052389145
2024-11-05 00:28:45,993 - INFO - [diffusion][Epoch 8051] diffusion learning rate: 0.001
2024-11-05 00:28:45,995 - INFO - [diffusion][Epoch 8051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:45,996 - INFO - [diffusion][Epoch 8052] Epoch 8053/12000
2024-11-05 00:28:50,106 - INFO - [diffusion][Epoch 8052] diffusion training Loss: 0.06848732754588127
2024-11-05 00:28:50,108 - INFO - [diffusion][Epoch 8052] diffusion learning rate: 0.001
2024-11-05 00:28:50,111 - INFO - [diffusion][Epoch 8052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:50,112 - INFO - [diffusion][Epoch 8053] Epoch 8054/12000
2024-11-05 00:28:54,287 - INFO - [diffusion][Epoch 8053] diffusion training Loss: 0.0687740221619606
2024-11-05 00:28:54,289 - INFO - [diffusion][Epoch 8053] diffusion learning rate: 0.001
2024-11-05 00:28:54,291 - INFO - [diffusion][Epoch 8053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:54,292 - INFO - [diffusion][Epoch 8054] Epoch 8055/12000
2024-11-05 00:28:58,365 - INFO - [diffusion][Epoch 8054] diffusion training Loss: 0.06286469660699368
2024-11-05 00:28:58,368 - INFO - [diffusion][Epoch 8054] diffusion learning rate: 0.001
2024-11-05 00:28:58,370 - INFO - [diffusion][Epoch 8054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:58,371 - INFO - [diffusion][Epoch 8055] Epoch 8056/12000
2024-11-05 00:29:02,508 - INFO - [diffusion][Epoch 8055] diffusion training Loss: 0.0693636629730463
2024-11-05 00:29:02,510 - INFO - [diffusion][Epoch 8055] diffusion learning rate: 0.001
2024-11-05 00:29:02,512 - INFO - [diffusion][Epoch 8055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:02,514 - INFO - [diffusion][Epoch 8056] Epoch 8057/12000
2024-11-05 00:29:06,557 - INFO - [diffusion][Epoch 8056] diffusion training Loss: 0.0687031913548708
2024-11-05 00:29:06,560 - INFO - [diffusion][Epoch 8056] diffusion learning rate: 0.001
2024-11-05 00:29:06,561 - INFO - [diffusion][Epoch 8056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:06,563 - INFO - [diffusion][Epoch 8057] Epoch 8058/12000
2024-11-05 00:29:10,710 - INFO - [diffusion][Epoch 8057] diffusion training Loss: 0.0628351978957653
2024-11-05 00:29:10,712 - INFO - [diffusion][Epoch 8057] diffusion learning rate: 0.001
2024-11-05 00:29:10,715 - INFO - [diffusion][Epoch 8057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:10,717 - INFO - [diffusion][Epoch 8058] Epoch 8059/12000
2024-11-05 00:29:14,965 - INFO - [diffusion][Epoch 8058] diffusion training Loss: 0.06989671103656292
2024-11-05 00:29:14,967 - INFO - [diffusion][Epoch 8058] diffusion learning rate: 0.001
2024-11-05 00:29:14,970 - INFO - [diffusion][Epoch 8058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:14,971 - INFO - [diffusion][Epoch 8059] Epoch 8060/12000
2024-11-05 00:29:19,262 - INFO - [diffusion][Epoch 8059] diffusion training Loss: 0.07598378881812096
2024-11-05 00:29:19,264 - INFO - [diffusion][Epoch 8059] diffusion learning rate: 0.001
2024-11-05 00:29:19,266 - INFO - [diffusion][Epoch 8059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:19,268 - INFO - [diffusion][Epoch 8060] Epoch 8061/12000
2024-11-05 00:29:23,598 - INFO - [diffusion][Epoch 8060] diffusion training Loss: 0.07374465838074684
2024-11-05 00:29:23,600 - INFO - [diffusion][Epoch 8060] diffusion learning rate: 0.001
2024-11-05 00:29:23,602 - INFO - [diffusion][Epoch 8060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:23,604 - INFO - [diffusion][Epoch 8061] Epoch 8062/12000
2024-11-05 00:29:27,766 - INFO - [diffusion][Epoch 8061] diffusion training Loss: 0.07040915079414845
2024-11-05 00:29:27,768 - INFO - [diffusion][Epoch 8061] diffusion learning rate: 0.001
2024-11-05 00:29:27,770 - INFO - [diffusion][Epoch 8061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:27,771 - INFO - [diffusion][Epoch 8062] Epoch 8063/12000
2024-11-05 00:29:32,769 - INFO - [diffusion][Epoch 8062] diffusion training Loss: 0.06690187007188797
2024-11-05 00:29:32,771 - INFO - [diffusion][Epoch 8062] diffusion learning rate: 0.001
2024-11-05 00:29:32,773 - INFO - [diffusion][Epoch 8062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:32,774 - INFO - [diffusion][Epoch 8063] Epoch 8064/12000
2024-11-05 00:29:36,889 - INFO - [diffusion][Epoch 8063] diffusion training Loss: 0.07135524693876505
2024-11-05 00:29:36,890 - INFO - [diffusion][Epoch 8063] diffusion learning rate: 0.001
2024-11-05 00:29:36,892 - INFO - [diffusion][Epoch 8063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:36,894 - INFO - [diffusion][Epoch 8064] Epoch 8065/12000
2024-11-05 00:29:41,118 - INFO - [diffusion][Epoch 8064] diffusion training Loss: 0.06658951099961996
2024-11-05 00:29:41,120 - INFO - [diffusion][Epoch 8064] diffusion learning rate: 0.001
2024-11-05 00:29:41,122 - INFO - [diffusion][Epoch 8064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:41,123 - INFO - [diffusion][Epoch 8065] Epoch 8066/12000
2024-11-05 00:29:45,181 - INFO - [diffusion][Epoch 8065] diffusion training Loss: 0.06695378199219704
2024-11-05 00:29:45,183 - INFO - [diffusion][Epoch 8065] diffusion learning rate: 0.001
2024-11-05 00:29:45,185 - INFO - [diffusion][Epoch 8065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:45,187 - INFO - [diffusion][Epoch 8066] Epoch 8067/12000
2024-11-05 00:29:49,380 - INFO - [diffusion][Epoch 8066] diffusion training Loss: 0.06882992014288902
2024-11-05 00:29:49,383 - INFO - [diffusion][Epoch 8066] diffusion learning rate: 0.001
2024-11-05 00:29:49,384 - INFO - [diffusion][Epoch 8066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:49,386 - INFO - [diffusion][Epoch 8067] Epoch 8068/12000
2024-11-05 00:29:53,631 - INFO - [diffusion][Epoch 8067] diffusion training Loss: 0.06267116405069828
2024-11-05 00:29:53,633 - INFO - [diffusion][Epoch 8067] diffusion learning rate: 0.001
2024-11-05 00:29:53,636 - INFO - [diffusion][Epoch 8067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:53,637 - INFO - [diffusion][Epoch 8068] Epoch 8069/12000
2024-11-05 00:29:57,826 - INFO - [diffusion][Epoch 8068] diffusion training Loss: 0.07136630639433861
2024-11-05 00:29:57,828 - INFO - [diffusion][Epoch 8068] diffusion learning rate: 0.001
2024-11-05 00:29:57,829 - INFO - [diffusion][Epoch 8068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:57,831 - INFO - [diffusion][Epoch 8069] Epoch 8070/12000
2024-11-05 00:30:01,983 - INFO - [diffusion][Epoch 8069] diffusion training Loss: 0.06430993229150772
2024-11-05 00:30:01,986 - INFO - [diffusion][Epoch 8069] diffusion learning rate: 0.001
2024-11-05 00:30:01,988 - INFO - [diffusion][Epoch 8069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:01,989 - INFO - [diffusion][Epoch 8070] Epoch 8071/12000
2024-11-05 00:30:06,144 - INFO - [diffusion][Epoch 8070] diffusion training Loss: 0.06759805791079998
2024-11-05 00:30:06,146 - INFO - [diffusion][Epoch 8070] diffusion learning rate: 0.001
2024-11-05 00:30:06,149 - INFO - [diffusion][Epoch 8070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:06,150 - INFO - [diffusion][Epoch 8071] Epoch 8072/12000
2024-11-05 00:30:10,451 - INFO - [diffusion][Epoch 8071] diffusion training Loss: 0.06984341703355312
2024-11-05 00:30:10,453 - INFO - [diffusion][Epoch 8071] diffusion learning rate: 0.001
2024-11-05 00:30:10,455 - INFO - [diffusion][Epoch 8071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:10,456 - INFO - [diffusion][Epoch 8072] Epoch 8073/12000
2024-11-05 00:30:14,665 - INFO - [diffusion][Epoch 8072] diffusion training Loss: 0.07204193249344826
2024-11-05 00:30:14,667 - INFO - [diffusion][Epoch 8072] diffusion learning rate: 0.001
2024-11-05 00:30:14,669 - INFO - [diffusion][Epoch 8072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:14,671 - INFO - [diffusion][Epoch 8073] Epoch 8074/12000
2024-11-05 00:30:18,864 - INFO - [diffusion][Epoch 8073] diffusion training Loss: 0.06600901670753956
2024-11-05 00:30:18,908 - INFO - [diffusion][Epoch 8073] diffusion learning rate: 0.001
2024-11-05 00:30:18,909 - INFO - [diffusion][Epoch 8073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:18,910 - INFO - [diffusion][Epoch 8074] Epoch 8075/12000
2024-11-05 00:30:23,049 - INFO - [diffusion][Epoch 8074] diffusion training Loss: 0.07426520064473152
2024-11-05 00:30:23,051 - INFO - [diffusion][Epoch 8074] diffusion learning rate: 0.001
2024-11-05 00:30:23,053 - INFO - [diffusion][Epoch 8074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:23,054 - INFO - [diffusion][Epoch 8075] Epoch 8076/12000
2024-11-05 00:30:27,202 - INFO - [diffusion][Epoch 8075] diffusion training Loss: 0.06999963894486427
2024-11-05 00:30:27,204 - INFO - [diffusion][Epoch 8075] diffusion learning rate: 0.001
2024-11-05 00:30:27,206 - INFO - [diffusion][Epoch 8075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:27,207 - INFO - [diffusion][Epoch 8076] Epoch 8077/12000
2024-11-05 00:30:31,458 - INFO - [diffusion][Epoch 8076] diffusion training Loss: 0.06904233619570732
2024-11-05 00:30:31,460 - INFO - [diffusion][Epoch 8076] diffusion learning rate: 0.001
2024-11-05 00:30:31,462 - INFO - [diffusion][Epoch 8076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:31,463 - INFO - [diffusion][Epoch 8077] Epoch 8078/12000
2024-11-05 00:30:35,726 - INFO - [diffusion][Epoch 8077] diffusion training Loss: 0.06693166960030794
2024-11-05 00:30:35,729 - INFO - [diffusion][Epoch 8077] diffusion learning rate: 0.001
2024-11-05 00:30:35,731 - INFO - [diffusion][Epoch 8077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:35,733 - INFO - [diffusion][Epoch 8078] Epoch 8079/12000
2024-11-05 00:30:40,089 - INFO - [diffusion][Epoch 8078] diffusion training Loss: 0.07230162806808949
2024-11-05 00:30:40,091 - INFO - [diffusion][Epoch 8078] diffusion learning rate: 0.001
2024-11-05 00:30:40,093 - INFO - [diffusion][Epoch 8078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:40,095 - INFO - [diffusion][Epoch 8079] Epoch 8080/12000
2024-11-05 00:30:44,138 - INFO - [diffusion][Epoch 8079] diffusion training Loss: 0.07048461213707924
2024-11-05 00:30:44,140 - INFO - [diffusion][Epoch 8079] diffusion learning rate: 0.001
2024-11-05 00:30:44,142 - INFO - [diffusion][Epoch 8079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:44,144 - INFO - [diffusion][Epoch 8080] Epoch 8081/12000
2024-11-05 00:30:48,349 - INFO - [diffusion][Epoch 8080] diffusion training Loss: 0.06300423759967089
2024-11-05 00:30:48,351 - INFO - [diffusion][Epoch 8080] diffusion learning rate: 0.001
2024-11-05 00:30:48,353 - INFO - [diffusion][Epoch 8080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:48,354 - INFO - [diffusion][Epoch 8081] Epoch 8082/12000
2024-11-05 00:30:52,629 - INFO - [diffusion][Epoch 8081] diffusion training Loss: 0.06478284671902657
2024-11-05 00:30:52,631 - INFO - [diffusion][Epoch 8081] diffusion learning rate: 0.001
2024-11-05 00:30:52,633 - INFO - [diffusion][Epoch 8081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:52,634 - INFO - [diffusion][Epoch 8082] Epoch 8083/12000
2024-11-05 00:30:57,695 - INFO - [diffusion][Epoch 8082] diffusion training Loss: 0.0679179523140192
2024-11-05 00:30:57,697 - INFO - [diffusion][Epoch 8082] diffusion learning rate: 0.001
2024-11-05 00:30:57,699 - INFO - [diffusion][Epoch 8082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:57,700 - INFO - [diffusion][Epoch 8083] Epoch 8084/12000
2024-11-05 00:31:01,957 - INFO - [diffusion][Epoch 8083] diffusion training Loss: 0.06921086739748716
2024-11-05 00:31:01,959 - INFO - [diffusion][Epoch 8083] diffusion learning rate: 0.001
2024-11-05 00:31:01,961 - INFO - [diffusion][Epoch 8083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:01,963 - INFO - [diffusion][Epoch 8084] Epoch 8085/12000
2024-11-05 00:31:06,192 - INFO - [diffusion][Epoch 8084] diffusion training Loss: 0.06782084796577692
2024-11-05 00:31:06,194 - INFO - [diffusion][Epoch 8084] diffusion learning rate: 0.001
2024-11-05 00:31:06,196 - INFO - [diffusion][Epoch 8084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:06,197 - INFO - [diffusion][Epoch 8085] Epoch 8086/12000
2024-11-05 00:31:10,327 - INFO - [diffusion][Epoch 8085] diffusion training Loss: 0.06889026612043381
2024-11-05 00:31:10,329 - INFO - [diffusion][Epoch 8085] diffusion learning rate: 0.001
2024-11-05 00:31:10,331 - INFO - [diffusion][Epoch 8085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:10,332 - INFO - [diffusion][Epoch 8086] Epoch 8087/12000
2024-11-05 00:31:14,454 - INFO - [diffusion][Epoch 8086] diffusion training Loss: 0.0668919961899519
2024-11-05 00:31:14,458 - INFO - [diffusion][Epoch 8086] diffusion learning rate: 0.001
2024-11-05 00:31:14,460 - INFO - [diffusion][Epoch 8086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:14,461 - INFO - [diffusion][Epoch 8087] Epoch 8088/12000
2024-11-05 00:31:18,702 - INFO - [diffusion][Epoch 8087] diffusion training Loss: 0.06171638611704111
2024-11-05 00:31:18,704 - INFO - [diffusion][Epoch 8087] diffusion learning rate: 0.001
2024-11-05 00:31:18,706 - INFO - [diffusion][Epoch 8087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:18,707 - INFO - [diffusion][Epoch 8088] Epoch 8089/12000
2024-11-05 00:31:22,889 - INFO - [diffusion][Epoch 8088] diffusion training Loss: 0.06687230616807938
2024-11-05 00:31:22,891 - INFO - [diffusion][Epoch 8088] diffusion learning rate: 0.001
2024-11-05 00:31:22,893 - INFO - [diffusion][Epoch 8088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:22,894 - INFO - [diffusion][Epoch 8089] Epoch 8090/12000
2024-11-05 00:31:26,985 - INFO - [diffusion][Epoch 8089] diffusion training Loss: 0.06992258876562119
2024-11-05 00:31:26,987 - INFO - [diffusion][Epoch 8089] diffusion learning rate: 0.001
2024-11-05 00:31:26,988 - INFO - [diffusion][Epoch 8089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:26,989 - INFO - [diffusion][Epoch 8090] Epoch 8091/12000
2024-11-05 00:31:31,231 - INFO - [diffusion][Epoch 8090] diffusion training Loss: 0.07004345487803221
2024-11-05 00:31:31,236 - INFO - [diffusion][Epoch 8090] diffusion learning rate: 0.001
2024-11-05 00:31:31,239 - INFO - [diffusion][Epoch 8090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:31,240 - INFO - [diffusion][Epoch 8091] Epoch 8092/12000
2024-11-05 00:31:35,556 - INFO - [diffusion][Epoch 8091] diffusion training Loss: 0.06847530324012041
2024-11-05 00:31:35,558 - INFO - [diffusion][Epoch 8091] diffusion learning rate: 0.001
2024-11-05 00:31:35,560 - INFO - [diffusion][Epoch 8091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:35,561 - INFO - [diffusion][Epoch 8092] Epoch 8093/12000
2024-11-05 00:31:39,756 - INFO - [diffusion][Epoch 8092] diffusion training Loss: 0.0685683935880661
2024-11-05 00:31:39,758 - INFO - [diffusion][Epoch 8092] diffusion learning rate: 0.001
2024-11-05 00:31:39,761 - INFO - [diffusion][Epoch 8092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:39,763 - INFO - [diffusion][Epoch 8093] Epoch 8094/12000
2024-11-05 00:31:43,981 - INFO - [diffusion][Epoch 8093] diffusion training Loss: 0.06769123114645481
2024-11-05 00:31:43,983 - INFO - [diffusion][Epoch 8093] diffusion learning rate: 0.001
2024-11-05 00:31:43,985 - INFO - [diffusion][Epoch 8093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:43,986 - INFO - [diffusion][Epoch 8094] Epoch 8095/12000
2024-11-05 00:31:47,892 - INFO - [diffusion][Epoch 8094] diffusion training Loss: 0.07033873349428177
2024-11-05 00:31:47,894 - INFO - [diffusion][Epoch 8094] diffusion learning rate: 0.001
2024-11-05 00:31:47,896 - INFO - [diffusion][Epoch 8094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:47,897 - INFO - [diffusion][Epoch 8095] Epoch 8096/12000
2024-11-05 00:31:52,086 - INFO - [diffusion][Epoch 8095] diffusion training Loss: 0.0693953987210989
2024-11-05 00:31:52,088 - INFO - [diffusion][Epoch 8095] diffusion learning rate: 0.001
2024-11-05 00:31:52,090 - INFO - [diffusion][Epoch 8095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:52,091 - INFO - [diffusion][Epoch 8096] Epoch 8097/12000
2024-11-05 00:31:56,347 - INFO - [diffusion][Epoch 8096] diffusion training Loss: 0.0698265079408884
2024-11-05 00:31:56,349 - INFO - [diffusion][Epoch 8096] diffusion learning rate: 0.001
2024-11-05 00:31:56,351 - INFO - [diffusion][Epoch 8096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:56,352 - INFO - [diffusion][Epoch 8097] Epoch 8098/12000
2024-11-05 00:32:00,509 - INFO - [diffusion][Epoch 8097] diffusion training Loss: 0.07169824279844761
2024-11-05 00:32:00,512 - INFO - [diffusion][Epoch 8097] diffusion learning rate: 0.001
2024-11-05 00:32:00,514 - INFO - [diffusion][Epoch 8097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:00,515 - INFO - [diffusion][Epoch 8098] Epoch 8099/12000
2024-11-05 00:32:04,635 - INFO - [diffusion][Epoch 8098] diffusion training Loss: 0.0676176343113184
2024-11-05 00:32:04,637 - INFO - [diffusion][Epoch 8098] diffusion learning rate: 0.001
2024-11-05 00:32:04,639 - INFO - [diffusion][Epoch 8098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:04,640 - INFO - [diffusion][Epoch 8099] Epoch 8100/12000
2024-11-05 00:32:08,767 - INFO - [diffusion][Epoch 8099] diffusion training Loss: 0.06905528716742992
2024-11-05 00:32:08,768 - INFO - [diffusion][Epoch 8099] diffusion learning rate: 0.001
2024-11-05 00:32:08,770 - INFO - [diffusion][Epoch 8099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:08,772 - INFO - [diffusion][Epoch 8100] Epoch 8101/12000
2024-11-05 00:32:13,060 - INFO - [diffusion][Epoch 8100] diffusion training Loss: 0.06779639050364494
2024-11-05 00:32:13,062 - INFO - [diffusion][Epoch 8100] diffusion learning rate: 0.001
2024-11-05 00:32:13,064 - INFO - [diffusion][Epoch 8100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:13,066 - INFO - [diffusion][Epoch 8101] Epoch 8102/12000
2024-11-05 00:32:17,187 - INFO - [diffusion][Epoch 8101] diffusion training Loss: 0.06967192329466343
2024-11-05 00:32:17,189 - INFO - [diffusion][Epoch 8101] diffusion learning rate: 0.001
2024-11-05 00:32:17,191 - INFO - [diffusion][Epoch 8101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:17,192 - INFO - [diffusion][Epoch 8102] Epoch 8103/12000
2024-11-05 00:32:21,328 - INFO - [diffusion][Epoch 8102] diffusion training Loss: 0.07675162330269814
2024-11-05 00:32:21,330 - INFO - [diffusion][Epoch 8102] diffusion learning rate: 0.001
2024-11-05 00:32:21,332 - INFO - [diffusion][Epoch 8102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:21,333 - INFO - [diffusion][Epoch 8103] Epoch 8104/12000
2024-11-05 00:32:25,632 - INFO - [diffusion][Epoch 8103] diffusion training Loss: 0.07252578530460596
2024-11-05 00:32:25,634 - INFO - [diffusion][Epoch 8103] diffusion learning rate: 0.001
2024-11-05 00:32:25,636 - INFO - [diffusion][Epoch 8103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:25,637 - INFO - [diffusion][Epoch 8104] Epoch 8105/12000
2024-11-05 00:32:30,276 - INFO - [diffusion][Epoch 8104] diffusion training Loss: 0.06494161859154701
2024-11-05 00:32:30,278 - INFO - [diffusion][Epoch 8104] diffusion learning rate: 0.001
2024-11-05 00:32:30,280 - INFO - [diffusion][Epoch 8104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:30,282 - INFO - [diffusion][Epoch 8105] Epoch 8106/12000
2024-11-05 00:32:34,345 - INFO - [diffusion][Epoch 8105] diffusion training Loss: 0.06756286323070526
2024-11-05 00:32:34,347 - INFO - [diffusion][Epoch 8105] diffusion learning rate: 0.001
2024-11-05 00:32:34,349 - INFO - [diffusion][Epoch 8105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:34,350 - INFO - [diffusion][Epoch 8106] Epoch 8107/12000
2024-11-05 00:32:38,536 - INFO - [diffusion][Epoch 8106] diffusion training Loss: 0.07065076194703579
2024-11-05 00:32:38,539 - INFO - [diffusion][Epoch 8106] diffusion learning rate: 0.001
2024-11-05 00:32:38,540 - INFO - [diffusion][Epoch 8106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:38,542 - INFO - [diffusion][Epoch 8107] Epoch 8108/12000
2024-11-05 00:32:42,731 - INFO - [diffusion][Epoch 8107] diffusion training Loss: 0.06883996911346912
2024-11-05 00:32:42,733 - INFO - [diffusion][Epoch 8107] diffusion learning rate: 0.001
2024-11-05 00:32:42,734 - INFO - [diffusion][Epoch 8107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:42,735 - INFO - [diffusion][Epoch 8108] Epoch 8109/12000
2024-11-05 00:32:46,888 - INFO - [diffusion][Epoch 8108] diffusion training Loss: 0.07054171804338694
2024-11-05 00:32:46,889 - INFO - [diffusion][Epoch 8108] diffusion learning rate: 0.001
2024-11-05 00:32:46,891 - INFO - [diffusion][Epoch 8108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:46,892 - INFO - [diffusion][Epoch 8109] Epoch 8110/12000
2024-11-05 00:32:50,871 - INFO - [diffusion][Epoch 8109] diffusion training Loss: 0.06751601956784725
2024-11-05 00:32:50,872 - INFO - [diffusion][Epoch 8109] diffusion learning rate: 0.001
2024-11-05 00:32:50,874 - INFO - [diffusion][Epoch 8109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:50,875 - INFO - [diffusion][Epoch 8110] Epoch 8111/12000
2024-11-05 00:32:54,927 - INFO - [diffusion][Epoch 8110] diffusion training Loss: 0.06838709115982056
2024-11-05 00:32:54,929 - INFO - [diffusion][Epoch 8110] diffusion learning rate: 0.001
2024-11-05 00:32:54,932 - INFO - [diffusion][Epoch 8110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:54,933 - INFO - [diffusion][Epoch 8111] Epoch 8112/12000
2024-11-05 00:32:59,056 - INFO - [diffusion][Epoch 8111] diffusion training Loss: 0.06533015891909599
2024-11-05 00:32:59,058 - INFO - [diffusion][Epoch 8111] diffusion learning rate: 0.001
2024-11-05 00:32:59,060 - INFO - [diffusion][Epoch 8111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:59,061 - INFO - [diffusion][Epoch 8112] Epoch 8113/12000
2024-11-05 00:33:03,131 - INFO - [diffusion][Epoch 8112] diffusion training Loss: 0.07400134392082691
2024-11-05 00:33:03,133 - INFO - [diffusion][Epoch 8112] diffusion learning rate: 0.001
2024-11-05 00:33:03,135 - INFO - [diffusion][Epoch 8112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:03,136 - INFO - [diffusion][Epoch 8113] Epoch 8114/12000
2024-11-05 00:33:07,226 - INFO - [diffusion][Epoch 8113] diffusion training Loss: 0.06685782223939896
2024-11-05 00:33:07,228 - INFO - [diffusion][Epoch 8113] diffusion learning rate: 0.001
2024-11-05 00:33:07,230 - INFO - [diffusion][Epoch 8113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:07,231 - INFO - [diffusion][Epoch 8114] Epoch 8115/12000
2024-11-05 00:33:11,269 - INFO - [diffusion][Epoch 8114] diffusion training Loss: 0.064662323333323
2024-11-05 00:33:11,600 - INFO - [diffusion][Epoch 8114] diffusion learning rate: 0.001
2024-11-05 00:33:11,602 - INFO - [diffusion][Epoch 8114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:11,603 - INFO - [diffusion][Epoch 8115] Epoch 8116/12000
2024-11-05 00:33:15,784 - INFO - [diffusion][Epoch 8115] diffusion training Loss: 0.06323020998388529
2024-11-05 00:33:15,787 - INFO - [diffusion][Epoch 8115] diffusion learning rate: 0.001
2024-11-05 00:33:15,788 - INFO - [diffusion][Epoch 8115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:15,790 - INFO - [diffusion][Epoch 8116] Epoch 8117/12000
2024-11-05 00:33:19,867 - INFO - [diffusion][Epoch 8116] diffusion training Loss: 0.0672679990530014
2024-11-05 00:33:19,870 - INFO - [diffusion][Epoch 8116] diffusion learning rate: 0.001
2024-11-05 00:33:19,871 - INFO - [diffusion][Epoch 8116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:19,872 - INFO - [diffusion][Epoch 8117] Epoch 8118/12000
2024-11-05 00:33:24,015 - INFO - [diffusion][Epoch 8117] diffusion training Loss: 0.07509845495223999
2024-11-05 00:33:24,017 - INFO - [diffusion][Epoch 8117] diffusion learning rate: 0.001
2024-11-05 00:33:24,018 - INFO - [diffusion][Epoch 8117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:24,020 - INFO - [diffusion][Epoch 8118] Epoch 8119/12000
2024-11-05 00:33:28,152 - INFO - [diffusion][Epoch 8118] diffusion training Loss: 0.07215970940887928
2024-11-05 00:33:28,154 - INFO - [diffusion][Epoch 8118] diffusion learning rate: 0.001
2024-11-05 00:33:28,157 - INFO - [diffusion][Epoch 8118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:28,158 - INFO - [diffusion][Epoch 8119] Epoch 8120/12000
2024-11-05 00:33:32,170 - INFO - [diffusion][Epoch 8119] diffusion training Loss: 0.06351971998810768
2024-11-05 00:33:32,172 - INFO - [diffusion][Epoch 8119] diffusion learning rate: 0.001
2024-11-05 00:33:32,174 - INFO - [diffusion][Epoch 8119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:32,175 - INFO - [diffusion][Epoch 8120] Epoch 8121/12000
2024-11-05 00:33:36,235 - INFO - [diffusion][Epoch 8120] diffusion training Loss: 0.0629621148109436
2024-11-05 00:33:36,237 - INFO - [diffusion][Epoch 8120] diffusion learning rate: 0.001
2024-11-05 00:33:36,239 - INFO - [diffusion][Epoch 8120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:36,240 - INFO - [diffusion][Epoch 8121] Epoch 8122/12000
2024-11-05 00:33:40,408 - INFO - [diffusion][Epoch 8121] diffusion training Loss: 0.06448008771985769
2024-11-05 00:33:40,410 - INFO - [diffusion][Epoch 8121] diffusion learning rate: 0.001
2024-11-05 00:33:40,412 - INFO - [diffusion][Epoch 8121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:40,413 - INFO - [diffusion][Epoch 8122] Epoch 8123/12000
2024-11-05 00:33:44,527 - INFO - [diffusion][Epoch 8122] diffusion training Loss: 0.062573098577559
2024-11-05 00:33:44,529 - INFO - [diffusion][Epoch 8122] diffusion learning rate: 0.001
2024-11-05 00:33:44,531 - INFO - [diffusion][Epoch 8122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:44,532 - INFO - [diffusion][Epoch 8123] Epoch 8124/12000
2024-11-05 00:33:48,829 - INFO - [diffusion][Epoch 8123] diffusion training Loss: 0.06869192235171795
2024-11-05 00:33:48,831 - INFO - [diffusion][Epoch 8123] diffusion learning rate: 0.001
2024-11-05 00:33:48,834 - INFO - [diffusion][Epoch 8123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:48,835 - INFO - [diffusion][Epoch 8124] Epoch 8125/12000
2024-11-05 00:33:53,058 - INFO - [diffusion][Epoch 8124] diffusion training Loss: 0.0657357145100832
2024-11-05 00:33:53,059 - INFO - [diffusion][Epoch 8124] diffusion learning rate: 0.001
2024-11-05 00:33:53,061 - INFO - [diffusion][Epoch 8124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:53,062 - INFO - [diffusion][Epoch 8125] Epoch 8126/12000
2024-11-05 00:33:57,231 - INFO - [diffusion][Epoch 8125] diffusion training Loss: 0.06600978970527649
2024-11-05 00:33:57,233 - INFO - [diffusion][Epoch 8125] diffusion learning rate: 0.001
2024-11-05 00:33:57,235 - INFO - [diffusion][Epoch 8125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:57,237 - INFO - [diffusion][Epoch 8126] Epoch 8127/12000
2024-11-05 00:34:01,070 - INFO - [diffusion][Epoch 8126] diffusion training Loss: 0.0705619640648365
2024-11-05 00:34:01,072 - INFO - [diffusion][Epoch 8126] diffusion learning rate: 0.001
2024-11-05 00:34:01,075 - INFO - [diffusion][Epoch 8126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:01,076 - INFO - [diffusion][Epoch 8127] Epoch 8128/12000
2024-11-05 00:34:05,190 - INFO - [diffusion][Epoch 8127] diffusion training Loss: 0.07196363806724548
2024-11-05 00:34:05,192 - INFO - [diffusion][Epoch 8127] diffusion learning rate: 0.001
2024-11-05 00:34:05,194 - INFO - [diffusion][Epoch 8127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:05,195 - INFO - [diffusion][Epoch 8128] Epoch 8129/12000
2024-11-05 00:34:09,237 - INFO - [diffusion][Epoch 8128] diffusion training Loss: 0.0689032468944788
2024-11-05 00:34:09,239 - INFO - [diffusion][Epoch 8128] diffusion learning rate: 0.001
2024-11-05 00:34:09,241 - INFO - [diffusion][Epoch 8128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:09,242 - INFO - [diffusion][Epoch 8129] Epoch 8130/12000
2024-11-05 00:34:13,346 - INFO - [diffusion][Epoch 8129] diffusion training Loss: 0.07176485657691956
2024-11-05 00:34:13,348 - INFO - [diffusion][Epoch 8129] diffusion learning rate: 0.001
2024-11-05 00:34:13,350 - INFO - [diffusion][Epoch 8129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:13,352 - INFO - [diffusion][Epoch 8130] Epoch 8131/12000
2024-11-05 00:34:17,516 - INFO - [diffusion][Epoch 8130] diffusion training Loss: 0.06871656700968742
2024-11-05 00:34:17,518 - INFO - [diffusion][Epoch 8130] diffusion learning rate: 0.001
2024-11-05 00:34:17,520 - INFO - [diffusion][Epoch 8130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:17,521 - INFO - [diffusion][Epoch 8131] Epoch 8132/12000
2024-11-05 00:34:21,447 - INFO - [diffusion][Epoch 8131] diffusion training Loss: 0.07061279192566872
2024-11-05 00:34:21,449 - INFO - [diffusion][Epoch 8131] diffusion learning rate: 0.001
2024-11-05 00:34:21,450 - INFO - [diffusion][Epoch 8131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:21,452 - INFO - [diffusion][Epoch 8132] Epoch 8133/12000
2024-11-05 00:34:25,689 - INFO - [diffusion][Epoch 8132] diffusion training Loss: 0.06529688835144043
2024-11-05 00:34:25,691 - INFO - [diffusion][Epoch 8132] diffusion learning rate: 0.001
2024-11-05 00:34:25,693 - INFO - [diffusion][Epoch 8132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:25,695 - INFO - [diffusion][Epoch 8133] Epoch 8134/12000
2024-11-05 00:34:29,666 - INFO - [diffusion][Epoch 8133] diffusion training Loss: 0.06481590773910284
2024-11-05 00:34:29,668 - INFO - [diffusion][Epoch 8133] diffusion learning rate: 0.001
2024-11-05 00:34:29,670 - INFO - [diffusion][Epoch 8133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:29,671 - INFO - [diffusion][Epoch 8134] Epoch 8135/12000
2024-11-05 00:34:33,836 - INFO - [diffusion][Epoch 8134] diffusion training Loss: 0.06763207726180553
2024-11-05 00:34:33,838 - INFO - [diffusion][Epoch 8134] diffusion learning rate: 0.001
2024-11-05 00:34:33,840 - INFO - [diffusion][Epoch 8134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:33,841 - INFO - [diffusion][Epoch 8135] Epoch 8136/12000
2024-11-05 00:34:37,853 - INFO - [diffusion][Epoch 8135] diffusion training Loss: 0.06976109743118286
2024-11-05 00:34:37,856 - INFO - [diffusion][Epoch 8135] diffusion learning rate: 0.001
2024-11-05 00:34:37,857 - INFO - [diffusion][Epoch 8135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:37,859 - INFO - [diffusion][Epoch 8136] Epoch 8137/12000
2024-11-05 00:34:41,795 - INFO - [diffusion][Epoch 8136] diffusion training Loss: 0.06641768850386143
2024-11-05 00:34:41,811 - INFO - [diffusion][Epoch 8136] diffusion learning rate: 0.001
2024-11-05 00:34:41,813 - INFO - [diffusion][Epoch 8136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:41,815 - INFO - [diffusion][Epoch 8137] Epoch 8138/12000
2024-11-05 00:34:45,762 - INFO - [diffusion][Epoch 8137] diffusion training Loss: 0.06708587519824505
2024-11-05 00:34:45,764 - INFO - [diffusion][Epoch 8137] diffusion learning rate: 0.001
2024-11-05 00:34:45,766 - INFO - [diffusion][Epoch 8137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:45,767 - INFO - [diffusion][Epoch 8138] Epoch 8139/12000
2024-11-05 00:34:49,757 - INFO - [diffusion][Epoch 8138] diffusion training Loss: 0.06838432606309652
2024-11-05 00:34:49,759 - INFO - [diffusion][Epoch 8138] diffusion learning rate: 0.001
2024-11-05 00:34:49,761 - INFO - [diffusion][Epoch 8138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:49,762 - INFO - [diffusion][Epoch 8139] Epoch 8140/12000
2024-11-05 00:34:53,970 - INFO - [diffusion][Epoch 8139] diffusion training Loss: 0.06987528130412102
2024-11-05 00:34:53,972 - INFO - [diffusion][Epoch 8139] diffusion learning rate: 0.001
2024-11-05 00:34:53,974 - INFO - [diffusion][Epoch 8139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:53,975 - INFO - [diffusion][Epoch 8140] Epoch 8141/12000
2024-11-05 00:34:58,161 - INFO - [diffusion][Epoch 8140] diffusion training Loss: 0.06509436294436455
2024-11-05 00:34:58,164 - INFO - [diffusion][Epoch 8140] diffusion learning rate: 0.001
2024-11-05 00:34:58,165 - INFO - [diffusion][Epoch 8140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:58,167 - INFO - [diffusion][Epoch 8141] Epoch 8142/12000
2024-11-05 00:35:02,417 - INFO - [diffusion][Epoch 8141] diffusion training Loss: 0.07012098841369152
2024-11-05 00:35:02,420 - INFO - [diffusion][Epoch 8141] diffusion learning rate: 0.001
2024-11-05 00:35:02,422 - INFO - [diffusion][Epoch 8141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:02,423 - INFO - [diffusion][Epoch 8142] Epoch 8143/12000
2024-11-05 00:35:06,716 - INFO - [diffusion][Epoch 8142] diffusion training Loss: 0.06562910415232182
2024-11-05 00:35:06,718 - INFO - [diffusion][Epoch 8142] diffusion learning rate: 0.001
2024-11-05 00:35:06,720 - INFO - [diffusion][Epoch 8142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:06,722 - INFO - [diffusion][Epoch 8143] Epoch 8144/12000
2024-11-05 00:35:11,196 - INFO - [diffusion][Epoch 8143] diffusion training Loss: 0.06822212412953377
2024-11-05 00:35:11,198 - INFO - [diffusion][Epoch 8143] diffusion learning rate: 0.001
2024-11-05 00:35:11,200 - INFO - [diffusion][Epoch 8143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:11,202 - INFO - [diffusion][Epoch 8144] Epoch 8145/12000
2024-11-05 00:35:15,422 - INFO - [diffusion][Epoch 8144] diffusion training Loss: 0.06513970717787743
2024-11-05 00:35:15,425 - INFO - [diffusion][Epoch 8144] diffusion learning rate: 0.001
2024-11-05 00:35:15,427 - INFO - [diffusion][Epoch 8144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:15,428 - INFO - [diffusion][Epoch 8145] Epoch 8146/12000
2024-11-05 00:35:19,738 - INFO - [diffusion][Epoch 8145] diffusion training Loss: 0.07354158535599709
2024-11-05 00:35:19,740 - INFO - [diffusion][Epoch 8145] diffusion learning rate: 0.001
2024-11-05 00:35:19,742 - INFO - [diffusion][Epoch 8145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:19,743 - INFO - [diffusion][Epoch 8146] Epoch 8147/12000
2024-11-05 00:35:23,916 - INFO - [diffusion][Epoch 8146] diffusion training Loss: 0.06885223276913166
2024-11-05 00:35:23,919 - INFO - [diffusion][Epoch 8146] diffusion learning rate: 0.001
2024-11-05 00:35:23,921 - INFO - [diffusion][Epoch 8146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:23,922 - INFO - [diffusion][Epoch 8147] Epoch 8148/12000
2024-11-05 00:35:28,026 - INFO - [diffusion][Epoch 8147] diffusion training Loss: 0.0733617153018713
2024-11-05 00:35:28,028 - INFO - [diffusion][Epoch 8147] diffusion learning rate: 0.001
2024-11-05 00:35:28,029 - INFO - [diffusion][Epoch 8147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:28,031 - INFO - [diffusion][Epoch 8148] Epoch 8149/12000
2024-11-05 00:35:32,093 - INFO - [diffusion][Epoch 8148] diffusion training Loss: 0.06950240768492222
2024-11-05 00:35:32,095 - INFO - [diffusion][Epoch 8148] diffusion learning rate: 0.001
2024-11-05 00:35:32,097 - INFO - [diffusion][Epoch 8148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:32,098 - INFO - [diffusion][Epoch 8149] Epoch 8150/12000
2024-11-05 00:35:36,233 - INFO - [diffusion][Epoch 8149] diffusion training Loss: 0.06689858995378017
2024-11-05 00:35:36,235 - INFO - [diffusion][Epoch 8149] diffusion learning rate: 0.001
2024-11-05 00:35:36,237 - INFO - [diffusion][Epoch 8149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:36,238 - INFO - [diffusion][Epoch 8150] Epoch 8151/12000
2024-11-05 00:35:40,246 - INFO - [diffusion][Epoch 8150] diffusion training Loss: 0.06264826003462076
2024-11-05 00:35:40,249 - INFO - [diffusion][Epoch 8150] diffusion learning rate: 0.001
2024-11-05 00:35:40,251 - INFO - [diffusion][Epoch 8150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:40,253 - INFO - [diffusion][Epoch 8151] Epoch 8152/12000
2024-11-05 00:35:44,397 - INFO - [diffusion][Epoch 8151] diffusion training Loss: 0.06554055493324995
2024-11-05 00:35:44,399 - INFO - [diffusion][Epoch 8151] diffusion learning rate: 0.001
2024-11-05 00:35:44,401 - INFO - [diffusion][Epoch 8151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:44,402 - INFO - [diffusion][Epoch 8152] Epoch 8153/12000
2024-11-05 00:35:48,672 - INFO - [diffusion][Epoch 8152] diffusion training Loss: 0.07230704091489315
2024-11-05 00:35:48,674 - INFO - [diffusion][Epoch 8152] diffusion learning rate: 0.001
2024-11-05 00:35:48,676 - INFO - [diffusion][Epoch 8152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:48,677 - INFO - [diffusion][Epoch 8153] Epoch 8154/12000
2024-11-05 00:35:52,696 - INFO - [diffusion][Epoch 8153] diffusion training Loss: 0.06734642200171947
2024-11-05 00:35:52,698 - INFO - [diffusion][Epoch 8153] diffusion learning rate: 0.001
2024-11-05 00:35:52,700 - INFO - [diffusion][Epoch 8153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:52,701 - INFO - [diffusion][Epoch 8154] Epoch 8155/12000
2024-11-05 00:35:56,942 - INFO - [diffusion][Epoch 8154] diffusion training Loss: 0.06925484072417021
2024-11-05 00:35:56,945 - INFO - [diffusion][Epoch 8154] diffusion learning rate: 0.001
2024-11-05 00:35:56,947 - INFO - [diffusion][Epoch 8154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:56,948 - INFO - [diffusion][Epoch 8155] Epoch 8156/12000
2024-11-05 00:36:01,233 - INFO - [diffusion][Epoch 8155] diffusion training Loss: 0.07203926239162683
2024-11-05 00:36:01,236 - INFO - [diffusion][Epoch 8155] diffusion learning rate: 0.001
2024-11-05 00:36:01,239 - INFO - [diffusion][Epoch 8155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:01,240 - INFO - [diffusion][Epoch 8156] Epoch 8157/12000
2024-11-05 00:36:05,516 - INFO - [diffusion][Epoch 8156] diffusion training Loss: 0.06425867788493633
2024-11-05 00:36:05,518 - INFO - [diffusion][Epoch 8156] diffusion learning rate: 0.001
2024-11-05 00:36:05,520 - INFO - [diffusion][Epoch 8156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:05,521 - INFO - [diffusion][Epoch 8157] Epoch 8158/12000
2024-11-05 00:36:09,752 - INFO - [diffusion][Epoch 8157] diffusion training Loss: 0.07190848141908646
2024-11-05 00:36:09,754 - INFO - [diffusion][Epoch 8157] diffusion learning rate: 0.001
2024-11-05 00:36:09,756 - INFO - [diffusion][Epoch 8157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:09,757 - INFO - [diffusion][Epoch 8158] Epoch 8159/12000
2024-11-05 00:36:13,962 - INFO - [diffusion][Epoch 8158] diffusion training Loss: 0.06656542234122753
2024-11-05 00:36:13,965 - INFO - [diffusion][Epoch 8158] diffusion learning rate: 0.001
2024-11-05 00:36:13,967 - INFO - [diffusion][Epoch 8158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:13,968 - INFO - [diffusion][Epoch 8159] Epoch 8160/12000
2024-11-05 00:36:18,086 - INFO - [diffusion][Epoch 8159] diffusion training Loss: 0.07118828035891056
2024-11-05 00:36:18,088 - INFO - [diffusion][Epoch 8159] diffusion learning rate: 0.001
2024-11-05 00:36:18,090 - INFO - [diffusion][Epoch 8159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:18,092 - INFO - [diffusion][Epoch 8160] Epoch 8161/12000
2024-11-05 00:36:22,265 - INFO - [diffusion][Epoch 8160] diffusion training Loss: 0.0685508418828249
2024-11-05 00:36:22,267 - INFO - [diffusion][Epoch 8160] diffusion learning rate: 0.001
2024-11-05 00:36:22,270 - INFO - [diffusion][Epoch 8160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:22,271 - INFO - [diffusion][Epoch 8161] Epoch 8162/12000
2024-11-05 00:36:26,405 - INFO - [diffusion][Epoch 8161] diffusion training Loss: 0.06695442274212837
2024-11-05 00:36:26,407 - INFO - [diffusion][Epoch 8161] diffusion learning rate: 0.001
2024-11-05 00:36:26,409 - INFO - [diffusion][Epoch 8161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:26,411 - INFO - [diffusion][Epoch 8162] Epoch 8163/12000
2024-11-05 00:36:30,760 - INFO - [diffusion][Epoch 8162] diffusion training Loss: 0.07121121883392334
2024-11-05 00:36:30,763 - INFO - [diffusion][Epoch 8162] diffusion learning rate: 0.001
2024-11-05 00:36:30,765 - INFO - [diffusion][Epoch 8162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:30,766 - INFO - [diffusion][Epoch 8163] Epoch 8164/12000
2024-11-05 00:36:35,033 - INFO - [diffusion][Epoch 8163] diffusion training Loss: 0.07456991448998451
2024-11-05 00:36:35,035 - INFO - [diffusion][Epoch 8163] diffusion learning rate: 0.001
2024-11-05 00:36:35,037 - INFO - [diffusion][Epoch 8163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:35,038 - INFO - [diffusion][Epoch 8164] Epoch 8165/12000
2024-11-05 00:36:39,278 - INFO - [diffusion][Epoch 8164] diffusion training Loss: 0.07258678879588842
2024-11-05 00:36:39,281 - INFO - [diffusion][Epoch 8164] diffusion learning rate: 0.001
2024-11-05 00:36:39,283 - INFO - [diffusion][Epoch 8164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:39,284 - INFO - [diffusion][Epoch 8165] Epoch 8166/12000
2024-11-05 00:36:43,703 - INFO - [diffusion][Epoch 8165] diffusion training Loss: 0.06421947572380304
2024-11-05 00:36:43,705 - INFO - [diffusion][Epoch 8165] diffusion learning rate: 0.001
2024-11-05 00:36:43,707 - INFO - [diffusion][Epoch 8165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:43,708 - INFO - [diffusion][Epoch 8166] Epoch 8167/12000
2024-11-05 00:36:47,897 - INFO - [diffusion][Epoch 8166] diffusion training Loss: 0.07184750586748123
2024-11-05 00:36:47,899 - INFO - [diffusion][Epoch 8166] diffusion learning rate: 0.001
2024-11-05 00:36:47,901 - INFO - [diffusion][Epoch 8166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:47,903 - INFO - [diffusion][Epoch 8167] Epoch 8168/12000
2024-11-05 00:36:52,024 - INFO - [diffusion][Epoch 8167] diffusion training Loss: 0.06928635761141777
2024-11-05 00:36:52,026 - INFO - [diffusion][Epoch 8167] diffusion learning rate: 0.001
2024-11-05 00:36:52,028 - INFO - [diffusion][Epoch 8167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:52,029 - INFO - [diffusion][Epoch 8168] Epoch 8169/12000
2024-11-05 00:36:56,258 - INFO - [diffusion][Epoch 8168] diffusion training Loss: 0.06426064297556877
2024-11-05 00:36:56,260 - INFO - [diffusion][Epoch 8168] diffusion learning rate: 0.001
2024-11-05 00:36:56,262 - INFO - [diffusion][Epoch 8168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:56,263 - INFO - [diffusion][Epoch 8169] Epoch 8170/12000
2024-11-05 00:37:00,416 - INFO - [diffusion][Epoch 8169] diffusion training Loss: 0.0629952335730195
2024-11-05 00:37:00,418 - INFO - [diffusion][Epoch 8169] diffusion learning rate: 0.001
2024-11-05 00:37:00,420 - INFO - [diffusion][Epoch 8169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:00,421 - INFO - [diffusion][Epoch 8170] Epoch 8171/12000
2024-11-05 00:37:04,665 - INFO - [diffusion][Epoch 8170] diffusion training Loss: 0.06703945342451334
2024-11-05 00:37:04,669 - INFO - [diffusion][Epoch 8170] diffusion learning rate: 0.001
2024-11-05 00:37:04,671 - INFO - [diffusion][Epoch 8170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:04,672 - INFO - [diffusion][Epoch 8171] Epoch 8172/12000
2024-11-05 00:37:08,865 - INFO - [diffusion][Epoch 8171] diffusion training Loss: 0.07171734422445297
2024-11-05 00:37:08,867 - INFO - [diffusion][Epoch 8171] diffusion learning rate: 0.001
2024-11-05 00:37:08,869 - INFO - [diffusion][Epoch 8171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:08,870 - INFO - [diffusion][Epoch 8172] Epoch 8173/12000
2024-11-05 00:37:12,941 - INFO - [diffusion][Epoch 8172] diffusion training Loss: 0.06491169612854719
2024-11-05 00:37:12,943 - INFO - [diffusion][Epoch 8172] diffusion learning rate: 0.001
2024-11-05 00:37:12,945 - INFO - [diffusion][Epoch 8172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:12,947 - INFO - [diffusion][Epoch 8173] Epoch 8174/12000
2024-11-05 00:37:17,114 - INFO - [diffusion][Epoch 8173] diffusion training Loss: 0.06789360754191875
2024-11-05 00:37:17,116 - INFO - [diffusion][Epoch 8173] diffusion learning rate: 0.001
2024-11-05 00:37:17,118 - INFO - [diffusion][Epoch 8173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:17,119 - INFO - [diffusion][Epoch 8174] Epoch 8175/12000
2024-11-05 00:37:21,447 - INFO - [diffusion][Epoch 8174] diffusion training Loss: 0.06618487928062677
2024-11-05 00:37:21,449 - INFO - [diffusion][Epoch 8174] diffusion learning rate: 0.001
2024-11-05 00:37:21,450 - INFO - [diffusion][Epoch 8174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:21,452 - INFO - [diffusion][Epoch 8175] Epoch 8176/12000
2024-11-05 00:37:25,754 - INFO - [diffusion][Epoch 8175] diffusion training Loss: 0.06746845506131649
2024-11-05 00:37:25,756 - INFO - [diffusion][Epoch 8175] diffusion learning rate: 0.001
2024-11-05 00:37:25,758 - INFO - [diffusion][Epoch 8175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:25,759 - INFO - [diffusion][Epoch 8176] Epoch 8177/12000
2024-11-05 00:37:29,964 - INFO - [diffusion][Epoch 8176] diffusion training Loss: 0.06649410165846348
2024-11-05 00:37:29,972 - INFO - [diffusion][Epoch 8176] diffusion learning rate: 0.001
2024-11-05 00:37:29,974 - INFO - [diffusion][Epoch 8176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:29,976 - INFO - [diffusion][Epoch 8177] Epoch 8178/12000
2024-11-05 00:37:34,143 - INFO - [diffusion][Epoch 8177] diffusion training Loss: 0.0704868696630001
2024-11-05 00:37:34,145 - INFO - [diffusion][Epoch 8177] diffusion learning rate: 0.001
2024-11-05 00:37:34,147 - INFO - [diffusion][Epoch 8177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:34,148 - INFO - [diffusion][Epoch 8178] Epoch 8179/12000
2024-11-05 00:37:38,272 - INFO - [diffusion][Epoch 8178] diffusion training Loss: 0.06545236520469189
2024-11-05 00:37:38,275 - INFO - [diffusion][Epoch 8178] diffusion learning rate: 0.001
2024-11-05 00:37:38,278 - INFO - [diffusion][Epoch 8178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:38,280 - INFO - [diffusion][Epoch 8179] Epoch 8180/12000
2024-11-05 00:37:42,480 - INFO - [diffusion][Epoch 8179] diffusion training Loss: 0.06405209749937057
2024-11-05 00:37:42,482 - INFO - [diffusion][Epoch 8179] diffusion learning rate: 0.001
2024-11-05 00:37:42,485 - INFO - [diffusion][Epoch 8179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:42,487 - INFO - [diffusion][Epoch 8180] Epoch 8181/12000
2024-11-05 00:37:46,576 - INFO - [diffusion][Epoch 8180] diffusion training Loss: 0.07229385431855917
2024-11-05 00:37:46,579 - INFO - [diffusion][Epoch 8180] diffusion learning rate: 0.001
2024-11-05 00:37:46,581 - INFO - [diffusion][Epoch 8180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:46,582 - INFO - [diffusion][Epoch 8181] Epoch 8182/12000
2024-11-05 00:37:50,829 - INFO - [diffusion][Epoch 8181] diffusion training Loss: 0.06352328136563301
2024-11-05 00:37:50,831 - INFO - [diffusion][Epoch 8181] diffusion learning rate: 0.001
2024-11-05 00:37:50,833 - INFO - [diffusion][Epoch 8181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:50,834 - INFO - [diffusion][Epoch 8182] Epoch 8183/12000
2024-11-05 00:37:55,012 - INFO - [diffusion][Epoch 8182] diffusion training Loss: 0.06423555593937635
2024-11-05 00:37:55,014 - INFO - [diffusion][Epoch 8182] diffusion learning rate: 0.001
2024-11-05 00:37:55,016 - INFO - [diffusion][Epoch 8182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:55,017 - INFO - [diffusion][Epoch 8183] Epoch 8184/12000
2024-11-05 00:37:59,173 - INFO - [diffusion][Epoch 8183] diffusion training Loss: 0.06796756759285927
2024-11-05 00:37:59,175 - INFO - [diffusion][Epoch 8183] diffusion learning rate: 0.001
2024-11-05 00:37:59,177 - INFO - [diffusion][Epoch 8183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:59,178 - INFO - [diffusion][Epoch 8184] Epoch 8185/12000
2024-11-05 00:38:03,337 - INFO - [diffusion][Epoch 8184] diffusion training Loss: 0.06457747053354979
2024-11-05 00:38:03,339 - INFO - [diffusion][Epoch 8184] diffusion learning rate: 0.001
2024-11-05 00:38:03,341 - INFO - [diffusion][Epoch 8184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:03,342 - INFO - [diffusion][Epoch 8185] Epoch 8186/12000
2024-11-05 00:38:07,719 - INFO - [diffusion][Epoch 8185] diffusion training Loss: 0.06746339797973633
2024-11-05 00:38:07,721 - INFO - [diffusion][Epoch 8185] diffusion learning rate: 0.001
2024-11-05 00:38:07,723 - INFO - [diffusion][Epoch 8185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:07,724 - INFO - [diffusion][Epoch 8186] Epoch 8187/12000
2024-11-05 00:38:12,059 - INFO - [diffusion][Epoch 8186] diffusion training Loss: 0.07366153039038181
2024-11-05 00:38:12,061 - INFO - [diffusion][Epoch 8186] diffusion learning rate: 0.001
2024-11-05 00:38:12,063 - INFO - [diffusion][Epoch 8186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:12,065 - INFO - [diffusion][Epoch 8187] Epoch 8188/12000
2024-11-05 00:38:16,312 - INFO - [diffusion][Epoch 8187] diffusion training Loss: 0.06310254894196987
2024-11-05 00:38:16,315 - INFO - [diffusion][Epoch 8187] diffusion learning rate: 0.001
2024-11-05 00:38:16,317 - INFO - [diffusion][Epoch 8187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:16,319 - INFO - [diffusion][Epoch 8188] Epoch 8189/12000
2024-11-05 00:38:20,523 - INFO - [diffusion][Epoch 8188] diffusion training Loss: 0.06532630324363708
2024-11-05 00:38:20,525 - INFO - [diffusion][Epoch 8188] diffusion learning rate: 0.001
2024-11-05 00:38:20,527 - INFO - [diffusion][Epoch 8188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:20,529 - INFO - [diffusion][Epoch 8189] Epoch 8190/12000
2024-11-05 00:38:24,758 - INFO - [diffusion][Epoch 8189] diffusion training Loss: 0.06401110254228115
2024-11-05 00:38:24,759 - INFO - [diffusion][Epoch 8189] diffusion learning rate: 0.001
2024-11-05 00:38:24,761 - INFO - [diffusion][Epoch 8189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:24,763 - INFO - [diffusion][Epoch 8190] Epoch 8191/12000
2024-11-05 00:38:28,983 - INFO - [diffusion][Epoch 8190] diffusion training Loss: 0.06694714725017548
2024-11-05 00:38:28,986 - INFO - [diffusion][Epoch 8190] diffusion learning rate: 0.001
2024-11-05 00:38:28,988 - INFO - [diffusion][Epoch 8190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:28,989 - INFO - [diffusion][Epoch 8191] Epoch 8192/12000
2024-11-05 00:38:32,956 - INFO - [diffusion][Epoch 8191] diffusion training Loss: 0.06694566458463669
2024-11-05 00:38:32,958 - INFO - [diffusion][Epoch 8191] diffusion learning rate: 0.001
2024-11-05 00:38:32,960 - INFO - [diffusion][Epoch 8191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:32,961 - INFO - [diffusion][Epoch 8192] Epoch 8193/12000
2024-11-05 00:38:37,200 - INFO - [diffusion][Epoch 8192] diffusion training Loss: 0.06702354550361633
2024-11-05 00:38:37,202 - INFO - [diffusion][Epoch 8192] diffusion learning rate: 0.001
2024-11-05 00:38:37,204 - INFO - [diffusion][Epoch 8192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:37,206 - INFO - [diffusion][Epoch 8193] Epoch 8194/12000
2024-11-05 00:38:41,519 - INFO - [diffusion][Epoch 8193] diffusion training Loss: 0.06474847625941038
2024-11-05 00:38:41,522 - INFO - [diffusion][Epoch 8193] diffusion learning rate: 0.001
2024-11-05 00:38:41,523 - INFO - [diffusion][Epoch 8193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:41,524 - INFO - [diffusion][Epoch 8194] Epoch 8195/12000
2024-11-05 00:38:45,708 - INFO - [diffusion][Epoch 8194] diffusion training Loss: 0.0690958984196186
2024-11-05 00:38:45,710 - INFO - [diffusion][Epoch 8194] diffusion learning rate: 0.001
2024-11-05 00:38:45,711 - INFO - [diffusion][Epoch 8194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:45,713 - INFO - [diffusion][Epoch 8195] Epoch 8196/12000
2024-11-05 00:38:49,758 - INFO - [diffusion][Epoch 8195] diffusion training Loss: 0.06586326286196709
2024-11-05 00:38:49,760 - INFO - [diffusion][Epoch 8195] diffusion learning rate: 0.001
2024-11-05 00:38:49,763 - INFO - [diffusion][Epoch 8195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:49,764 - INFO - [diffusion][Epoch 8196] Epoch 8197/12000
2024-11-05 00:38:53,815 - INFO - [diffusion][Epoch 8196] diffusion training Loss: 0.06954098679125309
2024-11-05 00:38:53,817 - INFO - [diffusion][Epoch 8196] diffusion learning rate: 0.001
2024-11-05 00:38:53,819 - INFO - [diffusion][Epoch 8196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:53,820 - INFO - [diffusion][Epoch 8197] Epoch 8198/12000
2024-11-05 00:38:57,809 - INFO - [diffusion][Epoch 8197] diffusion training Loss: 0.06595027446746826
2024-11-05 00:38:57,811 - INFO - [diffusion][Epoch 8197] diffusion learning rate: 0.001
2024-11-05 00:38:57,813 - INFO - [diffusion][Epoch 8197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:57,814 - INFO - [diffusion][Epoch 8198] Epoch 8199/12000
2024-11-05 00:39:02,103 - INFO - [diffusion][Epoch 8198] diffusion training Loss: 0.07105160970240831
2024-11-05 00:39:02,107 - INFO - [diffusion][Epoch 8198] diffusion learning rate: 0.001
2024-11-05 00:39:02,109 - INFO - [diffusion][Epoch 8198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:02,111 - INFO - [diffusion][Epoch 8199] Epoch 8200/12000
2024-11-05 00:39:06,292 - INFO - [diffusion][Epoch 8199] diffusion training Loss: 0.07016059290617704
2024-11-05 00:39:06,294 - INFO - [diffusion][Epoch 8199] diffusion learning rate: 0.001
2024-11-05 00:39:06,296 - INFO - [diffusion][Epoch 8199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:06,297 - INFO - [diffusion][Epoch 8200] Epoch 8201/12000
2024-11-05 00:39:10,400 - INFO - [diffusion][Epoch 8200] diffusion training Loss: 0.07053542509675026
2024-11-05 00:39:10,402 - INFO - [diffusion][Epoch 8200] diffusion learning rate: 0.001
2024-11-05 00:39:10,404 - INFO - [diffusion][Epoch 8200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:10,405 - INFO - [diffusion][Epoch 8201] Epoch 8202/12000
2024-11-05 00:39:14,399 - INFO - [diffusion][Epoch 8201] diffusion training Loss: 0.07293255999684334
2024-11-05 00:39:14,402 - INFO - [diffusion][Epoch 8201] diffusion learning rate: 0.001
2024-11-05 00:39:14,404 - INFO - [diffusion][Epoch 8201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:14,406 - INFO - [diffusion][Epoch 8202] Epoch 8203/12000
2024-11-05 00:39:18,439 - INFO - [diffusion][Epoch 8202] diffusion training Loss: 0.06873822771012783
2024-11-05 00:39:18,441 - INFO - [diffusion][Epoch 8202] diffusion learning rate: 0.001
2024-11-05 00:39:18,443 - INFO - [diffusion][Epoch 8202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:18,445 - INFO - [diffusion][Epoch 8203] Epoch 8204/12000
2024-11-05 00:39:22,587 - INFO - [diffusion][Epoch 8203] diffusion training Loss: 0.0679692905396223
2024-11-05 00:39:22,589 - INFO - [diffusion][Epoch 8203] diffusion learning rate: 0.001
2024-11-05 00:39:22,590 - INFO - [diffusion][Epoch 8203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:22,592 - INFO - [diffusion][Epoch 8204] Epoch 8205/12000
2024-11-05 00:39:26,501 - INFO - [diffusion][Epoch 8204] diffusion training Loss: 0.07196015678346157
2024-11-05 00:39:26,503 - INFO - [diffusion][Epoch 8204] diffusion learning rate: 0.001
2024-11-05 00:39:26,505 - INFO - [diffusion][Epoch 8204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:26,506 - INFO - [diffusion][Epoch 8205] Epoch 8206/12000
2024-11-05 00:39:30,646 - INFO - [diffusion][Epoch 8205] diffusion training Loss: 0.0671810545027256
2024-11-05 00:39:30,649 - INFO - [diffusion][Epoch 8205] diffusion learning rate: 0.001
2024-11-05 00:39:30,651 - INFO - [diffusion][Epoch 8205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:30,653 - INFO - [diffusion][Epoch 8206] Epoch 8207/12000
2024-11-05 00:39:34,864 - INFO - [diffusion][Epoch 8206] diffusion training Loss: 0.06905665621161461
2024-11-05 00:39:34,867 - INFO - [diffusion][Epoch 8206] diffusion learning rate: 0.001
2024-11-05 00:39:34,868 - INFO - [diffusion][Epoch 8206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:34,870 - INFO - [diffusion][Epoch 8207] Epoch 8208/12000
2024-11-05 00:39:39,773 - INFO - [diffusion][Epoch 8207] diffusion training Loss: 0.06629608850926161
2024-11-05 00:39:39,775 - INFO - [diffusion][Epoch 8207] diffusion learning rate: 0.001
2024-11-05 00:39:39,777 - INFO - [diffusion][Epoch 8207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:39,778 - INFO - [diffusion][Epoch 8208] Epoch 8209/12000
2024-11-05 00:39:43,800 - INFO - [diffusion][Epoch 8208] diffusion training Loss: 0.07084445655345917
2024-11-05 00:39:43,802 - INFO - [diffusion][Epoch 8208] diffusion learning rate: 0.001
2024-11-05 00:39:43,804 - INFO - [diffusion][Epoch 8208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:43,805 - INFO - [diffusion][Epoch 8209] Epoch 8210/12000
2024-11-05 00:39:47,910 - INFO - [diffusion][Epoch 8209] diffusion training Loss: 0.06660037860274315
2024-11-05 00:39:47,912 - INFO - [diffusion][Epoch 8209] diffusion learning rate: 0.001
2024-11-05 00:39:47,915 - INFO - [diffusion][Epoch 8209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:47,916 - INFO - [diffusion][Epoch 8210] Epoch 8211/12000
2024-11-05 00:39:52,043 - INFO - [diffusion][Epoch 8210] diffusion training Loss: 0.071266770362854
2024-11-05 00:39:52,046 - INFO - [diffusion][Epoch 8210] diffusion learning rate: 0.001
2024-11-05 00:39:52,048 - INFO - [diffusion][Epoch 8210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:52,049 - INFO - [diffusion][Epoch 8211] Epoch 8212/12000
2024-11-05 00:39:56,115 - INFO - [diffusion][Epoch 8211] diffusion training Loss: 0.06421810202300549
2024-11-05 00:39:56,117 - INFO - [diffusion][Epoch 8211] diffusion learning rate: 0.001
2024-11-05 00:39:56,119 - INFO - [diffusion][Epoch 8211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:56,120 - INFO - [diffusion][Epoch 8212] Epoch 8213/12000
2024-11-05 00:40:00,228 - INFO - [diffusion][Epoch 8212] diffusion training Loss: 0.07335209660232067
2024-11-05 00:40:00,231 - INFO - [diffusion][Epoch 8212] diffusion learning rate: 0.001
2024-11-05 00:40:00,233 - INFO - [diffusion][Epoch 8212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:00,234 - INFO - [diffusion][Epoch 8213] Epoch 8214/12000
2024-11-05 00:40:04,026 - INFO - [diffusion][Epoch 8213] diffusion training Loss: 0.06689008511602879
2024-11-05 00:40:04,028 - INFO - [diffusion][Epoch 8213] diffusion learning rate: 0.001
2024-11-05 00:40:04,030 - INFO - [diffusion][Epoch 8213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:04,031 - INFO - [diffusion][Epoch 8214] Epoch 8215/12000
2024-11-05 00:40:08,086 - INFO - [diffusion][Epoch 8214] diffusion training Loss: 0.06866863556206226
2024-11-05 00:40:08,089 - INFO - [diffusion][Epoch 8214] diffusion learning rate: 0.001
2024-11-05 00:40:08,091 - INFO - [diffusion][Epoch 8214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:08,092 - INFO - [diffusion][Epoch 8215] Epoch 8216/12000
2024-11-05 00:40:12,133 - INFO - [diffusion][Epoch 8215] diffusion training Loss: 0.07077918015420437
2024-11-05 00:40:12,135 - INFO - [diffusion][Epoch 8215] diffusion learning rate: 0.001
2024-11-05 00:40:12,137 - INFO - [diffusion][Epoch 8215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:12,139 - INFO - [diffusion][Epoch 8216] Epoch 8217/12000
2024-11-05 00:40:16,202 - INFO - [diffusion][Epoch 8216] diffusion training Loss: 0.06842286046594381
2024-11-05 00:40:16,204 - INFO - [diffusion][Epoch 8216] diffusion learning rate: 0.001
2024-11-05 00:40:16,207 - INFO - [diffusion][Epoch 8216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:16,209 - INFO - [diffusion][Epoch 8217] Epoch 8218/12000
2024-11-05 00:40:20,370 - INFO - [diffusion][Epoch 8217] diffusion training Loss: 0.06618406809866428
2024-11-05 00:40:20,372 - INFO - [diffusion][Epoch 8217] diffusion learning rate: 0.001
2024-11-05 00:40:20,374 - INFO - [diffusion][Epoch 8217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:20,376 - INFO - [diffusion][Epoch 8218] Epoch 8219/12000
2024-11-05 00:40:24,525 - INFO - [diffusion][Epoch 8218] diffusion training Loss: 0.06294052302837372
2024-11-05 00:40:24,527 - INFO - [diffusion][Epoch 8218] diffusion learning rate: 0.001
2024-11-05 00:40:24,529 - INFO - [diffusion][Epoch 8218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:24,532 - INFO - [diffusion][Epoch 8219] Epoch 8220/12000
2024-11-05 00:40:28,749 - INFO - [diffusion][Epoch 8219] diffusion training Loss: 0.0625363178551197
2024-11-05 00:40:28,751 - INFO - [diffusion][Epoch 8219] diffusion learning rate: 0.001
2024-11-05 00:40:28,753 - INFO - [diffusion][Epoch 8219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:28,755 - INFO - [diffusion][Epoch 8220] Epoch 8221/12000
2024-11-05 00:40:32,763 - INFO - [diffusion][Epoch 8220] diffusion training Loss: 0.06623536348342896
2024-11-05 00:40:32,765 - INFO - [diffusion][Epoch 8220] diffusion learning rate: 0.001
2024-11-05 00:40:32,813 - INFO - [diffusion][Epoch 8220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:32,814 - INFO - [diffusion][Epoch 8221] Epoch 8222/12000
2024-11-05 00:40:36,921 - INFO - [diffusion][Epoch 8221] diffusion training Loss: 0.07117726467549801
2024-11-05 00:40:36,923 - INFO - [diffusion][Epoch 8221] diffusion learning rate: 0.001
2024-11-05 00:40:36,925 - INFO - [diffusion][Epoch 8221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:36,926 - INFO - [diffusion][Epoch 8222] Epoch 8223/12000
2024-11-05 00:40:40,997 - INFO - [diffusion][Epoch 8222] diffusion training Loss: 0.06445518974214792
2024-11-05 00:40:41,000 - INFO - [diffusion][Epoch 8222] diffusion learning rate: 0.001
2024-11-05 00:40:41,002 - INFO - [diffusion][Epoch 8222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:41,004 - INFO - [diffusion][Epoch 8223] Epoch 8224/12000
2024-11-05 00:40:45,026 - INFO - [diffusion][Epoch 8223] diffusion training Loss: 0.0674698892980814
2024-11-05 00:40:45,028 - INFO - [diffusion][Epoch 8223] diffusion learning rate: 0.001
2024-11-05 00:40:45,030 - INFO - [diffusion][Epoch 8223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:45,031 - INFO - [diffusion][Epoch 8224] Epoch 8225/12000
2024-11-05 00:40:49,282 - INFO - [diffusion][Epoch 8224] diffusion training Loss: 0.0654884111136198
2024-11-05 00:40:49,284 - INFO - [diffusion][Epoch 8224] diffusion learning rate: 0.001
2024-11-05 00:40:49,286 - INFO - [diffusion][Epoch 8224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:49,287 - INFO - [diffusion][Epoch 8225] Epoch 8226/12000
2024-11-05 00:40:53,359 - INFO - [diffusion][Epoch 8225] diffusion training Loss: 0.07239937596023083
2024-11-05 00:40:53,362 - INFO - [diffusion][Epoch 8225] diffusion learning rate: 0.001
2024-11-05 00:40:53,364 - INFO - [diffusion][Epoch 8225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:53,365 - INFO - [diffusion][Epoch 8226] Epoch 8227/12000
2024-11-05 00:40:57,571 - INFO - [diffusion][Epoch 8226] diffusion training Loss: 0.06742916256189346
2024-11-05 00:40:57,573 - INFO - [diffusion][Epoch 8226] diffusion learning rate: 0.001
2024-11-05 00:40:57,575 - INFO - [diffusion][Epoch 8226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:57,576 - INFO - [diffusion][Epoch 8227] Epoch 8228/12000
2024-11-05 00:41:02,250 - INFO - [diffusion][Epoch 8227] diffusion training Loss: 0.07126698642969131
2024-11-05 00:41:02,251 - INFO - [diffusion][Epoch 8227] diffusion learning rate: 0.001
2024-11-05 00:41:02,253 - INFO - [diffusion][Epoch 8227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:02,255 - INFO - [diffusion][Epoch 8228] Epoch 8229/12000
2024-11-05 00:41:06,345 - INFO - [diffusion][Epoch 8228] diffusion training Loss: 0.0656621903181076
2024-11-05 00:41:06,347 - INFO - [diffusion][Epoch 8228] diffusion learning rate: 0.001
2024-11-05 00:41:06,348 - INFO - [diffusion][Epoch 8228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:06,350 - INFO - [diffusion][Epoch 8229] Epoch 8230/12000
2024-11-05 00:41:10,579 - INFO - [diffusion][Epoch 8229] diffusion training Loss: 0.06697474792599678
2024-11-05 00:41:10,581 - INFO - [diffusion][Epoch 8229] diffusion learning rate: 0.001
2024-11-05 00:41:10,582 - INFO - [diffusion][Epoch 8229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:10,584 - INFO - [diffusion][Epoch 8230] Epoch 8231/12000
2024-11-05 00:41:14,858 - INFO - [diffusion][Epoch 8230] diffusion training Loss: 0.06930187344551086
2024-11-05 00:41:14,860 - INFO - [diffusion][Epoch 8230] diffusion learning rate: 0.001
2024-11-05 00:41:14,862 - INFO - [diffusion][Epoch 8230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:14,863 - INFO - [diffusion][Epoch 8231] Epoch 8232/12000
2024-11-05 00:41:18,997 - INFO - [diffusion][Epoch 8231] diffusion training Loss: 0.06498891115188599
2024-11-05 00:41:18,999 - INFO - [diffusion][Epoch 8231] diffusion learning rate: 0.001
2024-11-05 00:41:19,001 - INFO - [diffusion][Epoch 8231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:19,002 - INFO - [diffusion][Epoch 8232] Epoch 8233/12000
2024-11-05 00:41:22,990 - INFO - [diffusion][Epoch 8232] diffusion training Loss: 0.06811660155653954
2024-11-05 00:41:22,991 - INFO - [diffusion][Epoch 8232] diffusion learning rate: 0.001
2024-11-05 00:41:22,993 - INFO - [diffusion][Epoch 8232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:22,995 - INFO - [diffusion][Epoch 8233] Epoch 8234/12000
2024-11-05 00:41:27,080 - INFO - [diffusion][Epoch 8233] diffusion training Loss: 0.06699468567967415
2024-11-05 00:41:27,082 - INFO - [diffusion][Epoch 8233] diffusion learning rate: 0.001
2024-11-05 00:41:27,083 - INFO - [diffusion][Epoch 8233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:27,085 - INFO - [diffusion][Epoch 8234] Epoch 8235/12000
2024-11-05 00:41:31,205 - INFO - [diffusion][Epoch 8234] diffusion training Loss: 0.07432455196976662
2024-11-05 00:41:31,207 - INFO - [diffusion][Epoch 8234] diffusion learning rate: 0.001
2024-11-05 00:41:31,209 - INFO - [diffusion][Epoch 8234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:31,211 - INFO - [diffusion][Epoch 8235] Epoch 8236/12000
2024-11-05 00:41:35,291 - INFO - [diffusion][Epoch 8235] diffusion training Loss: 0.06439192779362202
2024-11-05 00:41:35,293 - INFO - [diffusion][Epoch 8235] diffusion learning rate: 0.001
2024-11-05 00:41:35,296 - INFO - [diffusion][Epoch 8235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:35,297 - INFO - [diffusion][Epoch 8236] Epoch 8237/12000
2024-11-05 00:41:39,462 - INFO - [diffusion][Epoch 8236] diffusion training Loss: 0.06672350130975246
2024-11-05 00:41:39,464 - INFO - [diffusion][Epoch 8236] diffusion learning rate: 0.001
2024-11-05 00:41:39,466 - INFO - [diffusion][Epoch 8236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:39,467 - INFO - [diffusion][Epoch 8237] Epoch 8238/12000
2024-11-05 00:41:43,610 - INFO - [diffusion][Epoch 8237] diffusion training Loss: 0.07274062931537628
2024-11-05 00:41:43,612 - INFO - [diffusion][Epoch 8237] diffusion learning rate: 0.001
2024-11-05 00:41:43,614 - INFO - [diffusion][Epoch 8237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:43,615 - INFO - [diffusion][Epoch 8238] Epoch 8239/12000
2024-11-05 00:41:47,756 - INFO - [diffusion][Epoch 8238] diffusion training Loss: 0.06745932437479496
2024-11-05 00:41:47,758 - INFO - [diffusion][Epoch 8238] diffusion learning rate: 0.001
2024-11-05 00:41:47,759 - INFO - [diffusion][Epoch 8238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:47,760 - INFO - [diffusion][Epoch 8239] Epoch 8240/12000
2024-11-05 00:41:51,915 - INFO - [diffusion][Epoch 8239] diffusion training Loss: 0.06492962688207626
2024-11-05 00:41:51,918 - INFO - [diffusion][Epoch 8239] diffusion learning rate: 0.001
2024-11-05 00:41:51,919 - INFO - [diffusion][Epoch 8239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:51,921 - INFO - [diffusion][Epoch 8240] Epoch 8241/12000
2024-11-05 00:41:56,041 - INFO - [diffusion][Epoch 8240] diffusion training Loss: 0.07052026689052582
2024-11-05 00:41:56,043 - INFO - [diffusion][Epoch 8240] diffusion learning rate: 0.001
2024-11-05 00:41:56,045 - INFO - [diffusion][Epoch 8240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:56,047 - INFO - [diffusion][Epoch 8241] Epoch 8242/12000
2024-11-05 00:42:00,122 - INFO - [diffusion][Epoch 8241] diffusion training Loss: 0.06945449113845825
2024-11-05 00:42:00,124 - INFO - [diffusion][Epoch 8241] diffusion learning rate: 0.001
2024-11-05 00:42:00,126 - INFO - [diffusion][Epoch 8241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:00,127 - INFO - [diffusion][Epoch 8242] Epoch 8243/12000
2024-11-05 00:42:04,331 - INFO - [diffusion][Epoch 8242] diffusion training Loss: 0.0652414858341217
2024-11-05 00:42:04,332 - INFO - [diffusion][Epoch 8242] diffusion learning rate: 0.001
2024-11-05 00:42:04,334 - INFO - [diffusion][Epoch 8242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:04,336 - INFO - [diffusion][Epoch 8243] Epoch 8244/12000
2024-11-05 00:42:08,464 - INFO - [diffusion][Epoch 8243] diffusion training Loss: 0.06450885348021984
2024-11-05 00:42:08,466 - INFO - [diffusion][Epoch 8243] diffusion learning rate: 0.001
2024-11-05 00:42:08,468 - INFO - [diffusion][Epoch 8243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:08,469 - INFO - [diffusion][Epoch 8244] Epoch 8245/12000
2024-11-05 00:42:12,617 - INFO - [diffusion][Epoch 8244] diffusion training Loss: 0.07245905045419931
2024-11-05 00:42:12,620 - INFO - [diffusion][Epoch 8244] diffusion learning rate: 0.001
2024-11-05 00:42:12,622 - INFO - [diffusion][Epoch 8244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:12,623 - INFO - [diffusion][Epoch 8245] Epoch 8246/12000
2024-11-05 00:42:16,833 - INFO - [diffusion][Epoch 8245] diffusion training Loss: 0.07073674909770489
2024-11-05 00:42:16,835 - INFO - [diffusion][Epoch 8245] diffusion learning rate: 0.001
2024-11-05 00:42:16,837 - INFO - [diffusion][Epoch 8245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:16,838 - INFO - [diffusion][Epoch 8246] Epoch 8247/12000
2024-11-05 00:42:20,961 - INFO - [diffusion][Epoch 8246] diffusion training Loss: 0.07037071697413921
2024-11-05 00:42:20,963 - INFO - [diffusion][Epoch 8246] diffusion learning rate: 0.001
2024-11-05 00:42:20,965 - INFO - [diffusion][Epoch 8246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:20,967 - INFO - [diffusion][Epoch 8247] Epoch 8248/12000
2024-11-05 00:42:25,560 - INFO - [diffusion][Epoch 8247] diffusion training Loss: 0.06801778264343739
2024-11-05 00:42:25,563 - INFO - [diffusion][Epoch 8247] diffusion learning rate: 0.001
2024-11-05 00:42:25,602 - INFO - [diffusion][Epoch 8247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:25,604 - INFO - [diffusion][Epoch 8248] Epoch 8249/12000
2024-11-05 00:42:29,796 - INFO - [diffusion][Epoch 8248] diffusion training Loss: 0.07098162919282913
2024-11-05 00:42:29,799 - INFO - [diffusion][Epoch 8248] diffusion learning rate: 0.001
2024-11-05 00:42:29,801 - INFO - [diffusion][Epoch 8248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:29,803 - INFO - [diffusion][Epoch 8249] Epoch 8250/12000
2024-11-05 00:42:33,938 - INFO - [diffusion][Epoch 8249] diffusion training Loss: 0.06902982108294964
2024-11-05 00:42:33,940 - INFO - [diffusion][Epoch 8249] diffusion learning rate: 0.001
2024-11-05 00:42:33,942 - INFO - [diffusion][Epoch 8249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:33,943 - INFO - [diffusion][Epoch 8250] Epoch 8251/12000
2024-11-05 00:42:38,142 - INFO - [diffusion][Epoch 8250] diffusion training Loss: 0.06514305248856544
2024-11-05 00:42:38,144 - INFO - [diffusion][Epoch 8250] diffusion learning rate: 0.001
2024-11-05 00:42:38,146 - INFO - [diffusion][Epoch 8250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:38,147 - INFO - [diffusion][Epoch 8251] Epoch 8252/12000
2024-11-05 00:42:42,158 - INFO - [diffusion][Epoch 8251] diffusion training Loss: 0.06981070525944233
2024-11-05 00:42:42,160 - INFO - [diffusion][Epoch 8251] diffusion learning rate: 0.001
2024-11-05 00:42:42,162 - INFO - [diffusion][Epoch 8251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:42,163 - INFO - [diffusion][Epoch 8252] Epoch 8253/12000
2024-11-05 00:42:46,255 - INFO - [diffusion][Epoch 8252] diffusion training Loss: 0.06905018538236618
2024-11-05 00:42:46,257 - INFO - [diffusion][Epoch 8252] diffusion learning rate: 0.001
2024-11-05 00:42:46,258 - INFO - [diffusion][Epoch 8252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:46,260 - INFO - [diffusion][Epoch 8253] Epoch 8254/12000
2024-11-05 00:42:50,313 - INFO - [diffusion][Epoch 8253] diffusion training Loss: 0.06628259923309088
2024-11-05 00:42:50,315 - INFO - [diffusion][Epoch 8253] diffusion learning rate: 0.001
2024-11-05 00:42:50,316 - INFO - [diffusion][Epoch 8253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:50,318 - INFO - [diffusion][Epoch 8254] Epoch 8255/12000
2024-11-05 00:42:54,236 - INFO - [diffusion][Epoch 8254] diffusion training Loss: 0.07207184284925461
2024-11-05 00:42:54,239 - INFO - [diffusion][Epoch 8254] diffusion learning rate: 0.001
2024-11-05 00:42:54,241 - INFO - [diffusion][Epoch 8254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:54,242 - INFO - [diffusion][Epoch 8255] Epoch 8256/12000
2024-11-05 00:42:58,492 - INFO - [diffusion][Epoch 8255] diffusion training Loss: 0.07282873056828976
2024-11-05 00:42:58,494 - INFO - [diffusion][Epoch 8255] diffusion learning rate: 0.001
2024-11-05 00:42:58,496 - INFO - [diffusion][Epoch 8255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:58,497 - INFO - [diffusion][Epoch 8256] Epoch 8257/12000
2024-11-05 00:43:02,551 - INFO - [diffusion][Epoch 8256] diffusion training Loss: 0.07025394774973392
2024-11-05 00:43:02,553 - INFO - [diffusion][Epoch 8256] diffusion learning rate: 0.001
2024-11-05 00:43:02,556 - INFO - [diffusion][Epoch 8256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:02,558 - INFO - [diffusion][Epoch 8257] Epoch 8258/12000
2024-11-05 00:43:06,543 - INFO - [diffusion][Epoch 8257] diffusion training Loss: 0.06512719392776489
2024-11-05 00:43:06,545 - INFO - [diffusion][Epoch 8257] diffusion learning rate: 0.001
2024-11-05 00:43:06,549 - INFO - [diffusion][Epoch 8257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:06,550 - INFO - [diffusion][Epoch 8258] Epoch 8259/12000
2024-11-05 00:43:10,475 - INFO - [diffusion][Epoch 8258] diffusion training Loss: 0.06872906163334846
2024-11-05 00:43:10,477 - INFO - [diffusion][Epoch 8258] diffusion learning rate: 0.001
2024-11-05 00:43:10,479 - INFO - [diffusion][Epoch 8258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:10,480 - INFO - [diffusion][Epoch 8259] Epoch 8260/12000
2024-11-05 00:43:14,559 - INFO - [diffusion][Epoch 8259] diffusion training Loss: 0.06770148314535618
2024-11-05 00:43:14,562 - INFO - [diffusion][Epoch 8259] diffusion learning rate: 0.001
2024-11-05 00:43:14,564 - INFO - [diffusion][Epoch 8259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:14,565 - INFO - [diffusion][Epoch 8260] Epoch 8261/12000
2024-11-05 00:43:18,568 - INFO - [diffusion][Epoch 8260] diffusion training Loss: 0.06328495964407921
2024-11-05 00:43:18,570 - INFO - [diffusion][Epoch 8260] diffusion learning rate: 0.001
2024-11-05 00:43:18,572 - INFO - [diffusion][Epoch 8260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:18,573 - INFO - [diffusion][Epoch 8261] Epoch 8262/12000
2024-11-05 00:43:22,659 - INFO - [diffusion][Epoch 8261] diffusion training Loss: 0.0664036376401782
2024-11-05 00:43:22,661 - INFO - [diffusion][Epoch 8261] diffusion learning rate: 0.001
2024-11-05 00:43:22,663 - INFO - [diffusion][Epoch 8261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:22,664 - INFO - [diffusion][Epoch 8262] Epoch 8263/12000
2024-11-05 00:43:26,831 - INFO - [diffusion][Epoch 8262] diffusion training Loss: 0.06622486002743244
2024-11-05 00:43:26,835 - INFO - [diffusion][Epoch 8262] diffusion learning rate: 0.001
2024-11-05 00:43:26,837 - INFO - [diffusion][Epoch 8262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:26,839 - INFO - [diffusion][Epoch 8263] Epoch 8264/12000
2024-11-05 00:43:30,871 - INFO - [diffusion][Epoch 8263] diffusion training Loss: 0.07026323303580284
2024-11-05 00:43:30,873 - INFO - [diffusion][Epoch 8263] diffusion learning rate: 0.001
2024-11-05 00:43:30,875 - INFO - [diffusion][Epoch 8263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:30,876 - INFO - [diffusion][Epoch 8264] Epoch 8265/12000
2024-11-05 00:43:34,996 - INFO - [diffusion][Epoch 8264] diffusion training Loss: 0.06633222661912441
2024-11-05 00:43:35,044 - INFO - [diffusion][Epoch 8264] diffusion learning rate: 0.001
2024-11-05 00:43:35,046 - INFO - [diffusion][Epoch 8264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:35,048 - INFO - [diffusion][Epoch 8265] Epoch 8266/12000
2024-11-05 00:43:39,316 - INFO - [diffusion][Epoch 8265] diffusion training Loss: 0.07475702092051506
2024-11-05 00:43:39,318 - INFO - [diffusion][Epoch 8265] diffusion learning rate: 0.001
2024-11-05 00:43:39,319 - INFO - [diffusion][Epoch 8265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:39,321 - INFO - [diffusion][Epoch 8266] Epoch 8267/12000
2024-11-05 00:43:43,454 - INFO - [diffusion][Epoch 8266] diffusion training Loss: 0.07127182558178902
2024-11-05 00:43:43,456 - INFO - [diffusion][Epoch 8266] diffusion learning rate: 0.001
2024-11-05 00:43:43,457 - INFO - [diffusion][Epoch 8266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:43,459 - INFO - [diffusion][Epoch 8267] Epoch 8268/12000
2024-11-05 00:43:47,384 - INFO - [diffusion][Epoch 8267] diffusion training Loss: 0.06644199974834919
2024-11-05 00:43:47,386 - INFO - [diffusion][Epoch 8267] diffusion learning rate: 0.001
2024-11-05 00:43:47,387 - INFO - [diffusion][Epoch 8267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:47,388 - INFO - [diffusion][Epoch 8268] Epoch 8269/12000
2024-11-05 00:43:51,986 - INFO - [diffusion][Epoch 8268] diffusion training Loss: 0.06828695256263018
2024-11-05 00:43:51,988 - INFO - [diffusion][Epoch 8268] diffusion learning rate: 0.001
2024-11-05 00:43:51,989 - INFO - [diffusion][Epoch 8268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:51,991 - INFO - [diffusion][Epoch 8269] Epoch 8270/12000
2024-11-05 00:43:56,015 - INFO - [diffusion][Epoch 8269] diffusion training Loss: 0.07112724706530571
2024-11-05 00:43:56,017 - INFO - [diffusion][Epoch 8269] diffusion learning rate: 0.001
2024-11-05 00:43:56,020 - INFO - [diffusion][Epoch 8269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:56,021 - INFO - [diffusion][Epoch 8270] Epoch 8271/12000
2024-11-05 00:44:00,166 - INFO - [diffusion][Epoch 8270] diffusion training Loss: 0.06280304212123156
2024-11-05 00:44:00,168 - INFO - [diffusion][Epoch 8270] diffusion learning rate: 0.001
2024-11-05 00:44:00,190 - INFO - [diffusion][Epoch 8270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:00,192 - INFO - [diffusion][Epoch 8271] Epoch 8272/12000
2024-11-05 00:44:04,335 - INFO - [diffusion][Epoch 8271] diffusion training Loss: 0.06595590617507696
2024-11-05 00:44:04,336 - INFO - [diffusion][Epoch 8271] diffusion learning rate: 0.001
2024-11-05 00:44:04,338 - INFO - [diffusion][Epoch 8271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:04,340 - INFO - [diffusion][Epoch 8272] Epoch 8273/12000
2024-11-05 00:44:08,313 - INFO - [diffusion][Epoch 8272] diffusion training Loss: 0.06765503250062466
2024-11-05 00:44:08,315 - INFO - [diffusion][Epoch 8272] diffusion learning rate: 0.001
2024-11-05 00:44:08,317 - INFO - [diffusion][Epoch 8272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:08,318 - INFO - [diffusion][Epoch 8273] Epoch 8274/12000
2024-11-05 00:44:12,541 - INFO - [diffusion][Epoch 8273] diffusion training Loss: 0.06829722225666046
2024-11-05 00:44:12,543 - INFO - [diffusion][Epoch 8273] diffusion learning rate: 0.001
2024-11-05 00:44:12,545 - INFO - [diffusion][Epoch 8273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:12,546 - INFO - [diffusion][Epoch 8274] Epoch 8275/12000
2024-11-05 00:44:16,815 - INFO - [diffusion][Epoch 8274] diffusion training Loss: 0.06509324535727501
2024-11-05 00:44:16,817 - INFO - [diffusion][Epoch 8274] diffusion learning rate: 0.001
2024-11-05 00:44:16,819 - INFO - [diffusion][Epoch 8274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:16,820 - INFO - [diffusion][Epoch 8275] Epoch 8276/12000
2024-11-05 00:44:20,892 - INFO - [diffusion][Epoch 8275] diffusion training Loss: 0.0681493952870369
2024-11-05 00:44:20,894 - INFO - [diffusion][Epoch 8275] diffusion learning rate: 0.001
2024-11-05 00:44:20,950 - INFO - [diffusion][Epoch 8275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:20,951 - INFO - [diffusion][Epoch 8276] Epoch 8277/12000
2024-11-05 00:44:25,179 - INFO - [diffusion][Epoch 8276] diffusion training Loss: 0.0654813926666975
2024-11-05 00:44:25,181 - INFO - [diffusion][Epoch 8276] diffusion learning rate: 0.001
2024-11-05 00:44:25,183 - INFO - [diffusion][Epoch 8276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:25,184 - INFO - [diffusion][Epoch 8277] Epoch 8278/12000
2024-11-05 00:44:29,454 - INFO - [diffusion][Epoch 8277] diffusion training Loss: 0.06632088497281075
2024-11-05 00:44:29,456 - INFO - [diffusion][Epoch 8277] diffusion learning rate: 0.001
2024-11-05 00:44:29,458 - INFO - [diffusion][Epoch 8277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:29,459 - INFO - [diffusion][Epoch 8278] Epoch 8279/12000
2024-11-05 00:44:33,526 - INFO - [diffusion][Epoch 8278] diffusion training Loss: 0.062389859929680824
2024-11-05 00:44:33,528 - INFO - [diffusion][Epoch 8278] diffusion learning rate: 0.001
2024-11-05 00:44:33,530 - INFO - [diffusion][Epoch 8278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:33,531 - INFO - [diffusion][Epoch 8279] Epoch 8280/12000
2024-11-05 00:44:37,723 - INFO - [diffusion][Epoch 8279] diffusion training Loss: 0.06794152595102787
2024-11-05 00:44:37,770 - INFO - [diffusion][Epoch 8279] diffusion learning rate: 0.001
2024-11-05 00:44:37,772 - INFO - [diffusion][Epoch 8279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:37,773 - INFO - [diffusion][Epoch 8280] Epoch 8281/12000
2024-11-05 00:44:41,902 - INFO - [diffusion][Epoch 8280] diffusion training Loss: 0.06982973963022232
2024-11-05 00:44:41,904 - INFO - [diffusion][Epoch 8280] diffusion learning rate: 0.001
2024-11-05 00:44:41,905 - INFO - [diffusion][Epoch 8280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:41,907 - INFO - [diffusion][Epoch 8281] Epoch 8282/12000
2024-11-05 00:44:46,123 - INFO - [diffusion][Epoch 8281] diffusion training Loss: 0.06645946577191353
2024-11-05 00:44:46,125 - INFO - [diffusion][Epoch 8281] diffusion learning rate: 0.001
2024-11-05 00:44:46,127 - INFO - [diffusion][Epoch 8281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:46,128 - INFO - [diffusion][Epoch 8282] Epoch 8283/12000
2024-11-05 00:44:50,424 - INFO - [diffusion][Epoch 8282] diffusion training Loss: 0.06026601791381836
2024-11-05 00:44:50,426 - INFO - [diffusion][Epoch 8282] diffusion learning rate: 0.001
2024-11-05 00:44:50,427 - INFO - [diffusion][Epoch 8282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:50,429 - INFO - [diffusion][Epoch 8283] Epoch 8284/12000
2024-11-05 00:44:54,665 - INFO - [diffusion][Epoch 8283] diffusion training Loss: 0.06561119668185711
2024-11-05 00:44:54,667 - INFO - [diffusion][Epoch 8283] diffusion learning rate: 0.001
2024-11-05 00:44:54,669 - INFO - [diffusion][Epoch 8283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:54,670 - INFO - [diffusion][Epoch 8284] Epoch 8285/12000
2024-11-05 00:44:58,907 - INFO - [diffusion][Epoch 8284] diffusion training Loss: 0.06596755795180798
2024-11-05 00:44:58,910 - INFO - [diffusion][Epoch 8284] diffusion learning rate: 0.001
2024-11-05 00:44:58,912 - INFO - [diffusion][Epoch 8284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:58,913 - INFO - [diffusion][Epoch 8285] Epoch 8286/12000
2024-11-05 00:45:03,040 - INFO - [diffusion][Epoch 8285] diffusion training Loss: 0.06891948916018009
2024-11-05 00:45:03,043 - INFO - [diffusion][Epoch 8285] diffusion learning rate: 0.001
2024-11-05 00:45:03,044 - INFO - [diffusion][Epoch 8285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:03,046 - INFO - [diffusion][Epoch 8286] Epoch 8287/12000
2024-11-05 00:45:07,248 - INFO - [diffusion][Epoch 8286] diffusion training Loss: 0.06655121222138405
2024-11-05 00:45:07,251 - INFO - [diffusion][Epoch 8286] diffusion learning rate: 0.001
2024-11-05 00:45:07,253 - INFO - [diffusion][Epoch 8286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:07,254 - INFO - [diffusion][Epoch 8287] Epoch 8288/12000
2024-11-05 00:45:11,454 - INFO - [diffusion][Epoch 8287] diffusion training Loss: 0.06260202266275883
2024-11-05 00:45:11,456 - INFO - [diffusion][Epoch 8287] diffusion learning rate: 0.001
2024-11-05 00:45:11,458 - INFO - [diffusion][Epoch 8287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:11,460 - INFO - [diffusion][Epoch 8288] Epoch 8289/12000
2024-11-05 00:45:15,662 - INFO - [diffusion][Epoch 8288] diffusion training Loss: 0.0699189081788063
2024-11-05 00:45:15,664 - INFO - [diffusion][Epoch 8288] diffusion learning rate: 0.001
2024-11-05 00:45:15,666 - INFO - [diffusion][Epoch 8288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:15,668 - INFO - [diffusion][Epoch 8289] Epoch 8290/12000
2024-11-05 00:45:19,961 - INFO - [diffusion][Epoch 8289] diffusion training Loss: 0.06315623968839645
2024-11-05 00:45:19,963 - INFO - [diffusion][Epoch 8289] diffusion learning rate: 0.001
2024-11-05 00:45:19,965 - INFO - [diffusion][Epoch 8289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:19,967 - INFO - [diffusion][Epoch 8290] Epoch 8291/12000
2024-11-05 00:45:24,557 - INFO - [diffusion][Epoch 8290] diffusion training Loss: 0.07068909704685211
2024-11-05 00:45:24,558 - INFO - [diffusion][Epoch 8290] diffusion learning rate: 0.001
2024-11-05 00:45:24,560 - INFO - [diffusion][Epoch 8290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:24,561 - INFO - [diffusion][Epoch 8291] Epoch 8292/12000
2024-11-05 00:45:28,875 - INFO - [diffusion][Epoch 8291] diffusion training Loss: 0.0712377205491066
2024-11-05 00:45:28,878 - INFO - [diffusion][Epoch 8291] diffusion learning rate: 0.001
2024-11-05 00:45:28,880 - INFO - [diffusion][Epoch 8291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:28,881 - INFO - [diffusion][Epoch 8292] Epoch 8293/12000
2024-11-05 00:45:33,173 - INFO - [diffusion][Epoch 8292] diffusion training Loss: 0.06695627328008413
2024-11-05 00:45:33,175 - INFO - [diffusion][Epoch 8292] diffusion learning rate: 0.001
2024-11-05 00:45:33,177 - INFO - [diffusion][Epoch 8292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:33,178 - INFO - [diffusion][Epoch 8293] Epoch 8294/12000
2024-11-05 00:45:37,439 - INFO - [diffusion][Epoch 8293] diffusion training Loss: 0.06473527662456036
2024-11-05 00:45:37,441 - INFO - [diffusion][Epoch 8293] diffusion learning rate: 0.001
2024-11-05 00:45:37,443 - INFO - [diffusion][Epoch 8293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:37,444 - INFO - [diffusion][Epoch 8294] Epoch 8295/12000
2024-11-05 00:45:41,490 - INFO - [diffusion][Epoch 8294] diffusion training Loss: 0.06980323605239391
2024-11-05 00:45:41,493 - INFO - [diffusion][Epoch 8294] diffusion learning rate: 0.001
2024-11-05 00:45:41,494 - INFO - [diffusion][Epoch 8294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:41,496 - INFO - [diffusion][Epoch 8295] Epoch 8296/12000
2024-11-05 00:45:45,592 - INFO - [diffusion][Epoch 8295] diffusion training Loss: 0.07192347943782806
2024-11-05 00:45:45,595 - INFO - [diffusion][Epoch 8295] diffusion learning rate: 0.001
2024-11-05 00:45:45,596 - INFO - [diffusion][Epoch 8295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:45,598 - INFO - [diffusion][Epoch 8296] Epoch 8297/12000
2024-11-05 00:45:49,813 - INFO - [diffusion][Epoch 8296] diffusion training Loss: 0.06336802337318659
2024-11-05 00:45:49,816 - INFO - [diffusion][Epoch 8296] diffusion learning rate: 0.001
2024-11-05 00:45:49,817 - INFO - [diffusion][Epoch 8296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:49,819 - INFO - [diffusion][Epoch 8297] Epoch 8298/12000
2024-11-05 00:45:54,179 - INFO - [diffusion][Epoch 8297] diffusion training Loss: 0.0689790640026331
2024-11-05 00:45:54,182 - INFO - [diffusion][Epoch 8297] diffusion learning rate: 0.001
2024-11-05 00:45:54,184 - INFO - [diffusion][Epoch 8297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:54,185 - INFO - [diffusion][Epoch 8298] Epoch 8299/12000
2024-11-05 00:45:58,406 - INFO - [diffusion][Epoch 8298] diffusion training Loss: 0.06456712819635868
2024-11-05 00:45:58,409 - INFO - [diffusion][Epoch 8298] diffusion learning rate: 0.001
2024-11-05 00:45:58,413 - INFO - [diffusion][Epoch 8298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:58,415 - INFO - [diffusion][Epoch 8299] Epoch 8300/12000
2024-11-05 00:46:02,729 - INFO - [diffusion][Epoch 8299] diffusion training Loss: 0.06865007057785988
2024-11-05 00:46:02,732 - INFO - [diffusion][Epoch 8299] diffusion learning rate: 0.001
2024-11-05 00:46:02,733 - INFO - [diffusion][Epoch 8299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:02,735 - INFO - [diffusion][Epoch 8300] Epoch 8301/12000
2024-11-05 00:46:06,913 - INFO - [diffusion][Epoch 8300] diffusion training Loss: 0.0691080205142498
2024-11-05 00:46:06,915 - INFO - [diffusion][Epoch 8300] diffusion learning rate: 0.001
2024-11-05 00:46:06,917 - INFO - [diffusion][Epoch 8300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:06,918 - INFO - [diffusion][Epoch 8301] Epoch 8302/12000
2024-11-05 00:46:10,994 - INFO - [diffusion][Epoch 8301] diffusion training Loss: 0.06266663316637278
2024-11-05 00:46:10,997 - INFO - [diffusion][Epoch 8301] diffusion learning rate: 0.001
2024-11-05 00:46:10,998 - INFO - [diffusion][Epoch 8301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:11,000 - INFO - [diffusion][Epoch 8302] Epoch 8303/12000
2024-11-05 00:46:15,243 - INFO - [diffusion][Epoch 8302] diffusion training Loss: 0.06575925461947918
2024-11-05 00:46:15,245 - INFO - [diffusion][Epoch 8302] diffusion learning rate: 0.001
2024-11-05 00:46:15,246 - INFO - [diffusion][Epoch 8302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:15,248 - INFO - [diffusion][Epoch 8303] Epoch 8304/12000
2024-11-05 00:46:19,488 - INFO - [diffusion][Epoch 8303] diffusion training Loss: 0.06581830326467752
2024-11-05 00:46:19,490 - INFO - [diffusion][Epoch 8303] diffusion learning rate: 0.001
2024-11-05 00:46:19,492 - INFO - [diffusion][Epoch 8303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:19,493 - INFO - [diffusion][Epoch 8304] Epoch 8305/12000
2024-11-05 00:46:23,648 - INFO - [diffusion][Epoch 8304] diffusion training Loss: 0.07234676741063595
2024-11-05 00:46:23,650 - INFO - [diffusion][Epoch 8304] diffusion learning rate: 0.001
2024-11-05 00:46:23,652 - INFO - [diffusion][Epoch 8304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:23,653 - INFO - [diffusion][Epoch 8305] Epoch 8306/12000
2024-11-05 00:46:27,953 - INFO - [diffusion][Epoch 8305] diffusion training Loss: 0.0647381516173482
2024-11-05 00:46:27,955 - INFO - [diffusion][Epoch 8305] diffusion learning rate: 0.001
2024-11-05 00:46:27,957 - INFO - [diffusion][Epoch 8305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:27,958 - INFO - [diffusion][Epoch 8306] Epoch 8307/12000
2024-11-05 00:46:31,998 - INFO - [diffusion][Epoch 8306] diffusion training Loss: 0.06978531554341316
2024-11-05 00:46:32,000 - INFO - [diffusion][Epoch 8306] diffusion learning rate: 0.001
2024-11-05 00:46:32,002 - INFO - [diffusion][Epoch 8306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:32,004 - INFO - [diffusion][Epoch 8307] Epoch 8308/12000
2024-11-05 00:46:36,223 - INFO - [diffusion][Epoch 8307] diffusion training Loss: 0.0666673481464386
2024-11-05 00:46:36,225 - INFO - [diffusion][Epoch 8307] diffusion learning rate: 0.001
2024-11-05 00:46:36,227 - INFO - [diffusion][Epoch 8307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:36,228 - INFO - [diffusion][Epoch 8308] Epoch 8309/12000
2024-11-05 00:46:40,328 - INFO - [diffusion][Epoch 8308] diffusion training Loss: 0.06762392725795507
2024-11-05 00:46:40,330 - INFO - [diffusion][Epoch 8308] diffusion learning rate: 0.001
2024-11-05 00:46:40,332 - INFO - [diffusion][Epoch 8308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:40,333 - INFO - [diffusion][Epoch 8309] Epoch 8310/12000
2024-11-05 00:46:44,571 - INFO - [diffusion][Epoch 8309] diffusion training Loss: 0.06817224994301796
2024-11-05 00:46:44,573 - INFO - [diffusion][Epoch 8309] diffusion learning rate: 0.001
2024-11-05 00:46:44,575 - INFO - [diffusion][Epoch 8309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:44,576 - INFO - [diffusion][Epoch 8310] Epoch 8311/12000
2024-11-05 00:46:49,373 - INFO - [diffusion][Epoch 8310] diffusion training Loss: 0.06743616424500942
2024-11-05 00:46:49,375 - INFO - [diffusion][Epoch 8310] diffusion learning rate: 0.001
2024-11-05 00:46:49,377 - INFO - [diffusion][Epoch 8310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:49,378 - INFO - [diffusion][Epoch 8311] Epoch 8312/12000
2024-11-05 00:46:53,641 - INFO - [diffusion][Epoch 8311] diffusion training Loss: 0.06544907577335835
2024-11-05 00:46:53,643 - INFO - [diffusion][Epoch 8311] diffusion learning rate: 0.001
2024-11-05 00:46:53,645 - INFO - [diffusion][Epoch 8311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:53,646 - INFO - [diffusion][Epoch 8312] Epoch 8313/12000
2024-11-05 00:46:57,717 - INFO - [diffusion][Epoch 8312] diffusion training Loss: 0.06665468029677868
2024-11-05 00:46:57,720 - INFO - [diffusion][Epoch 8312] diffusion learning rate: 0.001
2024-11-05 00:46:57,721 - INFO - [diffusion][Epoch 8312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:57,723 - INFO - [diffusion][Epoch 8313] Epoch 8314/12000
2024-11-05 00:47:02,015 - INFO - [diffusion][Epoch 8313] diffusion training Loss: 0.06731109693646431
2024-11-05 00:47:02,017 - INFO - [diffusion][Epoch 8313] diffusion learning rate: 0.001
2024-11-05 00:47:02,019 - INFO - [diffusion][Epoch 8313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:02,021 - INFO - [diffusion][Epoch 8314] Epoch 8315/12000
2024-11-05 00:47:06,214 - INFO - [diffusion][Epoch 8314] diffusion training Loss: 0.073390893638134
2024-11-05 00:47:06,216 - INFO - [diffusion][Epoch 8314] diffusion learning rate: 0.001
2024-11-05 00:47:06,218 - INFO - [diffusion][Epoch 8314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:06,219 - INFO - [diffusion][Epoch 8315] Epoch 8316/12000
2024-11-05 00:47:10,414 - INFO - [diffusion][Epoch 8315] diffusion training Loss: 0.06967019103467464
2024-11-05 00:47:10,416 - INFO - [diffusion][Epoch 8315] diffusion learning rate: 0.001
2024-11-05 00:47:10,418 - INFO - [diffusion][Epoch 8315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:10,420 - INFO - [diffusion][Epoch 8316] Epoch 8317/12000
2024-11-05 00:47:14,634 - INFO - [diffusion][Epoch 8316] diffusion training Loss: 0.067114875651896
2024-11-05 00:47:14,636 - INFO - [diffusion][Epoch 8316] diffusion learning rate: 0.001
2024-11-05 00:47:14,639 - INFO - [diffusion][Epoch 8316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:14,640 - INFO - [diffusion][Epoch 8317] Epoch 8318/12000
2024-11-05 00:47:18,860 - INFO - [diffusion][Epoch 8317] diffusion training Loss: 0.065556975081563
2024-11-05 00:47:18,863 - INFO - [diffusion][Epoch 8317] diffusion learning rate: 0.001
2024-11-05 00:47:18,865 - INFO - [diffusion][Epoch 8317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:18,866 - INFO - [diffusion][Epoch 8318] Epoch 8319/12000
2024-11-05 00:47:23,097 - INFO - [diffusion][Epoch 8318] diffusion training Loss: 0.0669455137103796
2024-11-05 00:47:23,099 - INFO - [diffusion][Epoch 8318] diffusion learning rate: 0.001
2024-11-05 00:47:23,101 - INFO - [diffusion][Epoch 8318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:23,102 - INFO - [diffusion][Epoch 8319] Epoch 8320/12000
2024-11-05 00:47:27,116 - INFO - [diffusion][Epoch 8319] diffusion training Loss: 0.06502438150346279
2024-11-05 00:47:27,118 - INFO - [diffusion][Epoch 8319] diffusion learning rate: 0.001
2024-11-05 00:47:27,120 - INFO - [diffusion][Epoch 8319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:27,121 - INFO - [diffusion][Epoch 8320] Epoch 8321/12000
2024-11-05 00:47:31,114 - INFO - [diffusion][Epoch 8320] diffusion training Loss: 0.0702835489064455
2024-11-05 00:47:31,116 - INFO - [diffusion][Epoch 8320] diffusion learning rate: 0.001
2024-11-05 00:47:31,119 - INFO - [diffusion][Epoch 8320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:31,120 - INFO - [diffusion][Epoch 8321] Epoch 8322/12000
2024-11-05 00:47:35,279 - INFO - [diffusion][Epoch 8321] diffusion training Loss: 0.06726817414164543
2024-11-05 00:47:35,282 - INFO - [diffusion][Epoch 8321] diffusion learning rate: 0.001
2024-11-05 00:47:35,340 - INFO - [diffusion][Epoch 8321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:35,341 - INFO - [diffusion][Epoch 8322] Epoch 8323/12000
2024-11-05 00:47:39,529 - INFO - [diffusion][Epoch 8322] diffusion training Loss: 0.06820168159902096
2024-11-05 00:47:39,531 - INFO - [diffusion][Epoch 8322] diffusion learning rate: 0.001
2024-11-05 00:47:39,533 - INFO - [diffusion][Epoch 8322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:39,535 - INFO - [diffusion][Epoch 8323] Epoch 8324/12000
2024-11-05 00:47:43,760 - INFO - [diffusion][Epoch 8323] diffusion training Loss: 0.065863654948771
2024-11-05 00:47:43,763 - INFO - [diffusion][Epoch 8323] diffusion learning rate: 0.001
2024-11-05 00:47:43,765 - INFO - [diffusion][Epoch 8323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:43,766 - INFO - [diffusion][Epoch 8324] Epoch 8325/12000
2024-11-05 00:47:47,996 - INFO - [diffusion][Epoch 8324] diffusion training Loss: 0.06818867288529873
2024-11-05 00:47:47,999 - INFO - [diffusion][Epoch 8324] diffusion learning rate: 0.001
2024-11-05 00:47:48,001 - INFO - [diffusion][Epoch 8324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:48,003 - INFO - [diffusion][Epoch 8325] Epoch 8326/12000
2024-11-05 00:47:52,312 - INFO - [diffusion][Epoch 8325] diffusion training Loss: 0.06483218260109425
2024-11-05 00:47:52,314 - INFO - [diffusion][Epoch 8325] diffusion learning rate: 0.001
2024-11-05 00:47:52,316 - INFO - [diffusion][Epoch 8325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:52,318 - INFO - [diffusion][Epoch 8326] Epoch 8327/12000
2024-11-05 00:47:56,596 - INFO - [diffusion][Epoch 8326] diffusion training Loss: 0.0744827426970005
2024-11-05 00:47:56,598 - INFO - [diffusion][Epoch 8326] diffusion learning rate: 0.001
2024-11-05 00:47:56,599 - INFO - [diffusion][Epoch 8326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:56,601 - INFO - [diffusion][Epoch 8327] Epoch 8328/12000
2024-11-05 00:48:00,715 - INFO - [diffusion][Epoch 8327] diffusion training Loss: 0.06818908452987671
2024-11-05 00:48:00,718 - INFO - [diffusion][Epoch 8327] diffusion learning rate: 0.001
2024-11-05 00:48:00,720 - INFO - [diffusion][Epoch 8327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:00,721 - INFO - [diffusion][Epoch 8328] Epoch 8329/12000
2024-11-05 00:48:04,923 - INFO - [diffusion][Epoch 8328] diffusion training Loss: 0.0669593345373869
2024-11-05 00:48:04,925 - INFO - [diffusion][Epoch 8328] diffusion learning rate: 0.001
2024-11-05 00:48:04,927 - INFO - [diffusion][Epoch 8328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:04,929 - INFO - [diffusion][Epoch 8329] Epoch 8330/12000
2024-11-05 00:48:09,019 - INFO - [diffusion][Epoch 8329] diffusion training Loss: 0.07031990215182304
2024-11-05 00:48:09,021 - INFO - [diffusion][Epoch 8329] diffusion learning rate: 0.001
2024-11-05 00:48:09,062 - INFO - [diffusion][Epoch 8329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:09,064 - INFO - [diffusion][Epoch 8330] Epoch 8331/12000
2024-11-05 00:48:13,532 - INFO - [diffusion][Epoch 8330] diffusion training Loss: 0.07188557460904121
2024-11-05 00:48:13,535 - INFO - [diffusion][Epoch 8330] diffusion learning rate: 0.001
2024-11-05 00:48:13,537 - INFO - [diffusion][Epoch 8330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:13,538 - INFO - [diffusion][Epoch 8331] Epoch 8332/12000
2024-11-05 00:48:17,758 - INFO - [diffusion][Epoch 8331] diffusion training Loss: 0.0664275623857975
2024-11-05 00:48:17,760 - INFO - [diffusion][Epoch 8331] diffusion learning rate: 0.001
2024-11-05 00:48:17,762 - INFO - [diffusion][Epoch 8331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:17,763 - INFO - [diffusion][Epoch 8332] Epoch 8333/12000
2024-11-05 00:48:21,883 - INFO - [diffusion][Epoch 8332] diffusion training Loss: 0.06340951658785343
2024-11-05 00:48:21,885 - INFO - [diffusion][Epoch 8332] diffusion learning rate: 0.001
2024-11-05 00:48:21,886 - INFO - [diffusion][Epoch 8332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:21,888 - INFO - [diffusion][Epoch 8333] Epoch 8334/12000
2024-11-05 00:48:26,029 - INFO - [diffusion][Epoch 8333] diffusion training Loss: 0.06346566043794155
2024-11-05 00:48:26,031 - INFO - [diffusion][Epoch 8333] diffusion learning rate: 0.001
2024-11-05 00:48:26,033 - INFO - [diffusion][Epoch 8333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:26,034 - INFO - [diffusion][Epoch 8334] Epoch 8335/12000
2024-11-05 00:48:30,204 - INFO - [diffusion][Epoch 8334] diffusion training Loss: 0.06736453995108604
2024-11-05 00:48:30,206 - INFO - [diffusion][Epoch 8334] diffusion learning rate: 0.001
2024-11-05 00:48:30,208 - INFO - [diffusion][Epoch 8334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:30,210 - INFO - [diffusion][Epoch 8335] Epoch 8336/12000
2024-11-05 00:48:34,395 - INFO - [diffusion][Epoch 8335] diffusion training Loss: 0.06498623453080654
2024-11-05 00:48:34,397 - INFO - [diffusion][Epoch 8335] diffusion learning rate: 0.001
2024-11-05 00:48:34,398 - INFO - [diffusion][Epoch 8335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:34,400 - INFO - [diffusion][Epoch 8336] Epoch 8337/12000
2024-11-05 00:48:38,598 - INFO - [diffusion][Epoch 8336] diffusion training Loss: 0.06481554079800844
2024-11-05 00:48:38,600 - INFO - [diffusion][Epoch 8336] diffusion learning rate: 0.001
2024-11-05 00:48:38,602 - INFO - [diffusion][Epoch 8336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:38,603 - INFO - [diffusion][Epoch 8337] Epoch 8338/12000
2024-11-05 00:48:42,589 - INFO - [diffusion][Epoch 8337] diffusion training Loss: 0.06304431427270174
2024-11-05 00:48:42,590 - INFO - [diffusion][Epoch 8337] diffusion learning rate: 0.001
2024-11-05 00:48:42,592 - INFO - [diffusion][Epoch 8337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:42,594 - INFO - [diffusion][Epoch 8338] Epoch 8339/12000
2024-11-05 00:48:46,526 - INFO - [diffusion][Epoch 8338] diffusion training Loss: 0.06924264878034592
2024-11-05 00:48:46,528 - INFO - [diffusion][Epoch 8338] diffusion learning rate: 0.001
2024-11-05 00:48:46,530 - INFO - [diffusion][Epoch 8338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:46,531 - INFO - [diffusion][Epoch 8339] Epoch 8340/12000
2024-11-05 00:48:50,681 - INFO - [diffusion][Epoch 8339] diffusion training Loss: 0.06590519566088915
2024-11-05 00:48:50,683 - INFO - [diffusion][Epoch 8339] diffusion learning rate: 0.001
2024-11-05 00:48:50,685 - INFO - [diffusion][Epoch 8339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:50,687 - INFO - [diffusion][Epoch 8340] Epoch 8341/12000
2024-11-05 00:48:54,851 - INFO - [diffusion][Epoch 8340] diffusion training Loss: 0.06978782080113888
2024-11-05 00:48:54,853 - INFO - [diffusion][Epoch 8340] diffusion learning rate: 0.001
2024-11-05 00:48:54,855 - INFO - [diffusion][Epoch 8340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:54,856 - INFO - [diffusion][Epoch 8341] Epoch 8342/12000
2024-11-05 00:48:58,948 - INFO - [diffusion][Epoch 8341] diffusion training Loss: 0.06461803521960974
2024-11-05 00:48:58,950 - INFO - [diffusion][Epoch 8341] diffusion learning rate: 0.001
2024-11-05 00:48:58,952 - INFO - [diffusion][Epoch 8341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:58,953 - INFO - [diffusion][Epoch 8342] Epoch 8343/12000
2024-11-05 00:49:03,235 - INFO - [diffusion][Epoch 8342] diffusion training Loss: 0.06630199402570724
2024-11-05 00:49:03,281 - INFO - [diffusion][Epoch 8342] diffusion learning rate: 0.001
2024-11-05 00:49:03,283 - INFO - [diffusion][Epoch 8342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:03,284 - INFO - [diffusion][Epoch 8343] Epoch 8344/12000
2024-11-05 00:49:07,275 - INFO - [diffusion][Epoch 8343] diffusion training Loss: 0.07054131291806698
2024-11-05 00:49:07,277 - INFO - [diffusion][Epoch 8343] diffusion learning rate: 0.001
2024-11-05 00:49:07,279 - INFO - [diffusion][Epoch 8343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:07,280 - INFO - [diffusion][Epoch 8344] Epoch 8345/12000
2024-11-05 00:49:11,538 - INFO - [diffusion][Epoch 8344] diffusion training Loss: 0.06608734931796789
2024-11-05 00:49:11,541 - INFO - [diffusion][Epoch 8344] diffusion learning rate: 0.001
2024-11-05 00:49:11,543 - INFO - [diffusion][Epoch 8344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:11,544 - INFO - [diffusion][Epoch 8345] Epoch 8346/12000
2024-11-05 00:49:15,456 - INFO - [diffusion][Epoch 8345] diffusion training Loss: 0.06894930265843868
2024-11-05 00:49:15,458 - INFO - [diffusion][Epoch 8345] diffusion learning rate: 0.001
2024-11-05 00:49:15,460 - INFO - [diffusion][Epoch 8345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:15,461 - INFO - [diffusion][Epoch 8346] Epoch 8347/12000
2024-11-05 00:49:19,469 - INFO - [diffusion][Epoch 8346] diffusion training Loss: 0.07106747105717659
2024-11-05 00:49:19,471 - INFO - [diffusion][Epoch 8346] diffusion learning rate: 0.001
2024-11-05 00:49:19,473 - INFO - [diffusion][Epoch 8346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:19,474 - INFO - [diffusion][Epoch 8347] Epoch 8348/12000
2024-11-05 00:49:23,597 - INFO - [diffusion][Epoch 8347] diffusion training Loss: 0.07198364473879337
2024-11-05 00:49:23,599 - INFO - [diffusion][Epoch 8347] diffusion learning rate: 0.001
2024-11-05 00:49:23,602 - INFO - [diffusion][Epoch 8347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:23,604 - INFO - [diffusion][Epoch 8348] Epoch 8349/12000
2024-11-05 00:49:27,796 - INFO - [diffusion][Epoch 8348] diffusion training Loss: 0.06526327040046453
2024-11-05 00:49:27,797 - INFO - [diffusion][Epoch 8348] diffusion learning rate: 0.001
2024-11-05 00:49:27,799 - INFO - [diffusion][Epoch 8348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:27,801 - INFO - [diffusion][Epoch 8349] Epoch 8350/12000
2024-11-05 00:49:31,902 - INFO - [diffusion][Epoch 8349] diffusion training Loss: 0.06835820898413658
2024-11-05 00:49:31,904 - INFO - [diffusion][Epoch 8349] diffusion learning rate: 0.001
2024-11-05 00:49:31,906 - INFO - [diffusion][Epoch 8349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:31,907 - INFO - [diffusion][Epoch 8350] Epoch 8351/12000
2024-11-05 00:49:36,526 - INFO - [diffusion][Epoch 8350] diffusion training Loss: 0.06713856011629105
2024-11-05 00:49:36,528 - INFO - [diffusion][Epoch 8350] diffusion learning rate: 0.001
2024-11-05 00:49:36,530 - INFO - [diffusion][Epoch 8350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:36,531 - INFO - [diffusion][Epoch 8351] Epoch 8352/12000
2024-11-05 00:49:40,518 - INFO - [diffusion][Epoch 8351] diffusion training Loss: 0.06512972712516785
2024-11-05 00:49:40,520 - INFO - [diffusion][Epoch 8351] diffusion learning rate: 0.001
2024-11-05 00:49:40,553 - INFO - [diffusion][Epoch 8351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:40,555 - INFO - [diffusion][Epoch 8352] Epoch 8353/12000
2024-11-05 00:49:44,690 - INFO - [diffusion][Epoch 8352] diffusion training Loss: 0.06973027437925339
2024-11-05 00:49:44,692 - INFO - [diffusion][Epoch 8352] diffusion learning rate: 0.001
2024-11-05 00:49:44,694 - INFO - [diffusion][Epoch 8352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:44,695 - INFO - [diffusion][Epoch 8353] Epoch 8354/12000
2024-11-05 00:49:48,944 - INFO - [diffusion][Epoch 8353] diffusion training Loss: 0.06913569755852222
2024-11-05 00:49:48,947 - INFO - [diffusion][Epoch 8353] diffusion learning rate: 0.001
2024-11-05 00:49:48,949 - INFO - [diffusion][Epoch 8353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:48,950 - INFO - [diffusion][Epoch 8354] Epoch 8355/12000
2024-11-05 00:49:53,299 - INFO - [diffusion][Epoch 8354] diffusion training Loss: 0.06857281271368265
2024-11-05 00:49:53,301 - INFO - [diffusion][Epoch 8354] diffusion learning rate: 0.001
2024-11-05 00:49:53,303 - INFO - [diffusion][Epoch 8354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:53,305 - INFO - [diffusion][Epoch 8355] Epoch 8356/12000
2024-11-05 00:49:57,596 - INFO - [diffusion][Epoch 8355] diffusion training Loss: 0.06532631814479828
2024-11-05 00:49:57,598 - INFO - [diffusion][Epoch 8355] diffusion learning rate: 0.001
2024-11-05 00:49:57,600 - INFO - [diffusion][Epoch 8355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:57,601 - INFO - [diffusion][Epoch 8356] Epoch 8357/12000
2024-11-05 00:50:01,755 - INFO - [diffusion][Epoch 8356] diffusion training Loss: 0.061632126569747925
2024-11-05 00:50:01,760 - INFO - [diffusion][Epoch 8356] diffusion learning rate: 0.001
2024-11-05 00:50:01,762 - INFO - [diffusion][Epoch 8356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:01,764 - INFO - [diffusion][Epoch 8357] Epoch 8358/12000
2024-11-05 00:50:05,847 - INFO - [diffusion][Epoch 8357] diffusion training Loss: 0.06446847598999739
2024-11-05 00:50:05,849 - INFO - [diffusion][Epoch 8357] diffusion learning rate: 0.001
2024-11-05 00:50:05,851 - INFO - [diffusion][Epoch 8357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:05,852 - INFO - [diffusion][Epoch 8358] Epoch 8359/12000
2024-11-05 00:50:10,049 - INFO - [diffusion][Epoch 8358] diffusion training Loss: 0.06448126211762428
2024-11-05 00:50:10,051 - INFO - [diffusion][Epoch 8358] diffusion learning rate: 0.001
2024-11-05 00:50:10,052 - INFO - [diffusion][Epoch 8358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:10,053 - INFO - [diffusion][Epoch 8359] Epoch 8360/12000
2024-11-05 00:50:14,124 - INFO - [diffusion][Epoch 8359] diffusion training Loss: 0.06832285784184933
2024-11-05 00:50:14,126 - INFO - [diffusion][Epoch 8359] diffusion learning rate: 0.001
2024-11-05 00:50:14,128 - INFO - [diffusion][Epoch 8359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:14,130 - INFO - [diffusion][Epoch 8360] Epoch 8361/12000
2024-11-05 00:50:18,105 - INFO - [diffusion][Epoch 8360] diffusion training Loss: 0.06704321131110191
2024-11-05 00:50:18,107 - INFO - [diffusion][Epoch 8360] diffusion learning rate: 0.001
2024-11-05 00:50:18,109 - INFO - [diffusion][Epoch 8360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:18,110 - INFO - [diffusion][Epoch 8361] Epoch 8362/12000
2024-11-05 00:50:22,086 - INFO - [diffusion][Epoch 8361] diffusion training Loss: 0.07033660635352135
2024-11-05 00:50:22,088 - INFO - [diffusion][Epoch 8361] diffusion learning rate: 0.001
2024-11-05 00:50:22,090 - INFO - [diffusion][Epoch 8361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:22,091 - INFO - [diffusion][Epoch 8362] Epoch 8363/12000
2024-11-05 00:50:26,177 - INFO - [diffusion][Epoch 8362] diffusion training Loss: 0.06706840731203556
2024-11-05 00:50:26,179 - INFO - [diffusion][Epoch 8362] diffusion learning rate: 0.001
2024-11-05 00:50:26,181 - INFO - [diffusion][Epoch 8362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:26,183 - INFO - [diffusion][Epoch 8363] Epoch 8364/12000
2024-11-05 00:50:30,320 - INFO - [diffusion][Epoch 8363] diffusion training Loss: 0.0662747360765934
2024-11-05 00:50:30,322 - INFO - [diffusion][Epoch 8363] diffusion learning rate: 0.001
2024-11-05 00:50:30,324 - INFO - [diffusion][Epoch 8363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:30,325 - INFO - [diffusion][Epoch 8364] Epoch 8365/12000
2024-11-05 00:50:34,437 - INFO - [diffusion][Epoch 8364] diffusion training Loss: 0.06779836863279343
2024-11-05 00:50:34,439 - INFO - [diffusion][Epoch 8364] diffusion learning rate: 0.001
2024-11-05 00:50:34,441 - INFO - [diffusion][Epoch 8364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:34,442 - INFO - [diffusion][Epoch 8365] Epoch 8366/12000
2024-11-05 00:50:38,691 - INFO - [diffusion][Epoch 8365] diffusion training Loss: 0.06733859330415726
2024-11-05 00:50:38,694 - INFO - [diffusion][Epoch 8365] diffusion learning rate: 0.001
2024-11-05 00:50:38,696 - INFO - [diffusion][Epoch 8365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:38,697 - INFO - [diffusion][Epoch 8366] Epoch 8367/12000
2024-11-05 00:50:42,725 - INFO - [diffusion][Epoch 8366] diffusion training Loss: 0.06310317851603031
2024-11-05 00:50:42,727 - INFO - [diffusion][Epoch 8366] diffusion learning rate: 0.001
2024-11-05 00:50:42,729 - INFO - [diffusion][Epoch 8366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:42,730 - INFO - [diffusion][Epoch 8367] Epoch 8368/12000
2024-11-05 00:50:46,969 - INFO - [diffusion][Epoch 8367] diffusion training Loss: 0.06516041979193687
2024-11-05 00:50:46,971 - INFO - [diffusion][Epoch 8367] diffusion learning rate: 0.001
2024-11-05 00:50:46,973 - INFO - [diffusion][Epoch 8367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:46,975 - INFO - [diffusion][Epoch 8368] Epoch 8369/12000
2024-11-05 00:50:51,182 - INFO - [diffusion][Epoch 8368] diffusion training Loss: 0.06282247696071863
2024-11-05 00:50:51,183 - INFO - [diffusion][Epoch 8368] diffusion learning rate: 0.001
2024-11-05 00:50:51,185 - INFO - [diffusion][Epoch 8368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:51,187 - INFO - [diffusion][Epoch 8369] Epoch 8370/12000
2024-11-05 00:50:55,322 - INFO - [diffusion][Epoch 8369] diffusion training Loss: 0.06768722180277109
2024-11-05 00:50:55,325 - INFO - [diffusion][Epoch 8369] diffusion learning rate: 0.001
2024-11-05 00:50:55,327 - INFO - [diffusion][Epoch 8369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:55,329 - INFO - [diffusion][Epoch 8370] Epoch 8371/12000
2024-11-05 00:50:59,718 - INFO - [diffusion][Epoch 8370] diffusion training Loss: 0.06723511684685946
2024-11-05 00:50:59,720 - INFO - [diffusion][Epoch 8370] diffusion learning rate: 0.001
2024-11-05 00:50:59,722 - INFO - [diffusion][Epoch 8370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:59,723 - INFO - [diffusion][Epoch 8371] Epoch 8372/12000
2024-11-05 00:51:03,917 - INFO - [diffusion][Epoch 8371] diffusion training Loss: 0.06471261940896511
2024-11-05 00:51:03,919 - INFO - [diffusion][Epoch 8371] diffusion learning rate: 0.001
2024-11-05 00:51:03,921 - INFO - [diffusion][Epoch 8371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:03,922 - INFO - [diffusion][Epoch 8372] Epoch 8373/12000
2024-11-05 00:51:08,036 - INFO - [diffusion][Epoch 8372] diffusion training Loss: 0.07115066796541214
2024-11-05 00:51:08,040 - INFO - [diffusion][Epoch 8372] diffusion learning rate: 0.001
2024-11-05 00:51:08,042 - INFO - [diffusion][Epoch 8372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:08,044 - INFO - [diffusion][Epoch 8373] Epoch 8374/12000
2024-11-05 00:51:12,091 - INFO - [diffusion][Epoch 8373] diffusion training Loss: 0.06483260169625282
2024-11-05 00:51:12,093 - INFO - [diffusion][Epoch 8373] diffusion learning rate: 0.001
2024-11-05 00:51:12,095 - INFO - [diffusion][Epoch 8373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:12,096 - INFO - [diffusion][Epoch 8374] Epoch 8375/12000
2024-11-05 00:51:16,206 - INFO - [diffusion][Epoch 8374] diffusion training Loss: 0.06796001456677914
2024-11-05 00:51:16,208 - INFO - [diffusion][Epoch 8374] diffusion learning rate: 0.001
2024-11-05 00:51:16,210 - INFO - [diffusion][Epoch 8374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:16,211 - INFO - [diffusion][Epoch 8375] Epoch 8376/12000
2024-11-05 00:51:20,464 - INFO - [diffusion][Epoch 8375] diffusion training Loss: 0.0666999826207757
2024-11-05 00:51:20,466 - INFO - [diffusion][Epoch 8375] diffusion learning rate: 0.001
2024-11-05 00:51:20,468 - INFO - [diffusion][Epoch 8375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:20,469 - INFO - [diffusion][Epoch 8376] Epoch 8377/12000
2024-11-05 00:51:24,675 - INFO - [diffusion][Epoch 8376] diffusion training Loss: 0.0640763808041811
2024-11-05 00:51:24,677 - INFO - [diffusion][Epoch 8376] diffusion learning rate: 0.001
2024-11-05 00:51:24,678 - INFO - [diffusion][Epoch 8376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:24,680 - INFO - [diffusion][Epoch 8377] Epoch 8378/12000
2024-11-05 00:51:28,817 - INFO - [diffusion][Epoch 8377] diffusion training Loss: 0.06850917637348175
2024-11-05 00:51:28,819 - INFO - [diffusion][Epoch 8377] diffusion learning rate: 0.001
2024-11-05 00:51:28,821 - INFO - [diffusion][Epoch 8377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:28,822 - INFO - [diffusion][Epoch 8378] Epoch 8379/12000
2024-11-05 00:51:32,970 - INFO - [diffusion][Epoch 8378] diffusion training Loss: 0.06997372768819332
2024-11-05 00:51:32,972 - INFO - [diffusion][Epoch 8378] diffusion learning rate: 0.001
2024-11-05 00:51:32,974 - INFO - [diffusion][Epoch 8378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:32,975 - INFO - [diffusion][Epoch 8379] Epoch 8380/12000
2024-11-05 00:51:37,286 - INFO - [diffusion][Epoch 8379] diffusion training Loss: 0.06865742802619934
2024-11-05 00:51:37,288 - INFO - [diffusion][Epoch 8379] diffusion learning rate: 0.001
2024-11-05 00:51:37,290 - INFO - [diffusion][Epoch 8379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:37,291 - INFO - [diffusion][Epoch 8380] Epoch 8381/12000
2024-11-05 00:51:41,535 - INFO - [diffusion][Epoch 8380] diffusion training Loss: 0.06925972364842892
2024-11-05 00:51:41,538 - INFO - [diffusion][Epoch 8380] diffusion learning rate: 0.001
2024-11-05 00:51:41,540 - INFO - [diffusion][Epoch 8380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:41,541 - INFO - [diffusion][Epoch 8381] Epoch 8382/12000
2024-11-05 00:51:45,659 - INFO - [diffusion][Epoch 8381] diffusion training Loss: 0.06689329352229834
2024-11-05 00:51:45,661 - INFO - [diffusion][Epoch 8381] diffusion learning rate: 0.001
2024-11-05 00:51:45,663 - INFO - [diffusion][Epoch 8381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:45,664 - INFO - [diffusion][Epoch 8382] Epoch 8383/12000
2024-11-05 00:51:49,557 - INFO - [diffusion][Epoch 8382] diffusion training Loss: 0.06770194321870804
2024-11-05 00:51:49,559 - INFO - [diffusion][Epoch 8382] diffusion learning rate: 0.001
2024-11-05 00:51:49,560 - INFO - [diffusion][Epoch 8382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:49,562 - INFO - [diffusion][Epoch 8383] Epoch 8384/12000
2024-11-05 00:51:53,686 - INFO - [diffusion][Epoch 8383] diffusion training Loss: 0.06617947481572628
2024-11-05 00:51:53,688 - INFO - [diffusion][Epoch 8383] diffusion learning rate: 0.001
2024-11-05 00:51:53,689 - INFO - [diffusion][Epoch 8383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:53,691 - INFO - [diffusion][Epoch 8384] Epoch 8385/12000
2024-11-05 00:51:57,835 - INFO - [diffusion][Epoch 8384] diffusion training Loss: 0.06718477979302406
2024-11-05 00:51:57,837 - INFO - [diffusion][Epoch 8384] diffusion learning rate: 0.001
2024-11-05 00:51:57,839 - INFO - [diffusion][Epoch 8384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:57,841 - INFO - [diffusion][Epoch 8385] Epoch 8386/12000
2024-11-05 00:52:01,885 - INFO - [diffusion][Epoch 8385] diffusion training Loss: 0.06889396347105503
2024-11-05 00:52:01,888 - INFO - [diffusion][Epoch 8385] diffusion learning rate: 0.001
2024-11-05 00:52:01,890 - INFO - [diffusion][Epoch 8385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:01,892 - INFO - [diffusion][Epoch 8386] Epoch 8387/12000
2024-11-05 00:52:05,986 - INFO - [diffusion][Epoch 8386] diffusion training Loss: 0.07062535360455513
2024-11-05 00:52:05,988 - INFO - [diffusion][Epoch 8386] diffusion learning rate: 0.001
2024-11-05 00:52:05,990 - INFO - [diffusion][Epoch 8386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:05,991 - INFO - [diffusion][Epoch 8387] Epoch 8388/12000
2024-11-05 00:52:10,103 - INFO - [diffusion][Epoch 8387] diffusion training Loss: 0.07078911364078522
2024-11-05 00:52:10,105 - INFO - [diffusion][Epoch 8387] diffusion learning rate: 0.001
2024-11-05 00:52:10,107 - INFO - [diffusion][Epoch 8387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:10,108 - INFO - [diffusion][Epoch 8388] Epoch 8389/12000
2024-11-05 00:52:14,258 - INFO - [diffusion][Epoch 8388] diffusion training Loss: 0.06360735557973385
2024-11-05 00:52:14,261 - INFO - [diffusion][Epoch 8388] diffusion learning rate: 0.001
2024-11-05 00:52:14,263 - INFO - [diffusion][Epoch 8388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:14,264 - INFO - [diffusion][Epoch 8389] Epoch 8390/12000
2024-11-05 00:52:18,316 - INFO - [diffusion][Epoch 8389] diffusion training Loss: 0.06691192463040352
2024-11-05 00:52:18,319 - INFO - [diffusion][Epoch 8389] diffusion learning rate: 0.001
2024-11-05 00:52:18,321 - INFO - [diffusion][Epoch 8389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:18,323 - INFO - [diffusion][Epoch 8390] Epoch 8391/12000
2024-11-05 00:52:22,320 - INFO - [diffusion][Epoch 8390] diffusion training Loss: 0.07239757757633924
2024-11-05 00:52:22,322 - INFO - [diffusion][Epoch 8390] diffusion learning rate: 0.001
2024-11-05 00:52:22,324 - INFO - [diffusion][Epoch 8390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:22,325 - INFO - [diffusion][Epoch 8391] Epoch 8392/12000
2024-11-05 00:52:27,008 - INFO - [diffusion][Epoch 8391] diffusion training Loss: 0.07203279435634613
2024-11-05 00:52:27,010 - INFO - [diffusion][Epoch 8391] diffusion learning rate: 0.001
2024-11-05 00:52:27,011 - INFO - [diffusion][Epoch 8391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:27,013 - INFO - [diffusion][Epoch 8392] Epoch 8393/12000
2024-11-05 00:52:31,118 - INFO - [diffusion][Epoch 8392] diffusion training Loss: 0.06969792116433382
2024-11-05 00:52:31,120 - INFO - [diffusion][Epoch 8392] diffusion learning rate: 0.001
2024-11-05 00:52:31,121 - INFO - [diffusion][Epoch 8392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:31,123 - INFO - [diffusion][Epoch 8393] Epoch 8394/12000
2024-11-05 00:52:35,309 - INFO - [diffusion][Epoch 8393] diffusion training Loss: 0.06550958566367626
2024-11-05 00:52:35,311 - INFO - [diffusion][Epoch 8393] diffusion learning rate: 0.001
2024-11-05 00:52:35,313 - INFO - [diffusion][Epoch 8393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:35,315 - INFO - [diffusion][Epoch 8394] Epoch 8395/12000
2024-11-05 00:52:39,344 - INFO - [diffusion][Epoch 8394] diffusion training Loss: 0.06630745716392994
2024-11-05 00:52:39,346 - INFO - [diffusion][Epoch 8394] diffusion learning rate: 0.001
2024-11-05 00:52:39,348 - INFO - [diffusion][Epoch 8394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:39,349 - INFO - [diffusion][Epoch 8395] Epoch 8396/12000
2024-11-05 00:52:43,366 - INFO - [diffusion][Epoch 8395] diffusion training Loss: 0.06874450109899044
2024-11-05 00:52:43,368 - INFO - [diffusion][Epoch 8395] diffusion learning rate: 0.001
2024-11-05 00:52:43,400 - INFO - [diffusion][Epoch 8395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:43,401 - INFO - [diffusion][Epoch 8396] Epoch 8397/12000
2024-11-05 00:52:47,558 - INFO - [diffusion][Epoch 8396] diffusion training Loss: 0.07150103524327278
2024-11-05 00:52:47,560 - INFO - [diffusion][Epoch 8396] diffusion learning rate: 0.001
2024-11-05 00:52:47,562 - INFO - [diffusion][Epoch 8396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:47,563 - INFO - [diffusion][Epoch 8397] Epoch 8398/12000
2024-11-05 00:52:51,747 - INFO - [diffusion][Epoch 8397] diffusion training Loss: 0.06360235065221786
2024-11-05 00:52:51,749 - INFO - [diffusion][Epoch 8397] diffusion learning rate: 0.001
2024-11-05 00:52:51,751 - INFO - [diffusion][Epoch 8397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:51,752 - INFO - [diffusion][Epoch 8398] Epoch 8399/12000
2024-11-05 00:52:55,707 - INFO - [diffusion][Epoch 8398] diffusion training Loss: 0.06860189884901047
2024-11-05 00:52:55,709 - INFO - [diffusion][Epoch 8398] diffusion learning rate: 0.001
2024-11-05 00:52:55,711 - INFO - [diffusion][Epoch 8398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:55,712 - INFO - [diffusion][Epoch 8399] Epoch 8400/12000
2024-11-05 00:52:59,926 - INFO - [diffusion][Epoch 8399] diffusion training Loss: 0.06580011360347271
2024-11-05 00:52:59,928 - INFO - [diffusion][Epoch 8399] diffusion learning rate: 0.001
2024-11-05 00:52:59,948 - INFO - [diffusion][Epoch 8399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:59,950 - INFO - [diffusion][Epoch 8400] Epoch 8401/12000
2024-11-05 00:53:04,207 - INFO - [diffusion][Epoch 8400] diffusion training Loss: 0.07041379623115063
2024-11-05 00:53:04,209 - INFO - [diffusion][Epoch 8400] diffusion learning rate: 0.001
2024-11-05 00:53:04,210 - INFO - [diffusion][Epoch 8400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:04,212 - INFO - [diffusion][Epoch 8401] Epoch 8402/12000
2024-11-05 00:53:08,438 - INFO - [diffusion][Epoch 8401] diffusion training Loss: 0.06797287426888943
2024-11-05 00:53:08,440 - INFO - [diffusion][Epoch 8401] diffusion learning rate: 0.001
2024-11-05 00:53:08,442 - INFO - [diffusion][Epoch 8401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:08,443 - INFO - [diffusion][Epoch 8402] Epoch 8403/12000
2024-11-05 00:53:12,712 - INFO - [diffusion][Epoch 8402] diffusion training Loss: 0.06342686153948307
2024-11-05 00:53:12,714 - INFO - [diffusion][Epoch 8402] diffusion learning rate: 0.001
2024-11-05 00:53:12,716 - INFO - [diffusion][Epoch 8402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:12,717 - INFO - [diffusion][Epoch 8403] Epoch 8404/12000
2024-11-05 00:53:16,943 - INFO - [diffusion][Epoch 8403] diffusion training Loss: 0.0641036843881011
2024-11-05 00:53:16,945 - INFO - [diffusion][Epoch 8403] diffusion learning rate: 0.001
2024-11-05 00:53:16,947 - INFO - [diffusion][Epoch 8403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:16,949 - INFO - [diffusion][Epoch 8404] Epoch 8405/12000
2024-11-05 00:53:21,073 - INFO - [diffusion][Epoch 8404] diffusion training Loss: 0.06890172697603703
2024-11-05 00:53:21,075 - INFO - [diffusion][Epoch 8404] diffusion learning rate: 0.001
2024-11-05 00:53:21,077 - INFO - [diffusion][Epoch 8404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:21,078 - INFO - [diffusion][Epoch 8405] Epoch 8406/12000
2024-11-05 00:53:25,153 - INFO - [diffusion][Epoch 8405] diffusion training Loss: 0.06542999297380447
2024-11-05 00:53:25,155 - INFO - [diffusion][Epoch 8405] diffusion learning rate: 0.001
2024-11-05 00:53:25,157 - INFO - [diffusion][Epoch 8405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:25,158 - INFO - [diffusion][Epoch 8406] Epoch 8407/12000
2024-11-05 00:53:29,394 - INFO - [diffusion][Epoch 8406] diffusion training Loss: 0.07004299759864807
2024-11-05 00:53:29,396 - INFO - [diffusion][Epoch 8406] diffusion learning rate: 0.001
2024-11-05 00:53:29,397 - INFO - [diffusion][Epoch 8406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:29,399 - INFO - [diffusion][Epoch 8407] Epoch 8408/12000
2024-11-05 00:53:33,599 - INFO - [diffusion][Epoch 8407] diffusion training Loss: 0.07038157247006893
2024-11-05 00:53:33,602 - INFO - [diffusion][Epoch 8407] diffusion learning rate: 0.001
2024-11-05 00:53:33,605 - INFO - [diffusion][Epoch 8407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:33,607 - INFO - [diffusion][Epoch 8408] Epoch 8409/12000
2024-11-05 00:53:37,726 - INFO - [diffusion][Epoch 8408] diffusion training Loss: 0.06795691698789597
2024-11-05 00:53:37,728 - INFO - [diffusion][Epoch 8408] diffusion learning rate: 0.001
2024-11-05 00:53:37,729 - INFO - [diffusion][Epoch 8408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:37,730 - INFO - [diffusion][Epoch 8409] Epoch 8410/12000
2024-11-05 00:53:41,872 - INFO - [diffusion][Epoch 8409] diffusion training Loss: 0.06634310446679592
2024-11-05 00:53:41,874 - INFO - [diffusion][Epoch 8409] diffusion learning rate: 0.001
2024-11-05 00:53:41,876 - INFO - [diffusion][Epoch 8409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:41,877 - INFO - [diffusion][Epoch 8410] Epoch 8411/12000
2024-11-05 00:53:46,044 - INFO - [diffusion][Epoch 8410] diffusion training Loss: 0.07011529430747032
2024-11-05 00:53:46,046 - INFO - [diffusion][Epoch 8410] diffusion learning rate: 0.001
2024-11-05 00:53:46,048 - INFO - [diffusion][Epoch 8410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:46,049 - INFO - [diffusion][Epoch 8411] Epoch 8412/12000
2024-11-05 00:53:50,777 - INFO - [diffusion][Epoch 8411] diffusion training Loss: 0.06585938110947609
2024-11-05 00:53:50,779 - INFO - [diffusion][Epoch 8411] diffusion learning rate: 0.001
2024-11-05 00:53:50,812 - INFO - [diffusion][Epoch 8411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:50,814 - INFO - [diffusion][Epoch 8412] Epoch 8413/12000
2024-11-05 00:53:54,882 - INFO - [diffusion][Epoch 8412] diffusion training Loss: 0.06796619947999716
2024-11-05 00:53:54,884 - INFO - [diffusion][Epoch 8412] diffusion learning rate: 0.001
2024-11-05 00:53:54,886 - INFO - [diffusion][Epoch 8412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:54,887 - INFO - [diffusion][Epoch 8413] Epoch 8414/12000
2024-11-05 00:53:58,989 - INFO - [diffusion][Epoch 8413] diffusion training Loss: 0.06658395193517208
2024-11-05 00:53:58,991 - INFO - [diffusion][Epoch 8413] diffusion learning rate: 0.001
2024-11-05 00:53:58,993 - INFO - [diffusion][Epoch 8413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:58,994 - INFO - [diffusion][Epoch 8414] Epoch 8415/12000
2024-11-05 00:54:03,243 - INFO - [diffusion][Epoch 8414] diffusion training Loss: 0.06629299186170101
2024-11-05 00:54:03,245 - INFO - [diffusion][Epoch 8414] diffusion learning rate: 0.001
2024-11-05 00:54:03,247 - INFO - [diffusion][Epoch 8414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:03,248 - INFO - [diffusion][Epoch 8415] Epoch 8416/12000
2024-11-05 00:54:07,504 - INFO - [diffusion][Epoch 8415] diffusion training Loss: 0.0721624381840229
2024-11-05 00:54:07,506 - INFO - [diffusion][Epoch 8415] diffusion learning rate: 0.001
2024-11-05 00:54:07,508 - INFO - [diffusion][Epoch 8415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:07,509 - INFO - [diffusion][Epoch 8416] Epoch 8417/12000
2024-11-05 00:54:11,535 - INFO - [diffusion][Epoch 8416] diffusion training Loss: 0.0589868426322937
2024-11-05 00:54:11,537 - INFO - [diffusion][Epoch 8416] diffusion learning rate: 0.001
2024-11-05 00:54:11,538 - INFO - [diffusion][Epoch 8416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:11,539 - INFO - [diffusion][Epoch 8417] Epoch 8418/12000
2024-11-05 00:54:15,606 - INFO - [diffusion][Epoch 8417] diffusion training Loss: 0.06900862883776426
2024-11-05 00:54:15,608 - INFO - [diffusion][Epoch 8417] diffusion learning rate: 0.001
2024-11-05 00:54:15,610 - INFO - [diffusion][Epoch 8417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:15,612 - INFO - [diffusion][Epoch 8418] Epoch 8419/12000
2024-11-05 00:54:19,698 - INFO - [diffusion][Epoch 8418] diffusion training Loss: 0.06781912595033646
2024-11-05 00:54:19,700 - INFO - [diffusion][Epoch 8418] diffusion learning rate: 0.001
2024-11-05 00:54:19,702 - INFO - [diffusion][Epoch 8418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:19,703 - INFO - [diffusion][Epoch 8419] Epoch 8420/12000
2024-11-05 00:54:24,001 - INFO - [diffusion][Epoch 8419] diffusion training Loss: 0.07265631854534149
2024-11-05 00:54:24,003 - INFO - [diffusion][Epoch 8419] diffusion learning rate: 0.001
2024-11-05 00:54:24,005 - INFO - [diffusion][Epoch 8419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:24,006 - INFO - [diffusion][Epoch 8420] Epoch 8421/12000
2024-11-05 00:54:28,154 - INFO - [diffusion][Epoch 8420] diffusion training Loss: 0.06627943087369204
2024-11-05 00:54:28,156 - INFO - [diffusion][Epoch 8420] diffusion learning rate: 0.001
2024-11-05 00:54:28,158 - INFO - [diffusion][Epoch 8420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:28,159 - INFO - [diffusion][Epoch 8421] Epoch 8422/12000
2024-11-05 00:54:32,332 - INFO - [diffusion][Epoch 8421] diffusion training Loss: 0.06671174988150597
2024-11-05 00:54:32,335 - INFO - [diffusion][Epoch 8421] diffusion learning rate: 0.001
2024-11-05 00:54:32,337 - INFO - [diffusion][Epoch 8421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:32,338 - INFO - [diffusion][Epoch 8422] Epoch 8423/12000
2024-11-05 00:54:36,592 - INFO - [diffusion][Epoch 8422] diffusion training Loss: 0.06988954171538353
2024-11-05 00:54:36,594 - INFO - [diffusion][Epoch 8422] diffusion learning rate: 0.001
2024-11-05 00:54:36,596 - INFO - [diffusion][Epoch 8422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:36,598 - INFO - [diffusion][Epoch 8423] Epoch 8424/12000
2024-11-05 00:54:40,845 - INFO - [diffusion][Epoch 8423] diffusion training Loss: 0.06517427135258913
2024-11-05 00:54:40,847 - INFO - [diffusion][Epoch 8423] diffusion learning rate: 0.001
2024-11-05 00:54:40,849 - INFO - [diffusion][Epoch 8423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:40,850 - INFO - [diffusion][Epoch 8424] Epoch 8425/12000
2024-11-05 00:54:44,924 - INFO - [diffusion][Epoch 8424] diffusion training Loss: 0.06730235926806927
2024-11-05 00:54:44,926 - INFO - [diffusion][Epoch 8424] diffusion learning rate: 0.001
2024-11-05 00:54:44,928 - INFO - [diffusion][Epoch 8424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:44,929 - INFO - [diffusion][Epoch 8425] Epoch 8426/12000
2024-11-05 00:54:48,968 - INFO - [diffusion][Epoch 8425] diffusion training Loss: 0.06858136411756277
2024-11-05 00:54:48,971 - INFO - [diffusion][Epoch 8425] diffusion learning rate: 0.001
2024-11-05 00:54:48,973 - INFO - [diffusion][Epoch 8425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:48,974 - INFO - [diffusion][Epoch 8426] Epoch 8427/12000
2024-11-05 00:54:53,143 - INFO - [diffusion][Epoch 8426] diffusion training Loss: 0.06951839290559292
2024-11-05 00:54:53,145 - INFO - [diffusion][Epoch 8426] diffusion learning rate: 0.001
2024-11-05 00:54:53,146 - INFO - [diffusion][Epoch 8426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:53,148 - INFO - [diffusion][Epoch 8427] Epoch 8428/12000
2024-11-05 00:54:57,389 - INFO - [diffusion][Epoch 8427] diffusion training Loss: 0.06806482095271349
2024-11-05 00:54:57,392 - INFO - [diffusion][Epoch 8427] diffusion learning rate: 0.001
2024-11-05 00:54:57,394 - INFO - [diffusion][Epoch 8427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:57,395 - INFO - [diffusion][Epoch 8428] Epoch 8429/12000
2024-11-05 00:55:01,466 - INFO - [diffusion][Epoch 8428] diffusion training Loss: 0.06681507080793381
2024-11-05 00:55:01,468 - INFO - [diffusion][Epoch 8428] diffusion learning rate: 0.001
2024-11-05 00:55:01,469 - INFO - [diffusion][Epoch 8428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:01,471 - INFO - [diffusion][Epoch 8429] Epoch 8430/12000
2024-11-05 00:55:05,490 - INFO - [diffusion][Epoch 8429] diffusion training Loss: 0.064877238124609
2024-11-05 00:55:05,492 - INFO - [diffusion][Epoch 8429] diffusion learning rate: 0.001
2024-11-05 00:55:05,493 - INFO - [diffusion][Epoch 8429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:05,495 - INFO - [diffusion][Epoch 8430] Epoch 8431/12000
2024-11-05 00:55:10,038 - INFO - [diffusion][Epoch 8430] diffusion training Loss: 0.06820935755968094
2024-11-05 00:55:10,041 - INFO - [diffusion][Epoch 8430] diffusion learning rate: 0.001
2024-11-05 00:55:10,043 - INFO - [diffusion][Epoch 8430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:10,044 - INFO - [diffusion][Epoch 8431] Epoch 8432/12000
2024-11-05 00:55:14,272 - INFO - [diffusion][Epoch 8431] diffusion training Loss: 0.06383759807795286
2024-11-05 00:55:14,275 - INFO - [diffusion][Epoch 8431] diffusion learning rate: 0.001
2024-11-05 00:55:14,276 - INFO - [diffusion][Epoch 8431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:14,278 - INFO - [diffusion][Epoch 8432] Epoch 8433/12000
2024-11-05 00:55:18,321 - INFO - [diffusion][Epoch 8432] diffusion training Loss: 0.06908970884978771
2024-11-05 00:55:18,323 - INFO - [diffusion][Epoch 8432] diffusion learning rate: 0.001
2024-11-05 00:55:18,325 - INFO - [diffusion][Epoch 8432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:18,326 - INFO - [diffusion][Epoch 8433] Epoch 8434/12000
2024-11-05 00:55:22,196 - INFO - [diffusion][Epoch 8433] diffusion training Loss: 0.06600281409919262
2024-11-05 00:55:22,198 - INFO - [diffusion][Epoch 8433] diffusion learning rate: 0.001
2024-11-05 00:55:22,200 - INFO - [diffusion][Epoch 8433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:22,201 - INFO - [diffusion][Epoch 8434] Epoch 8435/12000
2024-11-05 00:55:26,185 - INFO - [diffusion][Epoch 8434] diffusion training Loss: 0.06613024417310953
2024-11-05 00:55:26,187 - INFO - [diffusion][Epoch 8434] diffusion learning rate: 0.001
2024-11-05 00:55:26,188 - INFO - [diffusion][Epoch 8434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:26,190 - INFO - [diffusion][Epoch 8435] Epoch 8436/12000
2024-11-05 00:55:30,205 - INFO - [diffusion][Epoch 8435] diffusion training Loss: 0.06569677125662565
2024-11-05 00:55:30,403 - INFO - [diffusion][Epoch 8435] diffusion learning rate: 0.001
2024-11-05 00:55:30,406 - INFO - [diffusion][Epoch 8435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:30,407 - INFO - [diffusion][Epoch 8436] Epoch 8437/12000
2024-11-05 00:55:34,537 - INFO - [diffusion][Epoch 8436] diffusion training Loss: 0.06511396821588278
2024-11-05 00:55:34,539 - INFO - [diffusion][Epoch 8436] diffusion learning rate: 0.001
2024-11-05 00:55:34,541 - INFO - [diffusion][Epoch 8436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:34,542 - INFO - [diffusion][Epoch 8437] Epoch 8438/12000
2024-11-05 00:55:38,565 - INFO - [diffusion][Epoch 8437] diffusion training Loss: 0.06472595036029816
2024-11-05 00:55:38,567 - INFO - [diffusion][Epoch 8437] diffusion learning rate: 0.001
2024-11-05 00:55:38,569 - INFO - [diffusion][Epoch 8437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:38,571 - INFO - [diffusion][Epoch 8438] Epoch 8439/12000
2024-11-05 00:55:42,773 - INFO - [diffusion][Epoch 8438] diffusion training Loss: 0.07158052176237106
2024-11-05 00:55:42,775 - INFO - [diffusion][Epoch 8438] diffusion learning rate: 0.001
2024-11-05 00:55:42,777 - INFO - [diffusion][Epoch 8438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:42,778 - INFO - [diffusion][Epoch 8439] Epoch 8440/12000
2024-11-05 00:55:46,841 - INFO - [diffusion][Epoch 8439] diffusion training Loss: 0.0650116614997387
2024-11-05 00:55:46,843 - INFO - [diffusion][Epoch 8439] diffusion learning rate: 0.001
2024-11-05 00:55:46,845 - INFO - [diffusion][Epoch 8439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:46,846 - INFO - [diffusion][Epoch 8440] Epoch 8441/12000
2024-11-05 00:55:50,989 - INFO - [diffusion][Epoch 8440] diffusion training Loss: 0.06815758906304836
2024-11-05 00:55:50,991 - INFO - [diffusion][Epoch 8440] diffusion learning rate: 0.001
2024-11-05 00:55:50,993 - INFO - [diffusion][Epoch 8440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:50,994 - INFO - [diffusion][Epoch 8441] Epoch 8442/12000
2024-11-05 00:55:55,106 - INFO - [diffusion][Epoch 8441] diffusion training Loss: 0.06823707185685635
2024-11-05 00:55:55,108 - INFO - [diffusion][Epoch 8441] diffusion learning rate: 0.001
2024-11-05 00:55:55,110 - INFO - [diffusion][Epoch 8441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:55,111 - INFO - [diffusion][Epoch 8442] Epoch 8443/12000
2024-11-05 00:55:59,320 - INFO - [diffusion][Epoch 8442] diffusion training Loss: 0.06378612387925386
2024-11-05 00:55:59,321 - INFO - [diffusion][Epoch 8442] diffusion learning rate: 0.001
2024-11-05 00:55:59,323 - INFO - [diffusion][Epoch 8442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:59,324 - INFO - [diffusion][Epoch 8443] Epoch 8444/12000
2024-11-05 00:56:03,453 - INFO - [diffusion][Epoch 8443] diffusion training Loss: 0.06721481587737799
2024-11-05 00:56:03,455 - INFO - [diffusion][Epoch 8443] diffusion learning rate: 0.001
2024-11-05 00:56:03,457 - INFO - [diffusion][Epoch 8443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:03,458 - INFO - [diffusion][Epoch 8444] Epoch 8445/12000
2024-11-05 00:56:07,756 - INFO - [diffusion][Epoch 8444] diffusion training Loss: 0.06420546397566795
2024-11-05 00:56:07,758 - INFO - [diffusion][Epoch 8444] diffusion learning rate: 0.001
2024-11-05 00:56:07,760 - INFO - [diffusion][Epoch 8444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:07,762 - INFO - [diffusion][Epoch 8445] Epoch 8446/12000
2024-11-05 00:56:11,857 - INFO - [diffusion][Epoch 8445] diffusion training Loss: 0.06250843685120344
2024-11-05 00:56:11,859 - INFO - [diffusion][Epoch 8445] diffusion learning rate: 0.001
2024-11-05 00:56:11,861 - INFO - [diffusion][Epoch 8445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:11,862 - INFO - [diffusion][Epoch 8446] Epoch 8447/12000
2024-11-05 00:56:16,121 - INFO - [diffusion][Epoch 8446] diffusion training Loss: 0.0617792010307312
2024-11-05 00:56:16,123 - INFO - [diffusion][Epoch 8446] diffusion learning rate: 0.001
2024-11-05 00:56:16,125 - INFO - [diffusion][Epoch 8446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:16,126 - INFO - [diffusion][Epoch 8447] Epoch 8448/12000
2024-11-05 00:56:20,329 - INFO - [diffusion][Epoch 8447] diffusion training Loss: 0.06838730350136757
2024-11-05 00:56:20,331 - INFO - [diffusion][Epoch 8447] diffusion learning rate: 0.001
2024-11-05 00:56:20,333 - INFO - [diffusion][Epoch 8447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:20,335 - INFO - [diffusion][Epoch 8448] Epoch 8449/12000
2024-11-05 00:56:24,508 - INFO - [diffusion][Epoch 8448] diffusion training Loss: 0.061182680539786816
2024-11-05 00:56:24,510 - INFO - [diffusion][Epoch 8448] diffusion learning rate: 0.001
2024-11-05 00:56:24,512 - INFO - [diffusion][Epoch 8448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:24,513 - INFO - [diffusion][Epoch 8449] Epoch 8450/12000
2024-11-05 00:56:28,585 - INFO - [diffusion][Epoch 8449] diffusion training Loss: 0.06670468114316463
2024-11-05 00:56:28,587 - INFO - [diffusion][Epoch 8449] diffusion learning rate: 0.001
2024-11-05 00:56:28,589 - INFO - [diffusion][Epoch 8449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:28,590 - INFO - [diffusion][Epoch 8450] Epoch 8451/12000
2024-11-05 00:56:32,762 - INFO - [diffusion][Epoch 8450] diffusion training Loss: 0.057517215609550476
2024-11-05 00:56:32,764 - INFO - [diffusion][Epoch 8450] diffusion learning rate: 0.001
2024-11-05 00:56:32,766 - INFO - [diffusion][Epoch 8450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:32,767 - INFO - [diffusion][Epoch 8451] Epoch 8452/12000
2024-11-05 00:56:36,904 - INFO - [diffusion][Epoch 8451] diffusion training Loss: 0.06669567991048098
2024-11-05 00:56:36,906 - INFO - [diffusion][Epoch 8451] diffusion learning rate: 0.001
2024-11-05 00:56:36,908 - INFO - [diffusion][Epoch 8451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:36,910 - INFO - [diffusion][Epoch 8452] Epoch 8453/12000
2024-11-05 00:56:41,215 - INFO - [diffusion][Epoch 8452] diffusion training Loss: 0.06999248452484608
2024-11-05 00:56:41,217 - INFO - [diffusion][Epoch 8452] diffusion learning rate: 0.001
2024-11-05 00:56:41,220 - INFO - [diffusion][Epoch 8452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:41,221 - INFO - [diffusion][Epoch 8453] Epoch 8454/12000
2024-11-05 00:56:45,168 - INFO - [diffusion][Epoch 8453] diffusion training Loss: 0.07017445005476475
2024-11-05 00:56:45,170 - INFO - [diffusion][Epoch 8453] diffusion learning rate: 0.001
2024-11-05 00:56:45,192 - INFO - [diffusion][Epoch 8453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:45,193 - INFO - [diffusion][Epoch 8454] Epoch 8455/12000
2024-11-05 00:56:49,295 - INFO - [diffusion][Epoch 8454] diffusion training Loss: 0.06714729871600866
2024-11-05 00:56:49,297 - INFO - [diffusion][Epoch 8454] diffusion learning rate: 0.001
2024-11-05 00:56:49,300 - INFO - [diffusion][Epoch 8454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:49,301 - INFO - [diffusion][Epoch 8455] Epoch 8456/12000
2024-11-05 00:56:53,560 - INFO - [diffusion][Epoch 8455] diffusion training Loss: 0.06707036960870028
2024-11-05 00:56:53,562 - INFO - [diffusion][Epoch 8455] diffusion learning rate: 0.001
2024-11-05 00:56:53,564 - INFO - [diffusion][Epoch 8455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:53,565 - INFO - [diffusion][Epoch 8456] Epoch 8457/12000
2024-11-05 00:56:57,735 - INFO - [diffusion][Epoch 8456] diffusion training Loss: 0.07128865644335747
2024-11-05 00:56:57,737 - INFO - [diffusion][Epoch 8456] diffusion learning rate: 0.001
2024-11-05 00:56:57,739 - INFO - [diffusion][Epoch 8456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:57,740 - INFO - [diffusion][Epoch 8457] Epoch 8458/12000
2024-11-05 00:57:01,784 - INFO - [diffusion][Epoch 8457] diffusion training Loss: 0.06912605091929436
2024-11-05 00:57:01,787 - INFO - [diffusion][Epoch 8457] diffusion learning rate: 0.001
2024-11-05 00:57:01,789 - INFO - [diffusion][Epoch 8457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:01,790 - INFO - [diffusion][Epoch 8458] Epoch 8459/12000
2024-11-05 00:57:05,781 - INFO - [diffusion][Epoch 8458] diffusion training Loss: 0.0646108640357852
2024-11-05 00:57:05,783 - INFO - [diffusion][Epoch 8458] diffusion learning rate: 0.001
2024-11-05 00:57:05,824 - INFO - [diffusion][Epoch 8458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:05,826 - INFO - [diffusion][Epoch 8459] Epoch 8460/12000
2024-11-05 00:57:09,903 - INFO - [diffusion][Epoch 8459] diffusion training Loss: 0.0669171204790473
2024-11-05 00:57:09,905 - INFO - [diffusion][Epoch 8459] diffusion learning rate: 0.001
2024-11-05 00:57:09,907 - INFO - [diffusion][Epoch 8459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:09,909 - INFO - [diffusion][Epoch 8460] Epoch 8461/12000
2024-11-05 00:57:14,189 - INFO - [diffusion][Epoch 8460] diffusion training Loss: 0.07307668216526508
2024-11-05 00:57:14,190 - INFO - [diffusion][Epoch 8460] diffusion learning rate: 0.001
2024-11-05 00:57:14,192 - INFO - [diffusion][Epoch 8460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:14,193 - INFO - [diffusion][Epoch 8461] Epoch 8462/12000
2024-11-05 00:57:18,414 - INFO - [diffusion][Epoch 8461] diffusion training Loss: 0.06640684884041548
2024-11-05 00:57:18,416 - INFO - [diffusion][Epoch 8461] diffusion learning rate: 0.001
2024-11-05 00:57:18,418 - INFO - [diffusion][Epoch 8461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:18,419 - INFO - [diffusion][Epoch 8462] Epoch 8463/12000
2024-11-05 00:57:22,598 - INFO - [diffusion][Epoch 8462] diffusion training Loss: 0.06501900218427181
2024-11-05 00:57:22,600 - INFO - [diffusion][Epoch 8462] diffusion learning rate: 0.001
2024-11-05 00:57:22,636 - INFO - [diffusion][Epoch 8462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:22,638 - INFO - [diffusion][Epoch 8463] Epoch 8464/12000
2024-11-05 00:57:26,949 - INFO - [diffusion][Epoch 8463] diffusion training Loss: 0.06518631801009178
2024-11-05 00:57:26,950 - INFO - [diffusion][Epoch 8463] diffusion learning rate: 0.001
2024-11-05 00:57:26,952 - INFO - [diffusion][Epoch 8463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:26,954 - INFO - [diffusion][Epoch 8464] Epoch 8465/12000
2024-11-05 00:57:31,212 - INFO - [diffusion][Epoch 8464] diffusion training Loss: 0.062369247898459435
2024-11-05 00:57:31,214 - INFO - [diffusion][Epoch 8464] diffusion learning rate: 0.001
2024-11-05 00:57:31,216 - INFO - [diffusion][Epoch 8464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:31,218 - INFO - [diffusion][Epoch 8465] Epoch 8466/12000
2024-11-05 00:57:35,137 - INFO - [diffusion][Epoch 8465] diffusion training Loss: 0.06666490994393826
2024-11-05 00:57:35,139 - INFO - [diffusion][Epoch 8465] diffusion learning rate: 0.001
2024-11-05 00:57:35,141 - INFO - [diffusion][Epoch 8465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:35,142 - INFO - [diffusion][Epoch 8466] Epoch 8467/12000
2024-11-05 00:57:39,367 - INFO - [diffusion][Epoch 8466] diffusion training Loss: 0.0732510257512331
2024-11-05 00:57:39,369 - INFO - [diffusion][Epoch 8466] diffusion learning rate: 0.001
2024-11-05 00:57:39,371 - INFO - [diffusion][Epoch 8466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:39,373 - INFO - [diffusion][Epoch 8467] Epoch 8468/12000
2024-11-05 00:57:43,496 - INFO - [diffusion][Epoch 8467] diffusion training Loss: 0.06456468813121319
2024-11-05 00:57:43,498 - INFO - [diffusion][Epoch 8467] diffusion learning rate: 0.001
2024-11-05 00:57:43,500 - INFO - [diffusion][Epoch 8467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:43,501 - INFO - [diffusion][Epoch 8468] Epoch 8469/12000
2024-11-05 00:57:47,783 - INFO - [diffusion][Epoch 8468] diffusion training Loss: 0.07435867190361023
2024-11-05 00:57:47,787 - INFO - [diffusion][Epoch 8468] diffusion learning rate: 0.001
2024-11-05 00:57:47,789 - INFO - [diffusion][Epoch 8468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:47,790 - INFO - [diffusion][Epoch 8469] Epoch 8470/12000
2024-11-05 00:57:51,880 - INFO - [diffusion][Epoch 8469] diffusion training Loss: 0.0640903813764453
2024-11-05 00:57:51,883 - INFO - [diffusion][Epoch 8469] diffusion learning rate: 0.001
2024-11-05 00:57:51,885 - INFO - [diffusion][Epoch 8469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:51,886 - INFO - [diffusion][Epoch 8470] Epoch 8471/12000
2024-11-05 00:57:56,248 - INFO - [diffusion][Epoch 8470] diffusion training Loss: 0.06284142564982176
2024-11-05 00:57:56,249 - INFO - [diffusion][Epoch 8470] diffusion learning rate: 0.001
2024-11-05 00:57:56,251 - INFO - [diffusion][Epoch 8470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:56,252 - INFO - [diffusion][Epoch 8471] Epoch 8472/12000
2024-11-05 00:58:00,434 - INFO - [diffusion][Epoch 8471] diffusion training Loss: 0.07207614555954933
2024-11-05 00:58:00,436 - INFO - [diffusion][Epoch 8471] diffusion learning rate: 0.001
2024-11-05 00:58:00,438 - INFO - [diffusion][Epoch 8471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:00,439 - INFO - [diffusion][Epoch 8472] Epoch 8473/12000
2024-11-05 00:58:04,551 - INFO - [diffusion][Epoch 8472] diffusion training Loss: 0.06742794625461102
2024-11-05 00:58:04,553 - INFO - [diffusion][Epoch 8472] diffusion learning rate: 0.001
2024-11-05 00:58:04,555 - INFO - [diffusion][Epoch 8472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:04,556 - INFO - [diffusion][Epoch 8473] Epoch 8474/12000
2024-11-05 00:58:08,550 - INFO - [diffusion][Epoch 8473] diffusion training Loss: 0.06603254284709692
2024-11-05 00:58:08,552 - INFO - [diffusion][Epoch 8473] diffusion learning rate: 0.001
2024-11-05 00:58:08,554 - INFO - [diffusion][Epoch 8473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:08,555 - INFO - [diffusion][Epoch 8474] Epoch 8475/12000
2024-11-05 00:58:12,629 - INFO - [diffusion][Epoch 8474] diffusion training Loss: 0.0689468402415514
2024-11-05 00:58:12,631 - INFO - [diffusion][Epoch 8474] diffusion learning rate: 0.001
2024-11-05 00:58:12,633 - INFO - [diffusion][Epoch 8474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:12,634 - INFO - [diffusion][Epoch 8475] Epoch 8476/12000
2024-11-05 00:58:16,601 - INFO - [diffusion][Epoch 8475] diffusion training Loss: 0.0656850365921855
2024-11-05 00:58:16,603 - INFO - [diffusion][Epoch 8475] diffusion learning rate: 0.001
2024-11-05 00:58:16,605 - INFO - [diffusion][Epoch 8475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:16,606 - INFO - [diffusion][Epoch 8476] Epoch 8477/12000
2024-11-05 00:58:20,742 - INFO - [diffusion][Epoch 8476] diffusion training Loss: 0.06871845573186874
2024-11-05 00:58:20,745 - INFO - [diffusion][Epoch 8476] diffusion learning rate: 0.001
2024-11-05 00:58:20,746 - INFO - [diffusion][Epoch 8476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:20,747 - INFO - [diffusion][Epoch 8477] Epoch 8478/12000
2024-11-05 00:58:24,835 - INFO - [diffusion][Epoch 8477] diffusion training Loss: 0.06585068348795176
2024-11-05 00:58:24,837 - INFO - [diffusion][Epoch 8477] diffusion learning rate: 0.001
2024-11-05 00:58:24,839 - INFO - [diffusion][Epoch 8477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:24,840 - INFO - [diffusion][Epoch 8478] Epoch 8479/12000
2024-11-05 00:58:28,768 - INFO - [diffusion][Epoch 8478] diffusion training Loss: 0.06670159474015236
2024-11-05 00:58:28,770 - INFO - [diffusion][Epoch 8478] diffusion learning rate: 0.001
2024-11-05 00:58:28,772 - INFO - [diffusion][Epoch 8478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:28,773 - INFO - [diffusion][Epoch 8479] Epoch 8480/12000
2024-11-05 00:58:32,988 - INFO - [diffusion][Epoch 8479] diffusion training Loss: 0.06553145591169596
2024-11-05 00:58:32,990 - INFO - [diffusion][Epoch 8479] diffusion learning rate: 0.001
2024-11-05 00:58:32,992 - INFO - [diffusion][Epoch 8479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:32,993 - INFO - [diffusion][Epoch 8480] Epoch 8481/12000
2024-11-05 00:58:37,198 - INFO - [diffusion][Epoch 8480] diffusion training Loss: 0.06827599741518497
2024-11-05 00:58:37,201 - INFO - [diffusion][Epoch 8480] diffusion learning rate: 0.001
2024-11-05 00:58:37,203 - INFO - [diffusion][Epoch 8480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:37,204 - INFO - [diffusion][Epoch 8481] Epoch 8482/12000
2024-11-05 00:58:41,256 - INFO - [diffusion][Epoch 8481] diffusion training Loss: 0.0700994748622179
2024-11-05 00:58:41,259 - INFO - [diffusion][Epoch 8481] diffusion learning rate: 0.001
2024-11-05 00:58:41,261 - INFO - [diffusion][Epoch 8481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:41,263 - INFO - [diffusion][Epoch 8482] Epoch 8483/12000
2024-11-05 00:58:45,337 - INFO - [diffusion][Epoch 8482] diffusion training Loss: 0.06962165981531143
2024-11-05 00:58:45,339 - INFO - [diffusion][Epoch 8482] diffusion learning rate: 0.001
2024-11-05 00:58:45,341 - INFO - [diffusion][Epoch 8482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:45,342 - INFO - [diffusion][Epoch 8483] Epoch 8484/12000
2024-11-05 00:58:49,454 - INFO - [diffusion][Epoch 8483] diffusion training Loss: 0.06100654322654009
2024-11-05 00:58:49,457 - INFO - [diffusion][Epoch 8483] diffusion learning rate: 0.001
2024-11-05 00:58:49,460 - INFO - [diffusion][Epoch 8483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:49,462 - INFO - [diffusion][Epoch 8484] Epoch 8485/12000
2024-11-05 00:58:53,653 - INFO - [diffusion][Epoch 8484] diffusion training Loss: 0.0701212864369154
2024-11-05 00:58:53,655 - INFO - [diffusion][Epoch 8484] diffusion learning rate: 0.001
2024-11-05 00:58:53,656 - INFO - [diffusion][Epoch 8484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:53,658 - INFO - [diffusion][Epoch 8485] Epoch 8486/12000
2024-11-05 00:58:57,862 - INFO - [diffusion][Epoch 8485] diffusion training Loss: 0.061715646646916866
2024-11-05 00:58:57,864 - INFO - [diffusion][Epoch 8485] diffusion learning rate: 0.001
2024-11-05 00:58:57,866 - INFO - [diffusion][Epoch 8485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:57,867 - INFO - [diffusion][Epoch 8486] Epoch 8487/12000
2024-11-05 00:59:01,931 - INFO - [diffusion][Epoch 8486] diffusion training Loss: 0.06822637282311916
2024-11-05 00:59:01,933 - INFO - [diffusion][Epoch 8486] diffusion learning rate: 0.001
2024-11-05 00:59:01,935 - INFO - [diffusion][Epoch 8486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:01,936 - INFO - [diffusion][Epoch 8487] Epoch 8488/12000
2024-11-05 00:59:06,168 - INFO - [diffusion][Epoch 8487] diffusion training Loss: 0.06248838920146227
2024-11-05 00:59:06,169 - INFO - [diffusion][Epoch 8487] diffusion learning rate: 0.001
2024-11-05 00:59:06,171 - INFO - [diffusion][Epoch 8487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:06,173 - INFO - [diffusion][Epoch 8488] Epoch 8489/12000
2024-11-05 00:59:10,435 - INFO - [diffusion][Epoch 8488] diffusion training Loss: 0.06572325527667999
2024-11-05 00:59:10,437 - INFO - [diffusion][Epoch 8488] diffusion learning rate: 0.001
2024-11-05 00:59:10,439 - INFO - [diffusion][Epoch 8488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:10,440 - INFO - [diffusion][Epoch 8489] Epoch 8490/12000
2024-11-05 00:59:14,625 - INFO - [diffusion][Epoch 8489] diffusion training Loss: 0.06453720573335886
2024-11-05 00:59:14,627 - INFO - [diffusion][Epoch 8489] diffusion learning rate: 0.001
2024-11-05 00:59:14,656 - INFO - [diffusion][Epoch 8489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:14,658 - INFO - [diffusion][Epoch 8490] Epoch 8491/12000
2024-11-05 00:59:18,723 - INFO - [diffusion][Epoch 8490] diffusion training Loss: 0.06516152806580067
2024-11-05 00:59:18,725 - INFO - [diffusion][Epoch 8490] diffusion learning rate: 0.001
2024-11-05 00:59:18,727 - INFO - [diffusion][Epoch 8490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:18,728 - INFO - [diffusion][Epoch 8491] Epoch 8492/12000
2024-11-05 00:59:22,705 - INFO - [diffusion][Epoch 8491] diffusion training Loss: 0.06923365592956543
2024-11-05 00:59:22,708 - INFO - [diffusion][Epoch 8491] diffusion learning rate: 0.001
2024-11-05 00:59:22,710 - INFO - [diffusion][Epoch 8491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:22,711 - INFO - [diffusion][Epoch 8492] Epoch 8493/12000
2024-11-05 00:59:27,313 - INFO - [diffusion][Epoch 8492] diffusion training Loss: 0.06697495095431805
2024-11-05 00:59:27,315 - INFO - [diffusion][Epoch 8492] diffusion learning rate: 0.001
2024-11-05 00:59:27,317 - INFO - [diffusion][Epoch 8492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:27,318 - INFO - [diffusion][Epoch 8493] Epoch 8494/12000
2024-11-05 00:59:31,389 - INFO - [diffusion][Epoch 8493] diffusion training Loss: 0.06365889124572277
2024-11-05 00:59:31,391 - INFO - [diffusion][Epoch 8493] diffusion learning rate: 0.001
2024-11-05 00:59:31,393 - INFO - [diffusion][Epoch 8493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:31,394 - INFO - [diffusion][Epoch 8494] Epoch 8495/12000
2024-11-05 00:59:35,426 - INFO - [diffusion][Epoch 8494] diffusion training Loss: 0.06915614940226078
2024-11-05 00:59:35,428 - INFO - [diffusion][Epoch 8494] diffusion learning rate: 0.001
2024-11-05 00:59:35,429 - INFO - [diffusion][Epoch 8494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:35,431 - INFO - [diffusion][Epoch 8495] Epoch 8496/12000
2024-11-05 00:59:39,530 - INFO - [diffusion][Epoch 8495] diffusion training Loss: 0.06707226671278477
2024-11-05 00:59:39,532 - INFO - [diffusion][Epoch 8495] diffusion learning rate: 0.001
2024-11-05 00:59:39,533 - INFO - [diffusion][Epoch 8495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:39,535 - INFO - [diffusion][Epoch 8496] Epoch 8497/12000
2024-11-05 00:59:43,691 - INFO - [diffusion][Epoch 8496] diffusion training Loss: 0.06066411454230547
2024-11-05 00:59:43,693 - INFO - [diffusion][Epoch 8496] diffusion learning rate: 0.001
2024-11-05 00:59:43,695 - INFO - [diffusion][Epoch 8496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:43,696 - INFO - [diffusion][Epoch 8497] Epoch 8498/12000
2024-11-05 00:59:47,747 - INFO - [diffusion][Epoch 8497] diffusion training Loss: 0.06581307295709848
2024-11-05 00:59:47,749 - INFO - [diffusion][Epoch 8497] diffusion learning rate: 0.001
2024-11-05 00:59:47,751 - INFO - [diffusion][Epoch 8497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:47,752 - INFO - [diffusion][Epoch 8498] Epoch 8499/12000
2024-11-05 00:59:51,970 - INFO - [diffusion][Epoch 8498] diffusion training Loss: 0.06672017462551594
2024-11-05 00:59:51,972 - INFO - [diffusion][Epoch 8498] diffusion learning rate: 0.001
2024-11-05 00:59:51,974 - INFO - [diffusion][Epoch 8498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:51,975 - INFO - [diffusion][Epoch 8499] Epoch 8500/12000
2024-11-05 00:59:56,050 - INFO - [diffusion][Epoch 8499] diffusion training Loss: 0.0674729784950614
2024-11-05 00:59:56,060 - INFO - [diffusion][Epoch 8499] diffusion learning rate: 0.001
2024-11-05 00:59:56,062 - INFO - [diffusion][Epoch 8499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:56,064 - INFO - [diffusion][Epoch 8500] Epoch 8501/12000
2024-11-05 01:00:00,167 - INFO - [diffusion][Epoch 8500] diffusion training Loss: 0.06728841550648212
2024-11-05 01:00:00,169 - INFO - [diffusion][Epoch 8500] diffusion learning rate: 0.001
2024-11-05 01:00:00,171 - INFO - [diffusion][Epoch 8500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:00,172 - INFO - [diffusion][Epoch 8501] Epoch 8502/12000
2024-11-05 01:00:04,338 - INFO - [diffusion][Epoch 8501] diffusion training Loss: 0.07047950848937035
2024-11-05 01:00:04,341 - INFO - [diffusion][Epoch 8501] diffusion learning rate: 0.001
2024-11-05 01:00:04,342 - INFO - [diffusion][Epoch 8501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:04,343 - INFO - [diffusion][Epoch 8502] Epoch 8503/12000
2024-11-05 01:00:08,276 - INFO - [diffusion][Epoch 8502] diffusion training Loss: 0.05962339881807566
2024-11-05 01:00:08,280 - INFO - [diffusion][Epoch 8502] diffusion learning rate: 0.001
2024-11-05 01:00:08,282 - INFO - [diffusion][Epoch 8502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:08,283 - INFO - [diffusion][Epoch 8503] Epoch 8504/12000
2024-11-05 01:00:12,528 - INFO - [diffusion][Epoch 8503] diffusion training Loss: 0.06638763844966888
2024-11-05 01:00:12,530 - INFO - [diffusion][Epoch 8503] diffusion learning rate: 0.001
2024-11-05 01:00:12,571 - INFO - [diffusion][Epoch 8503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:12,572 - INFO - [diffusion][Epoch 8504] Epoch 8505/12000
2024-11-05 01:00:17,008 - INFO - [diffusion][Epoch 8504] diffusion training Loss: 0.06448642257601023
2024-11-05 01:00:17,010 - INFO - [diffusion][Epoch 8504] diffusion learning rate: 0.001
2024-11-05 01:00:17,012 - INFO - [diffusion][Epoch 8504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:17,013 - INFO - [diffusion][Epoch 8505] Epoch 8506/12000
2024-11-05 01:00:21,095 - INFO - [diffusion][Epoch 8505] diffusion training Loss: 0.06491331104189157
2024-11-05 01:00:21,098 - INFO - [diffusion][Epoch 8505] diffusion learning rate: 0.001
2024-11-05 01:00:21,099 - INFO - [diffusion][Epoch 8505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:21,101 - INFO - [diffusion][Epoch 8506] Epoch 8507/12000
2024-11-05 01:00:25,142 - INFO - [diffusion][Epoch 8506] diffusion training Loss: 0.06542157009243965
2024-11-05 01:00:25,144 - INFO - [diffusion][Epoch 8506] diffusion learning rate: 0.001
2024-11-05 01:00:25,146 - INFO - [diffusion][Epoch 8506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:25,147 - INFO - [diffusion][Epoch 8507] Epoch 8508/12000
2024-11-05 01:00:29,128 - INFO - [diffusion][Epoch 8507] diffusion training Loss: 0.061845616437494755
2024-11-05 01:00:29,131 - INFO - [diffusion][Epoch 8507] diffusion learning rate: 0.001
2024-11-05 01:00:29,133 - INFO - [diffusion][Epoch 8507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:29,134 - INFO - [diffusion][Epoch 8508] Epoch 8509/12000
2024-11-05 01:00:33,259 - INFO - [diffusion][Epoch 8508] diffusion training Loss: 0.06677500903606415
2024-11-05 01:00:33,261 - INFO - [diffusion][Epoch 8508] diffusion learning rate: 0.001
2024-11-05 01:00:33,264 - INFO - [diffusion][Epoch 8508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:33,265 - INFO - [diffusion][Epoch 8509] Epoch 8510/12000
2024-11-05 01:00:37,414 - INFO - [diffusion][Epoch 8509] diffusion training Loss: 0.06712422613054514
2024-11-05 01:00:37,416 - INFO - [diffusion][Epoch 8509] diffusion learning rate: 0.001
2024-11-05 01:00:37,417 - INFO - [diffusion][Epoch 8509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:37,419 - INFO - [diffusion][Epoch 8510] Epoch 8511/12000
2024-11-05 01:00:41,476 - INFO - [diffusion][Epoch 8510] diffusion training Loss: 0.0715285949409008
2024-11-05 01:00:41,478 - INFO - [diffusion][Epoch 8510] diffusion learning rate: 0.001
2024-11-05 01:00:41,480 - INFO - [diffusion][Epoch 8510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:41,482 - INFO - [diffusion][Epoch 8511] Epoch 8512/12000
2024-11-05 01:00:45,671 - INFO - [diffusion][Epoch 8511] diffusion training Loss: 0.06503899954259396
2024-11-05 01:00:45,673 - INFO - [diffusion][Epoch 8511] diffusion learning rate: 0.001
2024-11-05 01:00:45,675 - INFO - [diffusion][Epoch 8511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:45,677 - INFO - [diffusion][Epoch 8512] Epoch 8513/12000
2024-11-05 01:00:50,311 - INFO - [diffusion][Epoch 8512] diffusion training Loss: 0.07098478637635708
2024-11-05 01:00:50,313 - INFO - [diffusion][Epoch 8512] diffusion learning rate: 0.001
2024-11-05 01:00:50,315 - INFO - [diffusion][Epoch 8512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:50,316 - INFO - [diffusion][Epoch 8513] Epoch 8514/12000
2024-11-05 01:00:54,433 - INFO - [diffusion][Epoch 8513] diffusion training Loss: 0.06828118581324816
2024-11-05 01:00:54,436 - INFO - [diffusion][Epoch 8513] diffusion learning rate: 0.001
2024-11-05 01:00:54,437 - INFO - [diffusion][Epoch 8513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:54,439 - INFO - [diffusion][Epoch 8514] Epoch 8515/12000
2024-11-05 01:00:58,341 - INFO - [diffusion][Epoch 8514] diffusion training Loss: 0.07070115208625793
2024-11-05 01:00:58,343 - INFO - [diffusion][Epoch 8514] diffusion learning rate: 0.001
2024-11-05 01:00:58,345 - INFO - [diffusion][Epoch 8514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:58,347 - INFO - [diffusion][Epoch 8515] Epoch 8516/12000
2024-11-05 01:01:02,416 - INFO - [diffusion][Epoch 8515] diffusion training Loss: 0.06704318895936012
2024-11-05 01:01:02,418 - INFO - [diffusion][Epoch 8515] diffusion learning rate: 0.001
2024-11-05 01:01:02,420 - INFO - [diffusion][Epoch 8515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:02,421 - INFO - [diffusion][Epoch 8516] Epoch 8517/12000
2024-11-05 01:01:06,630 - INFO - [diffusion][Epoch 8516] diffusion training Loss: 0.07151014171540737
2024-11-05 01:01:06,632 - INFO - [diffusion][Epoch 8516] diffusion learning rate: 0.001
2024-11-05 01:01:06,634 - INFO - [diffusion][Epoch 8516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:06,635 - INFO - [diffusion][Epoch 8517] Epoch 8518/12000
2024-11-05 01:01:10,833 - INFO - [diffusion][Epoch 8517] diffusion training Loss: 0.06850318051874638
2024-11-05 01:01:10,835 - INFO - [diffusion][Epoch 8517] diffusion learning rate: 0.001
2024-11-05 01:01:10,837 - INFO - [diffusion][Epoch 8517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:10,838 - INFO - [diffusion][Epoch 8518] Epoch 8519/12000
2024-11-05 01:01:14,942 - INFO - [diffusion][Epoch 8518] diffusion training Loss: 0.06875461339950562
2024-11-05 01:01:14,945 - INFO - [diffusion][Epoch 8518] diffusion learning rate: 0.001
2024-11-05 01:01:14,947 - INFO - [diffusion][Epoch 8518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:14,948 - INFO - [diffusion][Epoch 8519] Epoch 8520/12000
2024-11-05 01:01:19,012 - INFO - [diffusion][Epoch 8519] diffusion training Loss: 0.06753424927592278
2024-11-05 01:01:19,014 - INFO - [diffusion][Epoch 8519] diffusion learning rate: 0.001
2024-11-05 01:01:19,016 - INFO - [diffusion][Epoch 8519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:19,017 - INFO - [diffusion][Epoch 8520] Epoch 8521/12000
2024-11-05 01:01:23,210 - INFO - [diffusion][Epoch 8520] diffusion training Loss: 0.06849116832017899
2024-11-05 01:01:23,212 - INFO - [diffusion][Epoch 8520] diffusion learning rate: 0.001
2024-11-05 01:01:23,215 - INFO - [diffusion][Epoch 8520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:23,216 - INFO - [diffusion][Epoch 8521] Epoch 8522/12000
2024-11-05 01:01:27,342 - INFO - [diffusion][Epoch 8521] diffusion training Loss: 0.06750665418803692
2024-11-05 01:01:27,344 - INFO - [diffusion][Epoch 8521] diffusion learning rate: 0.001
2024-11-05 01:01:27,346 - INFO - [diffusion][Epoch 8521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:27,347 - INFO - [diffusion][Epoch 8522] Epoch 8523/12000
2024-11-05 01:01:31,431 - INFO - [diffusion][Epoch 8522] diffusion training Loss: 0.06821680255234241
2024-11-05 01:01:31,432 - INFO - [diffusion][Epoch 8522] diffusion learning rate: 0.001
2024-11-05 01:01:31,434 - INFO - [diffusion][Epoch 8522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:31,436 - INFO - [diffusion][Epoch 8523] Epoch 8524/12000
2024-11-05 01:01:35,598 - INFO - [diffusion][Epoch 8523] diffusion training Loss: 0.06281881593167782
2024-11-05 01:01:35,600 - INFO - [diffusion][Epoch 8523] diffusion learning rate: 0.001
2024-11-05 01:01:35,601 - INFO - [diffusion][Epoch 8523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:35,602 - INFO - [diffusion][Epoch 8524] Epoch 8525/12000
2024-11-05 01:01:39,633 - INFO - [diffusion][Epoch 8524] diffusion training Loss: 0.07351845875382423
2024-11-05 01:01:39,636 - INFO - [diffusion][Epoch 8524] diffusion learning rate: 0.001
2024-11-05 01:01:39,637 - INFO - [diffusion][Epoch 8524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:39,639 - INFO - [diffusion][Epoch 8525] Epoch 8526/12000
2024-11-05 01:01:43,690 - INFO - [diffusion][Epoch 8525] diffusion training Loss: 0.06588407978415489
2024-11-05 01:01:43,693 - INFO - [diffusion][Epoch 8525] diffusion learning rate: 0.001
2024-11-05 01:01:43,694 - INFO - [diffusion][Epoch 8525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:43,696 - INFO - [diffusion][Epoch 8526] Epoch 8527/12000
2024-11-05 01:01:47,689 - INFO - [diffusion][Epoch 8526] diffusion training Loss: 0.07041697762906551
2024-11-05 01:01:47,691 - INFO - [diffusion][Epoch 8526] diffusion learning rate: 0.001
2024-11-05 01:01:47,694 - INFO - [diffusion][Epoch 8526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:47,695 - INFO - [diffusion][Epoch 8527] Epoch 8528/12000
2024-11-05 01:01:51,681 - INFO - [diffusion][Epoch 8527] diffusion training Loss: 0.06406736746430397
2024-11-05 01:01:51,683 - INFO - [diffusion][Epoch 8527] diffusion learning rate: 0.001
2024-11-05 01:01:51,685 - INFO - [diffusion][Epoch 8527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:51,686 - INFO - [diffusion][Epoch 8528] Epoch 8529/12000
2024-11-05 01:01:55,790 - INFO - [diffusion][Epoch 8528] diffusion training Loss: 0.06737691722810268
2024-11-05 01:01:55,792 - INFO - [diffusion][Epoch 8528] diffusion learning rate: 0.001
2024-11-05 01:01:55,794 - INFO - [diffusion][Epoch 8528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:55,795 - INFO - [diffusion][Epoch 8529] Epoch 8530/12000
2024-11-05 01:02:00,013 - INFO - [diffusion][Epoch 8529] diffusion training Loss: 0.05940557271242142
2024-11-05 01:02:00,015 - INFO - [diffusion][Epoch 8529] diffusion learning rate: 0.001
2024-11-05 01:02:00,017 - INFO - [diffusion][Epoch 8529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:00,018 - INFO - [diffusion][Epoch 8530] Epoch 8531/12000
2024-11-05 01:02:04,241 - INFO - [diffusion][Epoch 8530] diffusion training Loss: 0.060562433674931526
2024-11-05 01:02:04,243 - INFO - [diffusion][Epoch 8530] diffusion learning rate: 0.001
2024-11-05 01:02:04,245 - INFO - [diffusion][Epoch 8530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:04,246 - INFO - [diffusion][Epoch 8531] Epoch 8532/12000
2024-11-05 01:02:08,382 - INFO - [diffusion][Epoch 8531] diffusion training Loss: 0.061380282044410706
2024-11-05 01:02:08,384 - INFO - [diffusion][Epoch 8531] diffusion learning rate: 0.001
2024-11-05 01:02:08,386 - INFO - [diffusion][Epoch 8531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:08,387 - INFO - [diffusion][Epoch 8532] Epoch 8533/12000
2024-11-05 01:02:12,431 - INFO - [diffusion][Epoch 8532] diffusion training Loss: 0.06889243703335524
2024-11-05 01:02:12,433 - INFO - [diffusion][Epoch 8532] diffusion learning rate: 0.001
2024-11-05 01:02:12,435 - INFO - [diffusion][Epoch 8532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:12,436 - INFO - [diffusion][Epoch 8533] Epoch 8534/12000
2024-11-05 01:02:16,922 - INFO - [diffusion][Epoch 8533] diffusion training Loss: 0.07034403830766678
2024-11-05 01:02:16,924 - INFO - [diffusion][Epoch 8533] diffusion learning rate: 0.001
2024-11-05 01:02:16,926 - INFO - [diffusion][Epoch 8533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:16,927 - INFO - [diffusion][Epoch 8534] Epoch 8535/12000
2024-11-05 01:02:21,085 - INFO - [diffusion][Epoch 8534] diffusion training Loss: 0.06595616042613983
2024-11-05 01:02:21,087 - INFO - [diffusion][Epoch 8534] diffusion learning rate: 0.001
2024-11-05 01:02:21,089 - INFO - [diffusion][Epoch 8534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:21,091 - INFO - [diffusion][Epoch 8535] Epoch 8536/12000
2024-11-05 01:02:25,112 - INFO - [diffusion][Epoch 8535] diffusion training Loss: 0.06733722612261772
2024-11-05 01:02:25,114 - INFO - [diffusion][Epoch 8535] diffusion learning rate: 0.001
2024-11-05 01:02:25,115 - INFO - [diffusion][Epoch 8535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:25,117 - INFO - [diffusion][Epoch 8536] Epoch 8537/12000
2024-11-05 01:02:29,029 - INFO - [diffusion][Epoch 8536] diffusion training Loss: 0.0652485815808177
2024-11-05 01:02:29,033 - INFO - [diffusion][Epoch 8536] diffusion learning rate: 0.001
2024-11-05 01:02:29,035 - INFO - [diffusion][Epoch 8536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:29,036 - INFO - [diffusion][Epoch 8537] Epoch 8538/12000
2024-11-05 01:02:33,359 - INFO - [diffusion][Epoch 8537] diffusion training Loss: 0.06785159930586815
2024-11-05 01:02:33,361 - INFO - [diffusion][Epoch 8537] diffusion learning rate: 0.001
2024-11-05 01:02:33,362 - INFO - [diffusion][Epoch 8537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:33,364 - INFO - [diffusion][Epoch 8538] Epoch 8539/12000
2024-11-05 01:02:37,620 - INFO - [diffusion][Epoch 8538] diffusion training Loss: 0.06938512250781059
2024-11-05 01:02:37,622 - INFO - [diffusion][Epoch 8538] diffusion learning rate: 0.001
2024-11-05 01:02:37,624 - INFO - [diffusion][Epoch 8538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:37,626 - INFO - [diffusion][Epoch 8539] Epoch 8540/12000
2024-11-05 01:02:41,653 - INFO - [diffusion][Epoch 8539] diffusion training Loss: 0.06632802169770002
2024-11-05 01:02:41,655 - INFO - [diffusion][Epoch 8539] diffusion learning rate: 0.001
2024-11-05 01:02:41,656 - INFO - [diffusion][Epoch 8539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:41,658 - INFO - [diffusion][Epoch 8540] Epoch 8541/12000
2024-11-05 01:02:45,944 - INFO - [diffusion][Epoch 8540] diffusion training Loss: 0.06257940828800201
2024-11-05 01:02:45,946 - INFO - [diffusion][Epoch 8540] diffusion learning rate: 0.001
2024-11-05 01:02:46,010 - INFO - [diffusion][Epoch 8540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:46,012 - INFO - [diffusion][Epoch 8541] Epoch 8542/12000
2024-11-05 01:02:50,139 - INFO - [diffusion][Epoch 8541] diffusion training Loss: 0.068653404712677
2024-11-05 01:02:50,141 - INFO - [diffusion][Epoch 8541] diffusion learning rate: 0.001
2024-11-05 01:02:50,143 - INFO - [diffusion][Epoch 8541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:50,144 - INFO - [diffusion][Epoch 8542] Epoch 8543/12000
2024-11-05 01:02:54,402 - INFO - [diffusion][Epoch 8542] diffusion training Loss: 0.07151593081653118
2024-11-05 01:02:54,404 - INFO - [diffusion][Epoch 8542] diffusion learning rate: 0.001
2024-11-05 01:02:54,406 - INFO - [diffusion][Epoch 8542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:54,407 - INFO - [diffusion][Epoch 8543] Epoch 8544/12000
2024-11-05 01:02:58,498 - INFO - [diffusion][Epoch 8543] diffusion training Loss: 0.06918212212622166
2024-11-05 01:02:58,500 - INFO - [diffusion][Epoch 8543] diffusion learning rate: 0.001
2024-11-05 01:02:58,502 - INFO - [diffusion][Epoch 8543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:58,503 - INFO - [diffusion][Epoch 8544] Epoch 8545/12000
2024-11-05 01:03:02,446 - INFO - [diffusion][Epoch 8544] diffusion training Loss: 0.06225545145571232
2024-11-05 01:03:02,448 - INFO - [diffusion][Epoch 8544] diffusion learning rate: 0.001
2024-11-05 01:03:02,450 - INFO - [diffusion][Epoch 8544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:02,451 - INFO - [diffusion][Epoch 8545] Epoch 8546/12000
2024-11-05 01:03:06,443 - INFO - [diffusion][Epoch 8545] diffusion training Loss: 0.06493292190134525
2024-11-05 01:03:06,445 - INFO - [diffusion][Epoch 8545] diffusion learning rate: 0.001
2024-11-05 01:03:06,446 - INFO - [diffusion][Epoch 8545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:06,448 - INFO - [diffusion][Epoch 8546] Epoch 8547/12000
2024-11-05 01:03:10,794 - INFO - [diffusion][Epoch 8546] diffusion training Loss: 0.07069651782512665
2024-11-05 01:03:10,796 - INFO - [diffusion][Epoch 8546] diffusion learning rate: 0.001
2024-11-05 01:03:10,797 - INFO - [diffusion][Epoch 8546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:10,799 - INFO - [diffusion][Epoch 8547] Epoch 8548/12000
2024-11-05 01:03:15,120 - INFO - [diffusion][Epoch 8547] diffusion training Loss: 0.06342437397688627
2024-11-05 01:03:15,122 - INFO - [diffusion][Epoch 8547] diffusion learning rate: 0.001
2024-11-05 01:03:15,123 - INFO - [diffusion][Epoch 8547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:15,125 - INFO - [diffusion][Epoch 8548] Epoch 8549/12000
2024-11-05 01:03:19,303 - INFO - [diffusion][Epoch 8548] diffusion training Loss: 0.07205620780587196
2024-11-05 01:03:19,304 - INFO - [diffusion][Epoch 8548] diffusion learning rate: 0.001
2024-11-05 01:03:19,306 - INFO - [diffusion][Epoch 8548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:19,307 - INFO - [diffusion][Epoch 8549] Epoch 8550/12000
2024-11-05 01:03:23,491 - INFO - [diffusion][Epoch 8549] diffusion training Loss: 0.06544208899140358
2024-11-05 01:03:23,493 - INFO - [diffusion][Epoch 8549] diffusion learning rate: 0.001
2024-11-05 01:03:23,494 - INFO - [diffusion][Epoch 8549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:23,496 - INFO - [diffusion][Epoch 8550] Epoch 8551/12000
2024-11-05 01:03:27,717 - INFO - [diffusion][Epoch 8550] diffusion training Loss: 0.06377363950014114
2024-11-05 01:03:27,719 - INFO - [diffusion][Epoch 8550] diffusion learning rate: 0.001
2024-11-05 01:03:27,721 - INFO - [diffusion][Epoch 8550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:27,722 - INFO - [diffusion][Epoch 8551] Epoch 8552/12000
2024-11-05 01:03:31,908 - INFO - [diffusion][Epoch 8551] diffusion training Loss: 0.07511894404888153
2024-11-05 01:03:31,910 - INFO - [diffusion][Epoch 8551] diffusion learning rate: 0.001
2024-11-05 01:03:31,912 - INFO - [diffusion][Epoch 8551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:31,913 - INFO - [diffusion][Epoch 8552] Epoch 8553/12000
2024-11-05 01:03:36,034 - INFO - [diffusion][Epoch 8552] diffusion training Loss: 0.07012397423386574
2024-11-05 01:03:36,036 - INFO - [diffusion][Epoch 8552] diffusion learning rate: 0.001
2024-11-05 01:03:36,038 - INFO - [diffusion][Epoch 8552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:36,039 - INFO - [diffusion][Epoch 8553] Epoch 8554/12000
2024-11-05 01:03:40,579 - INFO - [diffusion][Epoch 8553] diffusion training Loss: 0.06718788854777813
2024-11-05 01:03:40,581 - INFO - [diffusion][Epoch 8553] diffusion learning rate: 0.001
2024-11-05 01:03:40,583 - INFO - [diffusion][Epoch 8553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:40,584 - INFO - [diffusion][Epoch 8554] Epoch 8555/12000
2024-11-05 01:03:44,520 - INFO - [diffusion][Epoch 8554] diffusion training Loss: 0.06080413609743118
2024-11-05 01:03:44,522 - INFO - [diffusion][Epoch 8554] diffusion learning rate: 0.001
2024-11-05 01:03:44,524 - INFO - [diffusion][Epoch 8554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:44,525 - INFO - [diffusion][Epoch 8555] Epoch 8556/12000
2024-11-05 01:03:48,636 - INFO - [diffusion][Epoch 8555] diffusion training Loss: 0.06658911984413862
2024-11-05 01:03:48,638 - INFO - [diffusion][Epoch 8555] diffusion learning rate: 0.001
2024-11-05 01:03:48,640 - INFO - [diffusion][Epoch 8555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:48,642 - INFO - [diffusion][Epoch 8556] Epoch 8557/12000
2024-11-05 01:03:52,683 - INFO - [diffusion][Epoch 8556] diffusion training Loss: 0.06878175586462021
2024-11-05 01:03:52,685 - INFO - [diffusion][Epoch 8556] diffusion learning rate: 0.001
2024-11-05 01:03:52,688 - INFO - [diffusion][Epoch 8556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:52,689 - INFO - [diffusion][Epoch 8557] Epoch 8558/12000
2024-11-05 01:03:56,764 - INFO - [diffusion][Epoch 8557] diffusion training Loss: 0.06629692204296589
2024-11-05 01:03:56,766 - INFO - [diffusion][Epoch 8557] diffusion learning rate: 0.001
2024-11-05 01:03:56,767 - INFO - [diffusion][Epoch 8557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:56,769 - INFO - [diffusion][Epoch 8558] Epoch 8559/12000
2024-11-05 01:04:00,823 - INFO - [diffusion][Epoch 8558] diffusion training Loss: 0.0638450300320983
2024-11-05 01:04:00,826 - INFO - [diffusion][Epoch 8558] diffusion learning rate: 0.001
2024-11-05 01:04:00,828 - INFO - [diffusion][Epoch 8558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:00,829 - INFO - [diffusion][Epoch 8559] Epoch 8560/12000
2024-11-05 01:04:04,756 - INFO - [diffusion][Epoch 8559] diffusion training Loss: 0.06936807930469513
2024-11-05 01:04:04,758 - INFO - [diffusion][Epoch 8559] diffusion learning rate: 0.001
2024-11-05 01:04:04,759 - INFO - [diffusion][Epoch 8559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:04,760 - INFO - [diffusion][Epoch 8560] Epoch 8561/12000
2024-11-05 01:04:08,832 - INFO - [diffusion][Epoch 8560] diffusion training Loss: 0.06185685470700264
2024-11-05 01:04:08,834 - INFO - [diffusion][Epoch 8560] diffusion learning rate: 0.001
2024-11-05 01:04:08,836 - INFO - [diffusion][Epoch 8560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:08,837 - INFO - [diffusion][Epoch 8561] Epoch 8562/12000
2024-11-05 01:04:13,060 - INFO - [diffusion][Epoch 8561] diffusion training Loss: 0.06758199445903301
2024-11-05 01:04:13,062 - INFO - [diffusion][Epoch 8561] diffusion learning rate: 0.001
2024-11-05 01:04:13,064 - INFO - [diffusion][Epoch 8561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:13,065 - INFO - [diffusion][Epoch 8562] Epoch 8563/12000
2024-11-05 01:04:17,298 - INFO - [diffusion][Epoch 8562] diffusion training Loss: 0.06880601402372122
2024-11-05 01:04:17,300 - INFO - [diffusion][Epoch 8562] diffusion learning rate: 0.001
2024-11-05 01:04:17,302 - INFO - [diffusion][Epoch 8562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:17,303 - INFO - [diffusion][Epoch 8563] Epoch 8564/12000
2024-11-05 01:04:21,512 - INFO - [diffusion][Epoch 8563] diffusion training Loss: 0.06410455424338579
2024-11-05 01:04:21,514 - INFO - [diffusion][Epoch 8563] diffusion learning rate: 0.001
2024-11-05 01:04:21,516 - INFO - [diffusion][Epoch 8563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:21,517 - INFO - [diffusion][Epoch 8564] Epoch 8565/12000
2024-11-05 01:04:25,703 - INFO - [diffusion][Epoch 8564] diffusion training Loss: 0.06575612258166075
2024-11-05 01:04:25,705 - INFO - [diffusion][Epoch 8564] diffusion learning rate: 0.001
2024-11-05 01:04:25,732 - INFO - [diffusion][Epoch 8564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:25,733 - INFO - [diffusion][Epoch 8565] Epoch 8566/12000
2024-11-05 01:04:29,917 - INFO - [diffusion][Epoch 8565] diffusion training Loss: 0.06490018032491207
2024-11-05 01:04:29,919 - INFO - [diffusion][Epoch 8565] diffusion learning rate: 0.001
2024-11-05 01:04:29,922 - INFO - [diffusion][Epoch 8565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:29,924 - INFO - [diffusion][Epoch 8566] Epoch 8567/12000
2024-11-05 01:04:34,143 - INFO - [diffusion][Epoch 8566] diffusion training Loss: 0.06288223806768656
2024-11-05 01:04:34,146 - INFO - [diffusion][Epoch 8566] diffusion learning rate: 0.001
2024-11-05 01:04:34,147 - INFO - [diffusion][Epoch 8566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:34,149 - INFO - [diffusion][Epoch 8567] Epoch 8568/12000
2024-11-05 01:04:38,299 - INFO - [diffusion][Epoch 8567] diffusion training Loss: 0.06415065005421638
2024-11-05 01:04:38,301 - INFO - [diffusion][Epoch 8567] diffusion learning rate: 0.001
2024-11-05 01:04:38,303 - INFO - [diffusion][Epoch 8567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:38,304 - INFO - [diffusion][Epoch 8568] Epoch 8569/12000
2024-11-05 01:04:42,468 - INFO - [diffusion][Epoch 8568] diffusion training Loss: 0.06264892593026161
2024-11-05 01:04:42,470 - INFO - [diffusion][Epoch 8568] diffusion learning rate: 0.001
2024-11-05 01:04:42,471 - INFO - [diffusion][Epoch 8568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:42,473 - INFO - [diffusion][Epoch 8569] Epoch 8570/12000
2024-11-05 01:04:46,454 - INFO - [diffusion][Epoch 8569] diffusion training Loss: 0.06749423034489155
2024-11-05 01:04:46,458 - INFO - [diffusion][Epoch 8569] diffusion learning rate: 0.001
2024-11-05 01:04:46,460 - INFO - [diffusion][Epoch 8569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:46,462 - INFO - [diffusion][Epoch 8570] Epoch 8571/12000
2024-11-05 01:04:50,664 - INFO - [diffusion][Epoch 8570] diffusion training Loss: 0.06956114992499352
2024-11-05 01:04:50,666 - INFO - [diffusion][Epoch 8570] diffusion learning rate: 0.001
2024-11-05 01:04:50,668 - INFO - [diffusion][Epoch 8570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:50,669 - INFO - [diffusion][Epoch 8571] Epoch 8572/12000
2024-11-05 01:04:54,846 - INFO - [diffusion][Epoch 8571] diffusion training Loss: 0.06491581071168184
2024-11-05 01:04:54,848 - INFO - [diffusion][Epoch 8571] diffusion learning rate: 0.001
2024-11-05 01:04:54,850 - INFO - [diffusion][Epoch 8571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:54,851 - INFO - [diffusion][Epoch 8572] Epoch 8573/12000
2024-11-05 01:04:59,090 - INFO - [diffusion][Epoch 8572] diffusion training Loss: 0.07129130698740482
2024-11-05 01:04:59,092 - INFO - [diffusion][Epoch 8572] diffusion learning rate: 0.001
2024-11-05 01:04:59,094 - INFO - [diffusion][Epoch 8572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:59,095 - INFO - [diffusion][Epoch 8573] Epoch 8574/12000
2024-11-05 01:05:03,016 - INFO - [diffusion][Epoch 8573] diffusion training Loss: 0.06705708149820566
2024-11-05 01:05:03,017 - INFO - [diffusion][Epoch 8573] diffusion learning rate: 0.001
2024-11-05 01:05:03,019 - INFO - [diffusion][Epoch 8573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:03,020 - INFO - [diffusion][Epoch 8574] Epoch 8575/12000
2024-11-05 01:05:07,808 - INFO - [diffusion][Epoch 8574] diffusion training Loss: 0.07029146701097488
2024-11-05 01:05:07,810 - INFO - [diffusion][Epoch 8574] diffusion learning rate: 0.001
2024-11-05 01:05:07,812 - INFO - [diffusion][Epoch 8574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:07,814 - INFO - [diffusion][Epoch 8575] Epoch 8576/12000
2024-11-05 01:05:12,083 - INFO - [diffusion][Epoch 8575] diffusion training Loss: 0.0597896808758378
2024-11-05 01:05:12,085 - INFO - [diffusion][Epoch 8575] diffusion learning rate: 0.001
2024-11-05 01:05:12,087 - INFO - [diffusion][Epoch 8575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:12,088 - INFO - [diffusion][Epoch 8576] Epoch 8577/12000
2024-11-05 01:05:16,266 - INFO - [diffusion][Epoch 8576] diffusion training Loss: 0.06609832588583231
2024-11-05 01:05:16,269 - INFO - [diffusion][Epoch 8576] diffusion learning rate: 0.001
2024-11-05 01:05:16,270 - INFO - [diffusion][Epoch 8576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:16,271 - INFO - [diffusion][Epoch 8577] Epoch 8578/12000
2024-11-05 01:05:20,416 - INFO - [diffusion][Epoch 8577] diffusion training Loss: 0.06525116879492998
2024-11-05 01:05:20,418 - INFO - [diffusion][Epoch 8577] diffusion learning rate: 0.001
2024-11-05 01:05:20,443 - INFO - [diffusion][Epoch 8577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:20,445 - INFO - [diffusion][Epoch 8578] Epoch 8579/12000
2024-11-05 01:05:24,473 - INFO - [diffusion][Epoch 8578] diffusion training Loss: 0.06481871660798788
2024-11-05 01:05:24,475 - INFO - [diffusion][Epoch 8578] diffusion learning rate: 0.001
2024-11-05 01:05:24,477 - INFO - [diffusion][Epoch 8578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:24,478 - INFO - [diffusion][Epoch 8579] Epoch 8580/12000
2024-11-05 01:05:28,068 - INFO - [diffusion][Epoch 8579] diffusion training Loss: 0.07561988569796085
2024-11-05 01:05:28,071 - INFO - [diffusion][Epoch 8579] diffusion learning rate: 0.001
2024-11-05 01:05:28,072 - INFO - [diffusion][Epoch 8579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:28,074 - INFO - [diffusion][Epoch 8580] Epoch 8581/12000
2024-11-05 01:05:32,106 - INFO - [diffusion][Epoch 8580] diffusion training Loss: 0.06589614972472191
2024-11-05 01:05:32,108 - INFO - [diffusion][Epoch 8580] diffusion learning rate: 0.001
2024-11-05 01:05:32,110 - INFO - [diffusion][Epoch 8580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:32,111 - INFO - [diffusion][Epoch 8581] Epoch 8582/12000
2024-11-05 01:05:36,010 - INFO - [diffusion][Epoch 8581] diffusion training Loss: 0.0686136232689023
2024-11-05 01:05:36,013 - INFO - [diffusion][Epoch 8581] diffusion learning rate: 0.001
2024-11-05 01:05:36,014 - INFO - [diffusion][Epoch 8581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:36,016 - INFO - [diffusion][Epoch 8582] Epoch 8583/12000
2024-11-05 01:05:40,231 - INFO - [diffusion][Epoch 8582] diffusion training Loss: 0.0655724573880434
2024-11-05 01:05:40,233 - INFO - [diffusion][Epoch 8582] diffusion learning rate: 0.001
2024-11-05 01:05:40,235 - INFO - [diffusion][Epoch 8582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:40,236 - INFO - [diffusion][Epoch 8583] Epoch 8584/12000
2024-11-05 01:05:44,426 - INFO - [diffusion][Epoch 8583] diffusion training Loss: 0.06592060718685389
2024-11-05 01:05:44,428 - INFO - [diffusion][Epoch 8583] diffusion learning rate: 0.001
2024-11-05 01:05:44,430 - INFO - [diffusion][Epoch 8583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:44,431 - INFO - [diffusion][Epoch 8584] Epoch 8585/12000
2024-11-05 01:05:48,306 - INFO - [diffusion][Epoch 8584] diffusion training Loss: 0.06825740821659565
2024-11-05 01:05:48,308 - INFO - [diffusion][Epoch 8584] diffusion learning rate: 0.001
2024-11-05 01:05:48,311 - INFO - [diffusion][Epoch 8584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:48,312 - INFO - [diffusion][Epoch 8585] Epoch 8586/12000
2024-11-05 01:05:52,485 - INFO - [diffusion][Epoch 8585] diffusion training Loss: 0.06712261028587818
2024-11-05 01:05:52,487 - INFO - [diffusion][Epoch 8585] diffusion learning rate: 0.001
2024-11-05 01:05:52,490 - INFO - [diffusion][Epoch 8585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:52,492 - INFO - [diffusion][Epoch 8586] Epoch 8587/12000
2024-11-05 01:05:56,533 - INFO - [diffusion][Epoch 8586] diffusion training Loss: 0.06620382145047188
2024-11-05 01:05:56,535 - INFO - [diffusion][Epoch 8586] diffusion learning rate: 0.001
2024-11-05 01:05:56,537 - INFO - [diffusion][Epoch 8586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:56,538 - INFO - [diffusion][Epoch 8587] Epoch 8588/12000
2024-11-05 01:06:00,556 - INFO - [diffusion][Epoch 8587] diffusion training Loss: 0.07220826391130686
2024-11-05 01:06:00,558 - INFO - [diffusion][Epoch 8587] diffusion learning rate: 0.001
2024-11-05 01:06:00,559 - INFO - [diffusion][Epoch 8587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:00,561 - INFO - [diffusion][Epoch 8588] Epoch 8589/12000
2024-11-05 01:06:04,617 - INFO - [diffusion][Epoch 8588] diffusion training Loss: 0.06633084174245596
2024-11-05 01:06:04,619 - INFO - [diffusion][Epoch 8588] diffusion learning rate: 0.001
2024-11-05 01:06:04,621 - INFO - [diffusion][Epoch 8588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:04,622 - INFO - [diffusion][Epoch 8589] Epoch 8590/12000
2024-11-05 01:06:08,685 - INFO - [diffusion][Epoch 8589] diffusion training Loss: 0.06426115706562996
2024-11-05 01:06:08,687 - INFO - [diffusion][Epoch 8589] diffusion learning rate: 0.001
2024-11-05 01:06:08,689 - INFO - [diffusion][Epoch 8589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:08,690 - INFO - [diffusion][Epoch 8590] Epoch 8591/12000
2024-11-05 01:06:12,793 - INFO - [diffusion][Epoch 8590] diffusion training Loss: 0.06667189300060272
2024-11-05 01:06:12,818 - INFO - [diffusion][Epoch 8590] diffusion learning rate: 0.001
2024-11-05 01:06:12,820 - INFO - [diffusion][Epoch 8590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:12,822 - INFO - [diffusion][Epoch 8591] Epoch 8592/12000
2024-11-05 01:06:16,942 - INFO - [diffusion][Epoch 8591] diffusion training Loss: 0.06563359685242176
2024-11-05 01:06:16,944 - INFO - [diffusion][Epoch 8591] diffusion learning rate: 0.001
2024-11-05 01:06:16,946 - INFO - [diffusion][Epoch 8591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:16,947 - INFO - [diffusion][Epoch 8592] Epoch 8593/12000
2024-11-05 01:06:21,154 - INFO - [diffusion][Epoch 8592] diffusion training Loss: 0.06750601530075073
2024-11-05 01:06:21,156 - INFO - [diffusion][Epoch 8592] diffusion learning rate: 0.001
2024-11-05 01:06:21,158 - INFO - [diffusion][Epoch 8592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:21,159 - INFO - [diffusion][Epoch 8593] Epoch 8594/12000
2024-11-05 01:06:25,244 - INFO - [diffusion][Epoch 8593] diffusion training Loss: 0.06892101839184761
2024-11-05 01:06:25,246 - INFO - [diffusion][Epoch 8593] diffusion learning rate: 0.001
2024-11-05 01:06:25,247 - INFO - [diffusion][Epoch 8593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:25,249 - INFO - [diffusion][Epoch 8594] Epoch 8595/12000
2024-11-05 01:06:29,326 - INFO - [diffusion][Epoch 8594] diffusion training Loss: 0.0657324567437172
2024-11-05 01:06:29,328 - INFO - [diffusion][Epoch 8594] diffusion learning rate: 0.001
2024-11-05 01:06:29,330 - INFO - [diffusion][Epoch 8594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:29,331 - INFO - [diffusion][Epoch 8595] Epoch 8596/12000
2024-11-05 01:06:33,521 - INFO - [diffusion][Epoch 8595] diffusion training Loss: 0.06560165621340275
2024-11-05 01:06:33,523 - INFO - [diffusion][Epoch 8595] diffusion learning rate: 0.001
2024-11-05 01:06:33,525 - INFO - [diffusion][Epoch 8595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:33,526 - INFO - [diffusion][Epoch 8596] Epoch 8597/12000
2024-11-05 01:06:38,184 - INFO - [diffusion][Epoch 8596] diffusion training Loss: 0.06259007193148136
2024-11-05 01:06:38,186 - INFO - [diffusion][Epoch 8596] diffusion learning rate: 0.001
2024-11-05 01:06:38,187 - INFO - [diffusion][Epoch 8596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:38,188 - INFO - [diffusion][Epoch 8597] Epoch 8598/12000
2024-11-05 01:06:42,011 - INFO - [diffusion][Epoch 8597] diffusion training Loss: 0.06493798736482859
2024-11-05 01:06:42,015 - INFO - [diffusion][Epoch 8597] diffusion learning rate: 0.001
2024-11-05 01:06:42,017 - INFO - [diffusion][Epoch 8597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:42,019 - INFO - [diffusion][Epoch 8598] Epoch 8599/12000
2024-11-05 01:06:45,979 - INFO - [diffusion][Epoch 8598] diffusion training Loss: 0.06636085547506809
2024-11-05 01:06:45,981 - INFO - [diffusion][Epoch 8598] diffusion learning rate: 0.001
2024-11-05 01:06:45,983 - INFO - [diffusion][Epoch 8598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:45,984 - INFO - [diffusion][Epoch 8599] Epoch 8600/12000
2024-11-05 01:06:49,984 - INFO - [diffusion][Epoch 8599] diffusion training Loss: 0.06218624487519264
2024-11-05 01:06:49,986 - INFO - [diffusion][Epoch 8599] diffusion learning rate: 0.001
2024-11-05 01:06:49,989 - INFO - [diffusion][Epoch 8599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:49,991 - INFO - [diffusion][Epoch 8600] Epoch 8601/12000
2024-11-05 01:06:54,089 - INFO - [diffusion][Epoch 8600] diffusion training Loss: 0.07377242669463158
2024-11-05 01:06:54,091 - INFO - [diffusion][Epoch 8600] diffusion learning rate: 0.001
2024-11-05 01:06:54,093 - INFO - [diffusion][Epoch 8600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:54,098 - INFO - [diffusion][Epoch 8601] Epoch 8602/12000
2024-11-05 01:06:58,306 - INFO - [diffusion][Epoch 8601] diffusion training Loss: 0.06800288520753384
2024-11-05 01:06:58,308 - INFO - [diffusion][Epoch 8601] diffusion learning rate: 0.001
2024-11-05 01:06:58,310 - INFO - [diffusion][Epoch 8601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:58,312 - INFO - [diffusion][Epoch 8602] Epoch 8603/12000
2024-11-05 01:07:02,407 - INFO - [diffusion][Epoch 8602] diffusion training Loss: 0.06369754299521446
2024-11-05 01:07:02,409 - INFO - [diffusion][Epoch 8602] diffusion learning rate: 0.001
2024-11-05 01:07:02,411 - INFO - [diffusion][Epoch 8602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:02,413 - INFO - [diffusion][Epoch 8603] Epoch 8604/12000
2024-11-05 01:07:06,620 - INFO - [diffusion][Epoch 8603] diffusion training Loss: 0.06465790420770645
2024-11-05 01:07:06,623 - INFO - [diffusion][Epoch 8603] diffusion learning rate: 0.001
2024-11-05 01:07:06,624 - INFO - [diffusion][Epoch 8603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:06,626 - INFO - [diffusion][Epoch 8604] Epoch 8605/12000
2024-11-05 01:07:10,828 - INFO - [diffusion][Epoch 8604] diffusion training Loss: 0.06663410272449255
2024-11-05 01:07:10,830 - INFO - [diffusion][Epoch 8604] diffusion learning rate: 0.001
2024-11-05 01:07:10,870 - INFO - [diffusion][Epoch 8604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:10,871 - INFO - [diffusion][Epoch 8605] Epoch 8606/12000
2024-11-05 01:07:15,005 - INFO - [diffusion][Epoch 8605] diffusion training Loss: 0.06277371570467949
2024-11-05 01:07:15,007 - INFO - [diffusion][Epoch 8605] diffusion learning rate: 0.001
2024-11-05 01:07:15,009 - INFO - [diffusion][Epoch 8605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:15,010 - INFO - [diffusion][Epoch 8606] Epoch 8607/12000
2024-11-05 01:07:19,036 - INFO - [diffusion][Epoch 8606] diffusion training Loss: 0.068162951618433
2024-11-05 01:07:19,038 - INFO - [diffusion][Epoch 8606] diffusion learning rate: 0.001
2024-11-05 01:07:19,040 - INFO - [diffusion][Epoch 8606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:19,041 - INFO - [diffusion][Epoch 8607] Epoch 8608/12000
2024-11-05 01:07:23,021 - INFO - [diffusion][Epoch 8607] diffusion training Loss: 0.0715070553123951
2024-11-05 01:07:23,023 - INFO - [diffusion][Epoch 8607] diffusion learning rate: 0.001
2024-11-05 01:07:23,025 - INFO - [diffusion][Epoch 8607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:23,026 - INFO - [diffusion][Epoch 8608] Epoch 8609/12000
2024-11-05 01:07:27,272 - INFO - [diffusion][Epoch 8608] diffusion training Loss: 0.06713407300412655
2024-11-05 01:07:27,274 - INFO - [diffusion][Epoch 8608] diffusion learning rate: 0.001
2024-11-05 01:07:27,276 - INFO - [diffusion][Epoch 8608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:27,277 - INFO - [diffusion][Epoch 8609] Epoch 8610/12000
2024-11-05 01:07:31,255 - INFO - [diffusion][Epoch 8609] diffusion training Loss: 0.06883330270648003
2024-11-05 01:07:31,257 - INFO - [diffusion][Epoch 8609] diffusion learning rate: 0.001
2024-11-05 01:07:31,259 - INFO - [diffusion][Epoch 8609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:31,261 - INFO - [diffusion][Epoch 8610] Epoch 8611/12000
2024-11-05 01:07:35,388 - INFO - [diffusion][Epoch 8610] diffusion training Loss: 0.06368102971464396
2024-11-05 01:07:35,390 - INFO - [diffusion][Epoch 8610] diffusion learning rate: 0.001
2024-11-05 01:07:35,391 - INFO - [diffusion][Epoch 8610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:35,393 - INFO - [diffusion][Epoch 8611] Epoch 8612/12000
2024-11-05 01:07:39,552 - INFO - [diffusion][Epoch 8611] diffusion training Loss: 0.07138379663228989
2024-11-05 01:07:39,555 - INFO - [diffusion][Epoch 8611] diffusion learning rate: 0.001
2024-11-05 01:07:39,557 - INFO - [diffusion][Epoch 8611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:39,559 - INFO - [diffusion][Epoch 8612] Epoch 8613/12000
2024-11-05 01:07:43,530 - INFO - [diffusion][Epoch 8612] diffusion training Loss: 0.06423318106681108
2024-11-05 01:07:43,532 - INFO - [diffusion][Epoch 8612] diffusion learning rate: 0.001
2024-11-05 01:07:43,569 - INFO - [diffusion][Epoch 8612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:43,570 - INFO - [diffusion][Epoch 8613] Epoch 8614/12000
2024-11-05 01:07:47,747 - INFO - [diffusion][Epoch 8613] diffusion training Loss: 0.06819990836083889
2024-11-05 01:07:47,749 - INFO - [diffusion][Epoch 8613] diffusion learning rate: 0.001
2024-11-05 01:07:47,751 - INFO - [diffusion][Epoch 8613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:47,752 - INFO - [diffusion][Epoch 8614] Epoch 8615/12000
2024-11-05 01:07:51,792 - INFO - [diffusion][Epoch 8614] diffusion training Loss: 0.06553273368626833
2024-11-05 01:07:51,794 - INFO - [diffusion][Epoch 8614] diffusion learning rate: 0.001
2024-11-05 01:07:51,796 - INFO - [diffusion][Epoch 8614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:51,797 - INFO - [diffusion][Epoch 8615] Epoch 8616/12000
2024-11-05 01:07:55,839 - INFO - [diffusion][Epoch 8615] diffusion training Loss: 0.06739285960793495
2024-11-05 01:07:55,841 - INFO - [diffusion][Epoch 8615] diffusion learning rate: 0.001
2024-11-05 01:07:55,843 - INFO - [diffusion][Epoch 8615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:55,844 - INFO - [diffusion][Epoch 8616] Epoch 8617/12000
2024-11-05 01:07:59,904 - INFO - [diffusion][Epoch 8616] diffusion training Loss: 0.06646748445928097
2024-11-05 01:07:59,906 - INFO - [diffusion][Epoch 8616] diffusion learning rate: 0.001
2024-11-05 01:07:59,908 - INFO - [diffusion][Epoch 8616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:59,909 - INFO - [diffusion][Epoch 8617] Epoch 8618/12000
2024-11-05 01:08:04,052 - INFO - [diffusion][Epoch 8617] diffusion training Loss: 0.07158590294420719
2024-11-05 01:08:04,054 - INFO - [diffusion][Epoch 8617] diffusion learning rate: 0.001
2024-11-05 01:08:04,056 - INFO - [diffusion][Epoch 8617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:04,057 - INFO - [diffusion][Epoch 8618] Epoch 8619/12000
2024-11-05 01:08:08,262 - INFO - [diffusion][Epoch 8618] diffusion training Loss: 0.06483103986829519
2024-11-05 01:08:08,264 - INFO - [diffusion][Epoch 8618] diffusion learning rate: 0.001
2024-11-05 01:08:08,266 - INFO - [diffusion][Epoch 8618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:08,267 - INFO - [diffusion][Epoch 8619] Epoch 8620/12000
2024-11-05 01:08:12,381 - INFO - [diffusion][Epoch 8619] diffusion training Loss: 0.06398566160351038
2024-11-05 01:08:12,384 - INFO - [diffusion][Epoch 8619] diffusion learning rate: 0.001
2024-11-05 01:08:12,386 - INFO - [diffusion][Epoch 8619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:12,387 - INFO - [diffusion][Epoch 8620] Epoch 8621/12000
2024-11-05 01:08:16,591 - INFO - [diffusion][Epoch 8620] diffusion training Loss: 0.06464133504778147
2024-11-05 01:08:16,593 - INFO - [diffusion][Epoch 8620] diffusion learning rate: 0.001
2024-11-05 01:08:16,595 - INFO - [diffusion][Epoch 8620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:16,596 - INFO - [diffusion][Epoch 8621] Epoch 8622/12000
2024-11-05 01:08:20,644 - INFO - [diffusion][Epoch 8621] diffusion training Loss: 0.06936494074761868
2024-11-05 01:08:20,646 - INFO - [diffusion][Epoch 8621] diffusion learning rate: 0.001
2024-11-05 01:08:20,649 - INFO - [diffusion][Epoch 8621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:20,651 - INFO - [diffusion][Epoch 8622] Epoch 8623/12000
2024-11-05 01:08:24,686 - INFO - [diffusion][Epoch 8622] diffusion training Loss: 0.06490256078541279
2024-11-05 01:08:24,688 - INFO - [diffusion][Epoch 8622] diffusion learning rate: 0.001
2024-11-05 01:08:24,690 - INFO - [diffusion][Epoch 8622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:24,692 - INFO - [diffusion][Epoch 8623] Epoch 8624/12000
2024-11-05 01:08:28,894 - INFO - [diffusion][Epoch 8623] diffusion training Loss: 0.06529702711850405
2024-11-05 01:08:28,896 - INFO - [diffusion][Epoch 8623] diffusion learning rate: 0.001
2024-11-05 01:08:28,898 - INFO - [diffusion][Epoch 8623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:28,899 - INFO - [diffusion][Epoch 8624] Epoch 8625/12000
2024-11-05 01:08:33,168 - INFO - [diffusion][Epoch 8624] diffusion training Loss: 0.06426967866718769
2024-11-05 01:08:33,170 - INFO - [diffusion][Epoch 8624] diffusion learning rate: 0.001
2024-11-05 01:08:33,173 - INFO - [diffusion][Epoch 8624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:33,174 - INFO - [diffusion][Epoch 8625] Epoch 8626/12000
2024-11-05 01:08:37,429 - INFO - [diffusion][Epoch 8625] diffusion training Loss: 0.0614698426797986
2024-11-05 01:08:37,431 - INFO - [diffusion][Epoch 8625] diffusion learning rate: 0.001
2024-11-05 01:08:37,433 - INFO - [diffusion][Epoch 8625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:37,434 - INFO - [diffusion][Epoch 8626] Epoch 8627/12000
2024-11-05 01:08:41,722 - INFO - [diffusion][Epoch 8626] diffusion training Loss: 0.06615621875971556
2024-11-05 01:08:41,724 - INFO - [diffusion][Epoch 8626] diffusion learning rate: 0.001
2024-11-05 01:08:41,726 - INFO - [diffusion][Epoch 8626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:41,727 - INFO - [diffusion][Epoch 8627] Epoch 8628/12000
2024-11-05 01:08:45,971 - INFO - [diffusion][Epoch 8627] diffusion training Loss: 0.060886587016284466
2024-11-05 01:08:45,973 - INFO - [diffusion][Epoch 8627] diffusion learning rate: 0.001
2024-11-05 01:08:45,975 - INFO - [diffusion][Epoch 8627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:45,976 - INFO - [diffusion][Epoch 8628] Epoch 8629/12000
2024-11-05 01:08:50,141 - INFO - [diffusion][Epoch 8628] diffusion training Loss: 0.06425358075648546
2024-11-05 01:08:50,143 - INFO - [diffusion][Epoch 8628] diffusion learning rate: 0.001
2024-11-05 01:08:50,145 - INFO - [diffusion][Epoch 8628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:50,147 - INFO - [diffusion][Epoch 8629] Epoch 8630/12000
2024-11-05 01:08:54,338 - INFO - [diffusion][Epoch 8629] diffusion training Loss: 0.06208183616399765
2024-11-05 01:08:54,341 - INFO - [diffusion][Epoch 8629] diffusion learning rate: 0.001
2024-11-05 01:08:54,343 - INFO - [diffusion][Epoch 8629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:54,344 - INFO - [diffusion][Epoch 8630] Epoch 8631/12000
2024-11-05 01:08:58,606 - INFO - [diffusion][Epoch 8630] diffusion training Loss: 0.07199285924434662
2024-11-05 01:08:58,608 - INFO - [diffusion][Epoch 8630] diffusion learning rate: 0.001
2024-11-05 01:08:58,610 - INFO - [diffusion][Epoch 8630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:58,612 - INFO - [diffusion][Epoch 8631] Epoch 8632/12000
2024-11-05 01:09:02,692 - INFO - [diffusion][Epoch 8631] diffusion training Loss: 0.0630273288115859
2024-11-05 01:09:02,694 - INFO - [diffusion][Epoch 8631] diffusion learning rate: 0.001
2024-11-05 01:09:02,696 - INFO - [diffusion][Epoch 8631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:02,697 - INFO - [diffusion][Epoch 8632] Epoch 8633/12000
2024-11-05 01:09:06,834 - INFO - [diffusion][Epoch 8632] diffusion training Loss: 0.06635484006255865
2024-11-05 01:09:06,836 - INFO - [diffusion][Epoch 8632] diffusion learning rate: 0.001
2024-11-05 01:09:06,875 - INFO - [diffusion][Epoch 8632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:06,876 - INFO - [diffusion][Epoch 8633] Epoch 8634/12000
2024-11-05 01:09:11,051 - INFO - [diffusion][Epoch 8633] diffusion training Loss: 0.06509325746446848
2024-11-05 01:09:11,054 - INFO - [diffusion][Epoch 8633] diffusion learning rate: 0.001
2024-11-05 01:09:11,056 - INFO - [diffusion][Epoch 8633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:11,058 - INFO - [diffusion][Epoch 8634] Epoch 8635/12000
2024-11-05 01:09:15,196 - INFO - [diffusion][Epoch 8634] diffusion training Loss: 0.06568715907633305
2024-11-05 01:09:15,198 - INFO - [diffusion][Epoch 8634] diffusion learning rate: 0.001
2024-11-05 01:09:15,200 - INFO - [diffusion][Epoch 8634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:15,202 - INFO - [diffusion][Epoch 8635] Epoch 8636/12000
2024-11-05 01:09:19,306 - INFO - [diffusion][Epoch 8635] diffusion training Loss: 0.0686299204826355
2024-11-05 01:09:19,308 - INFO - [diffusion][Epoch 8635] diffusion learning rate: 0.001
2024-11-05 01:09:19,309 - INFO - [diffusion][Epoch 8635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:19,311 - INFO - [diffusion][Epoch 8636] Epoch 8637/12000
2024-11-05 01:09:24,054 - INFO - [diffusion][Epoch 8636] diffusion training Loss: 0.07592655904591084
2024-11-05 01:09:24,117 - INFO - [diffusion][Epoch 8636] diffusion learning rate: 0.001
2024-11-05 01:09:24,119 - INFO - [diffusion][Epoch 8636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:24,121 - INFO - [diffusion][Epoch 8637] Epoch 8638/12000
2024-11-05 01:09:28,293 - INFO - [diffusion][Epoch 8637] diffusion training Loss: 0.06412589084357023
2024-11-05 01:09:28,295 - INFO - [diffusion][Epoch 8637] diffusion learning rate: 0.001
2024-11-05 01:09:28,297 - INFO - [diffusion][Epoch 8637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:28,299 - INFO - [diffusion][Epoch 8638] Epoch 8639/12000
2024-11-05 01:09:32,574 - INFO - [diffusion][Epoch 8638] diffusion training Loss: 0.06523961387574673
2024-11-05 01:09:32,577 - INFO - [diffusion][Epoch 8638] diffusion learning rate: 0.001
2024-11-05 01:09:32,579 - INFO - [diffusion][Epoch 8638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:32,580 - INFO - [diffusion][Epoch 8639] Epoch 8640/12000
2024-11-05 01:09:36,744 - INFO - [diffusion][Epoch 8639] diffusion training Loss: 0.06849678047001362
2024-11-05 01:09:36,746 - INFO - [diffusion][Epoch 8639] diffusion learning rate: 0.001
2024-11-05 01:09:36,747 - INFO - [diffusion][Epoch 8639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:36,749 - INFO - [diffusion][Epoch 8640] Epoch 8641/12000
2024-11-05 01:09:40,985 - INFO - [diffusion][Epoch 8640] diffusion training Loss: 0.06802039500325918
2024-11-05 01:09:40,987 - INFO - [diffusion][Epoch 8640] diffusion learning rate: 0.001
2024-11-05 01:09:40,989 - INFO - [diffusion][Epoch 8640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:40,991 - INFO - [diffusion][Epoch 8641] Epoch 8642/12000
2024-11-05 01:09:45,307 - INFO - [diffusion][Epoch 8641] diffusion training Loss: 0.07692395150661469
2024-11-05 01:09:45,310 - INFO - [diffusion][Epoch 8641] diffusion learning rate: 0.001
2024-11-05 01:09:45,312 - INFO - [diffusion][Epoch 8641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:45,313 - INFO - [diffusion][Epoch 8642] Epoch 8643/12000
2024-11-05 01:09:49,578 - INFO - [diffusion][Epoch 8642] diffusion training Loss: 0.06615246273577213
2024-11-05 01:09:49,581 - INFO - [diffusion][Epoch 8642] diffusion learning rate: 0.001
2024-11-05 01:09:49,582 - INFO - [diffusion][Epoch 8642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:49,584 - INFO - [diffusion][Epoch 8643] Epoch 8644/12000
2024-11-05 01:09:53,753 - INFO - [diffusion][Epoch 8643] diffusion training Loss: 0.06773717515170574
2024-11-05 01:09:53,755 - INFO - [diffusion][Epoch 8643] diffusion learning rate: 0.001
2024-11-05 01:09:53,757 - INFO - [diffusion][Epoch 8643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:53,759 - INFO - [diffusion][Epoch 8644] Epoch 8645/12000
2024-11-05 01:09:57,901 - INFO - [diffusion][Epoch 8644] diffusion training Loss: 0.0697646951302886
2024-11-05 01:09:57,903 - INFO - [diffusion][Epoch 8644] diffusion learning rate: 0.001
2024-11-05 01:09:57,905 - INFO - [diffusion][Epoch 8644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:57,907 - INFO - [diffusion][Epoch 8645] Epoch 8646/12000
2024-11-05 01:10:02,072 - INFO - [diffusion][Epoch 8645] diffusion training Loss: 0.06674940790981054
2024-11-05 01:10:02,074 - INFO - [diffusion][Epoch 8645] diffusion learning rate: 0.001
2024-11-05 01:10:02,076 - INFO - [diffusion][Epoch 8645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:02,078 - INFO - [diffusion][Epoch 8646] Epoch 8647/12000
2024-11-05 01:10:06,274 - INFO - [diffusion][Epoch 8646] diffusion training Loss: 0.06546389497816563
2024-11-05 01:10:06,276 - INFO - [diffusion][Epoch 8646] diffusion learning rate: 0.001
2024-11-05 01:10:06,278 - INFO - [diffusion][Epoch 8646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:06,280 - INFO - [diffusion][Epoch 8647] Epoch 8648/12000
2024-11-05 01:10:10,580 - INFO - [diffusion][Epoch 8647] diffusion training Loss: 0.06370403617620468
2024-11-05 01:10:10,582 - INFO - [diffusion][Epoch 8647] diffusion learning rate: 0.001
2024-11-05 01:10:10,584 - INFO - [diffusion][Epoch 8647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:10,585 - INFO - [diffusion][Epoch 8648] Epoch 8649/12000
2024-11-05 01:10:14,692 - INFO - [diffusion][Epoch 8648] diffusion training Loss: 0.06903246976435184
2024-11-05 01:10:14,695 - INFO - [diffusion][Epoch 8648] diffusion learning rate: 0.001
2024-11-05 01:10:14,698 - INFO - [diffusion][Epoch 8648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:14,699 - INFO - [diffusion][Epoch 8649] Epoch 8650/12000
2024-11-05 01:10:19,002 - INFO - [diffusion][Epoch 8649] diffusion training Loss: 0.06412936840206385
2024-11-05 01:10:19,004 - INFO - [diffusion][Epoch 8649] diffusion learning rate: 0.001
2024-11-05 01:10:19,006 - INFO - [diffusion][Epoch 8649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:19,007 - INFO - [diffusion][Epoch 8650] Epoch 8651/12000
2024-11-05 01:10:23,301 - INFO - [diffusion][Epoch 8650] diffusion training Loss: 0.06974956393241882
2024-11-05 01:10:23,303 - INFO - [diffusion][Epoch 8650] diffusion learning rate: 0.001
2024-11-05 01:10:23,305 - INFO - [diffusion][Epoch 8650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:23,306 - INFO - [diffusion][Epoch 8651] Epoch 8652/12000
2024-11-05 01:10:27,569 - INFO - [diffusion][Epoch 8651] diffusion training Loss: 0.06440950464457273
2024-11-05 01:10:27,572 - INFO - [diffusion][Epoch 8651] diffusion learning rate: 0.001
2024-11-05 01:10:27,574 - INFO - [diffusion][Epoch 8651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:27,575 - INFO - [diffusion][Epoch 8652] Epoch 8653/12000
2024-11-05 01:10:31,951 - INFO - [diffusion][Epoch 8652] diffusion training Loss: 0.0590491546317935
2024-11-05 01:10:31,953 - INFO - [diffusion][Epoch 8652] diffusion learning rate: 0.001
2024-11-05 01:10:31,955 - INFO - [diffusion][Epoch 8652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:31,957 - INFO - [diffusion][Epoch 8653] Epoch 8654/12000
2024-11-05 01:10:36,267 - INFO - [diffusion][Epoch 8653] diffusion training Loss: 0.061397070065140724
2024-11-05 01:10:36,270 - INFO - [diffusion][Epoch 8653] diffusion learning rate: 0.001
2024-11-05 01:10:36,272 - INFO - [diffusion][Epoch 8653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:36,273 - INFO - [diffusion][Epoch 8654] Epoch 8655/12000
2024-11-05 01:10:40,467 - INFO - [diffusion][Epoch 8654] diffusion training Loss: 0.06905052810907364
2024-11-05 01:10:40,469 - INFO - [diffusion][Epoch 8654] diffusion learning rate: 0.001
2024-11-05 01:10:40,472 - INFO - [diffusion][Epoch 8654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:40,474 - INFO - [diffusion][Epoch 8655] Epoch 8656/12000
2024-11-05 01:10:44,761 - INFO - [diffusion][Epoch 8655] diffusion training Loss: 0.06773331202566624
2024-11-05 01:10:44,763 - INFO - [diffusion][Epoch 8655] diffusion learning rate: 0.001
2024-11-05 01:10:44,765 - INFO - [diffusion][Epoch 8655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:44,767 - INFO - [diffusion][Epoch 8656] Epoch 8657/12000
2024-11-05 01:10:48,903 - INFO - [diffusion][Epoch 8656] diffusion training Loss: 0.06225158926099539
2024-11-05 01:10:48,906 - INFO - [diffusion][Epoch 8656] diffusion learning rate: 0.001
2024-11-05 01:10:48,908 - INFO - [diffusion][Epoch 8656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:48,909 - INFO - [diffusion][Epoch 8657] Epoch 8658/12000
2024-11-05 01:10:53,675 - INFO - [diffusion][Epoch 8657] diffusion training Loss: 0.05841862037777901
2024-11-05 01:10:53,678 - INFO - [diffusion][Epoch 8657] diffusion learning rate: 0.001
2024-11-05 01:10:53,680 - INFO - [diffusion][Epoch 8657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:53,683 - INFO - [diffusion][Epoch 8658] Epoch 8659/12000
2024-11-05 01:10:57,765 - INFO - [diffusion][Epoch 8658] diffusion training Loss: 0.06520074047148228
2024-11-05 01:10:57,767 - INFO - [diffusion][Epoch 8658] diffusion learning rate: 0.001
2024-11-05 01:10:57,768 - INFO - [diffusion][Epoch 8658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:57,770 - INFO - [diffusion][Epoch 8659] Epoch 8660/12000
2024-11-05 01:11:01,938 - INFO - [diffusion][Epoch 8659] diffusion training Loss: 0.06708409823477268
2024-11-05 01:11:01,940 - INFO - [diffusion][Epoch 8659] diffusion learning rate: 0.001
2024-11-05 01:11:01,942 - INFO - [diffusion][Epoch 8659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:01,943 - INFO - [diffusion][Epoch 8660] Epoch 8661/12000
2024-11-05 01:11:06,122 - INFO - [diffusion][Epoch 8660] diffusion training Loss: 0.06689430214464664
2024-11-05 01:11:06,124 - INFO - [diffusion][Epoch 8660] diffusion learning rate: 0.001
2024-11-05 01:11:06,126 - INFO - [diffusion][Epoch 8660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:06,127 - INFO - [diffusion][Epoch 8661] Epoch 8662/12000
2024-11-05 01:11:10,296 - INFO - [diffusion][Epoch 8661] diffusion training Loss: 0.06827452592551708
2024-11-05 01:11:10,298 - INFO - [diffusion][Epoch 8661] diffusion learning rate: 0.001
2024-11-05 01:11:10,300 - INFO - [diffusion][Epoch 8661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:10,301 - INFO - [diffusion][Epoch 8662] Epoch 8663/12000
2024-11-05 01:11:14,333 - INFO - [diffusion][Epoch 8662] diffusion training Loss: 0.07121849805116653
2024-11-05 01:11:14,335 - INFO - [diffusion][Epoch 8662] diffusion learning rate: 0.001
2024-11-05 01:11:14,337 - INFO - [diffusion][Epoch 8662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:14,339 - INFO - [diffusion][Epoch 8663] Epoch 8664/12000
2024-11-05 01:11:18,486 - INFO - [diffusion][Epoch 8663] diffusion training Loss: 0.07099864818155766
2024-11-05 01:11:18,488 - INFO - [diffusion][Epoch 8663] diffusion learning rate: 0.001
2024-11-05 01:11:18,490 - INFO - [diffusion][Epoch 8663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:18,492 - INFO - [diffusion][Epoch 8664] Epoch 8665/12000
2024-11-05 01:11:22,720 - INFO - [diffusion][Epoch 8664] diffusion training Loss: 0.0639646453782916
2024-11-05 01:11:22,722 - INFO - [diffusion][Epoch 8664] diffusion learning rate: 0.001
2024-11-05 01:11:22,724 - INFO - [diffusion][Epoch 8664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:22,726 - INFO - [diffusion][Epoch 8665] Epoch 8666/12000
2024-11-05 01:11:26,976 - INFO - [diffusion][Epoch 8665] diffusion training Loss: 0.06206085346639156
2024-11-05 01:11:26,978 - INFO - [diffusion][Epoch 8665] diffusion learning rate: 0.001
2024-11-05 01:11:26,980 - INFO - [diffusion][Epoch 8665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:26,981 - INFO - [diffusion][Epoch 8666] Epoch 8667/12000
2024-11-05 01:11:31,290 - INFO - [diffusion][Epoch 8666] diffusion training Loss: 0.0629332847893238
2024-11-05 01:11:31,292 - INFO - [diffusion][Epoch 8666] diffusion learning rate: 0.001
2024-11-05 01:11:31,295 - INFO - [diffusion][Epoch 8666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:31,296 - INFO - [diffusion][Epoch 8667] Epoch 8668/12000
2024-11-05 01:11:35,553 - INFO - [diffusion][Epoch 8667] diffusion training Loss: 0.06696647591888905
2024-11-05 01:11:35,555 - INFO - [diffusion][Epoch 8667] diffusion learning rate: 0.001
2024-11-05 01:11:35,661 - INFO - [diffusion][Epoch 8667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:35,663 - INFO - [diffusion][Epoch 8668] Epoch 8669/12000
2024-11-05 01:11:39,909 - INFO - [diffusion][Epoch 8668] diffusion training Loss: 0.06480108015239239
2024-11-05 01:11:39,911 - INFO - [diffusion][Epoch 8668] diffusion learning rate: 0.001
2024-11-05 01:11:39,913 - INFO - [diffusion][Epoch 8668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:39,915 - INFO - [diffusion][Epoch 8669] Epoch 8670/12000
2024-11-05 01:11:44,235 - INFO - [diffusion][Epoch 8669] diffusion training Loss: 0.06179139204323292
2024-11-05 01:11:44,238 - INFO - [diffusion][Epoch 8669] diffusion learning rate: 0.001
2024-11-05 01:11:44,240 - INFO - [diffusion][Epoch 8669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:44,241 - INFO - [diffusion][Epoch 8670] Epoch 8671/12000
2024-11-05 01:11:48,498 - INFO - [diffusion][Epoch 8670] diffusion training Loss: 0.06786426529288292
2024-11-05 01:11:48,500 - INFO - [diffusion][Epoch 8670] diffusion learning rate: 0.001
2024-11-05 01:11:48,502 - INFO - [diffusion][Epoch 8670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:48,504 - INFO - [diffusion][Epoch 8671] Epoch 8672/12000
2024-11-05 01:11:52,858 - INFO - [diffusion][Epoch 8671] diffusion training Loss: 0.06266168877482414
2024-11-05 01:11:52,860 - INFO - [diffusion][Epoch 8671] diffusion learning rate: 0.001
2024-11-05 01:11:52,937 - INFO - [diffusion][Epoch 8671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:52,938 - INFO - [diffusion][Epoch 8672] Epoch 8673/12000
2024-11-05 01:11:57,051 - INFO - [diffusion][Epoch 8672] diffusion training Loss: 0.06701336987316608
2024-11-05 01:11:57,053 - INFO - [diffusion][Epoch 8672] diffusion learning rate: 0.001
2024-11-05 01:11:57,055 - INFO - [diffusion][Epoch 8672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:57,057 - INFO - [diffusion][Epoch 8673] Epoch 8674/12000
2024-11-05 01:12:01,263 - INFO - [diffusion][Epoch 8673] diffusion training Loss: 0.06703076139092445
2024-11-05 01:12:01,265 - INFO - [diffusion][Epoch 8673] diffusion learning rate: 0.001
2024-11-05 01:12:01,266 - INFO - [diffusion][Epoch 8673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:01,268 - INFO - [diffusion][Epoch 8674] Epoch 8675/12000
2024-11-05 01:12:05,497 - INFO - [diffusion][Epoch 8674] diffusion training Loss: 0.06646705977618694
2024-11-05 01:12:05,500 - INFO - [diffusion][Epoch 8674] diffusion learning rate: 0.001
2024-11-05 01:12:05,502 - INFO - [diffusion][Epoch 8674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:05,503 - INFO - [diffusion][Epoch 8675] Epoch 8676/12000
2024-11-05 01:12:09,676 - INFO - [diffusion][Epoch 8675] diffusion training Loss: 0.06374903675168753
2024-11-05 01:12:09,679 - INFO - [diffusion][Epoch 8675] diffusion learning rate: 0.001
2024-11-05 01:12:09,681 - INFO - [diffusion][Epoch 8675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:09,683 - INFO - [diffusion][Epoch 8676] Epoch 8677/12000
2024-11-05 01:12:13,865 - INFO - [diffusion][Epoch 8676] diffusion training Loss: 0.06380057707428932
2024-11-05 01:12:13,867 - INFO - [diffusion][Epoch 8676] diffusion learning rate: 0.001
2024-11-05 01:12:13,870 - INFO - [diffusion][Epoch 8676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:13,871 - INFO - [diffusion][Epoch 8677] Epoch 8678/12000
2024-11-05 01:12:17,905 - INFO - [diffusion][Epoch 8677] diffusion training Loss: 0.06613445654511452
2024-11-05 01:12:17,907 - INFO - [diffusion][Epoch 8677] diffusion learning rate: 0.001
2024-11-05 01:12:17,909 - INFO - [diffusion][Epoch 8677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:17,910 - INFO - [diffusion][Epoch 8678] Epoch 8679/12000
2024-11-05 01:12:22,317 - INFO - [diffusion][Epoch 8678] diffusion training Loss: 0.06315801478922367
2024-11-05 01:12:22,319 - INFO - [diffusion][Epoch 8678] diffusion learning rate: 0.001
2024-11-05 01:12:22,322 - INFO - [diffusion][Epoch 8678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:22,323 - INFO - [diffusion][Epoch 8679] Epoch 8680/12000
2024-11-05 01:12:26,667 - INFO - [diffusion][Epoch 8679] diffusion training Loss: 0.06975622661411762
2024-11-05 01:12:26,669 - INFO - [diffusion][Epoch 8679] diffusion learning rate: 0.001
2024-11-05 01:12:26,672 - INFO - [diffusion][Epoch 8679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:26,673 - INFO - [diffusion][Epoch 8680] Epoch 8681/12000
2024-11-05 01:12:30,913 - INFO - [diffusion][Epoch 8680] diffusion training Loss: 0.061711231246590614
2024-11-05 01:12:30,915 - INFO - [diffusion][Epoch 8680] diffusion learning rate: 0.001
2024-11-05 01:12:30,916 - INFO - [diffusion][Epoch 8680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:30,918 - INFO - [diffusion][Epoch 8681] Epoch 8682/12000
2024-11-05 01:12:35,192 - INFO - [diffusion][Epoch 8681] diffusion training Loss: 0.0676925927400589
2024-11-05 01:12:35,194 - INFO - [diffusion][Epoch 8681] diffusion learning rate: 0.001
2024-11-05 01:12:35,196 - INFO - [diffusion][Epoch 8681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:35,198 - INFO - [diffusion][Epoch 8682] Epoch 8683/12000
2024-11-05 01:12:39,528 - INFO - [diffusion][Epoch 8682] diffusion training Loss: 0.07488606497645378
2024-11-05 01:12:39,530 - INFO - [diffusion][Epoch 8682] diffusion learning rate: 0.001
2024-11-05 01:12:39,532 - INFO - [diffusion][Epoch 8682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:39,533 - INFO - [diffusion][Epoch 8683] Epoch 8684/12000
2024-11-05 01:12:43,722 - INFO - [diffusion][Epoch 8683] diffusion training Loss: 0.06932943873107433
2024-11-05 01:12:43,724 - INFO - [diffusion][Epoch 8683] diffusion learning rate: 0.001
2024-11-05 01:12:43,726 - INFO - [diffusion][Epoch 8683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:43,727 - INFO - [diffusion][Epoch 8684] Epoch 8685/12000
2024-11-05 01:12:47,941 - INFO - [diffusion][Epoch 8684] diffusion training Loss: 0.06538431160151958
2024-11-05 01:12:47,943 - INFO - [diffusion][Epoch 8684] diffusion learning rate: 0.001
2024-11-05 01:12:47,945 - INFO - [diffusion][Epoch 8684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:47,946 - INFO - [diffusion][Epoch 8685] Epoch 8686/12000
2024-11-05 01:12:52,144 - INFO - [diffusion][Epoch 8685] diffusion training Loss: 0.06902816705405712
2024-11-05 01:12:52,146 - INFO - [diffusion][Epoch 8685] diffusion learning rate: 0.001
2024-11-05 01:12:52,148 - INFO - [diffusion][Epoch 8685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:52,150 - INFO - [diffusion][Epoch 8686] Epoch 8687/12000
2024-11-05 01:12:56,340 - INFO - [diffusion][Epoch 8686] diffusion training Loss: 0.0719776563346386
2024-11-05 01:12:56,342 - INFO - [diffusion][Epoch 8686] diffusion learning rate: 0.001
2024-11-05 01:12:56,343 - INFO - [diffusion][Epoch 8686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:56,345 - INFO - [diffusion][Epoch 8687] Epoch 8688/12000
2024-11-05 01:13:00,597 - INFO - [diffusion][Epoch 8687] diffusion training Loss: 0.06731914263218641
2024-11-05 01:13:00,599 - INFO - [diffusion][Epoch 8687] diffusion learning rate: 0.001
2024-11-05 01:13:00,601 - INFO - [diffusion][Epoch 8687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:00,602 - INFO - [diffusion][Epoch 8688] Epoch 8689/12000
2024-11-05 01:13:04,817 - INFO - [diffusion][Epoch 8688] diffusion training Loss: 0.06313325185328722
2024-11-05 01:13:04,819 - INFO - [diffusion][Epoch 8688] diffusion learning rate: 0.001
2024-11-05 01:13:04,820 - INFO - [diffusion][Epoch 8688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:04,822 - INFO - [diffusion][Epoch 8689] Epoch 8690/12000
2024-11-05 01:13:09,116 - INFO - [diffusion][Epoch 8689] diffusion training Loss: 0.06642585713416338
2024-11-05 01:13:09,119 - INFO - [diffusion][Epoch 8689] diffusion learning rate: 0.001
2024-11-05 01:13:09,121 - INFO - [diffusion][Epoch 8689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:09,122 - INFO - [diffusion][Epoch 8690] Epoch 8691/12000
2024-11-05 01:13:13,411 - INFO - [diffusion][Epoch 8690] diffusion training Loss: 0.06144085805863142
2024-11-05 01:13:13,414 - INFO - [diffusion][Epoch 8690] diffusion learning rate: 0.001
2024-11-05 01:13:13,415 - INFO - [diffusion][Epoch 8690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:13,417 - INFO - [diffusion][Epoch 8691] Epoch 8692/12000
2024-11-05 01:13:17,616 - INFO - [diffusion][Epoch 8691] diffusion training Loss: 0.06312383525073528
2024-11-05 01:13:17,620 - INFO - [diffusion][Epoch 8691] diffusion learning rate: 0.001
2024-11-05 01:13:17,622 - INFO - [diffusion][Epoch 8691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:17,624 - INFO - [diffusion][Epoch 8692] Epoch 8693/12000
2024-11-05 01:13:21,760 - INFO - [diffusion][Epoch 8692] diffusion training Loss: 0.06693070661276579
2024-11-05 01:13:21,762 - INFO - [diffusion][Epoch 8692] diffusion learning rate: 0.001
2024-11-05 01:13:21,764 - INFO - [diffusion][Epoch 8692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:21,765 - INFO - [diffusion][Epoch 8693] Epoch 8694/12000
2024-11-05 01:13:25,570 - INFO - [diffusion][Epoch 8693] diffusion training Loss: 0.07359477505087852
2024-11-05 01:13:25,572 - INFO - [diffusion][Epoch 8693] diffusion learning rate: 0.001
2024-11-05 01:13:25,621 - INFO - [diffusion][Epoch 8693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:25,622 - INFO - [diffusion][Epoch 8694] Epoch 8695/12000
2024-11-05 01:13:29,821 - INFO - [diffusion][Epoch 8694] diffusion training Loss: 0.06275773793458939
2024-11-05 01:13:29,823 - INFO - [diffusion][Epoch 8694] diffusion learning rate: 0.001
2024-11-05 01:13:29,825 - INFO - [diffusion][Epoch 8694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:29,826 - INFO - [diffusion][Epoch 8695] Epoch 8696/12000
2024-11-05 01:13:34,068 - INFO - [diffusion][Epoch 8695] diffusion training Loss: 0.07054498698562384
2024-11-05 01:13:34,070 - INFO - [diffusion][Epoch 8695] diffusion learning rate: 0.001
2024-11-05 01:13:34,072 - INFO - [diffusion][Epoch 8695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:34,073 - INFO - [diffusion][Epoch 8696] Epoch 8697/12000
2024-11-05 01:13:38,283 - INFO - [diffusion][Epoch 8696] diffusion training Loss: 0.06935526430606842
2024-11-05 01:13:38,285 - INFO - [diffusion][Epoch 8696] diffusion learning rate: 0.001
2024-11-05 01:13:38,287 - INFO - [diffusion][Epoch 8696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:38,288 - INFO - [diffusion][Epoch 8697] Epoch 8698/12000
2024-11-05 01:13:42,560 - INFO - [diffusion][Epoch 8697] diffusion training Loss: 0.07395157217979431
2024-11-05 01:13:42,562 - INFO - [diffusion][Epoch 8697] diffusion learning rate: 0.001
2024-11-05 01:13:42,564 - INFO - [diffusion][Epoch 8697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:42,565 - INFO - [diffusion][Epoch 8698] Epoch 8699/12000
2024-11-05 01:13:47,308 - INFO - [diffusion][Epoch 8698] diffusion training Loss: 0.06321527250111103
2024-11-05 01:13:47,310 - INFO - [diffusion][Epoch 8698] diffusion learning rate: 0.001
2024-11-05 01:13:47,311 - INFO - [diffusion][Epoch 8698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:47,313 - INFO - [diffusion][Epoch 8699] Epoch 8700/12000
2024-11-05 01:13:51,469 - INFO - [diffusion][Epoch 8699] diffusion training Loss: 0.07047707587480545
2024-11-05 01:13:51,471 - INFO - [diffusion][Epoch 8699] diffusion learning rate: 0.001
2024-11-05 01:13:51,473 - INFO - [diffusion][Epoch 8699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:51,474 - INFO - [diffusion][Epoch 8700] Epoch 8701/12000
2024-11-05 01:13:55,575 - INFO - [diffusion][Epoch 8700] diffusion training Loss: 0.07079773396253586
2024-11-05 01:13:55,578 - INFO - [diffusion][Epoch 8700] diffusion learning rate: 0.001
2024-11-05 01:13:55,580 - INFO - [diffusion][Epoch 8700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:55,581 - INFO - [diffusion][Epoch 8701] Epoch 8702/12000
2024-11-05 01:13:59,632 - INFO - [diffusion][Epoch 8701] diffusion training Loss: 0.06522417906671762
2024-11-05 01:13:59,633 - INFO - [diffusion][Epoch 8701] diffusion learning rate: 0.001
2024-11-05 01:13:59,635 - INFO - [diffusion][Epoch 8701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:59,636 - INFO - [diffusion][Epoch 8702] Epoch 8703/12000
2024-11-05 01:14:03,827 - INFO - [diffusion][Epoch 8702] diffusion training Loss: 0.06411795038729906
2024-11-05 01:14:03,829 - INFO - [diffusion][Epoch 8702] diffusion learning rate: 0.001
2024-11-05 01:14:03,831 - INFO - [diffusion][Epoch 8702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:03,832 - INFO - [diffusion][Epoch 8703] Epoch 8704/12000
2024-11-05 01:14:08,083 - INFO - [diffusion][Epoch 8703] diffusion training Loss: 0.060836534947156906
2024-11-05 01:14:08,085 - INFO - [diffusion][Epoch 8703] diffusion learning rate: 0.001
2024-11-05 01:14:08,087 - INFO - [diffusion][Epoch 8703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:08,088 - INFO - [diffusion][Epoch 8704] Epoch 8705/12000
2024-11-05 01:14:12,195 - INFO - [diffusion][Epoch 8704] diffusion training Loss: 0.06750387232750654
2024-11-05 01:14:12,197 - INFO - [diffusion][Epoch 8704] diffusion learning rate: 0.001
2024-11-05 01:14:12,199 - INFO - [diffusion][Epoch 8704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:12,200 - INFO - [diffusion][Epoch 8705] Epoch 8706/12000
2024-11-05 01:14:16,387 - INFO - [diffusion][Epoch 8705] diffusion training Loss: 0.06517762411385775
2024-11-05 01:14:16,389 - INFO - [diffusion][Epoch 8705] diffusion learning rate: 0.001
2024-11-05 01:14:16,391 - INFO - [diffusion][Epoch 8705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:16,392 - INFO - [diffusion][Epoch 8706] Epoch 8707/12000
2024-11-05 01:14:20,556 - INFO - [diffusion][Epoch 8706] diffusion training Loss: 0.06575736030936241
2024-11-05 01:14:20,558 - INFO - [diffusion][Epoch 8706] diffusion learning rate: 0.001
2024-11-05 01:14:20,560 - INFO - [diffusion][Epoch 8706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:20,561 - INFO - [diffusion][Epoch 8707] Epoch 8708/12000
2024-11-05 01:14:24,653 - INFO - [diffusion][Epoch 8707] diffusion training Loss: 0.06716272979974747
2024-11-05 01:14:24,655 - INFO - [diffusion][Epoch 8707] diffusion learning rate: 0.001
2024-11-05 01:14:24,657 - INFO - [diffusion][Epoch 8707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:24,658 - INFO - [diffusion][Epoch 8708] Epoch 8709/12000
2024-11-05 01:14:28,743 - INFO - [diffusion][Epoch 8708] diffusion training Loss: 0.0650932602584362
2024-11-05 01:14:28,745 - INFO - [diffusion][Epoch 8708] diffusion learning rate: 0.001
2024-11-05 01:14:28,747 - INFO - [diffusion][Epoch 8708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:28,748 - INFO - [diffusion][Epoch 8709] Epoch 8710/12000
2024-11-05 01:14:32,749 - INFO - [diffusion][Epoch 8709] diffusion training Loss: 0.06927454378455877
2024-11-05 01:14:32,751 - INFO - [diffusion][Epoch 8709] diffusion learning rate: 0.001
2024-11-05 01:14:32,753 - INFO - [diffusion][Epoch 8709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:32,755 - INFO - [diffusion][Epoch 8710] Epoch 8711/12000
2024-11-05 01:14:36,802 - INFO - [diffusion][Epoch 8710] diffusion training Loss: 0.06081773899495602
2024-11-05 01:14:36,804 - INFO - [diffusion][Epoch 8710] diffusion learning rate: 0.001
2024-11-05 01:14:36,806 - INFO - [diffusion][Epoch 8710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:36,807 - INFO - [diffusion][Epoch 8711] Epoch 8712/12000
2024-11-05 01:14:40,914 - INFO - [diffusion][Epoch 8711] diffusion training Loss: 0.06521646492183208
2024-11-05 01:14:40,918 - INFO - [diffusion][Epoch 8711] diffusion learning rate: 0.001
2024-11-05 01:14:40,920 - INFO - [diffusion][Epoch 8711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:40,922 - INFO - [diffusion][Epoch 8712] Epoch 8713/12000
2024-11-05 01:14:45,014 - INFO - [diffusion][Epoch 8712] diffusion training Loss: 0.06716890446841717
2024-11-05 01:14:45,016 - INFO - [diffusion][Epoch 8712] diffusion learning rate: 0.001
2024-11-05 01:14:45,018 - INFO - [diffusion][Epoch 8712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:45,019 - INFO - [diffusion][Epoch 8713] Epoch 8714/12000
2024-11-05 01:14:49,115 - INFO - [diffusion][Epoch 8713] diffusion training Loss: 0.0687296288087964
2024-11-05 01:14:49,118 - INFO - [diffusion][Epoch 8713] diffusion learning rate: 0.001
2024-11-05 01:14:49,121 - INFO - [diffusion][Epoch 8713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:49,122 - INFO - [diffusion][Epoch 8714] Epoch 8715/12000
2024-11-05 01:14:53,226 - INFO - [diffusion][Epoch 8714] diffusion training Loss: 0.0690491572022438
2024-11-05 01:14:53,228 - INFO - [diffusion][Epoch 8714] diffusion learning rate: 0.001
2024-11-05 01:14:53,230 - INFO - [diffusion][Epoch 8714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:53,231 - INFO - [diffusion][Epoch 8715] Epoch 8716/12000
2024-11-05 01:14:57,269 - INFO - [diffusion][Epoch 8715] diffusion training Loss: 0.06871325895190239
2024-11-05 01:14:57,271 - INFO - [diffusion][Epoch 8715] diffusion learning rate: 0.001
2024-11-05 01:14:57,272 - INFO - [diffusion][Epoch 8715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:57,273 - INFO - [diffusion][Epoch 8716] Epoch 8717/12000
2024-11-05 01:15:01,397 - INFO - [diffusion][Epoch 8716] diffusion training Loss: 0.06851087510585785
2024-11-05 01:15:01,399 - INFO - [diffusion][Epoch 8716] diffusion learning rate: 0.001
2024-11-05 01:15:01,401 - INFO - [diffusion][Epoch 8716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:01,402 - INFO - [diffusion][Epoch 8717] Epoch 8718/12000
2024-11-05 01:15:05,590 - INFO - [diffusion][Epoch 8717] diffusion training Loss: 0.06310653500258923
2024-11-05 01:15:05,592 - INFO - [diffusion][Epoch 8717] diffusion learning rate: 0.001
2024-11-05 01:15:05,593 - INFO - [diffusion][Epoch 8717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:05,595 - INFO - [diffusion][Epoch 8718] Epoch 8719/12000
2024-11-05 01:15:09,701 - INFO - [diffusion][Epoch 8718] diffusion training Loss: 0.06328821089118719
2024-11-05 01:15:09,703 - INFO - [diffusion][Epoch 8718] diffusion learning rate: 0.001
2024-11-05 01:15:09,704 - INFO - [diffusion][Epoch 8718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:09,706 - INFO - [diffusion][Epoch 8719] Epoch 8720/12000
2024-11-05 01:15:13,951 - INFO - [diffusion][Epoch 8719] diffusion training Loss: 0.06671649776399136
2024-11-05 01:15:13,953 - INFO - [diffusion][Epoch 8719] diffusion learning rate: 0.001
2024-11-05 01:15:13,955 - INFO - [diffusion][Epoch 8719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:13,956 - INFO - [diffusion][Epoch 8720] Epoch 8721/12000
2024-11-05 01:15:18,254 - INFO - [diffusion][Epoch 8720] diffusion training Loss: 0.06809159740805626
2024-11-05 01:15:18,256 - INFO - [diffusion][Epoch 8720] diffusion learning rate: 0.001
2024-11-05 01:15:18,258 - INFO - [diffusion][Epoch 8720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:18,259 - INFO - [diffusion][Epoch 8721] Epoch 8722/12000
2024-11-05 01:15:22,471 - INFO - [diffusion][Epoch 8721] diffusion training Loss: 0.06358758173882961
2024-11-05 01:15:22,473 - INFO - [diffusion][Epoch 8721] diffusion learning rate: 0.001
2024-11-05 01:15:22,475 - INFO - [diffusion][Epoch 8721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:22,476 - INFO - [diffusion][Epoch 8722] Epoch 8723/12000
2024-11-05 01:15:26,638 - INFO - [diffusion][Epoch 8722] diffusion training Loss: 0.06455499865114689
2024-11-05 01:15:26,640 - INFO - [diffusion][Epoch 8722] diffusion learning rate: 0.001
2024-11-05 01:15:26,641 - INFO - [diffusion][Epoch 8722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:26,643 - INFO - [diffusion][Epoch 8723] Epoch 8724/12000
2024-11-05 01:15:30,941 - INFO - [diffusion][Epoch 8723] diffusion training Loss: 0.0691585186868906
2024-11-05 01:15:30,943 - INFO - [diffusion][Epoch 8723] diffusion learning rate: 0.001
2024-11-05 01:15:30,945 - INFO - [diffusion][Epoch 8723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:30,947 - INFO - [diffusion][Epoch 8724] Epoch 8725/12000
2024-11-05 01:15:35,062 - INFO - [diffusion][Epoch 8724] diffusion training Loss: 0.0665268525481224
2024-11-05 01:15:35,064 - INFO - [diffusion][Epoch 8724] diffusion learning rate: 0.001
2024-11-05 01:15:35,066 - INFO - [diffusion][Epoch 8724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:35,067 - INFO - [diffusion][Epoch 8725] Epoch 8726/12000
2024-11-05 01:15:39,174 - INFO - [diffusion][Epoch 8725] diffusion training Loss: 0.0704058688133955
2024-11-05 01:15:39,209 - INFO - [diffusion][Epoch 8725] diffusion learning rate: 0.001
2024-11-05 01:15:39,211 - INFO - [diffusion][Epoch 8725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:39,212 - INFO - [diffusion][Epoch 8726] Epoch 8727/12000
2024-11-05 01:15:43,236 - INFO - [diffusion][Epoch 8726] diffusion training Loss: 0.06496097333729267
2024-11-05 01:15:43,239 - INFO - [diffusion][Epoch 8726] diffusion learning rate: 0.001
2024-11-05 01:15:43,241 - INFO - [diffusion][Epoch 8726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:43,242 - INFO - [diffusion][Epoch 8727] Epoch 8728/12000
2024-11-05 01:15:47,332 - INFO - [diffusion][Epoch 8727] diffusion training Loss: 0.06413038447499275
2024-11-05 01:15:47,335 - INFO - [diffusion][Epoch 8727] diffusion learning rate: 0.001
2024-11-05 01:15:47,337 - INFO - [diffusion][Epoch 8727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:47,338 - INFO - [diffusion][Epoch 8728] Epoch 8729/12000
2024-11-05 01:15:51,330 - INFO - [diffusion][Epoch 8728] diffusion training Loss: 0.07010023109614849
2024-11-05 01:15:51,331 - INFO - [diffusion][Epoch 8728] diffusion learning rate: 0.001
2024-11-05 01:15:51,333 - INFO - [diffusion][Epoch 8728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:51,335 - INFO - [diffusion][Epoch 8729] Epoch 8730/12000
2024-11-05 01:15:55,527 - INFO - [diffusion][Epoch 8729] diffusion training Loss: 0.0692950189113617
2024-11-05 01:15:55,529 - INFO - [diffusion][Epoch 8729] diffusion learning rate: 0.001
2024-11-05 01:15:55,532 - INFO - [diffusion][Epoch 8729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:55,533 - INFO - [diffusion][Epoch 8730] Epoch 8731/12000
2024-11-05 01:15:59,737 - INFO - [diffusion][Epoch 8730] diffusion training Loss: 0.06377569306641817
2024-11-05 01:15:59,740 - INFO - [diffusion][Epoch 8730] diffusion learning rate: 0.001
2024-11-05 01:15:59,741 - INFO - [diffusion][Epoch 8730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:59,743 - INFO - [diffusion][Epoch 8731] Epoch 8732/12000
2024-11-05 01:16:03,773 - INFO - [diffusion][Epoch 8731] diffusion training Loss: 0.06525126192718744
2024-11-05 01:16:03,775 - INFO - [diffusion][Epoch 8731] diffusion learning rate: 0.001
2024-11-05 01:16:03,777 - INFO - [diffusion][Epoch 8731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:03,778 - INFO - [diffusion][Epoch 8732] Epoch 8733/12000
2024-11-05 01:16:07,810 - INFO - [diffusion][Epoch 8732] diffusion training Loss: 0.06468375213444233
2024-11-05 01:16:07,813 - INFO - [diffusion][Epoch 8732] diffusion learning rate: 0.001
2024-11-05 01:16:07,815 - INFO - [diffusion][Epoch 8732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:07,816 - INFO - [diffusion][Epoch 8733] Epoch 8734/12000
2024-11-05 01:16:12,023 - INFO - [diffusion][Epoch 8733] diffusion training Loss: 0.06637846305966377
2024-11-05 01:16:12,025 - INFO - [diffusion][Epoch 8733] diffusion learning rate: 0.001
2024-11-05 01:16:12,027 - INFO - [diffusion][Epoch 8733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:12,028 - INFO - [diffusion][Epoch 8734] Epoch 8735/12000
2024-11-05 01:16:16,320 - INFO - [diffusion][Epoch 8734] diffusion training Loss: 0.06601797044277191
2024-11-05 01:16:16,322 - INFO - [diffusion][Epoch 8734] diffusion learning rate: 0.001
2024-11-05 01:16:16,324 - INFO - [diffusion][Epoch 8734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:16,325 - INFO - [diffusion][Epoch 8735] Epoch 8736/12000
2024-11-05 01:16:20,592 - INFO - [diffusion][Epoch 8735] diffusion training Loss: 0.0645179245620966
2024-11-05 01:16:20,595 - INFO - [diffusion][Epoch 8735] diffusion learning rate: 0.001
2024-11-05 01:16:20,596 - INFO - [diffusion][Epoch 8735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:20,597 - INFO - [diffusion][Epoch 8736] Epoch 8737/12000
2024-11-05 01:16:24,706 - INFO - [diffusion][Epoch 8736] diffusion training Loss: 0.061131179332733154
2024-11-05 01:16:24,708 - INFO - [diffusion][Epoch 8736] diffusion learning rate: 0.001
2024-11-05 01:16:24,710 - INFO - [diffusion][Epoch 8736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:24,712 - INFO - [diffusion][Epoch 8737] Epoch 8738/12000
2024-11-05 01:16:28,851 - INFO - [diffusion][Epoch 8737] diffusion training Loss: 0.0616205632686615
2024-11-05 01:16:28,853 - INFO - [diffusion][Epoch 8737] diffusion learning rate: 0.001
2024-11-05 01:16:28,856 - INFO - [diffusion][Epoch 8737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:28,857 - INFO - [diffusion][Epoch 8738] Epoch 8739/12000
2024-11-05 01:16:32,903 - INFO - [diffusion][Epoch 8738] diffusion training Loss: 0.06161720398813486
2024-11-05 01:16:32,905 - INFO - [diffusion][Epoch 8738] diffusion learning rate: 0.001
2024-11-05 01:16:32,906 - INFO - [diffusion][Epoch 8738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:32,908 - INFO - [diffusion][Epoch 8739] Epoch 8740/12000
2024-11-05 01:16:36,996 - INFO - [diffusion][Epoch 8739] diffusion training Loss: 0.06753778085112572
2024-11-05 01:16:37,009 - INFO - [diffusion][Epoch 8739] diffusion learning rate: 0.001
2024-11-05 01:16:37,011 - INFO - [diffusion][Epoch 8739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:37,012 - INFO - [diffusion][Epoch 8740] Epoch 8741/12000
2024-11-05 01:16:41,711 - INFO - [diffusion][Epoch 8740] diffusion training Loss: 0.06505951099097729
2024-11-05 01:16:41,713 - INFO - [diffusion][Epoch 8740] diffusion learning rate: 0.001
2024-11-05 01:16:41,714 - INFO - [diffusion][Epoch 8740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:41,715 - INFO - [diffusion][Epoch 8741] Epoch 8742/12000
2024-11-05 01:16:45,713 - INFO - [diffusion][Epoch 8741] diffusion training Loss: 0.06396234687417746
2024-11-05 01:16:45,716 - INFO - [diffusion][Epoch 8741] diffusion learning rate: 0.001
2024-11-05 01:16:45,718 - INFO - [diffusion][Epoch 8741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:45,720 - INFO - [diffusion][Epoch 8742] Epoch 8743/12000
2024-11-05 01:16:49,843 - INFO - [diffusion][Epoch 8742] diffusion training Loss: 0.0687957163900137
2024-11-05 01:16:49,845 - INFO - [diffusion][Epoch 8742] diffusion learning rate: 0.001
2024-11-05 01:16:49,847 - INFO - [diffusion][Epoch 8742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:49,848 - INFO - [diffusion][Epoch 8743] Epoch 8744/12000
2024-11-05 01:16:53,994 - INFO - [diffusion][Epoch 8743] diffusion training Loss: 0.06663101073354483
2024-11-05 01:16:53,996 - INFO - [diffusion][Epoch 8743] diffusion learning rate: 0.001
2024-11-05 01:16:53,998 - INFO - [diffusion][Epoch 8743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:53,999 - INFO - [diffusion][Epoch 8744] Epoch 8745/12000
2024-11-05 01:16:58,072 - INFO - [diffusion][Epoch 8744] diffusion training Loss: 0.07216674834489822
2024-11-05 01:16:58,074 - INFO - [diffusion][Epoch 8744] diffusion learning rate: 0.001
2024-11-05 01:16:58,076 - INFO - [diffusion][Epoch 8744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:58,077 - INFO - [diffusion][Epoch 8745] Epoch 8746/12000
2024-11-05 01:17:02,123 - INFO - [diffusion][Epoch 8745] diffusion training Loss: 0.06999227404594421
2024-11-05 01:17:02,125 - INFO - [diffusion][Epoch 8745] diffusion learning rate: 0.001
2024-11-05 01:17:02,127 - INFO - [diffusion][Epoch 8745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:02,128 - INFO - [diffusion][Epoch 8746] Epoch 8747/12000
2024-11-05 01:17:06,371 - INFO - [diffusion][Epoch 8746] diffusion training Loss: 0.06661153305321932
2024-11-05 01:17:06,373 - INFO - [diffusion][Epoch 8746] diffusion learning rate: 0.001
2024-11-05 01:17:06,374 - INFO - [diffusion][Epoch 8746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:06,376 - INFO - [diffusion][Epoch 8747] Epoch 8748/12000
2024-11-05 01:17:10,454 - INFO - [diffusion][Epoch 8747] diffusion training Loss: 0.06539497897028923
2024-11-05 01:17:10,456 - INFO - [diffusion][Epoch 8747] diffusion learning rate: 0.001
2024-11-05 01:17:10,458 - INFO - [diffusion][Epoch 8747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:10,459 - INFO - [diffusion][Epoch 8748] Epoch 8749/12000
2024-11-05 01:17:14,614 - INFO - [diffusion][Epoch 8748] diffusion training Loss: 0.07078776508569717
2024-11-05 01:17:14,616 - INFO - [diffusion][Epoch 8748] diffusion learning rate: 0.001
2024-11-05 01:17:14,617 - INFO - [diffusion][Epoch 8748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:14,619 - INFO - [diffusion][Epoch 8749] Epoch 8750/12000
2024-11-05 01:17:18,719 - INFO - [diffusion][Epoch 8749] diffusion training Loss: 0.06559209153056145
2024-11-05 01:17:18,721 - INFO - [diffusion][Epoch 8749] diffusion learning rate: 0.001
2024-11-05 01:17:18,724 - INFO - [diffusion][Epoch 8749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:18,725 - INFO - [diffusion][Epoch 8750] Epoch 8751/12000
2024-11-05 01:17:22,927 - INFO - [diffusion][Epoch 8750] diffusion training Loss: 0.0668261693790555
2024-11-05 01:17:22,929 - INFO - [diffusion][Epoch 8750] diffusion learning rate: 0.001
2024-11-05 01:17:22,931 - INFO - [diffusion][Epoch 8750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:22,932 - INFO - [diffusion][Epoch 8751] Epoch 8752/12000
2024-11-05 01:17:27,038 - INFO - [diffusion][Epoch 8751] diffusion training Loss: 0.06259091757237911
2024-11-05 01:17:27,040 - INFO - [diffusion][Epoch 8751] diffusion learning rate: 0.001
2024-11-05 01:17:27,042 - INFO - [diffusion][Epoch 8751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:27,043 - INFO - [diffusion][Epoch 8752] Epoch 8753/12000
2024-11-05 01:17:31,233 - INFO - [diffusion][Epoch 8752] diffusion training Loss: 0.06138109415769577
2024-11-05 01:17:31,235 - INFO - [diffusion][Epoch 8752] diffusion learning rate: 0.001
2024-11-05 01:17:31,237 - INFO - [diffusion][Epoch 8752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:31,238 - INFO - [diffusion][Epoch 8753] Epoch 8754/12000
2024-11-05 01:17:35,373 - INFO - [diffusion][Epoch 8753] diffusion training Loss: 0.06653543282300234
2024-11-05 01:17:35,375 - INFO - [diffusion][Epoch 8753] diffusion learning rate: 0.001
2024-11-05 01:17:35,377 - INFO - [diffusion][Epoch 8753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:35,379 - INFO - [diffusion][Epoch 8754] Epoch 8755/12000
2024-11-05 01:17:39,551 - INFO - [diffusion][Epoch 8754] diffusion training Loss: 0.0633986173197627
2024-11-05 01:17:39,553 - INFO - [diffusion][Epoch 8754] diffusion learning rate: 0.001
2024-11-05 01:17:39,555 - INFO - [diffusion][Epoch 8754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:39,557 - INFO - [diffusion][Epoch 8755] Epoch 8756/12000
2024-11-05 01:17:43,738 - INFO - [diffusion][Epoch 8755] diffusion training Loss: 0.06270322669297457
2024-11-05 01:17:43,740 - INFO - [diffusion][Epoch 8755] diffusion learning rate: 0.001
2024-11-05 01:17:43,742 - INFO - [diffusion][Epoch 8755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:43,743 - INFO - [diffusion][Epoch 8756] Epoch 8757/12000
2024-11-05 01:17:47,942 - INFO - [diffusion][Epoch 8756] diffusion training Loss: 0.0645302627235651
2024-11-05 01:17:47,946 - INFO - [diffusion][Epoch 8756] diffusion learning rate: 0.001
2024-11-05 01:17:47,948 - INFO - [diffusion][Epoch 8756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:47,950 - INFO - [diffusion][Epoch 8757] Epoch 8758/12000
2024-11-05 01:17:52,101 - INFO - [diffusion][Epoch 8757] diffusion training Loss: 0.06333337537944317
2024-11-05 01:17:52,104 - INFO - [diffusion][Epoch 8757] diffusion learning rate: 0.001
2024-11-05 01:17:52,106 - INFO - [diffusion][Epoch 8757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:52,107 - INFO - [diffusion][Epoch 8758] Epoch 8759/12000
2024-11-05 01:17:56,151 - INFO - [diffusion][Epoch 8758] diffusion training Loss: 0.06339148711413145
2024-11-05 01:17:56,153 - INFO - [diffusion][Epoch 8758] diffusion learning rate: 0.001
2024-11-05 01:17:56,155 - INFO - [diffusion][Epoch 8758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:56,156 - INFO - [diffusion][Epoch 8759] Epoch 8760/12000
2024-11-05 01:18:00,363 - INFO - [diffusion][Epoch 8759] diffusion training Loss: 0.0659007765352726
2024-11-05 01:18:00,365 - INFO - [diffusion][Epoch 8759] diffusion learning rate: 0.001
2024-11-05 01:18:00,367 - INFO - [diffusion][Epoch 8759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:00,368 - INFO - [diffusion][Epoch 8760] Epoch 8761/12000
2024-11-05 01:18:04,543 - INFO - [diffusion][Epoch 8760] diffusion training Loss: 0.06702330429106951
2024-11-05 01:18:04,545 - INFO - [diffusion][Epoch 8760] diffusion learning rate: 0.001
2024-11-05 01:18:04,547 - INFO - [diffusion][Epoch 8760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:04,548 - INFO - [diffusion][Epoch 8761] Epoch 8762/12000
2024-11-05 01:18:08,685 - INFO - [diffusion][Epoch 8761] diffusion training Loss: 0.06521819066256285
2024-11-05 01:18:08,687 - INFO - [diffusion][Epoch 8761] diffusion learning rate: 0.001
2024-11-05 01:18:08,689 - INFO - [diffusion][Epoch 8761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:08,691 - INFO - [diffusion][Epoch 8762] Epoch 8763/12000
2024-11-05 01:18:12,667 - INFO - [diffusion][Epoch 8762] diffusion training Loss: 0.06502214819192886
2024-11-05 01:18:12,716 - INFO - [diffusion][Epoch 8762] diffusion learning rate: 0.001
2024-11-05 01:18:12,718 - INFO - [diffusion][Epoch 8762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:12,720 - INFO - [diffusion][Epoch 8763] Epoch 8764/12000
2024-11-05 01:18:16,912 - INFO - [diffusion][Epoch 8763] diffusion training Loss: 0.06530525721609592
2024-11-05 01:18:16,914 - INFO - [diffusion][Epoch 8763] diffusion learning rate: 0.001
2024-11-05 01:18:16,916 - INFO - [diffusion][Epoch 8763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:16,918 - INFO - [diffusion][Epoch 8764] Epoch 8765/12000
2024-11-05 01:18:21,026 - INFO - [diffusion][Epoch 8764] diffusion training Loss: 0.07079016603529453
2024-11-05 01:18:21,028 - INFO - [diffusion][Epoch 8764] diffusion learning rate: 0.001
2024-11-05 01:18:21,031 - INFO - [diffusion][Epoch 8764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:21,033 - INFO - [diffusion][Epoch 8765] Epoch 8766/12000
2024-11-05 01:18:25,041 - INFO - [diffusion][Epoch 8765] diffusion training Loss: 0.0649036392569542
2024-11-05 01:18:25,044 - INFO - [diffusion][Epoch 8765] diffusion learning rate: 0.001
2024-11-05 01:18:25,046 - INFO - [diffusion][Epoch 8765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:25,048 - INFO - [diffusion][Epoch 8766] Epoch 8767/12000
2024-11-05 01:18:29,267 - INFO - [diffusion][Epoch 8766] diffusion training Loss: 0.06240191496908665
2024-11-05 01:18:29,269 - INFO - [diffusion][Epoch 8766] diffusion learning rate: 0.001
2024-11-05 01:18:29,331 - INFO - [diffusion][Epoch 8766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:29,333 - INFO - [diffusion][Epoch 8767] Epoch 8768/12000
2024-11-05 01:18:33,419 - INFO - [diffusion][Epoch 8767] diffusion training Loss: 0.06242230720818043
2024-11-05 01:18:33,421 - INFO - [diffusion][Epoch 8767] diffusion learning rate: 0.001
2024-11-05 01:18:33,423 - INFO - [diffusion][Epoch 8767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:33,424 - INFO - [diffusion][Epoch 8768] Epoch 8769/12000
2024-11-05 01:18:37,525 - INFO - [diffusion][Epoch 8768] diffusion training Loss: 0.06630781386047602
2024-11-05 01:18:37,527 - INFO - [diffusion][Epoch 8768] diffusion learning rate: 0.001
2024-11-05 01:18:37,529 - INFO - [diffusion][Epoch 8768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:37,530 - INFO - [diffusion][Epoch 8769] Epoch 8770/12000
2024-11-05 01:18:41,732 - INFO - [diffusion][Epoch 8769] diffusion training Loss: 0.0661932434886694
2024-11-05 01:18:41,733 - INFO - [diffusion][Epoch 8769] diffusion learning rate: 0.001
2024-11-05 01:18:41,735 - INFO - [diffusion][Epoch 8769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:41,736 - INFO - [diffusion][Epoch 8770] Epoch 8771/12000
2024-11-05 01:18:45,715 - INFO - [diffusion][Epoch 8770] diffusion training Loss: 0.06612475402653217
2024-11-05 01:18:45,717 - INFO - [diffusion][Epoch 8770] diffusion learning rate: 0.001
2024-11-05 01:18:45,719 - INFO - [diffusion][Epoch 8770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:45,720 - INFO - [diffusion][Epoch 8771] Epoch 8772/12000
2024-11-05 01:18:49,894 - INFO - [diffusion][Epoch 8771] diffusion training Loss: 0.0627836873754859
2024-11-05 01:18:49,897 - INFO - [diffusion][Epoch 8771] diffusion learning rate: 0.001
2024-11-05 01:18:49,899 - INFO - [diffusion][Epoch 8771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:49,900 - INFO - [diffusion][Epoch 8772] Epoch 8773/12000
2024-11-05 01:18:54,008 - INFO - [diffusion][Epoch 8772] diffusion training Loss: 0.06585900206118822
2024-11-05 01:18:54,011 - INFO - [diffusion][Epoch 8772] diffusion learning rate: 0.001
2024-11-05 01:18:54,014 - INFO - [diffusion][Epoch 8772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:54,016 - INFO - [diffusion][Epoch 8773] Epoch 8774/12000
2024-11-05 01:18:58,247 - INFO - [diffusion][Epoch 8773] diffusion training Loss: 0.07664915174245834
2024-11-05 01:18:58,248 - INFO - [diffusion][Epoch 8773] diffusion learning rate: 0.001
2024-11-05 01:18:58,250 - INFO - [diffusion][Epoch 8773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:58,251 - INFO - [diffusion][Epoch 8774] Epoch 8775/12000
2024-11-05 01:19:02,166 - INFO - [diffusion][Epoch 8774] diffusion training Loss: 0.06681181117892265
2024-11-05 01:19:02,168 - INFO - [diffusion][Epoch 8774] diffusion learning rate: 0.001
2024-11-05 01:19:02,170 - INFO - [diffusion][Epoch 8774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:02,172 - INFO - [diffusion][Epoch 8775] Epoch 8776/12000
2024-11-05 01:19:06,347 - INFO - [diffusion][Epoch 8775] diffusion training Loss: 0.07202708162367344
2024-11-05 01:19:06,349 - INFO - [diffusion][Epoch 8775] diffusion learning rate: 0.001
2024-11-05 01:19:06,351 - INFO - [diffusion][Epoch 8775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:06,352 - INFO - [diffusion][Epoch 8776] Epoch 8777/12000
2024-11-05 01:19:10,572 - INFO - [diffusion][Epoch 8776] diffusion training Loss: 0.06658904999494553
2024-11-05 01:19:10,574 - INFO - [diffusion][Epoch 8776] diffusion learning rate: 0.001
2024-11-05 01:19:10,629 - INFO - [diffusion][Epoch 8776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:10,630 - INFO - [diffusion][Epoch 8777] Epoch 8778/12000
2024-11-05 01:19:14,770 - INFO - [diffusion][Epoch 8777] diffusion training Loss: 0.060479896143078804
2024-11-05 01:19:14,772 - INFO - [diffusion][Epoch 8777] diffusion learning rate: 0.001
2024-11-05 01:19:14,774 - INFO - [diffusion][Epoch 8777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:14,775 - INFO - [diffusion][Epoch 8778] Epoch 8779/12000
2024-11-05 01:19:18,767 - INFO - [diffusion][Epoch 8778] diffusion training Loss: 0.06999962497502565
2024-11-05 01:19:18,769 - INFO - [diffusion][Epoch 8778] diffusion learning rate: 0.001
2024-11-05 01:19:18,770 - INFO - [diffusion][Epoch 8778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:18,772 - INFO - [diffusion][Epoch 8779] Epoch 8780/12000
2024-11-05 01:19:22,866 - INFO - [diffusion][Epoch 8779] diffusion training Loss: 0.06360985245555639
2024-11-05 01:19:22,868 - INFO - [diffusion][Epoch 8779] diffusion learning rate: 0.001
2024-11-05 01:19:22,870 - INFO - [diffusion][Epoch 8779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:22,871 - INFO - [diffusion][Epoch 8780] Epoch 8781/12000
2024-11-05 01:19:27,096 - INFO - [diffusion][Epoch 8780] diffusion training Loss: 0.07125000096857548
2024-11-05 01:19:27,098 - INFO - [diffusion][Epoch 8780] diffusion learning rate: 0.001
2024-11-05 01:19:27,126 - INFO - [diffusion][Epoch 8780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:27,127 - INFO - [diffusion][Epoch 8781] Epoch 8782/12000
2024-11-05 01:19:31,332 - INFO - [diffusion][Epoch 8781] diffusion training Loss: 0.06737165246158838
2024-11-05 01:19:31,334 - INFO - [diffusion][Epoch 8781] diffusion learning rate: 0.001
2024-11-05 01:19:31,336 - INFO - [diffusion][Epoch 8781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:31,337 - INFO - [diffusion][Epoch 8782] Epoch 8783/12000
2024-11-05 01:19:35,612 - INFO - [diffusion][Epoch 8782] diffusion training Loss: 0.06300335377454758
2024-11-05 01:19:35,614 - INFO - [diffusion][Epoch 8782] diffusion learning rate: 0.001
2024-11-05 01:19:35,616 - INFO - [diffusion][Epoch 8782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:35,618 - INFO - [diffusion][Epoch 8783] Epoch 8784/12000
2024-11-05 01:19:39,610 - INFO - [diffusion][Epoch 8783] diffusion training Loss: 0.07110452279448509
2024-11-05 01:19:39,612 - INFO - [diffusion][Epoch 8783] diffusion learning rate: 0.001
2024-11-05 01:19:39,614 - INFO - [diffusion][Epoch 8783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:39,615 - INFO - [diffusion][Epoch 8784] Epoch 8785/12000
2024-11-05 01:19:43,790 - INFO - [diffusion][Epoch 8784] diffusion training Loss: 0.06674789264798164
2024-11-05 01:19:43,793 - INFO - [diffusion][Epoch 8784] diffusion learning rate: 0.001
2024-11-05 01:19:43,795 - INFO - [diffusion][Epoch 8784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:43,796 - INFO - [diffusion][Epoch 8785] Epoch 8786/12000
2024-11-05 01:19:47,980 - INFO - [diffusion][Epoch 8785] diffusion training Loss: 0.06694207526743412
2024-11-05 01:19:47,982 - INFO - [diffusion][Epoch 8785] diffusion learning rate: 0.001
2024-11-05 01:19:47,984 - INFO - [diffusion][Epoch 8785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:47,986 - INFO - [diffusion][Epoch 8786] Epoch 8787/12000
2024-11-05 01:19:52,020 - INFO - [diffusion][Epoch 8786] diffusion training Loss: 0.0654524564743042
2024-11-05 01:19:52,023 - INFO - [diffusion][Epoch 8786] diffusion learning rate: 0.001
2024-11-05 01:19:52,025 - INFO - [diffusion][Epoch 8786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:52,026 - INFO - [diffusion][Epoch 8787] Epoch 8788/12000
2024-11-05 01:19:56,128 - INFO - [diffusion][Epoch 8787] diffusion training Loss: 0.07052214629948139
2024-11-05 01:19:56,130 - INFO - [diffusion][Epoch 8787] diffusion learning rate: 0.001
2024-11-05 01:19:56,132 - INFO - [diffusion][Epoch 8787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:56,133 - INFO - [diffusion][Epoch 8788] Epoch 8789/12000
2024-11-05 01:20:00,171 - INFO - [diffusion][Epoch 8788] diffusion training Loss: 0.06878511421382427
2024-11-05 01:20:00,172 - INFO - [diffusion][Epoch 8788] diffusion learning rate: 0.001
2024-11-05 01:20:00,174 - INFO - [diffusion][Epoch 8788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:00,175 - INFO - [diffusion][Epoch 8789] Epoch 8790/12000
2024-11-05 01:20:04,368 - INFO - [diffusion][Epoch 8789] diffusion training Loss: 0.06395714730024338
2024-11-05 01:20:04,371 - INFO - [diffusion][Epoch 8789] diffusion learning rate: 0.001
2024-11-05 01:20:04,373 - INFO - [diffusion][Epoch 8789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:04,374 - INFO - [diffusion][Epoch 8790] Epoch 8791/12000
2024-11-05 01:20:08,262 - INFO - [diffusion][Epoch 8790] diffusion training Loss: 0.06766212172806263
2024-11-05 01:20:08,264 - INFO - [diffusion][Epoch 8790] diffusion learning rate: 0.001
2024-11-05 01:20:08,266 - INFO - [diffusion][Epoch 8790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:08,267 - INFO - [diffusion][Epoch 8791] Epoch 8792/12000
2024-11-05 01:20:12,283 - INFO - [diffusion][Epoch 8791] diffusion training Loss: 0.06538466177880764
2024-11-05 01:20:12,602 - INFO - [diffusion][Epoch 8791] diffusion learning rate: 0.001
2024-11-05 01:20:12,604 - INFO - [diffusion][Epoch 8791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:12,605 - INFO - [diffusion][Epoch 8792] Epoch 8793/12000
2024-11-05 01:20:16,912 - INFO - [diffusion][Epoch 8792] diffusion training Loss: 0.0635958556085825
2024-11-05 01:20:16,915 - INFO - [diffusion][Epoch 8792] diffusion learning rate: 0.001
2024-11-05 01:20:16,917 - INFO - [diffusion][Epoch 8792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:16,918 - INFO - [diffusion][Epoch 8793] Epoch 8794/12000
2024-11-05 01:20:20,925 - INFO - [diffusion][Epoch 8793] diffusion training Loss: 0.0627504512667656
2024-11-05 01:20:20,929 - INFO - [diffusion][Epoch 8793] diffusion learning rate: 0.001
2024-11-05 01:20:20,930 - INFO - [diffusion][Epoch 8793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:20,932 - INFO - [diffusion][Epoch 8794] Epoch 8795/12000
2024-11-05 01:20:25,140 - INFO - [diffusion][Epoch 8794] diffusion training Loss: 0.06638065446168184
2024-11-05 01:20:25,142 - INFO - [diffusion][Epoch 8794] diffusion learning rate: 0.001
2024-11-05 01:20:25,144 - INFO - [diffusion][Epoch 8794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:25,146 - INFO - [diffusion][Epoch 8795] Epoch 8796/12000
2024-11-05 01:20:29,429 - INFO - [diffusion][Epoch 8795] diffusion training Loss: 0.06336068734526634
2024-11-05 01:20:29,431 - INFO - [diffusion][Epoch 8795] diffusion learning rate: 0.001
2024-11-05 01:20:29,433 - INFO - [diffusion][Epoch 8795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:29,434 - INFO - [diffusion][Epoch 8796] Epoch 8797/12000
2024-11-05 01:20:33,779 - INFO - [diffusion][Epoch 8796] diffusion training Loss: 0.06551114842295647
2024-11-05 01:20:33,781 - INFO - [diffusion][Epoch 8796] diffusion learning rate: 0.001
2024-11-05 01:20:33,782 - INFO - [diffusion][Epoch 8796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:33,784 - INFO - [diffusion][Epoch 8797] Epoch 8798/12000
2024-11-05 01:20:37,968 - INFO - [diffusion][Epoch 8797] diffusion training Loss: 0.06477741710841656
2024-11-05 01:20:37,970 - INFO - [diffusion][Epoch 8797] diffusion learning rate: 0.001
2024-11-05 01:20:37,972 - INFO - [diffusion][Epoch 8797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:37,973 - INFO - [diffusion][Epoch 8798] Epoch 8799/12000
2024-11-05 01:20:42,103 - INFO - [diffusion][Epoch 8798] diffusion training Loss: 0.06533771567046642
2024-11-05 01:20:42,105 - INFO - [diffusion][Epoch 8798] diffusion learning rate: 0.001
2024-11-05 01:20:42,107 - INFO - [diffusion][Epoch 8798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:42,109 - INFO - [diffusion][Epoch 8799] Epoch 8800/12000
2024-11-05 01:20:46,365 - INFO - [diffusion][Epoch 8799] diffusion training Loss: 0.06453626975417137
2024-11-05 01:20:46,367 - INFO - [diffusion][Epoch 8799] diffusion learning rate: 0.001
2024-11-05 01:20:46,369 - INFO - [diffusion][Epoch 8799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:46,370 - INFO - [diffusion][Epoch 8800] Epoch 8801/12000
2024-11-05 01:20:50,623 - INFO - [diffusion][Epoch 8800] diffusion training Loss: 0.06630824133753777
2024-11-05 01:20:50,626 - INFO - [diffusion][Epoch 8800] diffusion learning rate: 0.001
2024-11-05 01:20:50,627 - INFO - [diffusion][Epoch 8800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:50,628 - INFO - [diffusion][Epoch 8801] Epoch 8802/12000
2024-11-05 01:20:54,821 - INFO - [diffusion][Epoch 8801] diffusion training Loss: 0.06385205127298832
2024-11-05 01:20:54,823 - INFO - [diffusion][Epoch 8801] diffusion learning rate: 0.001
2024-11-05 01:20:54,825 - INFO - [diffusion][Epoch 8801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:54,826 - INFO - [diffusion][Epoch 8802] Epoch 8803/12000
2024-11-05 01:20:58,963 - INFO - [diffusion][Epoch 8802] diffusion training Loss: 0.06709301844239235
2024-11-05 01:20:58,965 - INFO - [diffusion][Epoch 8802] diffusion learning rate: 0.001
2024-11-05 01:20:58,966 - INFO - [diffusion][Epoch 8802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:58,967 - INFO - [diffusion][Epoch 8803] Epoch 8804/12000
2024-11-05 01:21:03,883 - INFO - [diffusion][Epoch 8803] diffusion training Loss: 0.06691745296120644
2024-11-05 01:21:03,885 - INFO - [diffusion][Epoch 8803] diffusion learning rate: 0.001
2024-11-05 01:21:03,887 - INFO - [diffusion][Epoch 8803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:03,888 - INFO - [diffusion][Epoch 8804] Epoch 8805/12000
2024-11-05 01:21:08,023 - INFO - [diffusion][Epoch 8804] diffusion training Loss: 0.06601664610207081
2024-11-05 01:21:08,024 - INFO - [diffusion][Epoch 8804] diffusion learning rate: 0.001
2024-11-05 01:21:08,026 - INFO - [diffusion][Epoch 8804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:08,028 - INFO - [diffusion][Epoch 8805] Epoch 8806/12000
2024-11-05 01:21:12,014 - INFO - [diffusion][Epoch 8805] diffusion training Loss: 0.06878518033772707
2024-11-05 01:21:12,016 - INFO - [diffusion][Epoch 8805] diffusion learning rate: 0.001
2024-11-05 01:21:12,018 - INFO - [diffusion][Epoch 8805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:12,019 - INFO - [diffusion][Epoch 8806] Epoch 8807/12000
2024-11-05 01:21:16,166 - INFO - [diffusion][Epoch 8806] diffusion training Loss: 0.07085424847900867
2024-11-05 01:21:16,168 - INFO - [diffusion][Epoch 8806] diffusion learning rate: 0.001
2024-11-05 01:21:16,169 - INFO - [diffusion][Epoch 8806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:16,171 - INFO - [diffusion][Epoch 8807] Epoch 8808/12000
2024-11-05 01:21:20,325 - INFO - [diffusion][Epoch 8807] diffusion training Loss: 0.06443579215556383
2024-11-05 01:21:20,327 - INFO - [diffusion][Epoch 8807] diffusion learning rate: 0.001
2024-11-05 01:21:20,329 - INFO - [diffusion][Epoch 8807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:20,330 - INFO - [diffusion][Epoch 8808] Epoch 8809/12000
2024-11-05 01:21:24,474 - INFO - [diffusion][Epoch 8808] diffusion training Loss: 0.07065148651599884
2024-11-05 01:21:24,475 - INFO - [diffusion][Epoch 8808] diffusion learning rate: 0.001
2024-11-05 01:21:24,504 - INFO - [diffusion][Epoch 8808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:24,506 - INFO - [diffusion][Epoch 8809] Epoch 8810/12000
2024-11-05 01:21:28,540 - INFO - [diffusion][Epoch 8809] diffusion training Loss: 0.06490439921617508
2024-11-05 01:21:28,542 - INFO - [diffusion][Epoch 8809] diffusion learning rate: 0.001
2024-11-05 01:21:28,544 - INFO - [diffusion][Epoch 8809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:28,545 - INFO - [diffusion][Epoch 8810] Epoch 8811/12000
2024-11-05 01:21:32,628 - INFO - [diffusion][Epoch 8810] diffusion training Loss: 0.06565541587769985
2024-11-05 01:21:32,630 - INFO - [diffusion][Epoch 8810] diffusion learning rate: 0.001
2024-11-05 01:21:32,632 - INFO - [diffusion][Epoch 8810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:32,634 - INFO - [diffusion][Epoch 8811] Epoch 8812/12000
2024-11-05 01:21:36,643 - INFO - [diffusion][Epoch 8811] diffusion training Loss: 0.06455948855727911
2024-11-05 01:21:36,645 - INFO - [diffusion][Epoch 8811] diffusion learning rate: 0.001
2024-11-05 01:21:36,647 - INFO - [diffusion][Epoch 8811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:36,648 - INFO - [diffusion][Epoch 8812] Epoch 8813/12000
2024-11-05 01:21:40,748 - INFO - [diffusion][Epoch 8812] diffusion training Loss: 0.0650552473962307
2024-11-05 01:21:40,750 - INFO - [diffusion][Epoch 8812] diffusion learning rate: 0.001
2024-11-05 01:21:40,751 - INFO - [diffusion][Epoch 8812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:40,753 - INFO - [diffusion][Epoch 8813] Epoch 8814/12000
2024-11-05 01:21:44,726 - INFO - [diffusion][Epoch 8813] diffusion training Loss: 0.06649753637611866
2024-11-05 01:21:44,729 - INFO - [diffusion][Epoch 8813] diffusion learning rate: 0.001
2024-11-05 01:21:44,732 - INFO - [diffusion][Epoch 8813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:44,733 - INFO - [diffusion][Epoch 8814] Epoch 8815/12000
2024-11-05 01:21:48,700 - INFO - [diffusion][Epoch 8814] diffusion training Loss: 0.059310671873390675
2024-11-05 01:21:48,702 - INFO - [diffusion][Epoch 8814] diffusion learning rate: 0.001
2024-11-05 01:21:48,703 - INFO - [diffusion][Epoch 8814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:48,705 - INFO - [diffusion][Epoch 8815] Epoch 8816/12000
2024-11-05 01:21:52,846 - INFO - [diffusion][Epoch 8815] diffusion training Loss: 0.06479626521468163
2024-11-05 01:21:52,848 - INFO - [diffusion][Epoch 8815] diffusion learning rate: 0.001
2024-11-05 01:21:52,850 - INFO - [diffusion][Epoch 8815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:52,851 - INFO - [diffusion][Epoch 8816] Epoch 8817/12000
2024-11-05 01:21:56,924 - INFO - [diffusion][Epoch 8816] diffusion training Loss: 0.06898937933146954
2024-11-05 01:21:56,926 - INFO - [diffusion][Epoch 8816] diffusion learning rate: 0.001
2024-11-05 01:21:56,928 - INFO - [diffusion][Epoch 8816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:56,929 - INFO - [diffusion][Epoch 8817] Epoch 8818/12000
2024-11-05 01:22:01,064 - INFO - [diffusion][Epoch 8817] diffusion training Loss: 0.07123436033725739
2024-11-05 01:22:01,066 - INFO - [diffusion][Epoch 8817] diffusion learning rate: 0.001
2024-11-05 01:22:01,068 - INFO - [diffusion][Epoch 8817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:01,069 - INFO - [diffusion][Epoch 8818] Epoch 8819/12000
2024-11-05 01:22:05,327 - INFO - [diffusion][Epoch 8818] diffusion training Loss: 0.06791240349411964
2024-11-05 01:22:05,330 - INFO - [diffusion][Epoch 8818] diffusion learning rate: 0.001
2024-11-05 01:22:05,332 - INFO - [diffusion][Epoch 8818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:05,333 - INFO - [diffusion][Epoch 8819] Epoch 8820/12000
2024-11-05 01:22:09,493 - INFO - [diffusion][Epoch 8819] diffusion training Loss: 0.06346585880964994
2024-11-05 01:22:09,495 - INFO - [diffusion][Epoch 8819] diffusion learning rate: 0.001
2024-11-05 01:22:09,497 - INFO - [diffusion][Epoch 8819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:09,498 - INFO - [diffusion][Epoch 8820] Epoch 8821/12000
2024-11-05 01:22:13,699 - INFO - [diffusion][Epoch 8820] diffusion training Loss: 0.06470642611384392
2024-11-05 01:22:13,701 - INFO - [diffusion][Epoch 8820] diffusion learning rate: 0.001
2024-11-05 01:22:13,703 - INFO - [diffusion][Epoch 8820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:13,704 - INFO - [diffusion][Epoch 8821] Epoch 8822/12000
2024-11-05 01:22:17,916 - INFO - [diffusion][Epoch 8821] diffusion training Loss: 0.06477728951722383
2024-11-05 01:22:17,918 - INFO - [diffusion][Epoch 8821] diffusion learning rate: 0.001
2024-11-05 01:22:17,920 - INFO - [diffusion][Epoch 8821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:17,922 - INFO - [diffusion][Epoch 8822] Epoch 8823/12000
2024-11-05 01:22:22,014 - INFO - [diffusion][Epoch 8822] diffusion training Loss: 0.07229636050760746
2024-11-05 01:22:22,016 - INFO - [diffusion][Epoch 8822] diffusion learning rate: 0.001
2024-11-05 01:22:22,018 - INFO - [diffusion][Epoch 8822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:22,019 - INFO - [diffusion][Epoch 8823] Epoch 8824/12000
2024-11-05 01:22:26,037 - INFO - [diffusion][Epoch 8823] diffusion training Loss: 0.07066558301448822
2024-11-05 01:22:26,039 - INFO - [diffusion][Epoch 8823] diffusion learning rate: 0.001
2024-11-05 01:22:26,041 - INFO - [diffusion][Epoch 8823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:26,042 - INFO - [diffusion][Epoch 8824] Epoch 8825/12000
2024-11-05 01:22:30,164 - INFO - [diffusion][Epoch 8824] diffusion training Loss: 0.06552001088857651
2024-11-05 01:22:30,166 - INFO - [diffusion][Epoch 8824] diffusion learning rate: 0.001
2024-11-05 01:22:30,168 - INFO - [diffusion][Epoch 8824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:30,169 - INFO - [diffusion][Epoch 8825] Epoch 8826/12000
2024-11-05 01:22:34,302 - INFO - [diffusion][Epoch 8825] diffusion training Loss: 0.07171036675572395
2024-11-05 01:22:34,304 - INFO - [diffusion][Epoch 8825] diffusion learning rate: 0.001
2024-11-05 01:22:34,305 - INFO - [diffusion][Epoch 8825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:34,307 - INFO - [diffusion][Epoch 8826] Epoch 8827/12000
2024-11-05 01:22:38,393 - INFO - [diffusion][Epoch 8826] diffusion training Loss: 0.06645198352634907
2024-11-05 01:22:38,395 - INFO - [diffusion][Epoch 8826] diffusion learning rate: 0.001
2024-11-05 01:22:38,397 - INFO - [diffusion][Epoch 8826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:38,398 - INFO - [diffusion][Epoch 8827] Epoch 8828/12000
2024-11-05 01:22:42,640 - INFO - [diffusion][Epoch 8827] diffusion training Loss: 0.06485669687390327
2024-11-05 01:22:42,648 - INFO - [diffusion][Epoch 8827] diffusion learning rate: 0.001
2024-11-05 01:22:42,649 - INFO - [diffusion][Epoch 8827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:42,651 - INFO - [diffusion][Epoch 8828] Epoch 8829/12000
2024-11-05 01:22:46,809 - INFO - [diffusion][Epoch 8828] diffusion training Loss: 0.06628196593374014
2024-11-05 01:22:46,811 - INFO - [diffusion][Epoch 8828] diffusion learning rate: 0.001
2024-11-05 01:22:46,813 - INFO - [diffusion][Epoch 8828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:46,814 - INFO - [diffusion][Epoch 8829] Epoch 8830/12000
2024-11-05 01:22:50,971 - INFO - [diffusion][Epoch 8829] diffusion training Loss: 0.06582638993859291
2024-11-05 01:22:50,973 - INFO - [diffusion][Epoch 8829] diffusion learning rate: 0.001
2024-11-05 01:22:50,975 - INFO - [diffusion][Epoch 8829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:50,977 - INFO - [diffusion][Epoch 8830] Epoch 8831/12000
2024-11-05 01:22:55,019 - INFO - [diffusion][Epoch 8830] diffusion training Loss: 0.06419517006725073
2024-11-05 01:22:55,022 - INFO - [diffusion][Epoch 8830] diffusion learning rate: 0.001
2024-11-05 01:22:55,024 - INFO - [diffusion][Epoch 8830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:55,026 - INFO - [diffusion][Epoch 8831] Epoch 8832/12000
2024-11-05 01:22:59,034 - INFO - [diffusion][Epoch 8831] diffusion training Loss: 0.06860414706170559
2024-11-05 01:22:59,036 - INFO - [diffusion][Epoch 8831] diffusion learning rate: 0.001
2024-11-05 01:22:59,038 - INFO - [diffusion][Epoch 8831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:59,039 - INFO - [diffusion][Epoch 8832] Epoch 8833/12000
2024-11-05 01:23:03,201 - INFO - [diffusion][Epoch 8832] diffusion training Loss: 0.06845850124955177
2024-11-05 01:23:03,203 - INFO - [diffusion][Epoch 8832] diffusion learning rate: 0.001
2024-11-05 01:23:03,205 - INFO - [diffusion][Epoch 8832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:03,207 - INFO - [diffusion][Epoch 8833] Epoch 8834/12000
2024-11-05 01:23:07,453 - INFO - [diffusion][Epoch 8833] diffusion training Loss: 0.06988386809825897
2024-11-05 01:23:07,455 - INFO - [diffusion][Epoch 8833] diffusion learning rate: 0.001
2024-11-05 01:23:07,456 - INFO - [diffusion][Epoch 8833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:07,458 - INFO - [diffusion][Epoch 8834] Epoch 8835/12000
2024-11-05 01:23:11,620 - INFO - [diffusion][Epoch 8834] diffusion training Loss: 0.06213793717324734
2024-11-05 01:23:11,621 - INFO - [diffusion][Epoch 8834] diffusion learning rate: 0.001
2024-11-05 01:23:11,623 - INFO - [diffusion][Epoch 8834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:11,625 - INFO - [diffusion][Epoch 8835] Epoch 8836/12000
2024-11-05 01:23:15,846 - INFO - [diffusion][Epoch 8835] diffusion training Loss: 0.06079495791345835
2024-11-05 01:23:15,849 - INFO - [diffusion][Epoch 8835] diffusion learning rate: 0.001
2024-11-05 01:23:15,852 - INFO - [diffusion][Epoch 8835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:15,853 - INFO - [diffusion][Epoch 8836] Epoch 8837/12000
2024-11-05 01:23:20,014 - INFO - [diffusion][Epoch 8836] diffusion training Loss: 0.06410067714750767
2024-11-05 01:23:20,017 - INFO - [diffusion][Epoch 8836] diffusion learning rate: 0.001
2024-11-05 01:23:20,019 - INFO - [diffusion][Epoch 8836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:20,020 - INFO - [diffusion][Epoch 8837] Epoch 8838/12000
2024-11-05 01:23:24,111 - INFO - [diffusion][Epoch 8837] diffusion training Loss: 0.06147748790681362
2024-11-05 01:23:24,113 - INFO - [diffusion][Epoch 8837] diffusion learning rate: 0.001
2024-11-05 01:23:24,115 - INFO - [diffusion][Epoch 8837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:24,116 - INFO - [diffusion][Epoch 8838] Epoch 8839/12000
2024-11-05 01:23:28,136 - INFO - [diffusion][Epoch 8838] diffusion training Loss: 0.05926716607064009
2024-11-05 01:23:28,139 - INFO - [diffusion][Epoch 8838] diffusion learning rate: 0.001
2024-11-05 01:23:28,142 - INFO - [diffusion][Epoch 8838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:28,144 - INFO - [diffusion][Epoch 8839] Epoch 8840/12000
2024-11-05 01:23:31,898 - INFO - [diffusion][Epoch 8839] diffusion training Loss: 0.06345832627266645
2024-11-05 01:23:31,900 - INFO - [diffusion][Epoch 8839] diffusion learning rate: 0.001
2024-11-05 01:23:31,948 - INFO - [diffusion][Epoch 8839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:31,949 - INFO - [diffusion][Epoch 8840] Epoch 8841/12000
2024-11-05 01:23:36,083 - INFO - [diffusion][Epoch 8840] diffusion training Loss: 0.06383065041154623
2024-11-05 01:23:36,086 - INFO - [diffusion][Epoch 8840] diffusion learning rate: 0.001
2024-11-05 01:23:36,088 - INFO - [diffusion][Epoch 8840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:36,090 - INFO - [diffusion][Epoch 8841] Epoch 8842/12000
2024-11-05 01:23:40,345 - INFO - [diffusion][Epoch 8841] diffusion training Loss: 0.06334360502660275
2024-11-05 01:23:40,347 - INFO - [diffusion][Epoch 8841] diffusion learning rate: 0.001
2024-11-05 01:23:40,349 - INFO - [diffusion][Epoch 8841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:40,351 - INFO - [diffusion][Epoch 8842] Epoch 8843/12000
2024-11-05 01:23:44,601 - INFO - [diffusion][Epoch 8842] diffusion training Loss: 0.06666351296007633
2024-11-05 01:23:44,603 - INFO - [diffusion][Epoch 8842] diffusion learning rate: 0.001
2024-11-05 01:23:44,605 - INFO - [diffusion][Epoch 8842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:44,606 - INFO - [diffusion][Epoch 8843] Epoch 8844/12000
2024-11-05 01:23:48,622 - INFO - [diffusion][Epoch 8843] diffusion training Loss: 0.06167206075042486
2024-11-05 01:23:48,624 - INFO - [diffusion][Epoch 8843] diffusion learning rate: 0.001
2024-11-05 01:23:48,626 - INFO - [diffusion][Epoch 8843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:48,628 - INFO - [diffusion][Epoch 8844] Epoch 8845/12000
2024-11-05 01:23:53,383 - INFO - [diffusion][Epoch 8844] diffusion training Loss: 0.06400467921048403
2024-11-05 01:23:53,385 - INFO - [diffusion][Epoch 8844] diffusion learning rate: 0.001
2024-11-05 01:23:53,387 - INFO - [diffusion][Epoch 8844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:53,388 - INFO - [diffusion][Epoch 8845] Epoch 8846/12000
2024-11-05 01:23:57,647 - INFO - [diffusion][Epoch 8845] diffusion training Loss: 0.06753491703420877
2024-11-05 01:23:57,648 - INFO - [diffusion][Epoch 8845] diffusion learning rate: 0.001
2024-11-05 01:23:57,650 - INFO - [diffusion][Epoch 8845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:57,651 - INFO - [diffusion][Epoch 8846] Epoch 8847/12000
2024-11-05 01:24:01,797 - INFO - [diffusion][Epoch 8846] diffusion training Loss: 0.06865278445184231
2024-11-05 01:24:01,799 - INFO - [diffusion][Epoch 8846] diffusion learning rate: 0.001
2024-11-05 01:24:01,801 - INFO - [diffusion][Epoch 8846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:01,802 - INFO - [diffusion][Epoch 8847] Epoch 8848/12000
2024-11-05 01:24:05,914 - INFO - [diffusion][Epoch 8847] diffusion training Loss: 0.06354812812060118
2024-11-05 01:24:05,916 - INFO - [diffusion][Epoch 8847] diffusion learning rate: 0.001
2024-11-05 01:24:05,918 - INFO - [diffusion][Epoch 8847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:05,919 - INFO - [diffusion][Epoch 8848] Epoch 8849/12000
2024-11-05 01:24:10,056 - INFO - [diffusion][Epoch 8848] diffusion training Loss: 0.07158868573606014
2024-11-05 01:24:10,059 - INFO - [diffusion][Epoch 8848] diffusion learning rate: 0.001
2024-11-05 01:24:10,060 - INFO - [diffusion][Epoch 8848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:10,061 - INFO - [diffusion][Epoch 8849] Epoch 8850/12000
2024-11-05 01:24:14,275 - INFO - [diffusion][Epoch 8849] diffusion training Loss: 0.06871083099395037
2024-11-05 01:24:14,277 - INFO - [diffusion][Epoch 8849] diffusion learning rate: 0.001
2024-11-05 01:24:14,279 - INFO - [diffusion][Epoch 8849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:14,280 - INFO - [diffusion][Epoch 8850] Epoch 8851/12000
2024-11-05 01:24:18,355 - INFO - [diffusion][Epoch 8850] diffusion training Loss: 0.06388728972524405
2024-11-05 01:24:18,357 - INFO - [diffusion][Epoch 8850] diffusion learning rate: 0.001
2024-11-05 01:24:18,359 - INFO - [diffusion][Epoch 8850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:18,360 - INFO - [diffusion][Epoch 8851] Epoch 8852/12000
2024-11-05 01:24:22,394 - INFO - [diffusion][Epoch 8851] diffusion training Loss: 0.0654624467715621
2024-11-05 01:24:22,397 - INFO - [diffusion][Epoch 8851] diffusion learning rate: 0.001
2024-11-05 01:24:22,398 - INFO - [diffusion][Epoch 8851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:22,400 - INFO - [diffusion][Epoch 8852] Epoch 8853/12000
2024-11-05 01:24:26,589 - INFO - [diffusion][Epoch 8852] diffusion training Loss: 0.06555116269737482
2024-11-05 01:24:26,592 - INFO - [diffusion][Epoch 8852] diffusion learning rate: 0.001
2024-11-05 01:24:26,594 - INFO - [diffusion][Epoch 8852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:26,595 - INFO - [diffusion][Epoch 8853] Epoch 8854/12000
2024-11-05 01:24:30,712 - INFO - [diffusion][Epoch 8853] diffusion training Loss: 0.06870501860976219
2024-11-05 01:24:30,715 - INFO - [diffusion][Epoch 8853] diffusion learning rate: 0.001
2024-11-05 01:24:30,717 - INFO - [diffusion][Epoch 8853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:30,719 - INFO - [diffusion][Epoch 8854] Epoch 8855/12000
2024-11-05 01:24:34,796 - INFO - [diffusion][Epoch 8854] diffusion training Loss: 0.06561138946563005
2024-11-05 01:24:34,798 - INFO - [diffusion][Epoch 8854] diffusion learning rate: 0.001
2024-11-05 01:24:34,800 - INFO - [diffusion][Epoch 8854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:34,801 - INFO - [diffusion][Epoch 8855] Epoch 8856/12000
2024-11-05 01:24:38,909 - INFO - [diffusion][Epoch 8855] diffusion training Loss: 0.06419963948428631
2024-11-05 01:24:38,912 - INFO - [diffusion][Epoch 8855] diffusion learning rate: 0.001
2024-11-05 01:24:38,913 - INFO - [diffusion][Epoch 8855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:38,915 - INFO - [diffusion][Epoch 8856] Epoch 8857/12000
2024-11-05 01:24:43,054 - INFO - [diffusion][Epoch 8856] diffusion training Loss: 0.05879109539091587
2024-11-05 01:24:43,055 - INFO - [diffusion][Epoch 8856] diffusion learning rate: 0.001
2024-11-05 01:24:43,057 - INFO - [diffusion][Epoch 8856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:43,059 - INFO - [diffusion][Epoch 8857] Epoch 8858/12000
2024-11-05 01:24:47,178 - INFO - [diffusion][Epoch 8857] diffusion training Loss: 0.06478277500718832
2024-11-05 01:24:47,180 - INFO - [diffusion][Epoch 8857] diffusion learning rate: 0.001
2024-11-05 01:24:47,181 - INFO - [diffusion][Epoch 8857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:47,182 - INFO - [diffusion][Epoch 8858] Epoch 8859/12000
2024-11-05 01:24:51,405 - INFO - [diffusion][Epoch 8858] diffusion training Loss: 0.06314374972134829
2024-11-05 01:24:51,407 - INFO - [diffusion][Epoch 8858] diffusion learning rate: 0.001
2024-11-05 01:24:51,408 - INFO - [diffusion][Epoch 8858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:51,409 - INFO - [diffusion][Epoch 8859] Epoch 8860/12000
2024-11-05 01:24:55,570 - INFO - [diffusion][Epoch 8859] diffusion training Loss: 0.0641125999391079
2024-11-05 01:24:55,572 - INFO - [diffusion][Epoch 8859] diffusion learning rate: 0.001
2024-11-05 01:24:55,574 - INFO - [diffusion][Epoch 8859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:55,575 - INFO - [diffusion][Epoch 8860] Epoch 8861/12000
2024-11-05 01:24:59,709 - INFO - [diffusion][Epoch 8860] diffusion training Loss: 0.06248607952147722
2024-11-05 01:24:59,711 - INFO - [diffusion][Epoch 8860] diffusion learning rate: 0.001
2024-11-05 01:24:59,713 - INFO - [diffusion][Epoch 8860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:59,714 - INFO - [diffusion][Epoch 8861] Epoch 8862/12000
2024-11-05 01:25:03,884 - INFO - [diffusion][Epoch 8861] diffusion training Loss: 0.07198506966233253
2024-11-05 01:25:03,886 - INFO - [diffusion][Epoch 8861] diffusion learning rate: 0.001
2024-11-05 01:25:03,888 - INFO - [diffusion][Epoch 8861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:03,889 - INFO - [diffusion][Epoch 8862] Epoch 8863/12000
2024-11-05 01:25:08,088 - INFO - [diffusion][Epoch 8862] diffusion training Loss: 0.06516100373119116
2024-11-05 01:25:08,090 - INFO - [diffusion][Epoch 8862] diffusion learning rate: 0.001
2024-11-05 01:25:08,092 - INFO - [diffusion][Epoch 8862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:08,093 - INFO - [diffusion][Epoch 8863] Epoch 8864/12000
2024-11-05 01:25:12,330 - INFO - [diffusion][Epoch 8863] diffusion training Loss: 0.06367074232548475
2024-11-05 01:25:12,396 - INFO - [diffusion][Epoch 8863] diffusion learning rate: 0.001
2024-11-05 01:25:12,398 - INFO - [diffusion][Epoch 8863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:12,399 - INFO - [diffusion][Epoch 8864] Epoch 8865/12000
2024-11-05 01:25:16,627 - INFO - [diffusion][Epoch 8864] diffusion training Loss: 0.06789327040314674
2024-11-05 01:25:16,629 - INFO - [diffusion][Epoch 8864] diffusion learning rate: 0.001
2024-11-05 01:25:16,631 - INFO - [diffusion][Epoch 8864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:16,633 - INFO - [diffusion][Epoch 8865] Epoch 8866/12000
2024-11-05 01:25:20,828 - INFO - [diffusion][Epoch 8865] diffusion training Loss: 0.07265833392739296
2024-11-05 01:25:20,830 - INFO - [diffusion][Epoch 8865] diffusion learning rate: 0.001
2024-11-05 01:25:20,910 - INFO - [diffusion][Epoch 8865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:20,912 - INFO - [diffusion][Epoch 8866] Epoch 8867/12000
2024-11-05 01:25:25,219 - INFO - [diffusion][Epoch 8866] diffusion training Loss: 0.07097435742616653
2024-11-05 01:25:25,221 - INFO - [diffusion][Epoch 8866] diffusion learning rate: 0.001
2024-11-05 01:25:25,222 - INFO - [diffusion][Epoch 8866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:25,224 - INFO - [diffusion][Epoch 8867] Epoch 8868/12000
2024-11-05 01:25:29,492 - INFO - [diffusion][Epoch 8867] diffusion training Loss: 0.07077578082680702
2024-11-05 01:25:29,495 - INFO - [diffusion][Epoch 8867] diffusion learning rate: 0.001
2024-11-05 01:25:29,497 - INFO - [diffusion][Epoch 8867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:29,499 - INFO - [diffusion][Epoch 8868] Epoch 8869/12000
2024-11-05 01:25:33,778 - INFO - [diffusion][Epoch 8868] diffusion training Loss: 0.06799796223640442
2024-11-05 01:25:33,780 - INFO - [diffusion][Epoch 8868] diffusion learning rate: 0.001
2024-11-05 01:25:33,782 - INFO - [diffusion][Epoch 8868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:33,783 - INFO - [diffusion][Epoch 8869] Epoch 8870/12000
2024-11-05 01:25:37,990 - INFO - [diffusion][Epoch 8869] diffusion training Loss: 0.06585108116269112
2024-11-05 01:25:37,992 - INFO - [diffusion][Epoch 8869] diffusion learning rate: 0.001
2024-11-05 01:25:38,024 - INFO - [diffusion][Epoch 8869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:38,025 - INFO - [diffusion][Epoch 8870] Epoch 8871/12000
2024-11-05 01:25:42,277 - INFO - [diffusion][Epoch 8870] diffusion training Loss: 0.06479020044207573
2024-11-05 01:25:42,279 - INFO - [diffusion][Epoch 8870] diffusion learning rate: 0.001
2024-11-05 01:25:42,282 - INFO - [diffusion][Epoch 8870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:42,283 - INFO - [diffusion][Epoch 8871] Epoch 8872/12000
2024-11-05 01:25:46,643 - INFO - [diffusion][Epoch 8871] diffusion training Loss: 0.06679440103471279
2024-11-05 01:25:46,646 - INFO - [diffusion][Epoch 8871] diffusion learning rate: 0.001
2024-11-05 01:25:46,650 - INFO - [diffusion][Epoch 8871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:46,651 - INFO - [diffusion][Epoch 8872] Epoch 8873/12000
2024-11-05 01:25:50,802 - INFO - [diffusion][Epoch 8872] diffusion training Loss: 0.06724709831178188
2024-11-05 01:25:50,804 - INFO - [diffusion][Epoch 8872] diffusion learning rate: 0.001
2024-11-05 01:25:50,806 - INFO - [diffusion][Epoch 8872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:50,807 - INFO - [diffusion][Epoch 8873] Epoch 8874/12000
2024-11-05 01:25:54,945 - INFO - [diffusion][Epoch 8873] diffusion training Loss: 0.06857035495340824
2024-11-05 01:25:54,947 - INFO - [diffusion][Epoch 8873] diffusion learning rate: 0.001
2024-11-05 01:25:54,950 - INFO - [diffusion][Epoch 8873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:54,951 - INFO - [diffusion][Epoch 8874] Epoch 8875/12000
2024-11-05 01:25:58,863 - INFO - [diffusion][Epoch 8874] diffusion training Loss: 0.06819881591945887
2024-11-05 01:25:58,865 - INFO - [diffusion][Epoch 8874] diffusion learning rate: 0.001
2024-11-05 01:25:58,867 - INFO - [diffusion][Epoch 8874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:58,869 - INFO - [diffusion][Epoch 8875] Epoch 8876/12000
2024-11-05 01:26:03,112 - INFO - [diffusion][Epoch 8875] diffusion training Loss: 0.07097897864878178
2024-11-05 01:26:03,114 - INFO - [diffusion][Epoch 8875] diffusion learning rate: 0.001
2024-11-05 01:26:03,115 - INFO - [diffusion][Epoch 8875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:03,117 - INFO - [diffusion][Epoch 8876] Epoch 8877/12000
2024-11-05 01:26:07,135 - INFO - [diffusion][Epoch 8876] diffusion training Loss: 0.06170337647199631
2024-11-05 01:26:07,137 - INFO - [diffusion][Epoch 8876] diffusion learning rate: 0.001
2024-11-05 01:26:07,139 - INFO - [diffusion][Epoch 8876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:07,140 - INFO - [diffusion][Epoch 8877] Epoch 8878/12000
2024-11-05 01:26:11,297 - INFO - [diffusion][Epoch 8877] diffusion training Loss: 0.06597640831023455
2024-11-05 01:26:11,299 - INFO - [diffusion][Epoch 8877] diffusion learning rate: 0.001
2024-11-05 01:26:11,301 - INFO - [diffusion][Epoch 8877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:11,303 - INFO - [diffusion][Epoch 8878] Epoch 8879/12000
2024-11-05 01:26:15,549 - INFO - [diffusion][Epoch 8878] diffusion training Loss: 0.0733561385422945
2024-11-05 01:26:15,551 - INFO - [diffusion][Epoch 8878] diffusion learning rate: 0.001
2024-11-05 01:26:15,552 - INFO - [diffusion][Epoch 8878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:15,554 - INFO - [diffusion][Epoch 8879] Epoch 8880/12000
2024-11-05 01:26:19,717 - INFO - [diffusion][Epoch 8879] diffusion training Loss: 0.07273371983319521
2024-11-05 01:26:19,719 - INFO - [diffusion][Epoch 8879] diffusion learning rate: 0.001
2024-11-05 01:26:19,721 - INFO - [diffusion][Epoch 8879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:19,723 - INFO - [diffusion][Epoch 8880] Epoch 8881/12000
2024-11-05 01:26:23,711 - INFO - [diffusion][Epoch 8880] diffusion training Loss: 0.06676939502358437
2024-11-05 01:26:23,713 - INFO - [diffusion][Epoch 8880] diffusion learning rate: 0.001
2024-11-05 01:26:23,715 - INFO - [diffusion][Epoch 8880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:23,716 - INFO - [diffusion][Epoch 8881] Epoch 8882/12000
2024-11-05 01:26:28,023 - INFO - [diffusion][Epoch 8881] diffusion training Loss: 0.06603439711034298
2024-11-05 01:26:28,025 - INFO - [diffusion][Epoch 8881] diffusion learning rate: 0.001
2024-11-05 01:26:28,145 - INFO - [diffusion][Epoch 8881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:28,147 - INFO - [diffusion][Epoch 8882] Epoch 8883/12000
2024-11-05 01:26:32,329 - INFO - [diffusion][Epoch 8882] diffusion training Loss: 0.07018319051712751
2024-11-05 01:26:32,331 - INFO - [diffusion][Epoch 8882] diffusion learning rate: 0.001
2024-11-05 01:26:32,333 - INFO - [diffusion][Epoch 8882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:32,334 - INFO - [diffusion][Epoch 8883] Epoch 8884/12000
2024-11-05 01:26:36,650 - INFO - [diffusion][Epoch 8883] diffusion training Loss: 0.061002531088888645
2024-11-05 01:26:36,652 - INFO - [diffusion][Epoch 8883] diffusion learning rate: 0.001
2024-11-05 01:26:36,654 - INFO - [diffusion][Epoch 8883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:36,655 - INFO - [diffusion][Epoch 8884] Epoch 8885/12000
2024-11-05 01:26:40,933 - INFO - [diffusion][Epoch 8884] diffusion training Loss: 0.06632104143500328
2024-11-05 01:26:40,935 - INFO - [diffusion][Epoch 8884] diffusion learning rate: 0.001
2024-11-05 01:26:40,936 - INFO - [diffusion][Epoch 8884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:40,938 - INFO - [diffusion][Epoch 8885] Epoch 8886/12000
2024-11-05 01:26:45,168 - INFO - [diffusion][Epoch 8885] diffusion training Loss: 0.06771341525018215
2024-11-05 01:26:45,170 - INFO - [diffusion][Epoch 8885] diffusion learning rate: 0.001
2024-11-05 01:26:45,172 - INFO - [diffusion][Epoch 8885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:45,174 - INFO - [diffusion][Epoch 8886] Epoch 8887/12000
2024-11-05 01:26:49,334 - INFO - [diffusion][Epoch 8886] diffusion training Loss: 0.0653298245742917
2024-11-05 01:26:49,336 - INFO - [diffusion][Epoch 8886] diffusion learning rate: 0.001
2024-11-05 01:26:49,338 - INFO - [diffusion][Epoch 8886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:49,339 - INFO - [diffusion][Epoch 8887] Epoch 8888/12000
2024-11-05 01:26:53,553 - INFO - [diffusion][Epoch 8887] diffusion training Loss: 0.06440326757729053
2024-11-05 01:26:53,567 - INFO - [diffusion][Epoch 8887] diffusion learning rate: 0.001
2024-11-05 01:26:53,569 - INFO - [diffusion][Epoch 8887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:53,570 - INFO - [diffusion][Epoch 8888] Epoch 8889/12000
2024-11-05 01:26:57,785 - INFO - [diffusion][Epoch 8888] diffusion training Loss: 0.06601212359964848
2024-11-05 01:26:57,787 - INFO - [diffusion][Epoch 8888] diffusion learning rate: 0.001
2024-11-05 01:26:57,789 - INFO - [diffusion][Epoch 8888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:57,791 - INFO - [diffusion][Epoch 8889] Epoch 8890/12000
2024-11-05 01:27:01,840 - INFO - [diffusion][Epoch 8889] diffusion training Loss: 0.0689405556768179
2024-11-05 01:27:01,842 - INFO - [diffusion][Epoch 8889] diffusion learning rate: 0.001
2024-11-05 01:27:01,844 - INFO - [diffusion][Epoch 8889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:01,845 - INFO - [diffusion][Epoch 8890] Epoch 8891/12000
2024-11-05 01:27:06,063 - INFO - [diffusion][Epoch 8890] diffusion training Loss: 0.062171973288059235
2024-11-05 01:27:06,065 - INFO - [diffusion][Epoch 8890] diffusion learning rate: 0.001
2024-11-05 01:27:06,067 - INFO - [diffusion][Epoch 8890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:06,068 - INFO - [diffusion][Epoch 8891] Epoch 8892/12000
2024-11-05 01:27:10,200 - INFO - [diffusion][Epoch 8891] diffusion training Loss: 0.063509127125144
2024-11-05 01:27:10,202 - INFO - [diffusion][Epoch 8891] diffusion learning rate: 0.001
2024-11-05 01:27:10,204 - INFO - [diffusion][Epoch 8891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:10,206 - INFO - [diffusion][Epoch 8892] Epoch 8893/12000
2024-11-05 01:27:14,381 - INFO - [diffusion][Epoch 8892] diffusion training Loss: 0.05972389783710241
2024-11-05 01:27:14,383 - INFO - [diffusion][Epoch 8892] diffusion learning rate: 0.001
2024-11-05 01:27:14,385 - INFO - [diffusion][Epoch 8892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:14,386 - INFO - [diffusion][Epoch 8893] Epoch 8894/12000
2024-11-05 01:27:18,400 - INFO - [diffusion][Epoch 8893] diffusion training Loss: 0.06231484655290842
2024-11-05 01:27:18,403 - INFO - [diffusion][Epoch 8893] diffusion learning rate: 0.001
2024-11-05 01:27:18,404 - INFO - [diffusion][Epoch 8893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:18,406 - INFO - [diffusion][Epoch 8894] Epoch 8895/12000
2024-11-05 01:27:22,552 - INFO - [diffusion][Epoch 8894] diffusion training Loss: 0.06660126522183418
2024-11-05 01:27:22,554 - INFO - [diffusion][Epoch 8894] diffusion learning rate: 0.001
2024-11-05 01:27:22,556 - INFO - [diffusion][Epoch 8894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:22,557 - INFO - [diffusion][Epoch 8895] Epoch 8896/12000
2024-11-05 01:27:26,695 - INFO - [diffusion][Epoch 8895] diffusion training Loss: 0.06734942272305489
2024-11-05 01:27:26,697 - INFO - [diffusion][Epoch 8895] diffusion learning rate: 0.001
2024-11-05 01:27:26,699 - INFO - [diffusion][Epoch 8895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:26,700 - INFO - [diffusion][Epoch 8896] Epoch 8897/12000
2024-11-05 01:27:30,996 - INFO - [diffusion][Epoch 8896] diffusion training Loss: 0.060353923588991165
2024-11-05 01:27:30,998 - INFO - [diffusion][Epoch 8896] diffusion learning rate: 0.001
2024-11-05 01:27:31,001 - INFO - [diffusion][Epoch 8896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:31,002 - INFO - [diffusion][Epoch 8897] Epoch 8898/12000
2024-11-05 01:27:35,173 - INFO - [diffusion][Epoch 8897] diffusion training Loss: 0.06279906630516052
2024-11-05 01:27:35,175 - INFO - [diffusion][Epoch 8897] diffusion learning rate: 0.001
2024-11-05 01:27:35,177 - INFO - [diffusion][Epoch 8897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:35,179 - INFO - [diffusion][Epoch 8898] Epoch 8899/12000
2024-11-05 01:27:39,371 - INFO - [diffusion][Epoch 8898] diffusion training Loss: 0.06675883755087852
2024-11-05 01:27:39,373 - INFO - [diffusion][Epoch 8898] diffusion learning rate: 0.001
2024-11-05 01:27:39,375 - INFO - [diffusion][Epoch 8898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:39,376 - INFO - [diffusion][Epoch 8899] Epoch 8900/12000
2024-11-05 01:27:43,457 - INFO - [diffusion][Epoch 8899] diffusion training Loss: 0.06696427427232265
2024-11-05 01:27:43,459 - INFO - [diffusion][Epoch 8899] diffusion learning rate: 0.001
2024-11-05 01:27:43,461 - INFO - [diffusion][Epoch 8899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:43,462 - INFO - [diffusion][Epoch 8900] Epoch 8901/12000
2024-11-05 01:27:47,624 - INFO - [diffusion][Epoch 8900] diffusion training Loss: 0.06482718884944916
2024-11-05 01:27:47,626 - INFO - [diffusion][Epoch 8900] diffusion learning rate: 0.001
2024-11-05 01:27:47,628 - INFO - [diffusion][Epoch 8900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:47,629 - INFO - [diffusion][Epoch 8901] Epoch 8902/12000
2024-11-05 01:27:51,833 - INFO - [diffusion][Epoch 8901] diffusion training Loss: 0.0626549618318677
2024-11-05 01:27:51,835 - INFO - [diffusion][Epoch 8901] diffusion learning rate: 0.001
2024-11-05 01:27:51,837 - INFO - [diffusion][Epoch 8901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:51,838 - INFO - [diffusion][Epoch 8902] Epoch 8903/12000
2024-11-05 01:27:55,884 - INFO - [diffusion][Epoch 8902] diffusion training Loss: 0.06755871791392565
2024-11-05 01:27:55,886 - INFO - [diffusion][Epoch 8902] diffusion learning rate: 0.001
2024-11-05 01:27:55,888 - INFO - [diffusion][Epoch 8902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:55,889 - INFO - [diffusion][Epoch 8903] Epoch 8904/12000
2024-11-05 01:27:59,802 - INFO - [diffusion][Epoch 8903] diffusion training Loss: 0.0626883776858449
2024-11-05 01:27:59,804 - INFO - [diffusion][Epoch 8903] diffusion learning rate: 0.001
2024-11-05 01:27:59,806 - INFO - [diffusion][Epoch 8903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:59,807 - INFO - [diffusion][Epoch 8904] Epoch 8905/12000
2024-11-05 01:28:03,884 - INFO - [diffusion][Epoch 8904] diffusion training Loss: 0.06639557331800461
2024-11-05 01:28:03,886 - INFO - [diffusion][Epoch 8904] diffusion learning rate: 0.001
2024-11-05 01:28:03,888 - INFO - [diffusion][Epoch 8904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:03,889 - INFO - [diffusion][Epoch 8905] Epoch 8906/12000
2024-11-05 01:28:08,253 - INFO - [diffusion][Epoch 8905] diffusion training Loss: 0.06403475441038609
2024-11-05 01:28:08,255 - INFO - [diffusion][Epoch 8905] diffusion learning rate: 0.001
2024-11-05 01:28:08,256 - INFO - [diffusion][Epoch 8905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:08,258 - INFO - [diffusion][Epoch 8906] Epoch 8907/12000
2024-11-05 01:28:12,499 - INFO - [diffusion][Epoch 8906] diffusion training Loss: 0.06856123171746731
2024-11-05 01:28:12,502 - INFO - [diffusion][Epoch 8906] diffusion learning rate: 0.001
2024-11-05 01:28:12,503 - INFO - [diffusion][Epoch 8906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:12,504 - INFO - [diffusion][Epoch 8907] Epoch 8908/12000
2024-11-05 01:28:16,764 - INFO - [diffusion][Epoch 8907] diffusion training Loss: 0.055292231030762196
2024-11-05 01:28:16,766 - INFO - [diffusion][Epoch 8907] diffusion learning rate: 0.001
2024-11-05 01:28:16,768 - INFO - [diffusion][Epoch 8907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:16,769 - INFO - [diffusion][Epoch 8908] Epoch 8909/12000
2024-11-05 01:28:20,988 - INFO - [diffusion][Epoch 8908] diffusion training Loss: 0.06156652420759201
2024-11-05 01:28:20,990 - INFO - [diffusion][Epoch 8908] diffusion learning rate: 0.001
2024-11-05 01:28:20,992 - INFO - [diffusion][Epoch 8908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:20,993 - INFO - [diffusion][Epoch 8909] Epoch 8910/12000
2024-11-05 01:28:25,224 - INFO - [diffusion][Epoch 8909] diffusion training Loss: 0.061649518087506294
2024-11-05 01:28:25,226 - INFO - [diffusion][Epoch 8909] diffusion learning rate: 0.001
2024-11-05 01:28:25,228 - INFO - [diffusion][Epoch 8909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:25,229 - INFO - [diffusion][Epoch 8910] Epoch 8911/12000
2024-11-05 01:28:29,418 - INFO - [diffusion][Epoch 8910] diffusion training Loss: 0.0660120202228427
2024-11-05 01:28:29,420 - INFO - [diffusion][Epoch 8910] diffusion learning rate: 0.001
2024-11-05 01:28:29,422 - INFO - [diffusion][Epoch 8910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:29,424 - INFO - [diffusion][Epoch 8911] Epoch 8912/12000
2024-11-05 01:28:33,588 - INFO - [diffusion][Epoch 8911] diffusion training Loss: 0.06155016552656889
2024-11-05 01:28:33,590 - INFO - [diffusion][Epoch 8911] diffusion learning rate: 0.001
2024-11-05 01:28:33,592 - INFO - [diffusion][Epoch 8911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:33,593 - INFO - [diffusion][Epoch 8912] Epoch 8913/12000
2024-11-05 01:28:37,794 - INFO - [diffusion][Epoch 8912] diffusion training Loss: 0.06633905880153179
2024-11-05 01:28:37,796 - INFO - [diffusion][Epoch 8912] diffusion learning rate: 0.001
2024-11-05 01:28:37,798 - INFO - [diffusion][Epoch 8912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:37,799 - INFO - [diffusion][Epoch 8913] Epoch 8914/12000
2024-11-05 01:28:41,871 - INFO - [diffusion][Epoch 8913] diffusion training Loss: 0.06412439979612827
2024-11-05 01:28:41,874 - INFO - [diffusion][Epoch 8913] diffusion learning rate: 0.001
2024-11-05 01:28:41,875 - INFO - [diffusion][Epoch 8913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:41,877 - INFO - [diffusion][Epoch 8914] Epoch 8915/12000
2024-11-05 01:28:46,113 - INFO - [diffusion][Epoch 8914] diffusion training Loss: 0.06528154946863651
2024-11-05 01:28:46,115 - INFO - [diffusion][Epoch 8914] diffusion learning rate: 0.001
2024-11-05 01:28:46,117 - INFO - [diffusion][Epoch 8914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:46,118 - INFO - [diffusion][Epoch 8915] Epoch 8916/12000
2024-11-05 01:28:50,215 - INFO - [diffusion][Epoch 8915] diffusion training Loss: 0.06693035736680031
2024-11-05 01:28:50,217 - INFO - [diffusion][Epoch 8915] diffusion learning rate: 0.001
2024-11-05 01:28:50,219 - INFO - [diffusion][Epoch 8915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:50,221 - INFO - [diffusion][Epoch 8916] Epoch 8917/12000
2024-11-05 01:28:54,520 - INFO - [diffusion][Epoch 8916] diffusion training Loss: 0.06238681823015213
2024-11-05 01:28:54,522 - INFO - [diffusion][Epoch 8916] diffusion learning rate: 0.001
2024-11-05 01:28:54,524 - INFO - [diffusion][Epoch 8916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:54,525 - INFO - [diffusion][Epoch 8917] Epoch 8918/12000
2024-11-05 01:28:58,519 - INFO - [diffusion][Epoch 8917] diffusion training Loss: 0.06732039898633957
2024-11-05 01:28:58,521 - INFO - [diffusion][Epoch 8917] diffusion learning rate: 0.001
2024-11-05 01:28:58,523 - INFO - [diffusion][Epoch 8917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:58,524 - INFO - [diffusion][Epoch 8918] Epoch 8919/12000
2024-11-05 01:29:02,674 - INFO - [diffusion][Epoch 8918] diffusion training Loss: 0.0662549901753664
2024-11-05 01:29:02,676 - INFO - [diffusion][Epoch 8918] diffusion learning rate: 0.001
2024-11-05 01:29:02,678 - INFO - [diffusion][Epoch 8918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:02,679 - INFO - [diffusion][Epoch 8919] Epoch 8920/12000
2024-11-05 01:29:06,770 - INFO - [diffusion][Epoch 8919] diffusion training Loss: 0.06707390025258064
2024-11-05 01:29:06,772 - INFO - [diffusion][Epoch 8919] diffusion learning rate: 0.001
2024-11-05 01:29:06,774 - INFO - [diffusion][Epoch 8919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:06,775 - INFO - [diffusion][Epoch 8920] Epoch 8921/12000
2024-11-05 01:29:10,906 - INFO - [diffusion][Epoch 8920] diffusion training Loss: 0.0669883880764246
2024-11-05 01:29:10,908 - INFO - [diffusion][Epoch 8920] diffusion learning rate: 0.001
2024-11-05 01:29:10,910 - INFO - [diffusion][Epoch 8920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:10,911 - INFO - [diffusion][Epoch 8921] Epoch 8922/12000
2024-11-05 01:29:15,136 - INFO - [diffusion][Epoch 8921] diffusion training Loss: 0.05911604128777981
2024-11-05 01:29:15,138 - INFO - [diffusion][Epoch 8921] diffusion learning rate: 0.001
2024-11-05 01:29:15,140 - INFO - [diffusion][Epoch 8921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:15,141 - INFO - [diffusion][Epoch 8922] Epoch 8923/12000
2024-11-05 01:29:19,198 - INFO - [diffusion][Epoch 8922] diffusion training Loss: 0.06778714433312416
2024-11-05 01:29:19,201 - INFO - [diffusion][Epoch 8922] diffusion learning rate: 0.001
2024-11-05 01:29:19,204 - INFO - [diffusion][Epoch 8922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:19,205 - INFO - [diffusion][Epoch 8923] Epoch 8924/12000
2024-11-05 01:29:23,347 - INFO - [diffusion][Epoch 8923] diffusion training Loss: 0.06154287979006767
2024-11-05 01:29:23,349 - INFO - [diffusion][Epoch 8923] diffusion learning rate: 0.001
2024-11-05 01:29:23,388 - INFO - [diffusion][Epoch 8923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:23,390 - INFO - [diffusion][Epoch 8924] Epoch 8925/12000
2024-11-05 01:29:27,617 - INFO - [diffusion][Epoch 8924] diffusion training Loss: 0.06117502134293318
2024-11-05 01:29:27,619 - INFO - [diffusion][Epoch 8924] diffusion learning rate: 0.001
2024-11-05 01:29:27,621 - INFO - [diffusion][Epoch 8924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:27,622 - INFO - [diffusion][Epoch 8925] Epoch 8926/12000
2024-11-05 01:29:31,790 - INFO - [diffusion][Epoch 8925] diffusion training Loss: 0.06763998419046402
2024-11-05 01:29:31,793 - INFO - [diffusion][Epoch 8925] diffusion learning rate: 0.001
2024-11-05 01:29:31,796 - INFO - [diffusion][Epoch 8925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:31,797 - INFO - [diffusion][Epoch 8926] Epoch 8927/12000
2024-11-05 01:29:36,179 - INFO - [diffusion][Epoch 8926] diffusion training Loss: 0.06584252044558525
2024-11-05 01:29:36,182 - INFO - [diffusion][Epoch 8926] diffusion learning rate: 0.001
2024-11-05 01:29:36,184 - INFO - [diffusion][Epoch 8926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:36,185 - INFO - [diffusion][Epoch 8927] Epoch 8928/12000
2024-11-05 01:29:40,257 - INFO - [diffusion][Epoch 8927] diffusion training Loss: 0.06666283495724201
2024-11-05 01:29:40,259 - INFO - [diffusion][Epoch 8927] diffusion learning rate: 0.001
2024-11-05 01:29:40,260 - INFO - [diffusion][Epoch 8927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:40,261 - INFO - [diffusion][Epoch 8928] Epoch 8929/12000
2024-11-05 01:29:44,302 - INFO - [diffusion][Epoch 8928] diffusion training Loss: 0.06518595479428768
2024-11-05 01:29:44,305 - INFO - [diffusion][Epoch 8928] diffusion learning rate: 0.001
2024-11-05 01:29:44,307 - INFO - [diffusion][Epoch 8928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:44,308 - INFO - [diffusion][Epoch 8929] Epoch 8930/12000
2024-11-05 01:29:48,384 - INFO - [diffusion][Epoch 8929] diffusion training Loss: 0.06021660193800926
2024-11-05 01:29:48,386 - INFO - [diffusion][Epoch 8929] diffusion learning rate: 0.001
2024-11-05 01:29:48,388 - INFO - [diffusion][Epoch 8929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:48,389 - INFO - [diffusion][Epoch 8930] Epoch 8931/12000
2024-11-05 01:29:52,575 - INFO - [diffusion][Epoch 8930] diffusion training Loss: 0.06249658577144146
2024-11-05 01:29:52,577 - INFO - [diffusion][Epoch 8930] diffusion learning rate: 0.001
2024-11-05 01:29:52,579 - INFO - [diffusion][Epoch 8930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:52,580 - INFO - [diffusion][Epoch 8931] Epoch 8932/12000
2024-11-05 01:29:56,588 - INFO - [diffusion][Epoch 8931] diffusion training Loss: 0.06667723879218102
2024-11-05 01:29:56,590 - INFO - [diffusion][Epoch 8931] diffusion learning rate: 0.001
2024-11-05 01:29:56,591 - INFO - [diffusion][Epoch 8931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:56,592 - INFO - [diffusion][Epoch 8932] Epoch 8933/12000
2024-11-05 01:30:00,604 - INFO - [diffusion][Epoch 8932] diffusion training Loss: 0.06458974443376064
2024-11-05 01:30:00,606 - INFO - [diffusion][Epoch 8932] diffusion learning rate: 0.001
2024-11-05 01:30:00,608 - INFO - [diffusion][Epoch 8932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:00,609 - INFO - [diffusion][Epoch 8933] Epoch 8934/12000
2024-11-05 01:30:04,759 - INFO - [diffusion][Epoch 8933] diffusion training Loss: 0.06564164813607931
2024-11-05 01:30:04,761 - INFO - [diffusion][Epoch 8933] diffusion learning rate: 0.001
2024-11-05 01:30:04,762 - INFO - [diffusion][Epoch 8933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:04,763 - INFO - [diffusion][Epoch 8934] Epoch 8935/12000
2024-11-05 01:30:08,759 - INFO - [diffusion][Epoch 8934] diffusion training Loss: 0.0667007165029645
2024-11-05 01:30:08,761 - INFO - [diffusion][Epoch 8934] diffusion learning rate: 0.001
2024-11-05 01:30:08,763 - INFO - [diffusion][Epoch 8934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:08,764 - INFO - [diffusion][Epoch 8935] Epoch 8936/12000
2024-11-05 01:30:12,829 - INFO - [diffusion][Epoch 8935] diffusion training Loss: 0.061973029747605324
2024-11-05 01:30:12,831 - INFO - [diffusion][Epoch 8935] diffusion learning rate: 0.001
2024-11-05 01:30:13,088 - INFO - [diffusion][Epoch 8935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:13,090 - INFO - [diffusion][Epoch 8936] Epoch 8937/12000
2024-11-05 01:30:17,297 - INFO - [diffusion][Epoch 8936] diffusion training Loss: 0.0661585507914424
2024-11-05 01:30:17,299 - INFO - [diffusion][Epoch 8936] diffusion learning rate: 0.001
2024-11-05 01:30:17,301 - INFO - [diffusion][Epoch 8936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:17,302 - INFO - [diffusion][Epoch 8937] Epoch 8938/12000
2024-11-05 01:30:21,568 - INFO - [diffusion][Epoch 8937] diffusion training Loss: 0.06366981845349073
2024-11-05 01:30:21,571 - INFO - [diffusion][Epoch 8937] diffusion learning rate: 0.001
2024-11-05 01:30:21,572 - INFO - [diffusion][Epoch 8937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:21,574 - INFO - [diffusion][Epoch 8938] Epoch 8939/12000
2024-11-05 01:30:25,626 - INFO - [diffusion][Epoch 8938] diffusion training Loss: 0.05966223683208227
2024-11-05 01:30:25,629 - INFO - [diffusion][Epoch 8938] diffusion learning rate: 0.001
2024-11-05 01:30:25,630 - INFO - [diffusion][Epoch 8938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:25,634 - INFO - [diffusion][Epoch 8939] Epoch 8940/12000
2024-11-05 01:30:29,813 - INFO - [diffusion][Epoch 8939] diffusion training Loss: 0.06801235675811768
2024-11-05 01:30:29,815 - INFO - [diffusion][Epoch 8939] diffusion learning rate: 0.001
2024-11-05 01:30:29,817 - INFO - [diffusion][Epoch 8939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:29,818 - INFO - [diffusion][Epoch 8940] Epoch 8941/12000
2024-11-05 01:30:34,044 - INFO - [diffusion][Epoch 8940] diffusion training Loss: 0.07152551412582397
2024-11-05 01:30:34,045 - INFO - [diffusion][Epoch 8940] diffusion learning rate: 0.001
2024-11-05 01:30:34,047 - INFO - [diffusion][Epoch 8940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:34,048 - INFO - [diffusion][Epoch 8941] Epoch 8942/12000
2024-11-05 01:30:38,281 - INFO - [diffusion][Epoch 8941] diffusion training Loss: 0.06901715882122517
2024-11-05 01:30:38,283 - INFO - [diffusion][Epoch 8941] diffusion learning rate: 0.001
2024-11-05 01:30:38,284 - INFO - [diffusion][Epoch 8941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:38,286 - INFO - [diffusion][Epoch 8942] Epoch 8943/12000
2024-11-05 01:30:42,422 - INFO - [diffusion][Epoch 8942] diffusion training Loss: 0.06241430155932903
2024-11-05 01:30:42,424 - INFO - [diffusion][Epoch 8942] diffusion learning rate: 0.001
2024-11-05 01:30:42,426 - INFO - [diffusion][Epoch 8942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:42,427 - INFO - [diffusion][Epoch 8943] Epoch 8944/12000
2024-11-05 01:30:46,595 - INFO - [diffusion][Epoch 8943] diffusion training Loss: 0.06017881631851196
2024-11-05 01:30:46,597 - INFO - [diffusion][Epoch 8943] diffusion learning rate: 0.001
2024-11-05 01:30:46,599 - INFO - [diffusion][Epoch 8943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:46,601 - INFO - [diffusion][Epoch 8944] Epoch 8945/12000
2024-11-05 01:30:50,865 - INFO - [diffusion][Epoch 8944] diffusion training Loss: 0.06516673043370247
2024-11-05 01:30:50,867 - INFO - [diffusion][Epoch 8944] diffusion learning rate: 0.001
2024-11-05 01:30:50,870 - INFO - [diffusion][Epoch 8944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:50,871 - INFO - [diffusion][Epoch 8945] Epoch 8946/12000
2024-11-05 01:30:55,068 - INFO - [diffusion][Epoch 8945] diffusion training Loss: 0.06631774827837944
2024-11-05 01:30:55,071 - INFO - [diffusion][Epoch 8945] diffusion learning rate: 0.001
2024-11-05 01:30:55,072 - INFO - [diffusion][Epoch 8945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:55,074 - INFO - [diffusion][Epoch 8946] Epoch 8947/12000
2024-11-05 01:30:59,113 - INFO - [diffusion][Epoch 8946] diffusion training Loss: 0.06289444398134947
2024-11-05 01:30:59,115 - INFO - [diffusion][Epoch 8946] diffusion learning rate: 0.001
2024-11-05 01:30:59,116 - INFO - [diffusion][Epoch 8946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:59,118 - INFO - [diffusion][Epoch 8947] Epoch 8948/12000
2024-11-05 01:31:03,760 - INFO - [diffusion][Epoch 8947] diffusion training Loss: 0.06118748430162668
2024-11-05 01:31:03,762 - INFO - [diffusion][Epoch 8947] diffusion learning rate: 0.001
2024-11-05 01:31:03,764 - INFO - [diffusion][Epoch 8947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:03,765 - INFO - [diffusion][Epoch 8948] Epoch 8949/12000
2024-11-05 01:31:07,862 - INFO - [diffusion][Epoch 8948] diffusion training Loss: 0.06473691202700138
2024-11-05 01:31:07,864 - INFO - [diffusion][Epoch 8948] diffusion learning rate: 0.001
2024-11-05 01:31:07,866 - INFO - [diffusion][Epoch 8948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:07,867 - INFO - [diffusion][Epoch 8949] Epoch 8950/12000
2024-11-05 01:31:12,164 - INFO - [diffusion][Epoch 8949] diffusion training Loss: 0.06390501279383898
2024-11-05 01:31:12,166 - INFO - [diffusion][Epoch 8949] diffusion learning rate: 0.001
2024-11-05 01:31:12,168 - INFO - [diffusion][Epoch 8949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:12,169 - INFO - [diffusion][Epoch 8950] Epoch 8951/12000
2024-11-05 01:31:16,404 - INFO - [diffusion][Epoch 8950] diffusion training Loss: 0.06127624958753586
2024-11-05 01:31:16,406 - INFO - [diffusion][Epoch 8950] diffusion learning rate: 0.001
2024-11-05 01:31:16,408 - INFO - [diffusion][Epoch 8950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:16,410 - INFO - [diffusion][Epoch 8951] Epoch 8952/12000
2024-11-05 01:31:20,690 - INFO - [diffusion][Epoch 8951] diffusion training Loss: 0.06363237835466862
2024-11-05 01:31:20,692 - INFO - [diffusion][Epoch 8951] diffusion learning rate: 0.001
2024-11-05 01:31:20,694 - INFO - [diffusion][Epoch 8951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:20,695 - INFO - [diffusion][Epoch 8952] Epoch 8953/12000
2024-11-05 01:31:24,871 - INFO - [diffusion][Epoch 8952] diffusion training Loss: 0.05916385445743799
2024-11-05 01:31:24,872 - INFO - [diffusion][Epoch 8952] diffusion learning rate: 0.001
2024-11-05 01:31:24,874 - INFO - [diffusion][Epoch 8952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:24,875 - INFO - [diffusion][Epoch 8953] Epoch 8954/12000
2024-11-05 01:31:28,942 - INFO - [diffusion][Epoch 8953] diffusion training Loss: 0.06284382194280624
2024-11-05 01:31:28,944 - INFO - [diffusion][Epoch 8953] diffusion learning rate: 0.001
2024-11-05 01:31:28,946 - INFO - [diffusion][Epoch 8953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:28,948 - INFO - [diffusion][Epoch 8954] Epoch 8955/12000
2024-11-05 01:31:32,999 - INFO - [diffusion][Epoch 8954] diffusion training Loss: 0.060630565509200096
2024-11-05 01:31:33,002 - INFO - [diffusion][Epoch 8954] diffusion learning rate: 0.001
2024-11-05 01:31:33,003 - INFO - [diffusion][Epoch 8954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:33,005 - INFO - [diffusion][Epoch 8955] Epoch 8956/12000
2024-11-05 01:31:37,096 - INFO - [diffusion][Epoch 8955] diffusion training Loss: 0.06843282654881477
2024-11-05 01:31:37,111 - INFO - [diffusion][Epoch 8955] diffusion learning rate: 0.001
2024-11-05 01:31:37,113 - INFO - [diffusion][Epoch 8955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:37,114 - INFO - [diffusion][Epoch 8956] Epoch 8957/12000
2024-11-05 01:31:41,143 - INFO - [diffusion][Epoch 8956] diffusion training Loss: 0.06283749267458916
2024-11-05 01:31:41,145 - INFO - [diffusion][Epoch 8956] diffusion learning rate: 0.001
2024-11-05 01:31:41,147 - INFO - [diffusion][Epoch 8956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:41,148 - INFO - [diffusion][Epoch 8957] Epoch 8958/12000
2024-11-05 01:31:45,175 - INFO - [diffusion][Epoch 8957] diffusion training Loss: 0.06545895151793957
2024-11-05 01:31:45,177 - INFO - [diffusion][Epoch 8957] diffusion learning rate: 0.001
2024-11-05 01:31:45,181 - INFO - [diffusion][Epoch 8957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:45,182 - INFO - [diffusion][Epoch 8958] Epoch 8959/12000
2024-11-05 01:31:49,263 - INFO - [diffusion][Epoch 8958] diffusion training Loss: 0.07244939543306828
2024-11-05 01:31:49,265 - INFO - [diffusion][Epoch 8958] diffusion learning rate: 0.001
2024-11-05 01:31:49,321 - INFO - [diffusion][Epoch 8958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:49,322 - INFO - [diffusion][Epoch 8959] Epoch 8960/12000
2024-11-05 01:31:53,498 - INFO - [diffusion][Epoch 8959] diffusion training Loss: 0.06582805514335632
2024-11-05 01:31:53,500 - INFO - [diffusion][Epoch 8959] diffusion learning rate: 0.001
2024-11-05 01:31:53,502 - INFO - [diffusion][Epoch 8959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:53,503 - INFO - [diffusion][Epoch 8960] Epoch 8961/12000
2024-11-05 01:31:57,609 - INFO - [diffusion][Epoch 8960] diffusion training Loss: 0.06112643796950579
2024-11-05 01:31:57,611 - INFO - [diffusion][Epoch 8960] diffusion learning rate: 0.001
2024-11-05 01:31:57,613 - INFO - [diffusion][Epoch 8960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:57,615 - INFO - [diffusion][Epoch 8961] Epoch 8962/12000
2024-11-05 01:32:01,801 - INFO - [diffusion][Epoch 8961] diffusion training Loss: 0.061408502981066704
2024-11-05 01:32:01,803 - INFO - [diffusion][Epoch 8961] diffusion learning rate: 0.001
2024-11-05 01:32:01,805 - INFO - [diffusion][Epoch 8961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:01,806 - INFO - [diffusion][Epoch 8962] Epoch 8963/12000
2024-11-05 01:32:06,012 - INFO - [diffusion][Epoch 8962] diffusion training Loss: 0.06447470095008612
2024-11-05 01:32:06,014 - INFO - [diffusion][Epoch 8962] diffusion learning rate: 0.001
2024-11-05 01:32:06,015 - INFO - [diffusion][Epoch 8962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:06,017 - INFO - [diffusion][Epoch 8963] Epoch 8964/12000
2024-11-05 01:32:10,239 - INFO - [diffusion][Epoch 8963] diffusion training Loss: 0.06979726813733578
2024-11-05 01:32:10,241 - INFO - [diffusion][Epoch 8963] diffusion learning rate: 0.001
2024-11-05 01:32:10,242 - INFO - [diffusion][Epoch 8963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:10,244 - INFO - [diffusion][Epoch 8964] Epoch 8965/12000
2024-11-05 01:32:14,420 - INFO - [diffusion][Epoch 8964] diffusion training Loss: 0.06492201425135136
2024-11-05 01:32:14,423 - INFO - [diffusion][Epoch 8964] diffusion learning rate: 0.001
2024-11-05 01:32:14,426 - INFO - [diffusion][Epoch 8964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:14,427 - INFO - [diffusion][Epoch 8965] Epoch 8966/12000
2024-11-05 01:32:18,516 - INFO - [diffusion][Epoch 8965] diffusion training Loss: 0.0646508801728487
2024-11-05 01:32:18,518 - INFO - [diffusion][Epoch 8965] diffusion learning rate: 0.001
2024-11-05 01:32:18,520 - INFO - [diffusion][Epoch 8965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:18,521 - INFO - [diffusion][Epoch 8966] Epoch 8967/12000
2024-11-05 01:32:22,680 - INFO - [diffusion][Epoch 8966] diffusion training Loss: 0.062349818646907806
2024-11-05 01:32:22,682 - INFO - [diffusion][Epoch 8966] diffusion learning rate: 0.001
2024-11-05 01:32:22,684 - INFO - [diffusion][Epoch 8966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:22,685 - INFO - [diffusion][Epoch 8967] Epoch 8968/12000
2024-11-05 01:32:27,301 - INFO - [diffusion][Epoch 8967] diffusion training Loss: 0.06799057684838772
2024-11-05 01:32:27,303 - INFO - [diffusion][Epoch 8967] diffusion learning rate: 0.001
2024-11-05 01:32:27,305 - INFO - [diffusion][Epoch 8967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:27,306 - INFO - [diffusion][Epoch 8968] Epoch 8969/12000
2024-11-05 01:32:31,389 - INFO - [diffusion][Epoch 8968] diffusion training Loss: 0.05903156101703644
2024-11-05 01:32:31,391 - INFO - [diffusion][Epoch 8968] diffusion learning rate: 0.001
2024-11-05 01:32:31,393 - INFO - [diffusion][Epoch 8968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:31,395 - INFO - [diffusion][Epoch 8969] Epoch 8970/12000
2024-11-05 01:32:35,505 - INFO - [diffusion][Epoch 8969] diffusion training Loss: 0.06628595665097237
2024-11-05 01:32:35,507 - INFO - [diffusion][Epoch 8969] diffusion learning rate: 0.001
2024-11-05 01:32:35,509 - INFO - [diffusion][Epoch 8969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:35,511 - INFO - [diffusion][Epoch 8970] Epoch 8971/12000
2024-11-05 01:32:39,622 - INFO - [diffusion][Epoch 8970] diffusion training Loss: 0.06799224391579628
2024-11-05 01:32:39,625 - INFO - [diffusion][Epoch 8970] diffusion learning rate: 0.001
2024-11-05 01:32:39,627 - INFO - [diffusion][Epoch 8970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:39,628 - INFO - [diffusion][Epoch 8971] Epoch 8972/12000
2024-11-05 01:32:43,895 - INFO - [diffusion][Epoch 8971] diffusion training Loss: 0.0667876722291112
2024-11-05 01:32:43,898 - INFO - [diffusion][Epoch 8971] diffusion learning rate: 0.001
2024-11-05 01:32:43,901 - INFO - [diffusion][Epoch 8971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:43,902 - INFO - [diffusion][Epoch 8972] Epoch 8973/12000
2024-11-05 01:32:48,041 - INFO - [diffusion][Epoch 8972] diffusion training Loss: 0.06438056845217943
2024-11-05 01:32:48,043 - INFO - [diffusion][Epoch 8972] diffusion learning rate: 0.001
2024-11-05 01:32:48,045 - INFO - [diffusion][Epoch 8972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:48,046 - INFO - [diffusion][Epoch 8973] Epoch 8974/12000
2024-11-05 01:32:52,247 - INFO - [diffusion][Epoch 8973] diffusion training Loss: 0.07067521009594202
2024-11-05 01:32:52,249 - INFO - [diffusion][Epoch 8973] diffusion learning rate: 0.001
2024-11-05 01:32:52,251 - INFO - [diffusion][Epoch 8973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:52,252 - INFO - [diffusion][Epoch 8974] Epoch 8975/12000
2024-11-05 01:32:56,463 - INFO - [diffusion][Epoch 8974] diffusion training Loss: 0.06376127246767282
2024-11-05 01:32:56,465 - INFO - [diffusion][Epoch 8974] diffusion learning rate: 0.001
2024-11-05 01:32:56,466 - INFO - [diffusion][Epoch 8974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:56,467 - INFO - [diffusion][Epoch 8975] Epoch 8976/12000
2024-11-05 01:33:00,525 - INFO - [diffusion][Epoch 8975] diffusion training Loss: 0.0657815970480442
2024-11-05 01:33:00,527 - INFO - [diffusion][Epoch 8975] diffusion learning rate: 0.001
2024-11-05 01:33:00,529 - INFO - [diffusion][Epoch 8975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:00,530 - INFO - [diffusion][Epoch 8976] Epoch 8977/12000
2024-11-05 01:33:04,884 - INFO - [diffusion][Epoch 8976] diffusion training Loss: 0.07243485748767853
2024-11-05 01:33:04,886 - INFO - [diffusion][Epoch 8976] diffusion learning rate: 0.001
2024-11-05 01:33:04,888 - INFO - [diffusion][Epoch 8976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:04,889 - INFO - [diffusion][Epoch 8977] Epoch 8978/12000
2024-11-05 01:33:08,862 - INFO - [diffusion][Epoch 8977] diffusion training Loss: 0.06901570223271847
2024-11-05 01:33:08,864 - INFO - [diffusion][Epoch 8977] diffusion learning rate: 0.001
2024-11-05 01:33:08,865 - INFO - [diffusion][Epoch 8977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:08,867 - INFO - [diffusion][Epoch 8978] Epoch 8979/12000
2024-11-05 01:33:13,081 - INFO - [diffusion][Epoch 8978] diffusion training Loss: 0.06595195084810257
2024-11-05 01:33:13,083 - INFO - [diffusion][Epoch 8978] diffusion learning rate: 0.001
2024-11-05 01:33:13,084 - INFO - [diffusion][Epoch 8978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:13,085 - INFO - [diffusion][Epoch 8979] Epoch 8980/12000
2024-11-05 01:33:17,054 - INFO - [diffusion][Epoch 8979] diffusion training Loss: 0.06338301301002502
2024-11-05 01:33:17,056 - INFO - [diffusion][Epoch 8979] diffusion learning rate: 0.001
2024-11-05 01:33:17,058 - INFO - [diffusion][Epoch 8979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:17,059 - INFO - [diffusion][Epoch 8980] Epoch 8981/12000
2024-11-05 01:33:21,118 - INFO - [diffusion][Epoch 8980] diffusion training Loss: 0.06809197925031185
2024-11-05 01:33:21,120 - INFO - [diffusion][Epoch 8980] diffusion learning rate: 0.001
2024-11-05 01:33:21,122 - INFO - [diffusion][Epoch 8980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:21,123 - INFO - [diffusion][Epoch 8981] Epoch 8982/12000
2024-11-05 01:33:25,091 - INFO - [diffusion][Epoch 8981] diffusion training Loss: 0.07010155357420444
2024-11-05 01:33:25,093 - INFO - [diffusion][Epoch 8981] diffusion learning rate: 0.001
2024-11-05 01:33:25,095 - INFO - [diffusion][Epoch 8981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:25,096 - INFO - [diffusion][Epoch 8982] Epoch 8983/12000
2024-11-05 01:33:29,123 - INFO - [diffusion][Epoch 8982] diffusion training Loss: 0.06181270536035299
2024-11-05 01:33:29,125 - INFO - [diffusion][Epoch 8982] diffusion learning rate: 0.001
2024-11-05 01:33:29,127 - INFO - [diffusion][Epoch 8982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:29,128 - INFO - [diffusion][Epoch 8983] Epoch 8984/12000
2024-11-05 01:33:33,468 - INFO - [diffusion][Epoch 8983] diffusion training Loss: 0.061939376406371593
2024-11-05 01:33:33,471 - INFO - [diffusion][Epoch 8983] diffusion learning rate: 0.001
2024-11-05 01:33:33,473 - INFO - [diffusion][Epoch 8983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:33,474 - INFO - [diffusion][Epoch 8984] Epoch 8985/12000
2024-11-05 01:33:37,694 - INFO - [diffusion][Epoch 8984] diffusion training Loss: 0.062490349635481834
2024-11-05 01:33:37,696 - INFO - [diffusion][Epoch 8984] diffusion learning rate: 0.001
2024-11-05 01:33:37,698 - INFO - [diffusion][Epoch 8984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:37,700 - INFO - [diffusion][Epoch 8985] Epoch 8986/12000
2024-11-05 01:33:41,790 - INFO - [diffusion][Epoch 8985] diffusion training Loss: 0.06469557993113995
2024-11-05 01:33:41,792 - INFO - [diffusion][Epoch 8985] diffusion learning rate: 0.001
2024-11-05 01:33:41,794 - INFO - [diffusion][Epoch 8985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:41,795 - INFO - [diffusion][Epoch 8986] Epoch 8987/12000
2024-11-05 01:33:45,774 - INFO - [diffusion][Epoch 8986] diffusion training Loss: 0.06416797544807196
2024-11-05 01:33:45,777 - INFO - [diffusion][Epoch 8986] diffusion learning rate: 0.001
2024-11-05 01:33:45,779 - INFO - [diffusion][Epoch 8986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:45,780 - INFO - [diffusion][Epoch 8987] Epoch 8988/12000
2024-11-05 01:33:50,542 - INFO - [diffusion][Epoch 8987] diffusion training Loss: 0.06596329435706139
2024-11-05 01:33:50,544 - INFO - [diffusion][Epoch 8987] diffusion learning rate: 0.001
2024-11-05 01:33:50,546 - INFO - [diffusion][Epoch 8987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:50,548 - INFO - [diffusion][Epoch 8988] Epoch 8989/12000
2024-11-05 01:33:54,697 - INFO - [diffusion][Epoch 8988] diffusion training Loss: 0.07024952583014965
2024-11-05 01:33:54,699 - INFO - [diffusion][Epoch 8988] diffusion learning rate: 0.001
2024-11-05 01:33:54,700 - INFO - [diffusion][Epoch 8988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:54,701 - INFO - [diffusion][Epoch 8989] Epoch 8990/12000
2024-11-05 01:33:58,880 - INFO - [diffusion][Epoch 8989] diffusion training Loss: 0.06450740806758404
2024-11-05 01:33:58,882 - INFO - [diffusion][Epoch 8989] diffusion learning rate: 0.001
2024-11-05 01:33:58,884 - INFO - [diffusion][Epoch 8989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:58,885 - INFO - [diffusion][Epoch 8990] Epoch 8991/12000
2024-11-05 01:34:03,073 - INFO - [diffusion][Epoch 8990] diffusion training Loss: 0.0661425068974495
2024-11-05 01:34:03,076 - INFO - [diffusion][Epoch 8990] diffusion learning rate: 0.001
2024-11-05 01:34:03,078 - INFO - [diffusion][Epoch 8990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:03,079 - INFO - [diffusion][Epoch 8991] Epoch 8992/12000
2024-11-05 01:34:07,200 - INFO - [diffusion][Epoch 8991] diffusion training Loss: 0.061384858563542366
2024-11-05 01:34:07,201 - INFO - [diffusion][Epoch 8991] diffusion learning rate: 0.001
2024-11-05 01:34:07,203 - INFO - [diffusion][Epoch 8991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:07,204 - INFO - [diffusion][Epoch 8992] Epoch 8993/12000
2024-11-05 01:34:11,195 - INFO - [diffusion][Epoch 8992] diffusion training Loss: 0.060854602605104446
2024-11-05 01:34:11,197 - INFO - [diffusion][Epoch 8992] diffusion learning rate: 0.001
2024-11-05 01:34:11,200 - INFO - [diffusion][Epoch 8992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:11,202 - INFO - [diffusion][Epoch 8993] Epoch 8994/12000
2024-11-05 01:34:15,180 - INFO - [diffusion][Epoch 8993] diffusion training Loss: 0.06269226502627134
2024-11-05 01:34:15,182 - INFO - [diffusion][Epoch 8993] diffusion learning rate: 0.001
2024-11-05 01:34:15,184 - INFO - [diffusion][Epoch 8993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:15,186 - INFO - [diffusion][Epoch 8994] Epoch 8995/12000
2024-11-05 01:34:19,312 - INFO - [diffusion][Epoch 8994] diffusion training Loss: 0.06440393254160881
2024-11-05 01:34:19,314 - INFO - [diffusion][Epoch 8994] diffusion learning rate: 0.001
2024-11-05 01:34:19,316 - INFO - [diffusion][Epoch 8994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:19,317 - INFO - [diffusion][Epoch 8995] Epoch 8996/12000
2024-11-05 01:34:23,496 - INFO - [diffusion][Epoch 8995] diffusion training Loss: 0.06476663611829281
2024-11-05 01:34:23,498 - INFO - [diffusion][Epoch 8995] diffusion learning rate: 0.001
2024-11-05 01:34:23,499 - INFO - [diffusion][Epoch 8995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:23,501 - INFO - [diffusion][Epoch 8996] Epoch 8997/12000
2024-11-05 01:34:27,598 - INFO - [diffusion][Epoch 8996] diffusion training Loss: 0.05823680851608515
2024-11-05 01:34:27,601 - INFO - [diffusion][Epoch 8996] diffusion learning rate: 0.001
2024-11-05 01:34:27,603 - INFO - [diffusion][Epoch 8996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:27,604 - INFO - [diffusion][Epoch 8997] Epoch 8998/12000
2024-11-05 01:34:31,648 - INFO - [diffusion][Epoch 8997] diffusion training Loss: 0.056707548908889294
2024-11-05 01:34:31,650 - INFO - [diffusion][Epoch 8997] diffusion learning rate: 0.001
2024-11-05 01:34:31,652 - INFO - [diffusion][Epoch 8997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:31,653 - INFO - [diffusion][Epoch 8998] Epoch 8999/12000
2024-11-05 01:34:35,657 - INFO - [diffusion][Epoch 8998] diffusion training Loss: 0.060899337753653526
2024-11-05 01:34:35,659 - INFO - [diffusion][Epoch 8998] diffusion learning rate: 0.001
2024-11-05 01:34:35,661 - INFO - [diffusion][Epoch 8998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:35,662 - INFO - [diffusion][Epoch 8999] Epoch 9000/12000
2024-11-05 01:34:39,329 - INFO - [diffusion][Epoch 8999] diffusion training Loss: 0.07042841240763664
2024-11-05 01:34:39,331 - INFO - [diffusion][Epoch 8999] diffusion learning rate: 0.001
2024-11-05 01:34:39,440 - INFO - [diffusion][Epoch 8999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:39,441 - INFO - [diffusion][Epoch 9000] Epoch 9001/12000
2024-11-05 01:34:43,534 - INFO - [diffusion][Epoch 9000] diffusion training Loss: 0.06397600565105677
2024-11-05 01:34:43,536 - INFO - [diffusion][Epoch 9000] diffusion learning rate: 0.001
2024-11-05 01:34:43,538 - INFO - [diffusion][Epoch 9000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:43,540 - INFO - [diffusion][Epoch 9001] Epoch 9002/12000
2024-11-05 01:34:47,809 - INFO - [diffusion][Epoch 9001] diffusion training Loss: 0.06449668668210506
2024-11-05 01:34:47,811 - INFO - [diffusion][Epoch 9001] diffusion learning rate: 0.001
2024-11-05 01:34:47,813 - INFO - [diffusion][Epoch 9001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:47,815 - INFO - [diffusion][Epoch 9002] Epoch 9003/12000
2024-11-05 01:34:52,104 - INFO - [diffusion][Epoch 9002] diffusion training Loss: 0.06606030091643333
2024-11-05 01:34:52,106 - INFO - [diffusion][Epoch 9002] diffusion learning rate: 0.001
2024-11-05 01:34:52,108 - INFO - [diffusion][Epoch 9002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:52,109 - INFO - [diffusion][Epoch 9003] Epoch 9004/12000
2024-11-05 01:34:56,244 - INFO - [diffusion][Epoch 9003] diffusion training Loss: 0.0702590998262167
2024-11-05 01:34:56,262 - INFO - [diffusion][Epoch 9003] diffusion learning rate: 0.001
2024-11-05 01:34:56,264 - INFO - [diffusion][Epoch 9003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:56,267 - INFO - [diffusion][Epoch 9004] Epoch 9005/12000
2024-11-05 01:35:00,511 - INFO - [diffusion][Epoch 9004] diffusion training Loss: 0.0640726899728179
2024-11-05 01:35:00,513 - INFO - [diffusion][Epoch 9004] diffusion learning rate: 0.001
2024-11-05 01:35:00,514 - INFO - [diffusion][Epoch 9004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:00,516 - INFO - [diffusion][Epoch 9005] Epoch 9006/12000
2024-11-05 01:35:04,734 - INFO - [diffusion][Epoch 9005] diffusion training Loss: 0.06062791869044304
2024-11-05 01:35:04,737 - INFO - [diffusion][Epoch 9005] diffusion learning rate: 0.001
2024-11-05 01:35:04,740 - INFO - [diffusion][Epoch 9005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:04,742 - INFO - [diffusion][Epoch 9006] Epoch 9007/12000
2024-11-05 01:35:08,867 - INFO - [diffusion][Epoch 9006] diffusion training Loss: 0.06630536541342735
2024-11-05 01:35:08,869 - INFO - [diffusion][Epoch 9006] diffusion learning rate: 0.001
2024-11-05 01:35:08,870 - INFO - [diffusion][Epoch 9006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:08,872 - INFO - [diffusion][Epoch 9007] Epoch 9008/12000
2024-11-05 01:35:13,355 - INFO - [diffusion][Epoch 9007] diffusion training Loss: 0.07098177447915077
2024-11-05 01:35:13,357 - INFO - [diffusion][Epoch 9007] diffusion learning rate: 0.001
2024-11-05 01:35:13,359 - INFO - [diffusion][Epoch 9007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:13,360 - INFO - [diffusion][Epoch 9008] Epoch 9009/12000
2024-11-05 01:35:17,455 - INFO - [diffusion][Epoch 9008] diffusion training Loss: 0.06496318150311708
2024-11-05 01:35:17,457 - INFO - [diffusion][Epoch 9008] diffusion learning rate: 0.001
2024-11-05 01:35:17,459 - INFO - [diffusion][Epoch 9008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:17,461 - INFO - [diffusion][Epoch 9009] Epoch 9010/12000
2024-11-05 01:35:21,499 - INFO - [diffusion][Epoch 9009] diffusion training Loss: 0.06183280050754547
2024-11-05 01:35:21,501 - INFO - [diffusion][Epoch 9009] diffusion learning rate: 0.001
2024-11-05 01:35:21,503 - INFO - [diffusion][Epoch 9009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:21,504 - INFO - [diffusion][Epoch 9010] Epoch 9011/12000
2024-11-05 01:35:25,622 - INFO - [diffusion][Epoch 9010] diffusion training Loss: 0.06446312740445137
2024-11-05 01:35:25,625 - INFO - [diffusion][Epoch 9010] diffusion learning rate: 0.001
2024-11-05 01:35:25,626 - INFO - [diffusion][Epoch 9010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:25,628 - INFO - [diffusion][Epoch 9011] Epoch 9012/12000
2024-11-05 01:35:29,728 - INFO - [diffusion][Epoch 9011] diffusion training Loss: 0.06154307723045349
2024-11-05 01:35:29,731 - INFO - [diffusion][Epoch 9011] diffusion learning rate: 0.001
2024-11-05 01:35:29,732 - INFO - [diffusion][Epoch 9011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:29,734 - INFO - [diffusion][Epoch 9012] Epoch 9013/12000
2024-11-05 01:35:33,918 - INFO - [diffusion][Epoch 9012] diffusion training Loss: 0.06430136132985353
2024-11-05 01:35:33,920 - INFO - [diffusion][Epoch 9012] diffusion learning rate: 0.001
2024-11-05 01:35:33,922 - INFO - [diffusion][Epoch 9012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:33,923 - INFO - [diffusion][Epoch 9013] Epoch 9014/12000
2024-11-05 01:35:38,175 - INFO - [diffusion][Epoch 9013] diffusion training Loss: 0.06215847749263048
2024-11-05 01:35:38,176 - INFO - [diffusion][Epoch 9013] diffusion learning rate: 0.001
2024-11-05 01:35:38,178 - INFO - [diffusion][Epoch 9013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:38,179 - INFO - [diffusion][Epoch 9014] Epoch 9015/12000
2024-11-05 01:35:42,421 - INFO - [diffusion][Epoch 9014] diffusion training Loss: 0.0662667527794838
2024-11-05 01:35:42,422 - INFO - [diffusion][Epoch 9014] diffusion learning rate: 0.001
2024-11-05 01:35:42,424 - INFO - [diffusion][Epoch 9014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:42,425 - INFO - [diffusion][Epoch 9015] Epoch 9016/12000
2024-11-05 01:35:46,266 - INFO - [diffusion][Epoch 9015] diffusion training Loss: 0.06688265595585108
2024-11-05 01:35:46,267 - INFO - [diffusion][Epoch 9015] diffusion learning rate: 0.001
2024-11-05 01:35:46,269 - INFO - [diffusion][Epoch 9015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:46,270 - INFO - [diffusion][Epoch 9016] Epoch 9017/12000
2024-11-05 01:35:50,553 - INFO - [diffusion][Epoch 9016] diffusion training Loss: 0.0599218150600791
2024-11-05 01:35:50,555 - INFO - [diffusion][Epoch 9016] diffusion learning rate: 0.001
2024-11-05 01:35:50,557 - INFO - [diffusion][Epoch 9016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:50,558 - INFO - [diffusion][Epoch 9017] Epoch 9018/12000
2024-11-05 01:35:54,805 - INFO - [diffusion][Epoch 9017] diffusion training Loss: 0.0640961267054081
2024-11-05 01:35:54,807 - INFO - [diffusion][Epoch 9017] diffusion learning rate: 0.001
2024-11-05 01:35:54,809 - INFO - [diffusion][Epoch 9017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:54,810 - INFO - [diffusion][Epoch 9018] Epoch 9019/12000
2024-11-05 01:35:58,910 - INFO - [diffusion][Epoch 9018] diffusion training Loss: 0.06068541295826435
2024-11-05 01:35:58,912 - INFO - [diffusion][Epoch 9018] diffusion learning rate: 0.001
2024-11-05 01:35:58,914 - INFO - [diffusion][Epoch 9018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:58,915 - INFO - [diffusion][Epoch 9019] Epoch 9020/12000
2024-11-05 01:36:03,228 - INFO - [diffusion][Epoch 9019] diffusion training Loss: 0.06488261092454195
2024-11-05 01:36:03,231 - INFO - [diffusion][Epoch 9019] diffusion learning rate: 0.001
2024-11-05 01:36:03,233 - INFO - [diffusion][Epoch 9019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:03,236 - INFO - [diffusion][Epoch 9020] Epoch 9021/12000
2024-11-05 01:36:07,495 - INFO - [diffusion][Epoch 9020] diffusion training Loss: 0.06210179999470711
2024-11-05 01:36:07,497 - INFO - [diffusion][Epoch 9020] diffusion learning rate: 0.001
2024-11-05 01:36:07,499 - INFO - [diffusion][Epoch 9020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:07,500 - INFO - [diffusion][Epoch 9021] Epoch 9022/12000
2024-11-05 01:36:11,570 - INFO - [diffusion][Epoch 9021] diffusion training Loss: 0.06518546864390373
2024-11-05 01:36:11,572 - INFO - [diffusion][Epoch 9021] diffusion learning rate: 0.001
2024-11-05 01:36:11,575 - INFO - [diffusion][Epoch 9021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:11,576 - INFO - [diffusion][Epoch 9022] Epoch 9023/12000
2024-11-05 01:36:15,753 - INFO - [diffusion][Epoch 9022] diffusion training Loss: 0.06338413711637259
2024-11-05 01:36:15,755 - INFO - [diffusion][Epoch 9022] diffusion learning rate: 0.001
2024-11-05 01:36:15,824 - INFO - [diffusion][Epoch 9022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:15,826 - INFO - [diffusion][Epoch 9023] Epoch 9024/12000
2024-11-05 01:36:19,939 - INFO - [diffusion][Epoch 9023] diffusion training Loss: 0.06561189889907837
2024-11-05 01:36:19,942 - INFO - [diffusion][Epoch 9023] diffusion learning rate: 0.001
2024-11-05 01:36:19,944 - INFO - [diffusion][Epoch 9023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:19,945 - INFO - [diffusion][Epoch 9024] Epoch 9025/12000
2024-11-05 01:36:24,108 - INFO - [diffusion][Epoch 9024] diffusion training Loss: 0.06865675374865532
2024-11-05 01:36:24,110 - INFO - [diffusion][Epoch 9024] diffusion learning rate: 0.001
2024-11-05 01:36:24,111 - INFO - [diffusion][Epoch 9024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:24,113 - INFO - [diffusion][Epoch 9025] Epoch 9026/12000
2024-11-05 01:36:28,302 - INFO - [diffusion][Epoch 9025] diffusion training Loss: 0.06184072233736515
2024-11-05 01:36:28,304 - INFO - [diffusion][Epoch 9025] diffusion learning rate: 0.001
2024-11-05 01:36:28,306 - INFO - [diffusion][Epoch 9025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:28,307 - INFO - [diffusion][Epoch 9026] Epoch 9027/12000
2024-11-05 01:36:32,481 - INFO - [diffusion][Epoch 9026] diffusion training Loss: 0.067832225933671
2024-11-05 01:36:32,483 - INFO - [diffusion][Epoch 9026] diffusion learning rate: 0.001
2024-11-05 01:36:32,485 - INFO - [diffusion][Epoch 9026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:32,486 - INFO - [diffusion][Epoch 9027] Epoch 9028/12000
2024-11-05 01:36:37,051 - INFO - [diffusion][Epoch 9027] diffusion training Loss: 0.06400035228580236
2024-11-05 01:36:37,053 - INFO - [diffusion][Epoch 9027] diffusion learning rate: 0.001
2024-11-05 01:36:37,055 - INFO - [diffusion][Epoch 9027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:37,057 - INFO - [diffusion][Epoch 9028] Epoch 9029/12000
2024-11-05 01:36:41,131 - INFO - [diffusion][Epoch 9028] diffusion training Loss: 0.06183885782957077
2024-11-05 01:36:41,135 - INFO - [diffusion][Epoch 9028] diffusion learning rate: 0.001
2024-11-05 01:36:41,137 - INFO - [diffusion][Epoch 9028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:41,138 - INFO - [diffusion][Epoch 9029] Epoch 9030/12000
2024-11-05 01:36:45,138 - INFO - [diffusion][Epoch 9029] diffusion training Loss: 0.059313652105629444
2024-11-05 01:36:45,140 - INFO - [diffusion][Epoch 9029] diffusion learning rate: 0.001
2024-11-05 01:36:45,142 - INFO - [diffusion][Epoch 9029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:45,143 - INFO - [diffusion][Epoch 9030] Epoch 9031/12000
2024-11-05 01:36:49,293 - INFO - [diffusion][Epoch 9030] diffusion training Loss: 0.06663540564477444
2024-11-05 01:36:49,295 - INFO - [diffusion][Epoch 9030] diffusion learning rate: 0.001
2024-11-05 01:36:49,297 - INFO - [diffusion][Epoch 9030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:49,298 - INFO - [diffusion][Epoch 9031] Epoch 9032/12000
2024-11-05 01:36:53,582 - INFO - [diffusion][Epoch 9031] diffusion training Loss: 0.06129905767738819
2024-11-05 01:36:53,585 - INFO - [diffusion][Epoch 9031] diffusion learning rate: 0.001
2024-11-05 01:36:53,587 - INFO - [diffusion][Epoch 9031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:53,589 - INFO - [diffusion][Epoch 9032] Epoch 9033/12000
2024-11-05 01:36:57,626 - INFO - [diffusion][Epoch 9032] diffusion training Loss: 0.06379448436200619
2024-11-05 01:36:57,628 - INFO - [diffusion][Epoch 9032] diffusion learning rate: 0.001
2024-11-05 01:36:57,629 - INFO - [diffusion][Epoch 9032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:57,631 - INFO - [diffusion][Epoch 9033] Epoch 9034/12000
2024-11-05 01:37:01,759 - INFO - [diffusion][Epoch 9033] diffusion training Loss: 0.06681889109313488
2024-11-05 01:37:01,761 - INFO - [diffusion][Epoch 9033] diffusion learning rate: 0.001
2024-11-05 01:37:01,763 - INFO - [diffusion][Epoch 9033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:01,764 - INFO - [diffusion][Epoch 9034] Epoch 9035/12000
2024-11-05 01:37:05,919 - INFO - [diffusion][Epoch 9034] diffusion training Loss: 0.06869480945169926
2024-11-05 01:37:05,921 - INFO - [diffusion][Epoch 9034] diffusion learning rate: 0.001
2024-11-05 01:37:05,923 - INFO - [diffusion][Epoch 9034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:05,924 - INFO - [diffusion][Epoch 9035] Epoch 9036/12000
2024-11-05 01:37:10,063 - INFO - [diffusion][Epoch 9035] diffusion training Loss: 0.05903278850018978
2024-11-05 01:37:10,066 - INFO - [diffusion][Epoch 9035] diffusion learning rate: 0.001
2024-11-05 01:37:10,069 - INFO - [diffusion][Epoch 9035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:10,070 - INFO - [diffusion][Epoch 9036] Epoch 9037/12000
2024-11-05 01:37:14,265 - INFO - [diffusion][Epoch 9036] diffusion training Loss: 0.06548559758812189
2024-11-05 01:37:14,268 - INFO - [diffusion][Epoch 9036] diffusion learning rate: 0.001
2024-11-05 01:37:14,270 - INFO - [diffusion][Epoch 9036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:14,271 - INFO - [diffusion][Epoch 9037] Epoch 9038/12000
2024-11-05 01:37:18,504 - INFO - [diffusion][Epoch 9037] diffusion training Loss: 0.06518303323537111
2024-11-05 01:37:18,506 - INFO - [diffusion][Epoch 9037] diffusion learning rate: 0.001
2024-11-05 01:37:18,507 - INFO - [diffusion][Epoch 9037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:18,509 - INFO - [diffusion][Epoch 9038] Epoch 9039/12000
2024-11-05 01:37:22,651 - INFO - [diffusion][Epoch 9038] diffusion training Loss: 0.06746824830770493
2024-11-05 01:37:22,653 - INFO - [diffusion][Epoch 9038] diffusion learning rate: 0.001
2024-11-05 01:37:22,655 - INFO - [diffusion][Epoch 9038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:22,656 - INFO - [diffusion][Epoch 9039] Epoch 9040/12000
2024-11-05 01:37:26,752 - INFO - [diffusion][Epoch 9039] diffusion training Loss: 0.06260613817721605
2024-11-05 01:37:26,754 - INFO - [diffusion][Epoch 9039] diffusion learning rate: 0.001
2024-11-05 01:37:26,756 - INFO - [diffusion][Epoch 9039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:26,757 - INFO - [diffusion][Epoch 9040] Epoch 9041/12000
2024-11-05 01:37:30,924 - INFO - [diffusion][Epoch 9040] diffusion training Loss: 0.06443269178271294
2024-11-05 01:37:30,927 - INFO - [diffusion][Epoch 9040] diffusion learning rate: 0.001
2024-11-05 01:37:30,928 - INFO - [diffusion][Epoch 9040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:30,930 - INFO - [diffusion][Epoch 9041] Epoch 9042/12000
2024-11-05 01:37:35,197 - INFO - [diffusion][Epoch 9041] diffusion training Loss: 0.06522398442029953
2024-11-05 01:37:35,201 - INFO - [diffusion][Epoch 9041] diffusion learning rate: 0.001
2024-11-05 01:37:35,203 - INFO - [diffusion][Epoch 9041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:35,204 - INFO - [diffusion][Epoch 9042] Epoch 9043/12000
2024-11-05 01:37:39,467 - INFO - [diffusion][Epoch 9042] diffusion training Loss: 0.06412414275109768
2024-11-05 01:37:39,471 - INFO - [diffusion][Epoch 9042] diffusion learning rate: 0.001
2024-11-05 01:37:39,473 - INFO - [diffusion][Epoch 9042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:39,474 - INFO - [diffusion][Epoch 9043] Epoch 9044/12000
2024-11-05 01:37:43,731 - INFO - [diffusion][Epoch 9043] diffusion training Loss: 0.06684153247624636
2024-11-05 01:37:43,736 - INFO - [diffusion][Epoch 9043] diffusion learning rate: 0.001
2024-11-05 01:37:43,738 - INFO - [diffusion][Epoch 9043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:43,740 - INFO - [diffusion][Epoch 9044] Epoch 9045/12000
2024-11-05 01:37:47,904 - INFO - [diffusion][Epoch 9044] diffusion training Loss: 0.06530346721410751
2024-11-05 01:37:47,906 - INFO - [diffusion][Epoch 9044] diffusion learning rate: 0.001
2024-11-05 01:37:47,908 - INFO - [diffusion][Epoch 9044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:47,909 - INFO - [diffusion][Epoch 9045] Epoch 9046/12000
2024-11-05 01:37:52,156 - INFO - [diffusion][Epoch 9045] diffusion training Loss: 0.061718277633190155
2024-11-05 01:37:52,158 - INFO - [diffusion][Epoch 9045] diffusion learning rate: 0.001
2024-11-05 01:37:52,160 - INFO - [diffusion][Epoch 9045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:52,161 - INFO - [diffusion][Epoch 9046] Epoch 9047/12000
2024-11-05 01:37:56,404 - INFO - [diffusion][Epoch 9046] diffusion training Loss: 0.06826810725033283
2024-11-05 01:37:56,407 - INFO - [diffusion][Epoch 9046] diffusion learning rate: 0.001
2024-11-05 01:37:56,408 - INFO - [diffusion][Epoch 9046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:56,410 - INFO - [diffusion][Epoch 9047] Epoch 9048/12000
2024-11-05 01:38:00,993 - INFO - [diffusion][Epoch 9047] diffusion training Loss: 0.06902916915714741
2024-11-05 01:38:00,995 - INFO - [diffusion][Epoch 9047] diffusion learning rate: 0.001
2024-11-05 01:38:00,997 - INFO - [diffusion][Epoch 9047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:00,998 - INFO - [diffusion][Epoch 9048] Epoch 9049/12000
2024-11-05 01:38:05,204 - INFO - [diffusion][Epoch 9048] diffusion training Loss: 0.06183083448559046
2024-11-05 01:38:05,206 - INFO - [diffusion][Epoch 9048] diffusion learning rate: 0.001
2024-11-05 01:38:05,208 - INFO - [diffusion][Epoch 9048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:05,210 - INFO - [diffusion][Epoch 9049] Epoch 9050/12000
2024-11-05 01:38:09,395 - INFO - [diffusion][Epoch 9049] diffusion training Loss: 0.0683519821614027
2024-11-05 01:38:09,396 - INFO - [diffusion][Epoch 9049] diffusion learning rate: 0.001
2024-11-05 01:38:09,398 - INFO - [diffusion][Epoch 9049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:09,400 - INFO - [diffusion][Epoch 9050] Epoch 9051/12000
2024-11-05 01:38:13,566 - INFO - [diffusion][Epoch 9050] diffusion training Loss: 0.06657811999320984
2024-11-05 01:38:13,569 - INFO - [diffusion][Epoch 9050] diffusion learning rate: 0.001
2024-11-05 01:38:13,660 - INFO - [diffusion][Epoch 9050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:13,662 - INFO - [diffusion][Epoch 9051] Epoch 9052/12000
2024-11-05 01:38:17,902 - INFO - [diffusion][Epoch 9051] diffusion training Loss: 0.06581144407391548
2024-11-05 01:38:17,904 - INFO - [diffusion][Epoch 9051] diffusion learning rate: 0.001
2024-11-05 01:38:17,906 - INFO - [diffusion][Epoch 9051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:17,908 - INFO - [diffusion][Epoch 9052] Epoch 9053/12000
2024-11-05 01:38:22,218 - INFO - [diffusion][Epoch 9052] diffusion training Loss: 0.06435446813702583
2024-11-05 01:38:22,220 - INFO - [diffusion][Epoch 9052] diffusion learning rate: 0.001
2024-11-05 01:38:22,222 - INFO - [diffusion][Epoch 9052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:22,224 - INFO - [diffusion][Epoch 9053] Epoch 9054/12000
2024-11-05 01:38:26,357 - INFO - [diffusion][Epoch 9053] diffusion training Loss: 0.06588993594050407
2024-11-05 01:38:26,359 - INFO - [diffusion][Epoch 9053] diffusion learning rate: 0.001
2024-11-05 01:38:26,361 - INFO - [diffusion][Epoch 9053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:26,363 - INFO - [diffusion][Epoch 9054] Epoch 9055/12000
2024-11-05 01:38:30,433 - INFO - [diffusion][Epoch 9054] diffusion training Loss: 0.06624831911176443
2024-11-05 01:38:30,436 - INFO - [diffusion][Epoch 9054] diffusion learning rate: 0.001
2024-11-05 01:38:30,438 - INFO - [diffusion][Epoch 9054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:30,439 - INFO - [diffusion][Epoch 9055] Epoch 9056/12000
2024-11-05 01:38:34,522 - INFO - [diffusion][Epoch 9055] diffusion training Loss: 0.06592824775725603
2024-11-05 01:38:34,525 - INFO - [diffusion][Epoch 9055] diffusion learning rate: 0.001
2024-11-05 01:38:34,527 - INFO - [diffusion][Epoch 9055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:34,528 - INFO - [diffusion][Epoch 9056] Epoch 9057/12000
2024-11-05 01:38:38,737 - INFO - [diffusion][Epoch 9056] diffusion training Loss: 0.06432249676436186
2024-11-05 01:38:38,740 - INFO - [diffusion][Epoch 9056] diffusion learning rate: 0.001
2024-11-05 01:38:38,742 - INFO - [diffusion][Epoch 9056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:38,743 - INFO - [diffusion][Epoch 9057] Epoch 9058/12000
2024-11-05 01:38:42,931 - INFO - [diffusion][Epoch 9057] diffusion training Loss: 0.06345834769308567
2024-11-05 01:38:42,934 - INFO - [diffusion][Epoch 9057] diffusion learning rate: 0.001
2024-11-05 01:38:42,935 - INFO - [diffusion][Epoch 9057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:42,937 - INFO - [diffusion][Epoch 9058] Epoch 9059/12000
2024-11-05 01:38:47,068 - INFO - [diffusion][Epoch 9058] diffusion training Loss: 0.06709763780236244
2024-11-05 01:38:47,070 - INFO - [diffusion][Epoch 9058] diffusion learning rate: 0.001
2024-11-05 01:38:47,072 - INFO - [diffusion][Epoch 9058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:47,073 - INFO - [diffusion][Epoch 9059] Epoch 9060/12000
2024-11-05 01:38:51,326 - INFO - [diffusion][Epoch 9059] diffusion training Loss: 0.06341924704611301
2024-11-05 01:38:51,328 - INFO - [diffusion][Epoch 9059] diffusion learning rate: 0.001
2024-11-05 01:38:51,330 - INFO - [diffusion][Epoch 9059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:51,331 - INFO - [diffusion][Epoch 9060] Epoch 9061/12000
2024-11-05 01:38:55,444 - INFO - [diffusion][Epoch 9060] diffusion training Loss: 0.05984461307525635
2024-11-05 01:38:55,446 - INFO - [diffusion][Epoch 9060] diffusion learning rate: 0.001
2024-11-05 01:38:55,449 - INFO - [diffusion][Epoch 9060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:55,450 - INFO - [diffusion][Epoch 9061] Epoch 9062/12000
2024-11-05 01:38:59,578 - INFO - [diffusion][Epoch 9061] diffusion training Loss: 0.06212596409022808
2024-11-05 01:38:59,580 - INFO - [diffusion][Epoch 9061] diffusion learning rate: 0.001
2024-11-05 01:38:59,585 - INFO - [diffusion][Epoch 9061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:59,586 - INFO - [diffusion][Epoch 9062] Epoch 9063/12000
2024-11-05 01:39:03,703 - INFO - [diffusion][Epoch 9062] diffusion training Loss: 0.06937431544065475
2024-11-05 01:39:03,706 - INFO - [diffusion][Epoch 9062] diffusion learning rate: 0.001
2024-11-05 01:39:03,757 - INFO - [diffusion][Epoch 9062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:03,759 - INFO - [diffusion][Epoch 9063] Epoch 9064/12000
2024-11-05 01:39:08,026 - INFO - [diffusion][Epoch 9063] diffusion training Loss: 0.06755416095256805
2024-11-05 01:39:08,028 - INFO - [diffusion][Epoch 9063] diffusion learning rate: 0.001
2024-11-05 01:39:08,030 - INFO - [diffusion][Epoch 9063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:08,031 - INFO - [diffusion][Epoch 9064] Epoch 9065/12000
2024-11-05 01:39:12,336 - INFO - [diffusion][Epoch 9064] diffusion training Loss: 0.06345822475850582
2024-11-05 01:39:12,512 - INFO - [diffusion][Epoch 9064] diffusion learning rate: 0.001
2024-11-05 01:39:12,514 - INFO - [diffusion][Epoch 9064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:12,516 - INFO - [diffusion][Epoch 9065] Epoch 9066/12000
2024-11-05 01:39:16,316 - INFO - [diffusion][Epoch 9065] diffusion training Loss: 0.06664321385324001
2024-11-05 01:39:16,318 - INFO - [diffusion][Epoch 9065] diffusion learning rate: 0.001
2024-11-05 01:39:16,320 - INFO - [diffusion][Epoch 9065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:16,321 - INFO - [diffusion][Epoch 9066] Epoch 9067/12000
2024-11-05 01:39:20,392 - INFO - [diffusion][Epoch 9066] diffusion training Loss: 0.06589198298752308
2024-11-05 01:39:20,394 - INFO - [diffusion][Epoch 9066] diffusion learning rate: 0.001
2024-11-05 01:39:20,453 - INFO - [diffusion][Epoch 9066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:20,454 - INFO - [diffusion][Epoch 9067] Epoch 9068/12000
2024-11-05 01:39:24,596 - INFO - [diffusion][Epoch 9067] diffusion training Loss: 0.06529275793582201
2024-11-05 01:39:24,598 - INFO - [diffusion][Epoch 9067] diffusion learning rate: 0.001
2024-11-05 01:39:24,600 - INFO - [diffusion][Epoch 9067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:24,602 - INFO - [diffusion][Epoch 9068] Epoch 9069/12000
2024-11-05 01:39:28,857 - INFO - [diffusion][Epoch 9068] diffusion training Loss: 0.06328149139881134
2024-11-05 01:39:28,859 - INFO - [diffusion][Epoch 9068] diffusion learning rate: 0.001
2024-11-05 01:39:28,861 - INFO - [diffusion][Epoch 9068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:28,862 - INFO - [diffusion][Epoch 9069] Epoch 9070/12000
2024-11-05 01:39:32,859 - INFO - [diffusion][Epoch 9069] diffusion training Loss: 0.05678796395659447
2024-11-05 01:39:32,861 - INFO - [diffusion][Epoch 9069] diffusion learning rate: 0.001
2024-11-05 01:39:32,863 - INFO - [diffusion][Epoch 9069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:32,865 - INFO - [diffusion][Epoch 9070] Epoch 9071/12000
2024-11-05 01:39:37,227 - INFO - [diffusion][Epoch 9070] diffusion training Loss: 0.06170256994664669
2024-11-05 01:39:37,230 - INFO - [diffusion][Epoch 9070] diffusion learning rate: 0.001
2024-11-05 01:39:37,232 - INFO - [diffusion][Epoch 9070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:37,233 - INFO - [diffusion][Epoch 9071] Epoch 9072/12000
2024-11-05 01:39:41,421 - INFO - [diffusion][Epoch 9071] diffusion training Loss: 0.061200693249702454
2024-11-05 01:39:41,424 - INFO - [diffusion][Epoch 9071] diffusion learning rate: 0.001
2024-11-05 01:39:41,426 - INFO - [diffusion][Epoch 9071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:41,427 - INFO - [diffusion][Epoch 9072] Epoch 9073/12000
2024-11-05 01:39:45,717 - INFO - [diffusion][Epoch 9072] diffusion training Loss: 0.06931432895362377
2024-11-05 01:39:45,719 - INFO - [diffusion][Epoch 9072] diffusion learning rate: 0.001
2024-11-05 01:39:45,721 - INFO - [diffusion][Epoch 9072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:45,722 - INFO - [diffusion][Epoch 9073] Epoch 9074/12000
2024-11-05 01:39:49,907 - INFO - [diffusion][Epoch 9073] diffusion training Loss: 0.06457272358238697
2024-11-05 01:39:49,909 - INFO - [diffusion][Epoch 9073] diffusion learning rate: 0.001
2024-11-05 01:39:49,911 - INFO - [diffusion][Epoch 9073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:49,912 - INFO - [diffusion][Epoch 9074] Epoch 9075/12000
2024-11-05 01:39:54,149 - INFO - [diffusion][Epoch 9074] diffusion training Loss: 0.06693498231470585
2024-11-05 01:39:54,151 - INFO - [diffusion][Epoch 9074] diffusion learning rate: 0.001
2024-11-05 01:39:54,189 - INFO - [diffusion][Epoch 9074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:54,191 - INFO - [diffusion][Epoch 9075] Epoch 9076/12000
2024-11-05 01:39:58,436 - INFO - [diffusion][Epoch 9075] diffusion training Loss: 0.06912136357277632
2024-11-05 01:39:58,439 - INFO - [diffusion][Epoch 9075] diffusion learning rate: 0.001
2024-11-05 01:39:58,441 - INFO - [diffusion][Epoch 9075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:58,442 - INFO - [diffusion][Epoch 9076] Epoch 9077/12000
2024-11-05 01:40:02,625 - INFO - [diffusion][Epoch 9076] diffusion training Loss: 0.06703210808336735
2024-11-05 01:40:02,628 - INFO - [diffusion][Epoch 9076] diffusion learning rate: 0.001
2024-11-05 01:40:02,630 - INFO - [diffusion][Epoch 9076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:02,631 - INFO - [diffusion][Epoch 9077] Epoch 9078/12000
2024-11-05 01:40:06,884 - INFO - [diffusion][Epoch 9077] diffusion training Loss: 0.06998718902468681
2024-11-05 01:40:06,886 - INFO - [diffusion][Epoch 9077] diffusion learning rate: 0.001
2024-11-05 01:40:06,888 - INFO - [diffusion][Epoch 9077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:06,889 - INFO - [diffusion][Epoch 9078] Epoch 9079/12000
2024-11-05 01:40:11,100 - INFO - [diffusion][Epoch 9078] diffusion training Loss: 0.06554261781275272
2024-11-05 01:40:11,101 - INFO - [diffusion][Epoch 9078] diffusion learning rate: 0.001
2024-11-05 01:40:11,117 - INFO - [diffusion][Epoch 9078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:11,118 - INFO - [diffusion][Epoch 9079] Epoch 9080/12000
2024-11-05 01:40:15,343 - INFO - [diffusion][Epoch 9079] diffusion training Loss: 0.07560347951948643
2024-11-05 01:40:15,346 - INFO - [diffusion][Epoch 9079] diffusion learning rate: 0.001
2024-11-05 01:40:15,348 - INFO - [diffusion][Epoch 9079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:15,349 - INFO - [diffusion][Epoch 9080] Epoch 9081/12000
2024-11-05 01:40:19,659 - INFO - [diffusion][Epoch 9080] diffusion training Loss: 0.0680463220924139
2024-11-05 01:40:19,661 - INFO - [diffusion][Epoch 9080] diffusion learning rate: 0.001
2024-11-05 01:40:19,663 - INFO - [diffusion][Epoch 9080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:19,665 - INFO - [diffusion][Epoch 9081] Epoch 9082/12000
2024-11-05 01:40:23,924 - INFO - [diffusion][Epoch 9081] diffusion training Loss: 0.06681710388511419
2024-11-05 01:40:23,926 - INFO - [diffusion][Epoch 9081] diffusion learning rate: 0.001
2024-11-05 01:40:23,928 - INFO - [diffusion][Epoch 9081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:23,930 - INFO - [diffusion][Epoch 9082] Epoch 9083/12000
2024-11-05 01:40:28,048 - INFO - [diffusion][Epoch 9082] diffusion training Loss: 0.06598835159093142
2024-11-05 01:40:28,050 - INFO - [diffusion][Epoch 9082] diffusion learning rate: 0.001
2024-11-05 01:40:28,052 - INFO - [diffusion][Epoch 9082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:28,054 - INFO - [diffusion][Epoch 9083] Epoch 9084/12000
2024-11-05 01:40:32,301 - INFO - [diffusion][Epoch 9083] diffusion training Loss: 0.06376102101057768
2024-11-05 01:40:32,304 - INFO - [diffusion][Epoch 9083] diffusion learning rate: 0.001
2024-11-05 01:40:32,307 - INFO - [diffusion][Epoch 9083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:32,309 - INFO - [diffusion][Epoch 9084] Epoch 9085/12000
2024-11-05 01:40:36,378 - INFO - [diffusion][Epoch 9084] diffusion training Loss: 0.06450069136917591
2024-11-05 01:40:36,380 - INFO - [diffusion][Epoch 9084] diffusion learning rate: 0.001
2024-11-05 01:40:36,382 - INFO - [diffusion][Epoch 9084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:36,384 - INFO - [diffusion][Epoch 9085] Epoch 9086/12000
2024-11-05 01:40:40,619 - INFO - [diffusion][Epoch 9085] diffusion training Loss: 0.06259574834257364
2024-11-05 01:40:40,621 - INFO - [diffusion][Epoch 9085] diffusion learning rate: 0.001
2024-11-05 01:40:40,623 - INFO - [diffusion][Epoch 9085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:40,624 - INFO - [diffusion][Epoch 9086] Epoch 9087/12000
2024-11-05 01:40:44,893 - INFO - [diffusion][Epoch 9086] diffusion training Loss: 0.07058493793010712
2024-11-05 01:40:44,895 - INFO - [diffusion][Epoch 9086] diffusion learning rate: 0.001
2024-11-05 01:40:44,897 - INFO - [diffusion][Epoch 9086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:44,899 - INFO - [diffusion][Epoch 9087] Epoch 9088/12000
2024-11-05 01:40:49,123 - INFO - [diffusion][Epoch 9087] diffusion training Loss: 0.060420142486691475
2024-11-05 01:40:49,125 - INFO - [diffusion][Epoch 9087] diffusion learning rate: 0.001
2024-11-05 01:40:49,129 - INFO - [diffusion][Epoch 9087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:49,131 - INFO - [diffusion][Epoch 9088] Epoch 9089/12000
2024-11-05 01:40:53,152 - INFO - [diffusion][Epoch 9088] diffusion training Loss: 0.06231014709919691
2024-11-05 01:40:53,155 - INFO - [diffusion][Epoch 9088] diffusion learning rate: 0.001
2024-11-05 01:40:53,158 - INFO - [diffusion][Epoch 9088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:53,159 - INFO - [diffusion][Epoch 9089] Epoch 9090/12000
2024-11-05 01:40:57,443 - INFO - [diffusion][Epoch 9089] diffusion training Loss: 0.06478874012827873
2024-11-05 01:40:57,445 - INFO - [diffusion][Epoch 9089] diffusion learning rate: 0.001
2024-11-05 01:40:57,447 - INFO - [diffusion][Epoch 9089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:57,449 - INFO - [diffusion][Epoch 9090] Epoch 9091/12000
2024-11-05 01:41:01,729 - INFO - [diffusion][Epoch 9090] diffusion training Loss: 0.06535133253782988
2024-11-05 01:41:01,731 - INFO - [diffusion][Epoch 9090] diffusion learning rate: 0.001
2024-11-05 01:41:01,733 - INFO - [diffusion][Epoch 9090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:01,734 - INFO - [diffusion][Epoch 9091] Epoch 9092/12000
2024-11-05 01:41:06,009 - INFO - [diffusion][Epoch 9091] diffusion training Loss: 0.06640077754855156
2024-11-05 01:41:06,011 - INFO - [diffusion][Epoch 9091] diffusion learning rate: 0.001
2024-11-05 01:41:06,013 - INFO - [diffusion][Epoch 9091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:06,014 - INFO - [diffusion][Epoch 9092] Epoch 9093/12000
2024-11-05 01:41:10,229 - INFO - [diffusion][Epoch 9092] diffusion training Loss: 0.06581542547792196
2024-11-05 01:41:10,232 - INFO - [diffusion][Epoch 9092] diffusion learning rate: 0.001
2024-11-05 01:41:10,233 - INFO - [diffusion][Epoch 9092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:10,235 - INFO - [diffusion][Epoch 9093] Epoch 9094/12000
2024-11-05 01:41:14,370 - INFO - [diffusion][Epoch 9093] diffusion training Loss: 0.06384652853012085
2024-11-05 01:41:14,372 - INFO - [diffusion][Epoch 9093] diffusion learning rate: 0.001
2024-11-05 01:41:14,374 - INFO - [diffusion][Epoch 9093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:14,375 - INFO - [diffusion][Epoch 9094] Epoch 9095/12000
2024-11-05 01:41:18,446 - INFO - [diffusion][Epoch 9094] diffusion training Loss: 0.06774531118571758
2024-11-05 01:41:18,448 - INFO - [diffusion][Epoch 9094] diffusion learning rate: 0.001
2024-11-05 01:41:18,450 - INFO - [diffusion][Epoch 9094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:18,451 - INFO - [diffusion][Epoch 9095] Epoch 9096/12000
2024-11-05 01:41:22,616 - INFO - [diffusion][Epoch 9095] diffusion training Loss: 0.0678232479840517
2024-11-05 01:41:22,618 - INFO - [diffusion][Epoch 9095] diffusion learning rate: 0.001
2024-11-05 01:41:22,620 - INFO - [diffusion][Epoch 9095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:22,622 - INFO - [diffusion][Epoch 9096] Epoch 9097/12000
2024-11-05 01:41:26,709 - INFO - [diffusion][Epoch 9096] diffusion training Loss: 0.060613454319536686
2024-11-05 01:41:26,712 - INFO - [diffusion][Epoch 9096] diffusion learning rate: 0.001
2024-11-05 01:41:26,714 - INFO - [diffusion][Epoch 9096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:26,716 - INFO - [diffusion][Epoch 9097] Epoch 9098/12000
2024-11-05 01:41:30,895 - INFO - [diffusion][Epoch 9097] diffusion training Loss: 0.06698743253946304
2024-11-05 01:41:30,897 - INFO - [diffusion][Epoch 9097] diffusion learning rate: 0.001
2024-11-05 01:41:30,899 - INFO - [diffusion][Epoch 9097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:30,900 - INFO - [diffusion][Epoch 9098] Epoch 9099/12000
2024-11-05 01:41:34,782 - INFO - [diffusion][Epoch 9098] diffusion training Loss: 0.06563687138259411
2024-11-05 01:41:34,785 - INFO - [diffusion][Epoch 9098] diffusion learning rate: 0.001
2024-11-05 01:41:34,788 - INFO - [diffusion][Epoch 9098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:34,789 - INFO - [diffusion][Epoch 9099] Epoch 9100/12000
2024-11-05 01:41:39,037 - INFO - [diffusion][Epoch 9099] diffusion training Loss: 0.06550729088485241
2024-11-05 01:41:39,039 - INFO - [diffusion][Epoch 9099] diffusion learning rate: 0.001
2024-11-05 01:41:39,041 - INFO - [diffusion][Epoch 9099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:39,042 - INFO - [diffusion][Epoch 9100] Epoch 9101/12000
2024-11-05 01:41:43,005 - INFO - [diffusion][Epoch 9100] diffusion training Loss: 0.06105045974254608
2024-11-05 01:41:43,007 - INFO - [diffusion][Epoch 9100] diffusion learning rate: 0.001
2024-11-05 01:41:43,009 - INFO - [diffusion][Epoch 9100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:43,011 - INFO - [diffusion][Epoch 9101] Epoch 9102/12000
2024-11-05 01:41:47,113 - INFO - [diffusion][Epoch 9101] diffusion training Loss: 0.0622840104624629
2024-11-05 01:41:47,116 - INFO - [diffusion][Epoch 9101] diffusion learning rate: 0.001
2024-11-05 01:41:47,118 - INFO - [diffusion][Epoch 9101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:47,119 - INFO - [diffusion][Epoch 9102] Epoch 9103/12000
2024-11-05 01:41:51,266 - INFO - [diffusion][Epoch 9102] diffusion training Loss: 0.06039105448871851
2024-11-05 01:41:51,268 - INFO - [diffusion][Epoch 9102] diffusion learning rate: 0.001
2024-11-05 01:41:51,270 - INFO - [diffusion][Epoch 9102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:51,271 - INFO - [diffusion][Epoch 9103] Epoch 9104/12000
2024-11-05 01:41:55,410 - INFO - [diffusion][Epoch 9103] diffusion training Loss: 0.06733847502619028
2024-11-05 01:41:55,412 - INFO - [diffusion][Epoch 9103] diffusion learning rate: 0.001
2024-11-05 01:41:55,414 - INFO - [diffusion][Epoch 9103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:55,415 - INFO - [diffusion][Epoch 9104] Epoch 9105/12000
2024-11-05 01:41:59,755 - INFO - [diffusion][Epoch 9104] diffusion training Loss: 0.06662395969033241
2024-11-05 01:41:59,757 - INFO - [diffusion][Epoch 9104] diffusion learning rate: 0.001
2024-11-05 01:41:59,758 - INFO - [diffusion][Epoch 9104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:59,760 - INFO - [diffusion][Epoch 9105] Epoch 9106/12000
2024-11-05 01:42:03,874 - INFO - [diffusion][Epoch 9105] diffusion training Loss: 0.06738829985260963
2024-11-05 01:42:03,876 - INFO - [diffusion][Epoch 9105] diffusion learning rate: 0.001
2024-11-05 01:42:03,878 - INFO - [diffusion][Epoch 9105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:03,879 - INFO - [diffusion][Epoch 9106] Epoch 9107/12000
2024-11-05 01:42:08,045 - INFO - [diffusion][Epoch 9106] diffusion training Loss: 0.06155630759894848
2024-11-05 01:42:08,049 - INFO - [diffusion][Epoch 9106] diffusion learning rate: 0.001
2024-11-05 01:42:08,051 - INFO - [diffusion][Epoch 9106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:08,053 - INFO - [diffusion][Epoch 9107] Epoch 9108/12000
2024-11-05 01:42:12,339 - INFO - [diffusion][Epoch 9107] diffusion training Loss: 0.06665281299501657
2024-11-05 01:42:12,341 - INFO - [diffusion][Epoch 9107] diffusion learning rate: 0.001
2024-11-05 01:42:12,343 - INFO - [diffusion][Epoch 9107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:12,344 - INFO - [diffusion][Epoch 9108] Epoch 9109/12000
2024-11-05 01:42:16,538 - INFO - [diffusion][Epoch 9108] diffusion training Loss: 0.06850453466176987
2024-11-05 01:42:16,540 - INFO - [diffusion][Epoch 9108] diffusion learning rate: 0.001
2024-11-05 01:42:16,542 - INFO - [diffusion][Epoch 9108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:16,543 - INFO - [diffusion][Epoch 9109] Epoch 9110/12000
2024-11-05 01:42:20,777 - INFO - [diffusion][Epoch 9109] diffusion training Loss: 0.06437404826283455
2024-11-05 01:42:20,779 - INFO - [diffusion][Epoch 9109] diffusion learning rate: 0.001
2024-11-05 01:42:20,823 - INFO - [diffusion][Epoch 9109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:20,825 - INFO - [diffusion][Epoch 9110] Epoch 9111/12000
2024-11-05 01:42:25,022 - INFO - [diffusion][Epoch 9110] diffusion training Loss: 0.06715783756226301
2024-11-05 01:42:25,025 - INFO - [diffusion][Epoch 9110] diffusion learning rate: 0.001
2024-11-05 01:42:25,027 - INFO - [diffusion][Epoch 9110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:25,028 - INFO - [diffusion][Epoch 9111] Epoch 9112/12000
2024-11-05 01:42:29,280 - INFO - [diffusion][Epoch 9111] diffusion training Loss: 0.06356532033532858
2024-11-05 01:42:29,283 - INFO - [diffusion][Epoch 9111] diffusion learning rate: 0.001
2024-11-05 01:42:29,285 - INFO - [diffusion][Epoch 9111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:29,287 - INFO - [diffusion][Epoch 9112] Epoch 9113/12000
2024-11-05 01:42:33,131 - INFO - [diffusion][Epoch 9112] diffusion training Loss: 0.06074550002813339
2024-11-05 01:42:33,133 - INFO - [diffusion][Epoch 9112] diffusion learning rate: 0.001
2024-11-05 01:42:33,134 - INFO - [diffusion][Epoch 9112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:33,136 - INFO - [diffusion][Epoch 9113] Epoch 9114/12000
2024-11-05 01:42:36,970 - INFO - [diffusion][Epoch 9113] diffusion training Loss: 0.06228889524936676
2024-11-05 01:42:36,972 - INFO - [diffusion][Epoch 9113] diffusion learning rate: 0.001
2024-11-05 01:42:36,974 - INFO - [diffusion][Epoch 9113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:36,976 - INFO - [diffusion][Epoch 9114] Epoch 9115/12000
2024-11-05 01:42:41,130 - INFO - [diffusion][Epoch 9114] diffusion training Loss: 0.06039294693619013
2024-11-05 01:42:41,132 - INFO - [diffusion][Epoch 9114] diffusion learning rate: 0.001
2024-11-05 01:42:41,134 - INFO - [diffusion][Epoch 9114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:41,136 - INFO - [diffusion][Epoch 9115] Epoch 9116/12000
2024-11-05 01:42:45,316 - INFO - [diffusion][Epoch 9115] diffusion training Loss: 0.0590490996837616
2024-11-05 01:42:45,326 - INFO - [diffusion][Epoch 9115] diffusion learning rate: 0.001
2024-11-05 01:42:45,328 - INFO - [diffusion][Epoch 9115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:45,329 - INFO - [diffusion][Epoch 9116] Epoch 9117/12000
2024-11-05 01:42:49,478 - INFO - [diffusion][Epoch 9116] diffusion training Loss: 0.05996633507311344
2024-11-05 01:42:49,481 - INFO - [diffusion][Epoch 9116] diffusion learning rate: 0.001
2024-11-05 01:42:49,483 - INFO - [diffusion][Epoch 9116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:49,484 - INFO - [diffusion][Epoch 9117] Epoch 9118/12000
2024-11-05 01:42:53,408 - INFO - [diffusion][Epoch 9117] diffusion training Loss: 0.06639583315700293
2024-11-05 01:42:53,410 - INFO - [diffusion][Epoch 9117] diffusion learning rate: 0.001
2024-11-05 01:42:53,412 - INFO - [diffusion][Epoch 9117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:53,414 - INFO - [diffusion][Epoch 9118] Epoch 9119/12000
2024-11-05 01:42:57,463 - INFO - [diffusion][Epoch 9118] diffusion training Loss: 0.06701603345572948
2024-11-05 01:42:57,465 - INFO - [diffusion][Epoch 9118] diffusion learning rate: 0.001
2024-11-05 01:42:57,467 - INFO - [diffusion][Epoch 9118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:57,469 - INFO - [diffusion][Epoch 9119] Epoch 9120/12000
2024-11-05 01:43:01,543 - INFO - [diffusion][Epoch 9119] diffusion training Loss: 0.05668785888701677
2024-11-05 01:43:01,545 - INFO - [diffusion][Epoch 9119] diffusion learning rate: 0.001
2024-11-05 01:43:01,547 - INFO - [diffusion][Epoch 9119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:01,549 - INFO - [diffusion][Epoch 9120] Epoch 9121/12000
2024-11-05 01:43:05,741 - INFO - [diffusion][Epoch 9120] diffusion training Loss: 0.06730352714657784
2024-11-05 01:43:05,743 - INFO - [diffusion][Epoch 9120] diffusion learning rate: 0.001
2024-11-05 01:43:05,745 - INFO - [diffusion][Epoch 9120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:05,746 - INFO - [diffusion][Epoch 9121] Epoch 9122/12000
2024-11-05 01:43:09,843 - INFO - [diffusion][Epoch 9121] diffusion training Loss: 0.06633116118609905
2024-11-05 01:43:09,845 - INFO - [diffusion][Epoch 9121] diffusion learning rate: 0.001
2024-11-05 01:43:09,847 - INFO - [diffusion][Epoch 9121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:09,849 - INFO - [diffusion][Epoch 9122] Epoch 9123/12000
2024-11-05 01:43:13,891 - INFO - [diffusion][Epoch 9122] diffusion training Loss: 0.06588743720203638
2024-11-05 01:43:13,893 - INFO - [diffusion][Epoch 9122] diffusion learning rate: 0.001
2024-11-05 01:43:13,895 - INFO - [diffusion][Epoch 9122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:13,896 - INFO - [diffusion][Epoch 9123] Epoch 9124/12000
2024-11-05 01:43:18,012 - INFO - [diffusion][Epoch 9123] diffusion training Loss: 0.05980030074715614
2024-11-05 01:43:18,014 - INFO - [diffusion][Epoch 9123] diffusion learning rate: 0.001
2024-11-05 01:43:18,016 - INFO - [diffusion][Epoch 9123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:18,017 - INFO - [diffusion][Epoch 9124] Epoch 9125/12000
2024-11-05 01:43:22,185 - INFO - [diffusion][Epoch 9124] diffusion training Loss: 0.06682216189801693
2024-11-05 01:43:22,187 - INFO - [diffusion][Epoch 9124] diffusion learning rate: 0.001
2024-11-05 01:43:22,189 - INFO - [diffusion][Epoch 9124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:22,190 - INFO - [diffusion][Epoch 9125] Epoch 9126/12000
2024-11-05 01:43:26,234 - INFO - [diffusion][Epoch 9125] diffusion training Loss: 0.06309097725898027
2024-11-05 01:43:26,237 - INFO - [diffusion][Epoch 9125] diffusion learning rate: 0.001
2024-11-05 01:43:26,276 - INFO - [diffusion][Epoch 9125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:26,277 - INFO - [diffusion][Epoch 9126] Epoch 9127/12000
2024-11-05 01:43:30,446 - INFO - [diffusion][Epoch 9126] diffusion training Loss: 0.068318547680974
2024-11-05 01:43:30,448 - INFO - [diffusion][Epoch 9126] diffusion learning rate: 0.001
2024-11-05 01:43:30,450 - INFO - [diffusion][Epoch 9126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:30,451 - INFO - [diffusion][Epoch 9127] Epoch 9128/12000
2024-11-05 01:43:34,442 - INFO - [diffusion][Epoch 9127] diffusion training Loss: 0.06647239997982979
2024-11-05 01:43:34,443 - INFO - [diffusion][Epoch 9127] diffusion learning rate: 0.001
2024-11-05 01:43:34,445 - INFO - [diffusion][Epoch 9127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:34,446 - INFO - [diffusion][Epoch 9128] Epoch 9129/12000
2024-11-05 01:43:38,486 - INFO - [diffusion][Epoch 9128] diffusion training Loss: 0.061028916388750076
2024-11-05 01:43:38,488 - INFO - [diffusion][Epoch 9128] diffusion learning rate: 0.001
2024-11-05 01:43:38,490 - INFO - [diffusion][Epoch 9128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:38,492 - INFO - [diffusion][Epoch 9129] Epoch 9130/12000
2024-11-05 01:43:42,877 - INFO - [diffusion][Epoch 9129] diffusion training Loss: 0.06387309171259403
2024-11-05 01:43:42,879 - INFO - [diffusion][Epoch 9129] diffusion learning rate: 0.001
2024-11-05 01:43:42,881 - INFO - [diffusion][Epoch 9129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:42,882 - INFO - [diffusion][Epoch 9130] Epoch 9131/12000
2024-11-05 01:43:47,019 - INFO - [diffusion][Epoch 9130] diffusion training Loss: 0.0674052070826292
2024-11-05 01:43:47,021 - INFO - [diffusion][Epoch 9130] diffusion learning rate: 0.001
2024-11-05 01:43:47,023 - INFO - [diffusion][Epoch 9130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:47,024 - INFO - [diffusion][Epoch 9131] Epoch 9132/12000
2024-11-05 01:43:51,301 - INFO - [diffusion][Epoch 9131] diffusion training Loss: 0.0650259293615818
2024-11-05 01:43:51,303 - INFO - [diffusion][Epoch 9131] diffusion learning rate: 0.001
2024-11-05 01:43:51,304 - INFO - [diffusion][Epoch 9131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:51,305 - INFO - [diffusion][Epoch 9132] Epoch 9133/12000
2024-11-05 01:43:55,483 - INFO - [diffusion][Epoch 9132] diffusion training Loss: 0.06999134086072445
2024-11-05 01:43:55,485 - INFO - [diffusion][Epoch 9132] diffusion learning rate: 0.001
2024-11-05 01:43:55,487 - INFO - [diffusion][Epoch 9132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:55,488 - INFO - [diffusion][Epoch 9133] Epoch 9134/12000
2024-11-05 01:43:59,681 - INFO - [diffusion][Epoch 9133] diffusion training Loss: 0.06758791953325272
2024-11-05 01:43:59,683 - INFO - [diffusion][Epoch 9133] diffusion learning rate: 0.001
2024-11-05 01:43:59,684 - INFO - [diffusion][Epoch 9133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:59,686 - INFO - [diffusion][Epoch 9134] Epoch 9135/12000
2024-11-05 01:44:03,838 - INFO - [diffusion][Epoch 9134] diffusion training Loss: 0.06586107052862644
2024-11-05 01:44:03,840 - INFO - [diffusion][Epoch 9134] diffusion learning rate: 0.001
2024-11-05 01:44:03,842 - INFO - [diffusion][Epoch 9134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:03,843 - INFO - [diffusion][Epoch 9135] Epoch 9136/12000
2024-11-05 01:44:07,990 - INFO - [diffusion][Epoch 9135] diffusion training Loss: 0.06258699111640453
2024-11-05 01:44:07,992 - INFO - [diffusion][Epoch 9135] diffusion learning rate: 0.001
2024-11-05 01:44:07,994 - INFO - [diffusion][Epoch 9135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:07,995 - INFO - [diffusion][Epoch 9136] Epoch 9137/12000
2024-11-05 01:44:12,231 - INFO - [diffusion][Epoch 9136] diffusion training Loss: 0.06826316192746162
2024-11-05 01:44:12,233 - INFO - [diffusion][Epoch 9136] diffusion learning rate: 0.001
2024-11-05 01:44:12,235 - INFO - [diffusion][Epoch 9136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:12,236 - INFO - [diffusion][Epoch 9137] Epoch 9138/12000
2024-11-05 01:44:16,440 - INFO - [diffusion][Epoch 9137] diffusion training Loss: 0.06478970497846603
2024-11-05 01:44:16,442 - INFO - [diffusion][Epoch 9137] diffusion learning rate: 0.001
2024-11-05 01:44:16,443 - INFO - [diffusion][Epoch 9137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:16,445 - INFO - [diffusion][Epoch 9138] Epoch 9139/12000
2024-11-05 01:44:20,695 - INFO - [diffusion][Epoch 9138] diffusion training Loss: 0.06411720812320709
2024-11-05 01:44:20,697 - INFO - [diffusion][Epoch 9138] diffusion learning rate: 0.001
2024-11-05 01:44:20,698 - INFO - [diffusion][Epoch 9138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:20,700 - INFO - [diffusion][Epoch 9139] Epoch 9140/12000
2024-11-05 01:44:24,777 - INFO - [diffusion][Epoch 9139] diffusion training Loss: 0.06555006094276905
2024-11-05 01:44:24,779 - INFO - [diffusion][Epoch 9139] diffusion learning rate: 0.001
2024-11-05 01:44:24,781 - INFO - [diffusion][Epoch 9139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:24,782 - INFO - [diffusion][Epoch 9140] Epoch 9141/12000
2024-11-05 01:44:28,998 - INFO - [diffusion][Epoch 9140] diffusion training Loss: 0.06444862391799688
2024-11-05 01:44:29,001 - INFO - [diffusion][Epoch 9140] diffusion learning rate: 0.001
2024-11-05 01:44:29,002 - INFO - [diffusion][Epoch 9140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:29,004 - INFO - [diffusion][Epoch 9141] Epoch 9142/12000
2024-11-05 01:44:33,172 - INFO - [diffusion][Epoch 9141] diffusion training Loss: 0.06747456453740597
2024-11-05 01:44:33,175 - INFO - [diffusion][Epoch 9141] diffusion learning rate: 0.001
2024-11-05 01:44:33,178 - INFO - [diffusion][Epoch 9141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:33,179 - INFO - [diffusion][Epoch 9142] Epoch 9143/12000
2024-11-05 01:44:37,263 - INFO - [diffusion][Epoch 9142] diffusion training Loss: 0.0735142882913351
2024-11-05 01:44:37,265 - INFO - [diffusion][Epoch 9142] diffusion learning rate: 0.001
2024-11-05 01:44:37,267 - INFO - [diffusion][Epoch 9142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:37,269 - INFO - [diffusion][Epoch 9143] Epoch 9144/12000
2024-11-05 01:44:41,277 - INFO - [diffusion][Epoch 9143] diffusion training Loss: 0.0692977774888277
2024-11-05 01:44:41,280 - INFO - [diffusion][Epoch 9143] diffusion learning rate: 0.001
2024-11-05 01:44:41,281 - INFO - [diffusion][Epoch 9143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:41,283 - INFO - [diffusion][Epoch 9144] Epoch 9145/12000
2024-11-05 01:44:45,422 - INFO - [diffusion][Epoch 9144] diffusion training Loss: 0.06179925985634327
2024-11-05 01:44:45,424 - INFO - [diffusion][Epoch 9144] diffusion learning rate: 0.001
2024-11-05 01:44:45,426 - INFO - [diffusion][Epoch 9144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:45,427 - INFO - [diffusion][Epoch 9145] Epoch 9146/12000
2024-11-05 01:44:49,060 - INFO - [diffusion][Epoch 9145] diffusion training Loss: 0.06339601706713438
2024-11-05 01:44:49,062 - INFO - [diffusion][Epoch 9145] diffusion learning rate: 0.001
2024-11-05 01:44:49,063 - INFO - [diffusion][Epoch 9145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:49,064 - INFO - [diffusion][Epoch 9146] Epoch 9147/12000
2024-11-05 01:44:53,007 - INFO - [diffusion][Epoch 9146] diffusion training Loss: 0.06338951829820871
2024-11-05 01:44:53,009 - INFO - [diffusion][Epoch 9146] diffusion learning rate: 0.001
2024-11-05 01:44:53,011 - INFO - [diffusion][Epoch 9146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:53,013 - INFO - [diffusion][Epoch 9147] Epoch 9148/12000
2024-11-05 01:44:57,100 - INFO - [diffusion][Epoch 9147] diffusion training Loss: 0.0622271541506052
2024-11-05 01:44:57,102 - INFO - [diffusion][Epoch 9147] diffusion learning rate: 0.001
2024-11-05 01:44:57,104 - INFO - [diffusion][Epoch 9147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:57,105 - INFO - [diffusion][Epoch 9148] Epoch 9149/12000
2024-11-05 01:45:01,223 - INFO - [diffusion][Epoch 9148] diffusion training Loss: 0.06628010235726833
2024-11-05 01:45:01,225 - INFO - [diffusion][Epoch 9148] diffusion learning rate: 0.001
2024-11-05 01:45:01,226 - INFO - [diffusion][Epoch 9148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:01,228 - INFO - [diffusion][Epoch 9149] Epoch 9150/12000
2024-11-05 01:45:05,835 - INFO - [diffusion][Epoch 9149] diffusion training Loss: 0.06371301878243685
2024-11-05 01:45:05,837 - INFO - [diffusion][Epoch 9149] diffusion learning rate: 0.001
2024-11-05 01:45:05,838 - INFO - [diffusion][Epoch 9149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:05,840 - INFO - [diffusion][Epoch 9150] Epoch 9151/12000
2024-11-05 01:45:10,022 - INFO - [diffusion][Epoch 9150] diffusion training Loss: 0.06411758344620466
2024-11-05 01:45:10,025 - INFO - [diffusion][Epoch 9150] diffusion learning rate: 0.001
2024-11-05 01:45:10,027 - INFO - [diffusion][Epoch 9150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:10,028 - INFO - [diffusion][Epoch 9151] Epoch 9152/12000
2024-11-05 01:45:14,161 - INFO - [diffusion][Epoch 9151] diffusion training Loss: 0.06692501902580261
2024-11-05 01:45:14,163 - INFO - [diffusion][Epoch 9151] diffusion learning rate: 0.001
2024-11-05 01:45:14,165 - INFO - [diffusion][Epoch 9151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:14,166 - INFO - [diffusion][Epoch 9152] Epoch 9153/12000
2024-11-05 01:45:18,470 - INFO - [diffusion][Epoch 9152] diffusion training Loss: 0.06838136911392212
2024-11-05 01:45:18,472 - INFO - [diffusion][Epoch 9152] diffusion learning rate: 0.001
2024-11-05 01:45:18,474 - INFO - [diffusion][Epoch 9152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:18,475 - INFO - [diffusion][Epoch 9153] Epoch 9154/12000
2024-11-05 01:45:22,564 - INFO - [diffusion][Epoch 9153] diffusion training Loss: 0.06331321410834789
2024-11-05 01:45:22,565 - INFO - [diffusion][Epoch 9153] diffusion learning rate: 0.001
2024-11-05 01:45:22,567 - INFO - [diffusion][Epoch 9153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:22,568 - INFO - [diffusion][Epoch 9154] Epoch 9155/12000
2024-11-05 01:45:26,686 - INFO - [diffusion][Epoch 9154] diffusion training Loss: 0.064966706559062
2024-11-05 01:45:26,688 - INFO - [diffusion][Epoch 9154] diffusion learning rate: 0.001
2024-11-05 01:45:26,690 - INFO - [diffusion][Epoch 9154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:26,691 - INFO - [diffusion][Epoch 9155] Epoch 9156/12000
2024-11-05 01:45:30,753 - INFO - [diffusion][Epoch 9155] diffusion training Loss: 0.06308106053620577
2024-11-05 01:45:30,755 - INFO - [diffusion][Epoch 9155] diffusion learning rate: 0.001
2024-11-05 01:45:30,757 - INFO - [diffusion][Epoch 9155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:30,758 - INFO - [diffusion][Epoch 9156] Epoch 9157/12000
2024-11-05 01:45:34,931 - INFO - [diffusion][Epoch 9156] diffusion training Loss: 0.06579544208943844
2024-11-05 01:45:34,933 - INFO - [diffusion][Epoch 9156] diffusion learning rate: 0.001
2024-11-05 01:45:34,935 - INFO - [diffusion][Epoch 9156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:34,936 - INFO - [diffusion][Epoch 9157] Epoch 9158/12000
2024-11-05 01:45:38,960 - INFO - [diffusion][Epoch 9157] diffusion training Loss: 0.06941573694348335
2024-11-05 01:45:38,963 - INFO - [diffusion][Epoch 9157] diffusion learning rate: 0.001
2024-11-05 01:45:39,063 - INFO - [diffusion][Epoch 9157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:39,065 - INFO - [diffusion][Epoch 9158] Epoch 9159/12000
2024-11-05 01:45:43,137 - INFO - [diffusion][Epoch 9158] diffusion training Loss: 0.06241881474852562
2024-11-05 01:45:43,139 - INFO - [diffusion][Epoch 9158] diffusion learning rate: 0.001
2024-11-05 01:45:43,141 - INFO - [diffusion][Epoch 9158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:43,142 - INFO - [diffusion][Epoch 9159] Epoch 9160/12000
2024-11-05 01:45:47,243 - INFO - [diffusion][Epoch 9159] diffusion training Loss: 0.0665572788566351
2024-11-05 01:45:47,245 - INFO - [diffusion][Epoch 9159] diffusion learning rate: 0.001
2024-11-05 01:45:47,246 - INFO - [diffusion][Epoch 9159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:47,248 - INFO - [diffusion][Epoch 9160] Epoch 9161/12000
2024-11-05 01:45:51,378 - INFO - [diffusion][Epoch 9160] diffusion training Loss: 0.06727535184472799
2024-11-05 01:45:51,380 - INFO - [diffusion][Epoch 9160] diffusion learning rate: 0.001
2024-11-05 01:45:51,382 - INFO - [diffusion][Epoch 9160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:51,383 - INFO - [diffusion][Epoch 9161] Epoch 9162/12000
2024-11-05 01:45:55,423 - INFO - [diffusion][Epoch 9161] diffusion training Loss: 0.0631895661354065
2024-11-05 01:45:55,425 - INFO - [diffusion][Epoch 9161] diffusion learning rate: 0.001
2024-11-05 01:45:55,426 - INFO - [diffusion][Epoch 9161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:55,428 - INFO - [diffusion][Epoch 9162] Epoch 9163/12000
2024-11-05 01:45:59,655 - INFO - [diffusion][Epoch 9162] diffusion training Loss: 0.06643124856054783
2024-11-05 01:45:59,657 - INFO - [diffusion][Epoch 9162] diffusion learning rate: 0.001
2024-11-05 01:45:59,659 - INFO - [diffusion][Epoch 9162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:59,660 - INFO - [diffusion][Epoch 9163] Epoch 9164/12000
2024-11-05 01:46:03,826 - INFO - [diffusion][Epoch 9163] diffusion training Loss: 0.06507130619138479
2024-11-05 01:46:03,828 - INFO - [diffusion][Epoch 9163] diffusion learning rate: 0.001
2024-11-05 01:46:03,830 - INFO - [diffusion][Epoch 9163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:03,831 - INFO - [diffusion][Epoch 9164] Epoch 9165/12000
2024-11-05 01:46:07,972 - INFO - [diffusion][Epoch 9164] diffusion training Loss: 0.06186906062066555
2024-11-05 01:46:07,974 - INFO - [diffusion][Epoch 9164] diffusion learning rate: 0.001
2024-11-05 01:46:07,976 - INFO - [diffusion][Epoch 9164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:07,977 - INFO - [diffusion][Epoch 9165] Epoch 9166/12000
2024-11-05 01:46:12,004 - INFO - [diffusion][Epoch 9165] diffusion training Loss: 0.06443080492317677
2024-11-05 01:46:12,006 - INFO - [diffusion][Epoch 9165] diffusion learning rate: 0.001
2024-11-05 01:46:12,008 - INFO - [diffusion][Epoch 9165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:12,009 - INFO - [diffusion][Epoch 9166] Epoch 9167/12000
2024-11-05 01:46:16,096 - INFO - [diffusion][Epoch 9166] diffusion training Loss: 0.06355317123234272
2024-11-05 01:46:16,099 - INFO - [diffusion][Epoch 9166] diffusion learning rate: 0.001
2024-11-05 01:46:16,104 - INFO - [diffusion][Epoch 9166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:16,106 - INFO - [diffusion][Epoch 9167] Epoch 9168/12000
2024-11-05 01:46:20,191 - INFO - [diffusion][Epoch 9167] diffusion training Loss: 0.062072454020380974
2024-11-05 01:46:20,193 - INFO - [diffusion][Epoch 9167] diffusion learning rate: 0.001
2024-11-05 01:46:20,196 - INFO - [diffusion][Epoch 9167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:20,197 - INFO - [diffusion][Epoch 9168] Epoch 9169/12000
2024-11-05 01:46:24,203 - INFO - [diffusion][Epoch 9168] diffusion training Loss: 0.061163405887782574
2024-11-05 01:46:24,204 - INFO - [diffusion][Epoch 9168] diffusion learning rate: 0.001
2024-11-05 01:46:24,206 - INFO - [diffusion][Epoch 9168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:24,207 - INFO - [diffusion][Epoch 9169] Epoch 9170/12000
2024-11-05 01:46:28,327 - INFO - [diffusion][Epoch 9169] diffusion training Loss: 0.060110838152468204
2024-11-05 01:46:28,329 - INFO - [diffusion][Epoch 9169] diffusion learning rate: 0.001
2024-11-05 01:46:28,331 - INFO - [diffusion][Epoch 9169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:28,332 - INFO - [diffusion][Epoch 9170] Epoch 9171/12000
2024-11-05 01:46:32,400 - INFO - [diffusion][Epoch 9170] diffusion training Loss: 0.06652039662003517
2024-11-05 01:46:32,402 - INFO - [diffusion][Epoch 9170] diffusion learning rate: 0.001
2024-11-05 01:46:32,404 - INFO - [diffusion][Epoch 9170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:32,405 - INFO - [diffusion][Epoch 9171] Epoch 9172/12000
2024-11-05 01:46:36,673 - INFO - [diffusion][Epoch 9171] diffusion training Loss: 0.06808597408235073
2024-11-05 01:46:36,676 - INFO - [diffusion][Epoch 9171] diffusion learning rate: 0.001
2024-11-05 01:46:36,678 - INFO - [diffusion][Epoch 9171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:36,680 - INFO - [diffusion][Epoch 9172] Epoch 9173/12000
2024-11-05 01:46:40,761 - INFO - [diffusion][Epoch 9172] diffusion training Loss: 0.06123473681509495
2024-11-05 01:46:40,764 - INFO - [diffusion][Epoch 9172] diffusion learning rate: 0.001
2024-11-05 01:46:40,766 - INFO - [diffusion][Epoch 9172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:40,767 - INFO - [diffusion][Epoch 9173] Epoch 9174/12000
2024-11-05 01:46:44,899 - INFO - [diffusion][Epoch 9173] diffusion training Loss: 0.05460103042423725
2024-11-05 01:46:44,901 - INFO - [diffusion][Epoch 9173] diffusion learning rate: 0.001
2024-11-05 01:46:44,903 - INFO - [diffusion][Epoch 9173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:44,904 - INFO - [diffusion][Epoch 9174] Epoch 9175/12000
2024-11-05 01:46:49,006 - INFO - [diffusion][Epoch 9174] diffusion training Loss: 0.06642920803278685
2024-11-05 01:46:49,008 - INFO - [diffusion][Epoch 9174] diffusion learning rate: 0.001
2024-11-05 01:46:49,010 - INFO - [diffusion][Epoch 9174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:49,011 - INFO - [diffusion][Epoch 9175] Epoch 9176/12000
2024-11-05 01:46:53,123 - INFO - [diffusion][Epoch 9175] diffusion training Loss: 0.060218170285224915
2024-11-05 01:46:53,126 - INFO - [diffusion][Epoch 9175] diffusion learning rate: 0.001
2024-11-05 01:46:53,128 - INFO - [diffusion][Epoch 9175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:53,129 - INFO - [diffusion][Epoch 9176] Epoch 9177/12000
2024-11-05 01:46:57,212 - INFO - [diffusion][Epoch 9176] diffusion training Loss: 0.06449887901544571
2024-11-05 01:46:57,214 - INFO - [diffusion][Epoch 9176] diffusion learning rate: 0.001
2024-11-05 01:46:57,216 - INFO - [diffusion][Epoch 9176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:57,217 - INFO - [diffusion][Epoch 9177] Epoch 9178/12000
2024-11-05 01:47:01,279 - INFO - [diffusion][Epoch 9177] diffusion training Loss: 0.06074772588908672
2024-11-05 01:47:01,281 - INFO - [diffusion][Epoch 9177] diffusion learning rate: 0.001
2024-11-05 01:47:01,284 - INFO - [diffusion][Epoch 9177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:01,286 - INFO - [diffusion][Epoch 9178] Epoch 9179/12000
2024-11-05 01:47:05,149 - INFO - [diffusion][Epoch 9178] diffusion training Loss: 0.0647045411169529
2024-11-05 01:47:05,151 - INFO - [diffusion][Epoch 9178] diffusion learning rate: 0.001
2024-11-05 01:47:05,153 - INFO - [diffusion][Epoch 9178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:05,154 - INFO - [diffusion][Epoch 9179] Epoch 9180/12000
2024-11-05 01:47:09,246 - INFO - [diffusion][Epoch 9179] diffusion training Loss: 0.05938975606113672
2024-11-05 01:47:09,249 - INFO - [diffusion][Epoch 9179] diffusion learning rate: 0.001
2024-11-05 01:47:09,251 - INFO - [diffusion][Epoch 9179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:09,252 - INFO - [diffusion][Epoch 9180] Epoch 9181/12000
2024-11-05 01:47:13,170 - INFO - [diffusion][Epoch 9180] diffusion training Loss: 0.06249812990427017
2024-11-05 01:47:13,171 - INFO - [diffusion][Epoch 9180] diffusion learning rate: 0.001
2024-11-05 01:47:13,173 - INFO - [diffusion][Epoch 9180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:13,174 - INFO - [diffusion][Epoch 9181] Epoch 9182/12000
2024-11-05 01:47:17,209 - INFO - [diffusion][Epoch 9181] diffusion training Loss: 0.05961462389677763
2024-11-05 01:47:17,211 - INFO - [diffusion][Epoch 9181] diffusion learning rate: 0.001
2024-11-05 01:47:17,213 - INFO - [diffusion][Epoch 9181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:17,214 - INFO - [diffusion][Epoch 9182] Epoch 9183/12000
2024-11-05 01:47:21,392 - INFO - [diffusion][Epoch 9182] diffusion training Loss: 0.06259450502693653
2024-11-05 01:47:21,414 - INFO - [diffusion][Epoch 9182] diffusion learning rate: 0.001
2024-11-05 01:47:21,416 - INFO - [diffusion][Epoch 9182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:21,417 - INFO - [diffusion][Epoch 9183] Epoch 9184/12000
2024-11-05 01:47:25,546 - INFO - [diffusion][Epoch 9183] diffusion training Loss: 0.06501666270196438
2024-11-05 01:47:25,548 - INFO - [diffusion][Epoch 9183] diffusion learning rate: 0.001
2024-11-05 01:47:25,550 - INFO - [diffusion][Epoch 9183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:25,552 - INFO - [diffusion][Epoch 9184] Epoch 9185/12000
2024-11-05 01:47:29,625 - INFO - [diffusion][Epoch 9184] diffusion training Loss: 0.06495688296854496
2024-11-05 01:47:29,627 - INFO - [diffusion][Epoch 9184] diffusion learning rate: 0.001
2024-11-05 01:47:29,629 - INFO - [diffusion][Epoch 9184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:29,630 - INFO - [diffusion][Epoch 9185] Epoch 9186/12000
2024-11-05 01:47:33,831 - INFO - [diffusion][Epoch 9185] diffusion training Loss: 0.06327721569687128
2024-11-05 01:47:33,833 - INFO - [diffusion][Epoch 9185] diffusion learning rate: 0.001
2024-11-05 01:47:33,835 - INFO - [diffusion][Epoch 9185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:33,837 - INFO - [diffusion][Epoch 9186] Epoch 9187/12000
2024-11-05 01:47:38,106 - INFO - [diffusion][Epoch 9186] diffusion training Loss: 0.0658805025741458
2024-11-05 01:47:38,108 - INFO - [diffusion][Epoch 9186] diffusion learning rate: 0.001
2024-11-05 01:47:38,110 - INFO - [diffusion][Epoch 9186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:38,111 - INFO - [diffusion][Epoch 9187] Epoch 9188/12000
2024-11-05 01:47:42,296 - INFO - [diffusion][Epoch 9187] diffusion training Loss: 0.06364437378942966
2024-11-05 01:47:42,298 - INFO - [diffusion][Epoch 9187] diffusion learning rate: 0.001
2024-11-05 01:47:42,300 - INFO - [diffusion][Epoch 9187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:42,301 - INFO - [diffusion][Epoch 9188] Epoch 9189/12000
2024-11-05 01:47:46,307 - INFO - [diffusion][Epoch 9188] diffusion training Loss: 0.0629073865711689
2024-11-05 01:47:46,309 - INFO - [diffusion][Epoch 9188] diffusion learning rate: 0.001
2024-11-05 01:47:46,311 - INFO - [diffusion][Epoch 9188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:46,312 - INFO - [diffusion][Epoch 9189] Epoch 9190/12000
2024-11-05 01:47:50,513 - INFO - [diffusion][Epoch 9189] diffusion training Loss: 0.06449294462800026
2024-11-05 01:47:50,515 - INFO - [diffusion][Epoch 9189] diffusion learning rate: 0.001
2024-11-05 01:47:50,517 - INFO - [diffusion][Epoch 9189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:50,518 - INFO - [diffusion][Epoch 9190] Epoch 9191/12000
2024-11-05 01:47:54,767 - INFO - [diffusion][Epoch 9190] diffusion training Loss: 0.05887953285127878
2024-11-05 01:47:54,769 - INFO - [diffusion][Epoch 9190] diffusion learning rate: 0.001
2024-11-05 01:47:54,771 - INFO - [diffusion][Epoch 9190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:54,772 - INFO - [diffusion][Epoch 9191] Epoch 9192/12000
2024-11-05 01:47:58,968 - INFO - [diffusion][Epoch 9191] diffusion training Loss: 0.06328451912850142
2024-11-05 01:47:58,970 - INFO - [diffusion][Epoch 9191] diffusion learning rate: 0.001
2024-11-05 01:47:58,972 - INFO - [diffusion][Epoch 9191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:58,973 - INFO - [diffusion][Epoch 9192] Epoch 9193/12000
2024-11-05 01:48:03,621 - INFO - [diffusion][Epoch 9192] diffusion training Loss: 0.06554949749261141
2024-11-05 01:48:03,623 - INFO - [diffusion][Epoch 9192] diffusion learning rate: 0.001
2024-11-05 01:48:03,625 - INFO - [diffusion][Epoch 9192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:03,626 - INFO - [diffusion][Epoch 9193] Epoch 9194/12000
2024-11-05 01:48:07,883 - INFO - [diffusion][Epoch 9193] diffusion training Loss: 0.06427737139165401
2024-11-05 01:48:07,885 - INFO - [diffusion][Epoch 9193] diffusion learning rate: 0.001
2024-11-05 01:48:07,887 - INFO - [diffusion][Epoch 9193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:07,888 - INFO - [diffusion][Epoch 9194] Epoch 9195/12000
2024-11-05 01:48:11,855 - INFO - [diffusion][Epoch 9194] diffusion training Loss: 0.06327272672206163
2024-11-05 01:48:11,857 - INFO - [diffusion][Epoch 9194] diffusion learning rate: 0.001
2024-11-05 01:48:11,859 - INFO - [diffusion][Epoch 9194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:11,860 - INFO - [diffusion][Epoch 9195] Epoch 9196/12000
2024-11-05 01:48:16,030 - INFO - [diffusion][Epoch 9195] diffusion training Loss: 0.06254089251160622
2024-11-05 01:48:16,032 - INFO - [diffusion][Epoch 9195] diffusion learning rate: 0.001
2024-11-05 01:48:16,034 - INFO - [diffusion][Epoch 9195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:16,035 - INFO - [diffusion][Epoch 9196] Epoch 9197/12000
2024-11-05 01:48:20,087 - INFO - [diffusion][Epoch 9196] diffusion training Loss: 0.062116626650094986
2024-11-05 01:48:20,089 - INFO - [diffusion][Epoch 9196] diffusion learning rate: 0.001
2024-11-05 01:48:20,091 - INFO - [diffusion][Epoch 9196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:20,092 - INFO - [diffusion][Epoch 9197] Epoch 9198/12000
2024-11-05 01:48:24,186 - INFO - [diffusion][Epoch 9197] diffusion training Loss: 0.061855158768594265
2024-11-05 01:48:24,190 - INFO - [diffusion][Epoch 9197] diffusion learning rate: 0.001
2024-11-05 01:48:24,192 - INFO - [diffusion][Epoch 9197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:24,193 - INFO - [diffusion][Epoch 9198] Epoch 9199/12000
2024-11-05 01:48:28,433 - INFO - [diffusion][Epoch 9198] diffusion training Loss: 0.0659186951816082
2024-11-05 01:48:28,435 - INFO - [diffusion][Epoch 9198] diffusion learning rate: 0.001
2024-11-05 01:48:28,437 - INFO - [diffusion][Epoch 9198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:28,438 - INFO - [diffusion][Epoch 9199] Epoch 9200/12000
2024-11-05 01:48:32,532 - INFO - [diffusion][Epoch 9199] diffusion training Loss: 0.06383026484400034
2024-11-05 01:48:32,535 - INFO - [diffusion][Epoch 9199] diffusion learning rate: 0.001
2024-11-05 01:48:32,537 - INFO - [diffusion][Epoch 9199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:32,539 - INFO - [diffusion][Epoch 9200] Epoch 9201/12000
2024-11-05 01:48:36,798 - INFO - [diffusion][Epoch 9200] diffusion training Loss: 0.06553509552031755
2024-11-05 01:48:36,800 - INFO - [diffusion][Epoch 9200] diffusion learning rate: 0.001
2024-11-05 01:48:36,802 - INFO - [diffusion][Epoch 9200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:36,803 - INFO - [diffusion][Epoch 9201] Epoch 9202/12000
2024-11-05 01:48:40,829 - INFO - [diffusion][Epoch 9201] diffusion training Loss: 0.06049520708620548
2024-11-05 01:48:40,832 - INFO - [diffusion][Epoch 9201] diffusion learning rate: 0.001
2024-11-05 01:48:40,834 - INFO - [diffusion][Epoch 9201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:40,835 - INFO - [diffusion][Epoch 9202] Epoch 9203/12000
2024-11-05 01:48:45,051 - INFO - [diffusion][Epoch 9202] diffusion training Loss: 0.06527359038591385
2024-11-05 01:48:45,053 - INFO - [diffusion][Epoch 9202] diffusion learning rate: 0.001
2024-11-05 01:48:45,055 - INFO - [diffusion][Epoch 9202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:45,057 - INFO - [diffusion][Epoch 9203] Epoch 9204/12000
2024-11-05 01:48:49,292 - INFO - [diffusion][Epoch 9203] diffusion training Loss: 0.06881884299218655
2024-11-05 01:48:49,293 - INFO - [diffusion][Epoch 9203] diffusion learning rate: 0.001
2024-11-05 01:48:49,295 - INFO - [diffusion][Epoch 9203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:49,296 - INFO - [diffusion][Epoch 9204] Epoch 9205/12000
2024-11-05 01:48:53,307 - INFO - [diffusion][Epoch 9204] diffusion training Loss: 0.06999791041016579
2024-11-05 01:48:53,309 - INFO - [diffusion][Epoch 9204] diffusion learning rate: 0.001
2024-11-05 01:48:53,311 - INFO - [diffusion][Epoch 9204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:53,312 - INFO - [diffusion][Epoch 9205] Epoch 9206/12000
2024-11-05 01:48:57,470 - INFO - [diffusion][Epoch 9205] diffusion training Loss: 0.06873490288853645
2024-11-05 01:48:57,472 - INFO - [diffusion][Epoch 9205] diffusion learning rate: 0.001
2024-11-05 01:48:57,474 - INFO - [diffusion][Epoch 9205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:57,476 - INFO - [diffusion][Epoch 9206] Epoch 9207/12000
2024-11-05 01:49:01,830 - INFO - [diffusion][Epoch 9206] diffusion training Loss: 0.06768795475363731
2024-11-05 01:49:01,832 - INFO - [diffusion][Epoch 9206] diffusion learning rate: 0.001
2024-11-05 01:49:01,863 - INFO - [diffusion][Epoch 9206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:01,864 - INFO - [diffusion][Epoch 9207] Epoch 9208/12000
2024-11-05 01:49:06,074 - INFO - [diffusion][Epoch 9207] diffusion training Loss: 0.06291980110108852
2024-11-05 01:49:06,076 - INFO - [diffusion][Epoch 9207] diffusion learning rate: 0.001
2024-11-05 01:49:06,078 - INFO - [diffusion][Epoch 9207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:06,079 - INFO - [diffusion][Epoch 9208] Epoch 9209/12000
2024-11-05 01:49:10,262 - INFO - [diffusion][Epoch 9208] diffusion training Loss: 0.06324463430792093
2024-11-05 01:49:10,264 - INFO - [diffusion][Epoch 9208] diffusion learning rate: 0.001
2024-11-05 01:49:10,266 - INFO - [diffusion][Epoch 9208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:10,267 - INFO - [diffusion][Epoch 9209] Epoch 9210/12000
2024-11-05 01:49:14,384 - INFO - [diffusion][Epoch 9209] diffusion training Loss: 0.06947865523397923
2024-11-05 01:49:14,386 - INFO - [diffusion][Epoch 9209] diffusion learning rate: 0.001
2024-11-05 01:49:14,388 - INFO - [diffusion][Epoch 9209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:14,390 - INFO - [diffusion][Epoch 9210] Epoch 9211/12000
2024-11-05 01:49:18,725 - INFO - [diffusion][Epoch 9210] diffusion training Loss: 0.06415504682809114
2024-11-05 01:49:18,727 - INFO - [diffusion][Epoch 9210] diffusion learning rate: 0.001
2024-11-05 01:49:18,729 - INFO - [diffusion][Epoch 9210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:18,730 - INFO - [diffusion][Epoch 9211] Epoch 9212/12000
2024-11-05 01:49:22,531 - INFO - [diffusion][Epoch 9211] diffusion training Loss: 0.060165684670209885
2024-11-05 01:49:22,533 - INFO - [diffusion][Epoch 9211] diffusion learning rate: 0.001
2024-11-05 01:49:22,535 - INFO - [diffusion][Epoch 9211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:22,536 - INFO - [diffusion][Epoch 9212] Epoch 9213/12000
2024-11-05 01:49:27,313 - INFO - [diffusion][Epoch 9212] diffusion training Loss: 0.06534568220376968
2024-11-05 01:49:27,315 - INFO - [diffusion][Epoch 9212] diffusion learning rate: 0.001
2024-11-05 01:49:27,317 - INFO - [diffusion][Epoch 9212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:27,318 - INFO - [diffusion][Epoch 9213] Epoch 9214/12000
2024-11-05 01:49:31,471 - INFO - [diffusion][Epoch 9213] diffusion training Loss: 0.064752540551126
2024-11-05 01:49:31,473 - INFO - [diffusion][Epoch 9213] diffusion learning rate: 0.001
2024-11-05 01:49:31,474 - INFO - [diffusion][Epoch 9213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:31,476 - INFO - [diffusion][Epoch 9214] Epoch 9215/12000
2024-11-05 01:49:35,684 - INFO - [diffusion][Epoch 9214] diffusion training Loss: 0.06754151359200478
2024-11-05 01:49:35,686 - INFO - [diffusion][Epoch 9214] diffusion learning rate: 0.001
2024-11-05 01:49:35,688 - INFO - [diffusion][Epoch 9214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:35,689 - INFO - [diffusion][Epoch 9215] Epoch 9216/12000
2024-11-05 01:49:39,799 - INFO - [diffusion][Epoch 9215] diffusion training Loss: 0.06948467437177896
2024-11-05 01:49:39,802 - INFO - [diffusion][Epoch 9215] diffusion learning rate: 0.001
2024-11-05 01:49:39,804 - INFO - [diffusion][Epoch 9215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:39,806 - INFO - [diffusion][Epoch 9216] Epoch 9217/12000
2024-11-05 01:49:43,925 - INFO - [diffusion][Epoch 9216] diffusion training Loss: 0.06666859611868858
2024-11-05 01:49:43,927 - INFO - [diffusion][Epoch 9216] diffusion learning rate: 0.001
2024-11-05 01:49:43,929 - INFO - [diffusion][Epoch 9216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:43,930 - INFO - [diffusion][Epoch 9217] Epoch 9218/12000
2024-11-05 01:49:48,125 - INFO - [diffusion][Epoch 9217] diffusion training Loss: 0.07011022791266441
2024-11-05 01:49:48,126 - INFO - [diffusion][Epoch 9217] diffusion learning rate: 0.001
2024-11-05 01:49:48,128 - INFO - [diffusion][Epoch 9217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:48,129 - INFO - [diffusion][Epoch 9218] Epoch 9219/12000
2024-11-05 01:49:52,391 - INFO - [diffusion][Epoch 9218] diffusion training Loss: 0.06637769192457199
2024-11-05 01:49:52,393 - INFO - [diffusion][Epoch 9218] diffusion learning rate: 0.001
2024-11-05 01:49:52,395 - INFO - [diffusion][Epoch 9218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:52,396 - INFO - [diffusion][Epoch 9219] Epoch 9220/12000
2024-11-05 01:49:56,462 - INFO - [diffusion][Epoch 9219] diffusion training Loss: 0.06409877073019743
2024-11-05 01:49:56,464 - INFO - [diffusion][Epoch 9219] diffusion learning rate: 0.001
2024-11-05 01:49:56,466 - INFO - [diffusion][Epoch 9219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:56,467 - INFO - [diffusion][Epoch 9220] Epoch 9221/12000
2024-11-05 01:50:00,540 - INFO - [diffusion][Epoch 9220] diffusion training Loss: 0.06327404547482729
2024-11-05 01:50:00,542 - INFO - [diffusion][Epoch 9220] diffusion learning rate: 0.001
2024-11-05 01:50:00,544 - INFO - [diffusion][Epoch 9220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:00,545 - INFO - [diffusion][Epoch 9221] Epoch 9222/12000
2024-11-05 01:50:04,666 - INFO - [diffusion][Epoch 9221] diffusion training Loss: 0.06217336468398571
2024-11-05 01:50:04,668 - INFO - [diffusion][Epoch 9221] diffusion learning rate: 0.001
2024-11-05 01:50:04,670 - INFO - [diffusion][Epoch 9221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:04,671 - INFO - [diffusion][Epoch 9222] Epoch 9223/12000
2024-11-05 01:50:08,745 - INFO - [diffusion][Epoch 9222] diffusion training Loss: 0.06493286695331335
2024-11-05 01:50:08,746 - INFO - [diffusion][Epoch 9222] diffusion learning rate: 0.001
2024-11-05 01:50:08,749 - INFO - [diffusion][Epoch 9222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:08,750 - INFO - [diffusion][Epoch 9223] Epoch 9224/12000
2024-11-05 01:50:12,787 - INFO - [diffusion][Epoch 9223] diffusion training Loss: 0.06589298974722624
2024-11-05 01:50:12,789 - INFO - [diffusion][Epoch 9223] diffusion learning rate: 0.001
2024-11-05 01:50:12,791 - INFO - [diffusion][Epoch 9223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:12,792 - INFO - [diffusion][Epoch 9224] Epoch 9225/12000
2024-11-05 01:50:16,985 - INFO - [diffusion][Epoch 9224] diffusion training Loss: 0.06312668416649103
2024-11-05 01:50:16,986 - INFO - [diffusion][Epoch 9224] diffusion learning rate: 0.001
2024-11-05 01:50:16,988 - INFO - [diffusion][Epoch 9224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:16,989 - INFO - [diffusion][Epoch 9225] Epoch 9226/12000
2024-11-05 01:50:21,233 - INFO - [diffusion][Epoch 9225] diffusion training Loss: 0.06375336460769176
2024-11-05 01:50:21,235 - INFO - [diffusion][Epoch 9225] diffusion learning rate: 0.001
2024-11-05 01:50:21,236 - INFO - [diffusion][Epoch 9225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:21,238 - INFO - [diffusion][Epoch 9226] Epoch 9227/12000
2024-11-05 01:50:25,213 - INFO - [diffusion][Epoch 9226] diffusion training Loss: 0.062058466486632824
2024-11-05 01:50:25,215 - INFO - [diffusion][Epoch 9226] diffusion learning rate: 0.001
2024-11-05 01:50:25,217 - INFO - [diffusion][Epoch 9226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:25,218 - INFO - [diffusion][Epoch 9227] Epoch 9228/12000
2024-11-05 01:50:29,294 - INFO - [diffusion][Epoch 9227] diffusion training Loss: 0.06643521692603827
2024-11-05 01:50:29,296 - INFO - [diffusion][Epoch 9227] diffusion learning rate: 0.001
2024-11-05 01:50:29,298 - INFO - [diffusion][Epoch 9227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:29,299 - INFO - [diffusion][Epoch 9228] Epoch 9229/12000
2024-11-05 01:50:33,513 - INFO - [diffusion][Epoch 9228] diffusion training Loss: 0.06433345563709736
2024-11-05 01:50:33,515 - INFO - [diffusion][Epoch 9228] diffusion learning rate: 0.001
2024-11-05 01:50:33,517 - INFO - [diffusion][Epoch 9228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:33,518 - INFO - [diffusion][Epoch 9229] Epoch 9230/12000
2024-11-05 01:50:37,574 - INFO - [diffusion][Epoch 9229] diffusion training Loss: 0.06097511854022741
2024-11-05 01:50:37,576 - INFO - [diffusion][Epoch 9229] diffusion learning rate: 0.001
2024-11-05 01:50:37,578 - INFO - [diffusion][Epoch 9229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:37,579 - INFO - [diffusion][Epoch 9230] Epoch 9231/12000
2024-11-05 01:50:41,809 - INFO - [diffusion][Epoch 9230] diffusion training Loss: 0.06749576330184937
2024-11-05 01:50:41,812 - INFO - [diffusion][Epoch 9230] diffusion learning rate: 0.001
2024-11-05 01:50:41,814 - INFO - [diffusion][Epoch 9230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:41,815 - INFO - [diffusion][Epoch 9231] Epoch 9232/12000
2024-11-05 01:50:45,970 - INFO - [diffusion][Epoch 9231] diffusion training Loss: 0.06720729172229767
2024-11-05 01:50:45,972 - INFO - [diffusion][Epoch 9231] diffusion learning rate: 0.001
2024-11-05 01:50:45,974 - INFO - [diffusion][Epoch 9231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:45,975 - INFO - [diffusion][Epoch 9232] Epoch 9233/12000
2024-11-05 01:50:50,068 - INFO - [diffusion][Epoch 9232] diffusion training Loss: 0.06450353935360909
2024-11-05 01:50:50,070 - INFO - [diffusion][Epoch 9232] diffusion learning rate: 0.001
2024-11-05 01:50:50,071 - INFO - [diffusion][Epoch 9232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:50,072 - INFO - [diffusion][Epoch 9233] Epoch 9234/12000
2024-11-05 01:50:54,158 - INFO - [diffusion][Epoch 9233] diffusion training Loss: 0.06101992633193731
2024-11-05 01:50:54,160 - INFO - [diffusion][Epoch 9233] diffusion learning rate: 0.001
2024-11-05 01:50:54,162 - INFO - [diffusion][Epoch 9233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:54,163 - INFO - [diffusion][Epoch 9234] Epoch 9235/12000
2024-11-05 01:50:58,343 - INFO - [diffusion][Epoch 9234] diffusion training Loss: 0.07051465474069118
2024-11-05 01:50:58,346 - INFO - [diffusion][Epoch 9234] diffusion learning rate: 0.001
2024-11-05 01:50:58,347 - INFO - [diffusion][Epoch 9234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:58,349 - INFO - [diffusion][Epoch 9235] Epoch 9236/12000
2024-11-05 01:51:02,468 - INFO - [diffusion][Epoch 9235] diffusion training Loss: 0.06337476149201393
2024-11-05 01:51:02,470 - INFO - [diffusion][Epoch 9235] diffusion learning rate: 0.001
2024-11-05 01:51:02,472 - INFO - [diffusion][Epoch 9235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:02,473 - INFO - [diffusion][Epoch 9236] Epoch 9237/12000
2024-11-05 01:51:06,602 - INFO - [diffusion][Epoch 9236] diffusion training Loss: 0.06281694956123829
2024-11-05 01:51:06,604 - INFO - [diffusion][Epoch 9236] diffusion learning rate: 0.001
2024-11-05 01:51:06,606 - INFO - [diffusion][Epoch 9236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:06,607 - INFO - [diffusion][Epoch 9237] Epoch 9238/12000
2024-11-05 01:51:10,805 - INFO - [diffusion][Epoch 9237] diffusion training Loss: 0.06024439074099064
2024-11-05 01:51:10,807 - INFO - [diffusion][Epoch 9237] diffusion learning rate: 0.001
2024-11-05 01:51:10,809 - INFO - [diffusion][Epoch 9237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:10,810 - INFO - [diffusion][Epoch 9238] Epoch 9239/12000
2024-11-05 01:51:14,862 - INFO - [diffusion][Epoch 9238] diffusion training Loss: 0.06532708648592234
2024-11-05 01:51:14,864 - INFO - [diffusion][Epoch 9238] diffusion learning rate: 0.001
2024-11-05 01:51:14,865 - INFO - [diffusion][Epoch 9238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:14,867 - INFO - [diffusion][Epoch 9239] Epoch 9240/12000
2024-11-05 01:51:18,980 - INFO - [diffusion][Epoch 9239] diffusion training Loss: 0.05707935709506273
2024-11-05 01:51:18,983 - INFO - [diffusion][Epoch 9239] diffusion learning rate: 0.001
2024-11-05 01:51:18,985 - INFO - [diffusion][Epoch 9239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:18,986 - INFO - [diffusion][Epoch 9240] Epoch 9241/12000
2024-11-05 01:51:23,086 - INFO - [diffusion][Epoch 9240] diffusion training Loss: 0.06459813471883535
2024-11-05 01:51:23,088 - INFO - [diffusion][Epoch 9240] diffusion learning rate: 0.001
2024-11-05 01:51:23,090 - INFO - [diffusion][Epoch 9240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:23,091 - INFO - [diffusion][Epoch 9241] Epoch 9242/12000
2024-11-05 01:51:27,179 - INFO - [diffusion][Epoch 9241] diffusion training Loss: 0.06506202463060617
2024-11-05 01:51:27,180 - INFO - [diffusion][Epoch 9241] diffusion learning rate: 0.001
2024-11-05 01:51:27,182 - INFO - [diffusion][Epoch 9241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:27,183 - INFO - [diffusion][Epoch 9242] Epoch 9243/12000
2024-11-05 01:51:31,256 - INFO - [diffusion][Epoch 9242] diffusion training Loss: 0.06594385579228401
2024-11-05 01:51:31,259 - INFO - [diffusion][Epoch 9242] diffusion learning rate: 0.001
2024-11-05 01:51:31,260 - INFO - [diffusion][Epoch 9242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:31,262 - INFO - [diffusion][Epoch 9243] Epoch 9244/12000
2024-11-05 01:51:35,293 - INFO - [diffusion][Epoch 9243] diffusion training Loss: 0.0634823851287365
2024-11-05 01:51:35,295 - INFO - [diffusion][Epoch 9243] diffusion learning rate: 0.001
2024-11-05 01:51:35,297 - INFO - [diffusion][Epoch 9243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:35,298 - INFO - [diffusion][Epoch 9244] Epoch 9245/12000
2024-11-05 01:51:39,544 - INFO - [diffusion][Epoch 9244] diffusion training Loss: 0.06481382995843887
2024-11-05 01:51:39,546 - INFO - [diffusion][Epoch 9244] diffusion learning rate: 0.001
2024-11-05 01:51:39,548 - INFO - [diffusion][Epoch 9244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:39,550 - INFO - [diffusion][Epoch 9245] Epoch 9246/12000
2024-11-05 01:51:43,737 - INFO - [diffusion][Epoch 9245] diffusion training Loss: 0.06691277772188187
2024-11-05 01:51:43,740 - INFO - [diffusion][Epoch 9245] diffusion learning rate: 0.001
2024-11-05 01:51:43,741 - INFO - [diffusion][Epoch 9245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:43,743 - INFO - [diffusion][Epoch 9246] Epoch 9247/12000
2024-11-05 01:51:47,794 - INFO - [diffusion][Epoch 9246] diffusion training Loss: 0.06246268004179001
2024-11-05 01:51:47,796 - INFO - [diffusion][Epoch 9246] diffusion learning rate: 0.001
2024-11-05 01:51:47,797 - INFO - [diffusion][Epoch 9246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:47,798 - INFO - [diffusion][Epoch 9247] Epoch 9248/12000
2024-11-05 01:51:52,021 - INFO - [diffusion][Epoch 9247] diffusion training Loss: 0.06445511244237423
2024-11-05 01:51:52,023 - INFO - [diffusion][Epoch 9247] diffusion learning rate: 0.001
2024-11-05 01:51:52,025 - INFO - [diffusion][Epoch 9247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:52,026 - INFO - [diffusion][Epoch 9248] Epoch 9249/12000
2024-11-05 01:51:56,083 - INFO - [diffusion][Epoch 9248] diffusion training Loss: 0.06754553690552711
2024-11-05 01:51:56,085 - INFO - [diffusion][Epoch 9248] diffusion learning rate: 0.001
2024-11-05 01:51:56,087 - INFO - [diffusion][Epoch 9248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:56,089 - INFO - [diffusion][Epoch 9249] Epoch 9250/12000
2024-11-05 01:52:00,184 - INFO - [diffusion][Epoch 9249] diffusion training Loss: 0.06617971882224083
2024-11-05 01:52:00,187 - INFO - [diffusion][Epoch 9249] diffusion learning rate: 0.001
2024-11-05 01:52:00,190 - INFO - [diffusion][Epoch 9249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:00,191 - INFO - [diffusion][Epoch 9250] Epoch 9251/12000
2024-11-05 01:52:04,251 - INFO - [diffusion][Epoch 9250] diffusion training Loss: 0.06718297861516476
2024-11-05 01:52:04,253 - INFO - [diffusion][Epoch 9250] diffusion learning rate: 0.001
2024-11-05 01:52:04,255 - INFO - [diffusion][Epoch 9250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:04,257 - INFO - [diffusion][Epoch 9251] Epoch 9252/12000
2024-11-05 01:52:08,430 - INFO - [diffusion][Epoch 9251] diffusion training Loss: 0.06316738296300173
2024-11-05 01:52:08,432 - INFO - [diffusion][Epoch 9251] diffusion learning rate: 0.001
2024-11-05 01:52:08,434 - INFO - [diffusion][Epoch 9251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:08,435 - INFO - [diffusion][Epoch 9252] Epoch 9253/12000
2024-11-05 01:52:12,542 - INFO - [diffusion][Epoch 9252] diffusion training Loss: 0.06230602785944939
2024-11-05 01:52:12,544 - INFO - [diffusion][Epoch 9252] diffusion learning rate: 0.001
2024-11-05 01:52:12,545 - INFO - [diffusion][Epoch 9252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:12,547 - INFO - [diffusion][Epoch 9253] Epoch 9254/12000
2024-11-05 01:52:16,685 - INFO - [diffusion][Epoch 9253] diffusion training Loss: 0.06276605743914843
2024-11-05 01:52:16,687 - INFO - [diffusion][Epoch 9253] diffusion learning rate: 0.001
2024-11-05 01:52:16,689 - INFO - [diffusion][Epoch 9253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:16,690 - INFO - [diffusion][Epoch 9254] Epoch 9255/12000
2024-11-05 01:52:20,754 - INFO - [diffusion][Epoch 9254] diffusion training Loss: 0.06703535281121731
2024-11-05 01:52:20,828 - INFO - [diffusion][Epoch 9254] diffusion learning rate: 0.001
2024-11-05 01:52:20,830 - INFO - [diffusion][Epoch 9254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:20,832 - INFO - [diffusion][Epoch 9255] Epoch 9256/12000
2024-11-05 01:52:24,836 - INFO - [diffusion][Epoch 9255] diffusion training Loss: 0.05923023819923401
2024-11-05 01:52:24,838 - INFO - [diffusion][Epoch 9255] diffusion learning rate: 0.001
2024-11-05 01:52:24,840 - INFO - [diffusion][Epoch 9255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:24,841 - INFO - [diffusion][Epoch 9256] Epoch 9257/12000
2024-11-05 01:52:28,843 - INFO - [diffusion][Epoch 9256] diffusion training Loss: 0.06352574564516544
2024-11-05 01:52:28,846 - INFO - [diffusion][Epoch 9256] diffusion learning rate: 0.001
2024-11-05 01:52:28,848 - INFO - [diffusion][Epoch 9256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:28,850 - INFO - [diffusion][Epoch 9257] Epoch 9258/12000
2024-11-05 01:52:32,988 - INFO - [diffusion][Epoch 9257] diffusion training Loss: 0.06011560559272766
2024-11-05 01:52:32,990 - INFO - [diffusion][Epoch 9257] diffusion learning rate: 0.001
2024-11-05 01:52:32,992 - INFO - [diffusion][Epoch 9257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:32,994 - INFO - [diffusion][Epoch 9258] Epoch 9259/12000
2024-11-05 01:52:36,845 - INFO - [diffusion][Epoch 9258] diffusion training Loss: 0.06308428011834621
2024-11-05 01:52:36,848 - INFO - [diffusion][Epoch 9258] diffusion learning rate: 0.001
2024-11-05 01:52:36,850 - INFO - [diffusion][Epoch 9258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:36,851 - INFO - [diffusion][Epoch 9259] Epoch 9260/12000
2024-11-05 01:52:40,907 - INFO - [diffusion][Epoch 9259] diffusion training Loss: 0.06121067050844431
2024-11-05 01:52:40,909 - INFO - [diffusion][Epoch 9259] diffusion learning rate: 0.001
2024-11-05 01:52:40,911 - INFO - [diffusion][Epoch 9259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:40,912 - INFO - [diffusion][Epoch 9260] Epoch 9261/12000
2024-11-05 01:52:44,864 - INFO - [diffusion][Epoch 9260] diffusion training Loss: 0.06631255149841309
2024-11-05 01:52:44,866 - INFO - [diffusion][Epoch 9260] diffusion learning rate: 0.001
2024-11-05 01:52:44,868 - INFO - [diffusion][Epoch 9260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:44,869 - INFO - [diffusion][Epoch 9261] Epoch 9262/12000
2024-11-05 01:52:48,887 - INFO - [diffusion][Epoch 9261] diffusion training Loss: 0.06485637743026018
2024-11-05 01:52:48,889 - INFO - [diffusion][Epoch 9261] diffusion learning rate: 0.001
2024-11-05 01:52:48,892 - INFO - [diffusion][Epoch 9261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:48,893 - INFO - [diffusion][Epoch 9262] Epoch 9263/12000
2024-11-05 01:52:53,039 - INFO - [diffusion][Epoch 9262] diffusion training Loss: 0.06174591649323702
2024-11-05 01:52:53,042 - INFO - [diffusion][Epoch 9262] diffusion learning rate: 0.001
2024-11-05 01:52:53,043 - INFO - [diffusion][Epoch 9262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:53,045 - INFO - [diffusion][Epoch 9263] Epoch 9264/12000
2024-11-05 01:52:57,187 - INFO - [diffusion][Epoch 9263] diffusion training Loss: 0.06224218849092722
2024-11-05 01:52:57,189 - INFO - [diffusion][Epoch 9263] diffusion learning rate: 0.001
2024-11-05 01:52:57,190 - INFO - [diffusion][Epoch 9263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:57,192 - INFO - [diffusion][Epoch 9264] Epoch 9265/12000
2024-11-05 01:53:01,336 - INFO - [diffusion][Epoch 9264] diffusion training Loss: 0.06562216393649578
2024-11-05 01:53:01,338 - INFO - [diffusion][Epoch 9264] diffusion learning rate: 0.001
2024-11-05 01:53:01,340 - INFO - [diffusion][Epoch 9264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:01,341 - INFO - [diffusion][Epoch 9265] Epoch 9266/12000
2024-11-05 01:53:05,587 - INFO - [diffusion][Epoch 9265] diffusion training Loss: 0.06290426198393106
2024-11-05 01:53:05,589 - INFO - [diffusion][Epoch 9265] diffusion learning rate: 0.001
2024-11-05 01:53:05,591 - INFO - [diffusion][Epoch 9265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:05,592 - INFO - [diffusion][Epoch 9266] Epoch 9267/12000
2024-11-05 01:53:09,792 - INFO - [diffusion][Epoch 9266] diffusion training Loss: 0.06302944384515285
2024-11-05 01:53:09,794 - INFO - [diffusion][Epoch 9266] diffusion learning rate: 0.001
2024-11-05 01:53:09,797 - INFO - [diffusion][Epoch 9266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:09,798 - INFO - [diffusion][Epoch 9267] Epoch 9268/12000
2024-11-05 01:53:13,897 - INFO - [diffusion][Epoch 9267] diffusion training Loss: 0.06171696167439222
2024-11-05 01:53:13,900 - INFO - [diffusion][Epoch 9267] diffusion learning rate: 0.001
2024-11-05 01:53:13,902 - INFO - [diffusion][Epoch 9267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:13,904 - INFO - [diffusion][Epoch 9268] Epoch 9269/12000
2024-11-05 01:53:18,025 - INFO - [diffusion][Epoch 9268] diffusion training Loss: 0.06588476430624723
2024-11-05 01:53:18,027 - INFO - [diffusion][Epoch 9268] diffusion learning rate: 0.001
2024-11-05 01:53:18,028 - INFO - [diffusion][Epoch 9268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:18,030 - INFO - [diffusion][Epoch 9269] Epoch 9270/12000
2024-11-05 01:53:22,303 - INFO - [diffusion][Epoch 9269] diffusion training Loss: 0.06036970019340515
2024-11-05 01:53:22,305 - INFO - [diffusion][Epoch 9269] diffusion learning rate: 0.001
2024-11-05 01:53:22,307 - INFO - [diffusion][Epoch 9269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:22,308 - INFO - [diffusion][Epoch 9270] Epoch 9271/12000
2024-11-05 01:53:26,596 - INFO - [diffusion][Epoch 9270] diffusion training Loss: 0.06805781740695238
2024-11-05 01:53:26,598 - INFO - [diffusion][Epoch 9270] diffusion learning rate: 0.001
2024-11-05 01:53:26,600 - INFO - [diffusion][Epoch 9270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:26,601 - INFO - [diffusion][Epoch 9271] Epoch 9272/12000
2024-11-05 01:53:30,743 - INFO - [diffusion][Epoch 9271] diffusion training Loss: 0.0650878082960844
2024-11-05 01:53:30,745 - INFO - [diffusion][Epoch 9271] diffusion learning rate: 0.001
2024-11-05 01:53:30,747 - INFO - [diffusion][Epoch 9271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:30,748 - INFO - [diffusion][Epoch 9272] Epoch 9273/12000
2024-11-05 01:53:34,829 - INFO - [diffusion][Epoch 9272] diffusion training Loss: 0.06415804289281368
2024-11-05 01:53:34,830 - INFO - [diffusion][Epoch 9272] diffusion learning rate: 0.001
2024-11-05 01:53:34,832 - INFO - [diffusion][Epoch 9272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:34,834 - INFO - [diffusion][Epoch 9273] Epoch 9274/12000
2024-11-05 01:53:39,609 - INFO - [diffusion][Epoch 9273] diffusion training Loss: 0.059025910682976246
2024-11-05 01:53:39,611 - INFO - [diffusion][Epoch 9273] diffusion learning rate: 0.001
2024-11-05 01:53:39,613 - INFO - [diffusion][Epoch 9273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:39,615 - INFO - [diffusion][Epoch 9274] Epoch 9275/12000
2024-11-05 01:53:43,709 - INFO - [diffusion][Epoch 9274] diffusion training Loss: 0.06466386001557112
2024-11-05 01:53:43,711 - INFO - [diffusion][Epoch 9274] diffusion learning rate: 0.001
2024-11-05 01:53:43,713 - INFO - [diffusion][Epoch 9274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:43,715 - INFO - [diffusion][Epoch 9275] Epoch 9276/12000
2024-11-05 01:53:47,868 - INFO - [diffusion][Epoch 9275] diffusion training Loss: 0.06223647203296423
2024-11-05 01:53:47,870 - INFO - [diffusion][Epoch 9275] diffusion learning rate: 0.001
2024-11-05 01:53:47,872 - INFO - [diffusion][Epoch 9275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:47,873 - INFO - [diffusion][Epoch 9276] Epoch 9277/12000
2024-11-05 01:53:51,932 - INFO - [diffusion][Epoch 9276] diffusion training Loss: 0.06551663111895323
2024-11-05 01:53:51,934 - INFO - [diffusion][Epoch 9276] diffusion learning rate: 0.001
2024-11-05 01:53:51,936 - INFO - [diffusion][Epoch 9276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:51,937 - INFO - [diffusion][Epoch 9277] Epoch 9278/12000
2024-11-05 01:53:56,034 - INFO - [diffusion][Epoch 9277] diffusion training Loss: 0.06497929990291595
2024-11-05 01:53:56,036 - INFO - [diffusion][Epoch 9277] diffusion learning rate: 0.001
2024-11-05 01:53:56,038 - INFO - [diffusion][Epoch 9277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:56,039 - INFO - [diffusion][Epoch 9278] Epoch 9279/12000
2024-11-05 01:54:00,236 - INFO - [diffusion][Epoch 9278] diffusion training Loss: 0.05983300972729921
2024-11-05 01:54:00,238 - INFO - [diffusion][Epoch 9278] diffusion learning rate: 0.001
2024-11-05 01:54:00,240 - INFO - [diffusion][Epoch 9278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:00,242 - INFO - [diffusion][Epoch 9279] Epoch 9280/12000
2024-11-05 01:54:04,247 - INFO - [diffusion][Epoch 9279] diffusion training Loss: 0.067381102591753
2024-11-05 01:54:04,249 - INFO - [diffusion][Epoch 9279] diffusion learning rate: 0.001
2024-11-05 01:54:04,251 - INFO - [diffusion][Epoch 9279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:04,252 - INFO - [diffusion][Epoch 9280] Epoch 9281/12000
2024-11-05 01:54:08,201 - INFO - [diffusion][Epoch 9280] diffusion training Loss: 0.06800408661365509
2024-11-05 01:54:08,203 - INFO - [diffusion][Epoch 9280] diffusion learning rate: 0.001
2024-11-05 01:54:08,204 - INFO - [diffusion][Epoch 9280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:08,205 - INFO - [diffusion][Epoch 9281] Epoch 9282/12000
2024-11-05 01:54:12,488 - INFO - [diffusion][Epoch 9281] diffusion training Loss: 0.06360464449971914
2024-11-05 01:54:12,490 - INFO - [diffusion][Epoch 9281] diffusion learning rate: 0.001
2024-11-05 01:54:12,492 - INFO - [diffusion][Epoch 9281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:12,493 - INFO - [diffusion][Epoch 9282] Epoch 9283/12000
2024-11-05 01:54:16,601 - INFO - [diffusion][Epoch 9282] diffusion training Loss: 0.06317033153027296
2024-11-05 01:54:16,605 - INFO - [diffusion][Epoch 9282] diffusion learning rate: 0.001
2024-11-05 01:54:16,607 - INFO - [diffusion][Epoch 9282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:16,608 - INFO - [diffusion][Epoch 9283] Epoch 9284/12000
2024-11-05 01:54:20,717 - INFO - [diffusion][Epoch 9283] diffusion training Loss: 0.06118359416723251
2024-11-05 01:54:20,719 - INFO - [diffusion][Epoch 9283] diffusion learning rate: 0.001
2024-11-05 01:54:20,721 - INFO - [diffusion][Epoch 9283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:20,722 - INFO - [diffusion][Epoch 9284] Epoch 9285/12000
2024-11-05 01:54:24,933 - INFO - [diffusion][Epoch 9284] diffusion training Loss: 0.06598277390003204
2024-11-05 01:54:24,935 - INFO - [diffusion][Epoch 9284] diffusion learning rate: 0.001
2024-11-05 01:54:24,937 - INFO - [diffusion][Epoch 9284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:24,938 - INFO - [diffusion][Epoch 9285] Epoch 9286/12000
2024-11-05 01:54:29,113 - INFO - [diffusion][Epoch 9285] diffusion training Loss: 0.06648905575275421
2024-11-05 01:54:29,114 - INFO - [diffusion][Epoch 9285] diffusion learning rate: 0.001
2024-11-05 01:54:29,116 - INFO - [diffusion][Epoch 9285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:29,118 - INFO - [diffusion][Epoch 9286] Epoch 9287/12000
2024-11-05 01:54:33,175 - INFO - [diffusion][Epoch 9286] diffusion training Loss: 0.061491687782108784
2024-11-05 01:54:33,178 - INFO - [diffusion][Epoch 9286] diffusion learning rate: 0.001
2024-11-05 01:54:33,180 - INFO - [diffusion][Epoch 9286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:33,181 - INFO - [diffusion][Epoch 9287] Epoch 9288/12000
2024-11-05 01:54:37,343 - INFO - [diffusion][Epoch 9287] diffusion training Loss: 0.06323236413300037
2024-11-05 01:54:37,345 - INFO - [diffusion][Epoch 9287] diffusion learning rate: 0.001
2024-11-05 01:54:37,347 - INFO - [diffusion][Epoch 9287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:37,348 - INFO - [diffusion][Epoch 9288] Epoch 9289/12000
2024-11-05 01:54:41,428 - INFO - [diffusion][Epoch 9288] diffusion training Loss: 0.06339570786803961
2024-11-05 01:54:41,430 - INFO - [diffusion][Epoch 9288] diffusion learning rate: 0.001
2024-11-05 01:54:41,432 - INFO - [diffusion][Epoch 9288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:41,433 - INFO - [diffusion][Epoch 9289] Epoch 9290/12000
2024-11-05 01:54:45,476 - INFO - [diffusion][Epoch 9289] diffusion training Loss: 0.0652425792068243
2024-11-05 01:54:45,478 - INFO - [diffusion][Epoch 9289] diffusion learning rate: 0.001
2024-11-05 01:54:45,479 - INFO - [diffusion][Epoch 9289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:45,481 - INFO - [diffusion][Epoch 9290] Epoch 9291/12000
2024-11-05 01:54:49,519 - INFO - [diffusion][Epoch 9290] diffusion training Loss: 0.06399765331298113
2024-11-05 01:54:49,521 - INFO - [diffusion][Epoch 9290] diffusion learning rate: 0.001
2024-11-05 01:54:49,523 - INFO - [diffusion][Epoch 9290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:49,524 - INFO - [diffusion][Epoch 9291] Epoch 9292/12000
2024-11-05 01:54:53,504 - INFO - [diffusion][Epoch 9291] diffusion training Loss: 0.06414646841585636
2024-11-05 01:54:53,505 - INFO - [diffusion][Epoch 9291] diffusion learning rate: 0.001
2024-11-05 01:54:53,507 - INFO - [diffusion][Epoch 9291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:53,509 - INFO - [diffusion][Epoch 9292] Epoch 9293/12000
2024-11-05 01:54:57,515 - INFO - [diffusion][Epoch 9292] diffusion training Loss: 0.0607594707980752
2024-11-05 01:54:57,518 - INFO - [diffusion][Epoch 9292] diffusion learning rate: 0.001
2024-11-05 01:54:57,520 - INFO - [diffusion][Epoch 9292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:57,521 - INFO - [diffusion][Epoch 9293] Epoch 9294/12000
2024-11-05 01:55:02,112 - INFO - [diffusion][Epoch 9293] diffusion training Loss: 0.06011901888996363
2024-11-05 01:55:02,115 - INFO - [diffusion][Epoch 9293] diffusion learning rate: 0.001
2024-11-05 01:55:02,117 - INFO - [diffusion][Epoch 9293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:02,118 - INFO - [diffusion][Epoch 9294] Epoch 9295/12000
2024-11-05 01:55:06,246 - INFO - [diffusion][Epoch 9294] diffusion training Loss: 0.057666633278131485
2024-11-05 01:55:06,248 - INFO - [diffusion][Epoch 9294] diffusion learning rate: 0.001
2024-11-05 01:55:06,249 - INFO - [diffusion][Epoch 9294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:06,251 - INFO - [diffusion][Epoch 9295] Epoch 9296/12000
2024-11-05 01:55:10,503 - INFO - [diffusion][Epoch 9295] diffusion training Loss: 0.061152550391852856
2024-11-05 01:55:10,506 - INFO - [diffusion][Epoch 9295] diffusion learning rate: 0.001
2024-11-05 01:55:10,535 - INFO - [diffusion][Epoch 9295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:10,537 - INFO - [diffusion][Epoch 9296] Epoch 9297/12000
2024-11-05 01:55:14,699 - INFO - [diffusion][Epoch 9296] diffusion training Loss: 0.06335531827062368
2024-11-05 01:55:14,701 - INFO - [diffusion][Epoch 9296] diffusion learning rate: 0.001
2024-11-05 01:55:14,703 - INFO - [diffusion][Epoch 9296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:14,705 - INFO - [diffusion][Epoch 9297] Epoch 9298/12000
2024-11-05 01:55:18,863 - INFO - [diffusion][Epoch 9297] diffusion training Loss: 0.0641781659796834
2024-11-05 01:55:18,865 - INFO - [diffusion][Epoch 9297] diffusion learning rate: 0.001
2024-11-05 01:55:18,867 - INFO - [diffusion][Epoch 9297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:18,868 - INFO - [diffusion][Epoch 9298] Epoch 9299/12000
2024-11-05 01:55:23,116 - INFO - [diffusion][Epoch 9298] diffusion training Loss: 0.05963519401848316
2024-11-05 01:55:23,119 - INFO - [diffusion][Epoch 9298] diffusion learning rate: 0.001
2024-11-05 01:55:23,121 - INFO - [diffusion][Epoch 9298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:23,122 - INFO - [diffusion][Epoch 9299] Epoch 9300/12000
2024-11-05 01:55:27,395 - INFO - [diffusion][Epoch 9299] diffusion training Loss: 0.06367158982902765
2024-11-05 01:55:27,397 - INFO - [diffusion][Epoch 9299] diffusion learning rate: 0.001
2024-11-05 01:55:27,399 - INFO - [diffusion][Epoch 9299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:27,400 - INFO - [diffusion][Epoch 9300] Epoch 9301/12000
2024-11-05 01:55:31,692 - INFO - [diffusion][Epoch 9300] diffusion training Loss: 0.06422539055347443
2024-11-05 01:55:31,694 - INFO - [diffusion][Epoch 9300] diffusion learning rate: 0.001
2024-11-05 01:55:31,744 - INFO - [diffusion][Epoch 9300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:31,746 - INFO - [diffusion][Epoch 9301] Epoch 9302/12000
2024-11-05 01:55:35,794 - INFO - [diffusion][Epoch 9301] diffusion training Loss: 0.05800433922559023
2024-11-05 01:55:35,797 - INFO - [diffusion][Epoch 9301] diffusion learning rate: 0.001
2024-11-05 01:55:35,799 - INFO - [diffusion][Epoch 9301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:35,800 - INFO - [diffusion][Epoch 9302] Epoch 9303/12000
2024-11-05 01:55:40,081 - INFO - [diffusion][Epoch 9302] diffusion training Loss: 0.0626801485195756
2024-11-05 01:55:40,083 - INFO - [diffusion][Epoch 9302] diffusion learning rate: 0.001
2024-11-05 01:55:40,086 - INFO - [diffusion][Epoch 9302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:40,087 - INFO - [diffusion][Epoch 9303] Epoch 9304/12000
2024-11-05 01:55:44,204 - INFO - [diffusion][Epoch 9303] diffusion training Loss: 0.06873442139476538
2024-11-05 01:55:44,206 - INFO - [diffusion][Epoch 9303] diffusion learning rate: 0.001
2024-11-05 01:55:44,208 - INFO - [diffusion][Epoch 9303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:44,209 - INFO - [diffusion][Epoch 9304] Epoch 9305/12000
2024-11-05 01:55:48,350 - INFO - [diffusion][Epoch 9304] diffusion training Loss: 0.06205303035676479
2024-11-05 01:55:48,352 - INFO - [diffusion][Epoch 9304] diffusion learning rate: 0.001
2024-11-05 01:55:48,354 - INFO - [diffusion][Epoch 9304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:48,355 - INFO - [diffusion][Epoch 9305] Epoch 9306/12000
2024-11-05 01:55:52,650 - INFO - [diffusion][Epoch 9305] diffusion training Loss: 0.06520401034504175
2024-11-05 01:55:52,653 - INFO - [diffusion][Epoch 9305] diffusion learning rate: 0.001
2024-11-05 01:55:52,655 - INFO - [diffusion][Epoch 9305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:52,656 - INFO - [diffusion][Epoch 9306] Epoch 9307/12000
2024-11-05 01:55:56,809 - INFO - [diffusion][Epoch 9306] diffusion training Loss: 0.06544878054410219
2024-11-05 01:55:56,811 - INFO - [diffusion][Epoch 9306] diffusion learning rate: 0.001
2024-11-05 01:55:56,813 - INFO - [diffusion][Epoch 9306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:56,814 - INFO - [diffusion][Epoch 9307] Epoch 9308/12000
2024-11-05 01:56:00,989 - INFO - [diffusion][Epoch 9307] diffusion training Loss: 0.0633219601586461
2024-11-05 01:56:00,991 - INFO - [diffusion][Epoch 9307] diffusion learning rate: 0.001
2024-11-05 01:56:00,993 - INFO - [diffusion][Epoch 9307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:00,994 - INFO - [diffusion][Epoch 9308] Epoch 9309/12000
2024-11-05 01:56:05,277 - INFO - [diffusion][Epoch 9308] diffusion training Loss: 0.06502551306039095
2024-11-05 01:56:05,279 - INFO - [diffusion][Epoch 9308] diffusion learning rate: 0.001
2024-11-05 01:56:05,281 - INFO - [diffusion][Epoch 9308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:05,282 - INFO - [diffusion][Epoch 9309] Epoch 9310/12000
2024-11-05 01:56:09,289 - INFO - [diffusion][Epoch 9309] diffusion training Loss: 0.06672871951013803
2024-11-05 01:56:09,292 - INFO - [diffusion][Epoch 9309] diffusion learning rate: 0.001
2024-11-05 01:56:09,294 - INFO - [diffusion][Epoch 9309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:09,295 - INFO - [diffusion][Epoch 9310] Epoch 9311/12000
2024-11-05 01:56:13,518 - INFO - [diffusion][Epoch 9310] diffusion training Loss: 0.06558359321206808
2024-11-05 01:56:13,520 - INFO - [diffusion][Epoch 9310] diffusion learning rate: 0.001
2024-11-05 01:56:13,522 - INFO - [diffusion][Epoch 9310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:13,523 - INFO - [diffusion][Epoch 9311] Epoch 9312/12000
2024-11-05 01:56:17,718 - INFO - [diffusion][Epoch 9311] diffusion training Loss: 0.06881311163306236
2024-11-05 01:56:17,720 - INFO - [diffusion][Epoch 9311] diffusion learning rate: 0.001
2024-11-05 01:56:17,722 - INFO - [diffusion][Epoch 9311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:17,724 - INFO - [diffusion][Epoch 9312] Epoch 9313/12000
2024-11-05 01:56:21,966 - INFO - [diffusion][Epoch 9312] diffusion training Loss: 0.06185527052730322
2024-11-05 01:56:21,968 - INFO - [diffusion][Epoch 9312] diffusion learning rate: 0.001
2024-11-05 01:56:21,970 - INFO - [diffusion][Epoch 9312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:21,971 - INFO - [diffusion][Epoch 9313] Epoch 9314/12000
2024-11-05 01:56:26,081 - INFO - [diffusion][Epoch 9313] diffusion training Loss: 0.05900677479803562
2024-11-05 01:56:26,084 - INFO - [diffusion][Epoch 9313] diffusion learning rate: 0.001
2024-11-05 01:56:26,086 - INFO - [diffusion][Epoch 9313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:26,087 - INFO - [diffusion][Epoch 9314] Epoch 9315/12000
2024-11-05 01:56:30,554 - INFO - [diffusion][Epoch 9314] diffusion training Loss: 0.06011584307998419
2024-11-05 01:56:30,614 - INFO - [diffusion][Epoch 9314] diffusion learning rate: 0.001
2024-11-05 01:56:30,616 - INFO - [diffusion][Epoch 9314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:30,617 - INFO - [diffusion][Epoch 9315] Epoch 9316/12000
2024-11-05 01:56:34,687 - INFO - [diffusion][Epoch 9315] diffusion training Loss: 0.06524793989956379
2024-11-05 01:56:34,689 - INFO - [diffusion][Epoch 9315] diffusion learning rate: 0.001
2024-11-05 01:56:34,692 - INFO - [diffusion][Epoch 9315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:34,693 - INFO - [diffusion][Epoch 9316] Epoch 9317/12000
2024-11-05 01:56:38,844 - INFO - [diffusion][Epoch 9316] diffusion training Loss: 0.06144147552549839
2024-11-05 01:56:38,846 - INFO - [diffusion][Epoch 9316] diffusion learning rate: 0.001
2024-11-05 01:56:38,848 - INFO - [diffusion][Epoch 9316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:38,849 - INFO - [diffusion][Epoch 9317] Epoch 9318/12000
2024-11-05 01:56:42,898 - INFO - [diffusion][Epoch 9317] diffusion training Loss: 0.06332617532461882
2024-11-05 01:56:42,900 - INFO - [diffusion][Epoch 9317] diffusion learning rate: 0.001
2024-11-05 01:56:42,903 - INFO - [diffusion][Epoch 9317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:42,905 - INFO - [diffusion][Epoch 9318] Epoch 9319/12000
2024-11-05 01:56:46,883 - INFO - [diffusion][Epoch 9318] diffusion training Loss: 0.06949963234364986
2024-11-05 01:56:46,885 - INFO - [diffusion][Epoch 9318] diffusion learning rate: 0.001
2024-11-05 01:56:46,887 - INFO - [diffusion][Epoch 9318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:46,888 - INFO - [diffusion][Epoch 9319] Epoch 9320/12000
2024-11-05 01:56:50,802 - INFO - [diffusion][Epoch 9319] diffusion training Loss: 0.06479878351092339
2024-11-05 01:56:50,804 - INFO - [diffusion][Epoch 9319] diffusion learning rate: 0.001
2024-11-05 01:56:50,807 - INFO - [diffusion][Epoch 9319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:50,808 - INFO - [diffusion][Epoch 9320] Epoch 9321/12000
2024-11-05 01:56:54,537 - INFO - [diffusion][Epoch 9320] diffusion training Loss: 0.0654681958258152
2024-11-05 01:56:54,539 - INFO - [diffusion][Epoch 9320] diffusion learning rate: 0.001
2024-11-05 01:56:54,541 - INFO - [diffusion][Epoch 9320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:54,542 - INFO - [diffusion][Epoch 9321] Epoch 9322/12000
2024-11-05 01:56:58,644 - INFO - [diffusion][Epoch 9321] diffusion training Loss: 0.06348186917603016
2024-11-05 01:56:58,647 - INFO - [diffusion][Epoch 9321] diffusion learning rate: 0.001
2024-11-05 01:56:58,649 - INFO - [diffusion][Epoch 9321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:58,650 - INFO - [diffusion][Epoch 9322] Epoch 9323/12000
2024-11-05 01:57:02,828 - INFO - [diffusion][Epoch 9322] diffusion training Loss: 0.06582764629274607
2024-11-05 01:57:02,830 - INFO - [diffusion][Epoch 9322] diffusion learning rate: 0.001
2024-11-05 01:57:02,832 - INFO - [diffusion][Epoch 9322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:02,833 - INFO - [diffusion][Epoch 9323] Epoch 9324/12000
2024-11-05 01:57:06,941 - INFO - [diffusion][Epoch 9323] diffusion training Loss: 0.06331579945981503
2024-11-05 01:57:06,942 - INFO - [diffusion][Epoch 9323] diffusion learning rate: 0.001
2024-11-05 01:57:06,944 - INFO - [diffusion][Epoch 9323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:06,946 - INFO - [diffusion][Epoch 9324] Epoch 9325/12000
2024-11-05 01:57:10,978 - INFO - [diffusion][Epoch 9324] diffusion training Loss: 0.06728421244770288
2024-11-05 01:57:10,980 - INFO - [diffusion][Epoch 9324] diffusion learning rate: 0.001
2024-11-05 01:57:10,982 - INFO - [diffusion][Epoch 9324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:10,983 - INFO - [diffusion][Epoch 9325] Epoch 9326/12000
2024-11-05 01:57:15,133 - INFO - [diffusion][Epoch 9325] diffusion training Loss: 0.06435376405715942
2024-11-05 01:57:15,135 - INFO - [diffusion][Epoch 9325] diffusion learning rate: 0.001
2024-11-05 01:57:15,137 - INFO - [diffusion][Epoch 9325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:15,138 - INFO - [diffusion][Epoch 9326] Epoch 9327/12000
2024-11-05 01:57:19,306 - INFO - [diffusion][Epoch 9326] diffusion training Loss: 0.07034283317625523
2024-11-05 01:57:19,308 - INFO - [diffusion][Epoch 9326] diffusion learning rate: 0.001
2024-11-05 01:57:19,310 - INFO - [diffusion][Epoch 9326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:19,311 - INFO - [diffusion][Epoch 9327] Epoch 9328/12000
2024-11-05 01:57:23,554 - INFO - [diffusion][Epoch 9327] diffusion training Loss: 0.06634214892983437
2024-11-05 01:57:23,556 - INFO - [diffusion][Epoch 9327] diffusion learning rate: 0.001
2024-11-05 01:57:23,558 - INFO - [diffusion][Epoch 9327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:23,559 - INFO - [diffusion][Epoch 9328] Epoch 9329/12000
2024-11-05 01:57:27,864 - INFO - [diffusion][Epoch 9328] diffusion training Loss: 0.06050894223153591
2024-11-05 01:57:27,866 - INFO - [diffusion][Epoch 9328] diffusion learning rate: 0.001
2024-11-05 01:57:27,869 - INFO - [diffusion][Epoch 9328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:27,871 - INFO - [diffusion][Epoch 9329] Epoch 9330/12000
2024-11-05 01:57:32,217 - INFO - [diffusion][Epoch 9329] diffusion training Loss: 0.06532449461519718
2024-11-05 01:57:32,219 - INFO - [diffusion][Epoch 9329] diffusion learning rate: 0.001
2024-11-05 01:57:32,247 - INFO - [diffusion][Epoch 9329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:32,249 - INFO - [diffusion][Epoch 9330] Epoch 9331/12000
2024-11-05 01:57:36,277 - INFO - [diffusion][Epoch 9330] diffusion training Loss: 0.06186181399971247
2024-11-05 01:57:36,279 - INFO - [diffusion][Epoch 9330] diffusion learning rate: 0.001
2024-11-05 01:57:36,281 - INFO - [diffusion][Epoch 9330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:36,282 - INFO - [diffusion][Epoch 9331] Epoch 9332/12000
2024-11-05 01:57:40,578 - INFO - [diffusion][Epoch 9331] diffusion training Loss: 0.05919521953910589
2024-11-05 01:57:40,581 - INFO - [diffusion][Epoch 9331] diffusion learning rate: 0.001
2024-11-05 01:57:40,583 - INFO - [diffusion][Epoch 9331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:40,585 - INFO - [diffusion][Epoch 9332] Epoch 9333/12000
2024-11-05 01:57:44,728 - INFO - [diffusion][Epoch 9332] diffusion training Loss: 0.062329383566975594
2024-11-05 01:57:44,730 - INFO - [diffusion][Epoch 9332] diffusion learning rate: 0.001
2024-11-05 01:57:44,732 - INFO - [diffusion][Epoch 9332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:44,733 - INFO - [diffusion][Epoch 9333] Epoch 9334/12000
2024-11-05 01:57:48,896 - INFO - [diffusion][Epoch 9333] diffusion training Loss: 0.06333361193537712
2024-11-05 01:57:48,898 - INFO - [diffusion][Epoch 9333] diffusion learning rate: 0.001
2024-11-05 01:57:48,900 - INFO - [diffusion][Epoch 9333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:48,901 - INFO - [diffusion][Epoch 9334] Epoch 9335/12000
2024-11-05 01:57:53,058 - INFO - [diffusion][Epoch 9334] diffusion training Loss: 0.062834526412189
2024-11-05 01:57:53,060 - INFO - [diffusion][Epoch 9334] diffusion learning rate: 0.001
2024-11-05 01:57:53,061 - INFO - [diffusion][Epoch 9334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:53,062 - INFO - [diffusion][Epoch 9335] Epoch 9336/12000
2024-11-05 01:57:57,115 - INFO - [diffusion][Epoch 9335] diffusion training Loss: 0.06616820581257343
2024-11-05 01:57:57,117 - INFO - [diffusion][Epoch 9335] diffusion learning rate: 0.001
2024-11-05 01:57:57,119 - INFO - [diffusion][Epoch 9335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:57,120 - INFO - [diffusion][Epoch 9336] Epoch 9337/12000
2024-11-05 01:58:01,159 - INFO - [diffusion][Epoch 9336] diffusion training Loss: 0.06614255532622337
2024-11-05 01:58:01,161 - INFO - [diffusion][Epoch 9336] diffusion learning rate: 0.001
2024-11-05 01:58:01,162 - INFO - [diffusion][Epoch 9336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:01,164 - INFO - [diffusion][Epoch 9337] Epoch 9338/12000
2024-11-05 01:58:05,225 - INFO - [diffusion][Epoch 9337] diffusion training Loss: 0.06898263283073902
2024-11-05 01:58:05,227 - INFO - [diffusion][Epoch 9337] diffusion learning rate: 0.001
2024-11-05 01:58:05,229 - INFO - [diffusion][Epoch 9337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:05,230 - INFO - [diffusion][Epoch 9338] Epoch 9339/12000
2024-11-05 01:58:09,346 - INFO - [diffusion][Epoch 9338] diffusion training Loss: 0.060547408647835255
2024-11-05 01:58:09,348 - INFO - [diffusion][Epoch 9338] diffusion learning rate: 0.001
2024-11-05 01:58:09,350 - INFO - [diffusion][Epoch 9338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:09,351 - INFO - [diffusion][Epoch 9339] Epoch 9340/12000
2024-11-05 01:58:13,482 - INFO - [diffusion][Epoch 9339] diffusion training Loss: 0.06506403535604477
2024-11-05 01:58:13,484 - INFO - [diffusion][Epoch 9339] diffusion learning rate: 0.001
2024-11-05 01:58:13,486 - INFO - [diffusion][Epoch 9339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:13,487 - INFO - [diffusion][Epoch 9340] Epoch 9341/12000
2024-11-05 01:58:17,697 - INFO - [diffusion][Epoch 9340] diffusion training Loss: 0.06167272012680769
2024-11-05 01:58:17,699 - INFO - [diffusion][Epoch 9340] diffusion learning rate: 0.001
2024-11-05 01:58:17,701 - INFO - [diffusion][Epoch 9340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:17,702 - INFO - [diffusion][Epoch 9341] Epoch 9342/12000
2024-11-05 01:58:21,787 - INFO - [diffusion][Epoch 9341] diffusion training Loss: 0.0646525053307414
2024-11-05 01:58:21,813 - INFO - [diffusion][Epoch 9341] diffusion learning rate: 0.001
2024-11-05 01:58:21,815 - INFO - [diffusion][Epoch 9341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:21,816 - INFO - [diffusion][Epoch 9342] Epoch 9343/12000
2024-11-05 01:58:25,908 - INFO - [diffusion][Epoch 9342] diffusion training Loss: 0.06731050554662943
2024-11-05 01:58:25,910 - INFO - [diffusion][Epoch 9342] diffusion learning rate: 0.001
2024-11-05 01:58:25,912 - INFO - [diffusion][Epoch 9342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:25,913 - INFO - [diffusion][Epoch 9343] Epoch 9344/12000
2024-11-05 01:58:29,878 - INFO - [diffusion][Epoch 9343] diffusion training Loss: 0.06226570624858141
2024-11-05 01:58:29,880 - INFO - [diffusion][Epoch 9343] diffusion learning rate: 0.001
2024-11-05 01:58:29,882 - INFO - [diffusion][Epoch 9343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:29,883 - INFO - [diffusion][Epoch 9344] Epoch 9345/12000
2024-11-05 01:58:33,958 - INFO - [diffusion][Epoch 9344] diffusion training Loss: 0.059755600057542324
2024-11-05 01:58:33,960 - INFO - [diffusion][Epoch 9344] diffusion learning rate: 0.001
2024-11-05 01:58:33,962 - INFO - [diffusion][Epoch 9344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:33,963 - INFO - [diffusion][Epoch 9345] Epoch 9346/12000
2024-11-05 01:58:38,109 - INFO - [diffusion][Epoch 9345] diffusion training Loss: 0.06470142491161823
2024-11-05 01:58:38,111 - INFO - [diffusion][Epoch 9345] diffusion learning rate: 0.001
2024-11-05 01:58:38,164 - INFO - [diffusion][Epoch 9345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:38,166 - INFO - [diffusion][Epoch 9346] Epoch 9347/12000
2024-11-05 01:58:42,389 - INFO - [diffusion][Epoch 9346] diffusion training Loss: 0.05997327156364918
2024-11-05 01:58:42,391 - INFO - [diffusion][Epoch 9346] diffusion learning rate: 0.001
2024-11-05 01:58:42,393 - INFO - [diffusion][Epoch 9346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:42,394 - INFO - [diffusion][Epoch 9347] Epoch 9348/12000
2024-11-05 01:58:46,587 - INFO - [diffusion][Epoch 9347] diffusion training Loss: 0.06292273662984371
2024-11-05 01:58:46,589 - INFO - [diffusion][Epoch 9347] diffusion learning rate: 0.001
2024-11-05 01:58:46,590 - INFO - [diffusion][Epoch 9347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:46,592 - INFO - [diffusion][Epoch 9348] Epoch 9349/12000
2024-11-05 01:58:50,763 - INFO - [diffusion][Epoch 9348] diffusion training Loss: 0.062232499942183495
2024-11-05 01:58:50,766 - INFO - [diffusion][Epoch 9348] diffusion learning rate: 0.001
2024-11-05 01:58:50,768 - INFO - [diffusion][Epoch 9348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:50,770 - INFO - [diffusion][Epoch 9349] Epoch 9350/12000
2024-11-05 01:58:54,810 - INFO - [diffusion][Epoch 9349] diffusion training Loss: 0.06207408104091883
2024-11-05 01:58:54,812 - INFO - [diffusion][Epoch 9349] diffusion learning rate: 0.001
2024-11-05 01:58:54,814 - INFO - [diffusion][Epoch 9349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:54,816 - INFO - [diffusion][Epoch 9350] Epoch 9351/12000
2024-11-05 01:58:59,013 - INFO - [diffusion][Epoch 9350] diffusion training Loss: 0.0623423783108592
2024-11-05 01:58:59,015 - INFO - [diffusion][Epoch 9350] diffusion learning rate: 0.001
2024-11-05 01:58:59,017 - INFO - [diffusion][Epoch 9350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:59,018 - INFO - [diffusion][Epoch 9351] Epoch 9352/12000
2024-11-05 01:59:03,181 - INFO - [diffusion][Epoch 9351] diffusion training Loss: 0.060460666194558144
2024-11-05 01:59:03,184 - INFO - [diffusion][Epoch 9351] diffusion learning rate: 0.001
2024-11-05 01:59:03,186 - INFO - [diffusion][Epoch 9351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:03,187 - INFO - [diffusion][Epoch 9352] Epoch 9353/12000
2024-11-05 01:59:07,440 - INFO - [diffusion][Epoch 9352] diffusion training Loss: 0.06856392696499825
2024-11-05 01:59:07,442 - INFO - [diffusion][Epoch 9352] diffusion learning rate: 0.001
2024-11-05 01:59:07,444 - INFO - [diffusion][Epoch 9352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:07,445 - INFO - [diffusion][Epoch 9353] Epoch 9354/12000
2024-11-05 01:59:11,420 - INFO - [diffusion][Epoch 9353] diffusion training Loss: 0.0616152323782444
2024-11-05 01:59:11,422 - INFO - [diffusion][Epoch 9353] diffusion learning rate: 0.001
2024-11-05 01:59:11,424 - INFO - [diffusion][Epoch 9353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:11,426 - INFO - [diffusion][Epoch 9354] Epoch 9355/12000
2024-11-05 01:59:15,709 - INFO - [diffusion][Epoch 9354] diffusion training Loss: 0.057791984640061855
2024-11-05 01:59:15,711 - INFO - [diffusion][Epoch 9354] diffusion learning rate: 0.001
2024-11-05 01:59:15,713 - INFO - [diffusion][Epoch 9354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:15,714 - INFO - [diffusion][Epoch 9355] Epoch 9356/12000
2024-11-05 01:59:20,064 - INFO - [diffusion][Epoch 9355] diffusion training Loss: 0.05912435334175825
2024-11-05 01:59:20,066 - INFO - [diffusion][Epoch 9355] diffusion learning rate: 0.001
2024-11-05 01:59:20,068 - INFO - [diffusion][Epoch 9355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:20,069 - INFO - [diffusion][Epoch 9356] Epoch 9357/12000
2024-11-05 01:59:24,278 - INFO - [diffusion][Epoch 9356] diffusion training Loss: 0.05883785802870989
2024-11-05 01:59:24,281 - INFO - [diffusion][Epoch 9356] diffusion learning rate: 0.001
2024-11-05 01:59:24,283 - INFO - [diffusion][Epoch 9356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:24,284 - INFO - [diffusion][Epoch 9357] Epoch 9358/12000
2024-11-05 01:59:28,459 - INFO - [diffusion][Epoch 9357] diffusion training Loss: 0.0647789379581809
2024-11-05 01:59:28,461 - INFO - [diffusion][Epoch 9357] diffusion learning rate: 0.001
2024-11-05 01:59:28,463 - INFO - [diffusion][Epoch 9357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:28,464 - INFO - [diffusion][Epoch 9358] Epoch 9359/12000
2024-11-05 01:59:32,700 - INFO - [diffusion][Epoch 9358] diffusion training Loss: 0.06535624619573355
2024-11-05 01:59:32,702 - INFO - [diffusion][Epoch 9358] diffusion learning rate: 0.001
2024-11-05 01:59:32,705 - INFO - [diffusion][Epoch 9358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:32,706 - INFO - [diffusion][Epoch 9359] Epoch 9360/12000
2024-11-05 01:59:36,861 - INFO - [diffusion][Epoch 9359] diffusion training Loss: 0.0572028998285532
2024-11-05 01:59:36,863 - INFO - [diffusion][Epoch 9359] diffusion learning rate: 0.001
2024-11-05 01:59:36,865 - INFO - [diffusion][Epoch 9359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:36,866 - INFO - [diffusion][Epoch 9360] Epoch 9361/12000
2024-11-05 01:59:41,037 - INFO - [diffusion][Epoch 9360] diffusion training Loss: 0.06660584546625614
2024-11-05 01:59:41,040 - INFO - [diffusion][Epoch 9360] diffusion learning rate: 0.001
2024-11-05 01:59:41,041 - INFO - [diffusion][Epoch 9360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:41,044 - INFO - [diffusion][Epoch 9361] Epoch 9362/12000
2024-11-05 01:59:45,115 - INFO - [diffusion][Epoch 9361] diffusion training Loss: 0.0728148240596056
2024-11-05 01:59:45,117 - INFO - [diffusion][Epoch 9361] diffusion learning rate: 0.001
2024-11-05 01:59:45,119 - INFO - [diffusion][Epoch 9361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:45,120 - INFO - [diffusion][Epoch 9362] Epoch 9363/12000
2024-11-05 01:59:49,182 - INFO - [diffusion][Epoch 9362] diffusion training Loss: 0.06831616908311844
2024-11-05 01:59:49,184 - INFO - [diffusion][Epoch 9362] diffusion learning rate: 0.001
2024-11-05 01:59:49,186 - INFO - [diffusion][Epoch 9362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:49,188 - INFO - [diffusion][Epoch 9363] Epoch 9364/12000
2024-11-05 01:59:53,241 - INFO - [diffusion][Epoch 9363] diffusion training Loss: 0.06176880840212107
2024-11-05 01:59:53,243 - INFO - [diffusion][Epoch 9363] diffusion learning rate: 0.001
2024-11-05 01:59:53,245 - INFO - [diffusion][Epoch 9363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:53,246 - INFO - [diffusion][Epoch 9364] Epoch 9365/12000
2024-11-05 01:59:57,362 - INFO - [diffusion][Epoch 9364] diffusion training Loss: 0.06662189401686192
2024-11-05 01:59:57,365 - INFO - [diffusion][Epoch 9364] diffusion learning rate: 0.001
2024-11-05 01:59:57,367 - INFO - [diffusion][Epoch 9364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:57,368 - INFO - [diffusion][Epoch 9365] Epoch 9366/12000
2024-11-05 02:00:01,483 - INFO - [diffusion][Epoch 9365] diffusion training Loss: 0.06569555401802063
2024-11-05 02:00:01,485 - INFO - [diffusion][Epoch 9365] diffusion learning rate: 0.001
2024-11-05 02:00:01,487 - INFO - [diffusion][Epoch 9365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:01,489 - INFO - [diffusion][Epoch 9366] Epoch 9367/12000
2024-11-05 02:00:05,632 - INFO - [diffusion][Epoch 9366] diffusion training Loss: 0.061516713351011276
2024-11-05 02:00:05,635 - INFO - [diffusion][Epoch 9366] diffusion learning rate: 0.001
2024-11-05 02:00:05,636 - INFO - [diffusion][Epoch 9366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:05,638 - INFO - [diffusion][Epoch 9367] Epoch 9368/12000
2024-11-05 02:00:09,775 - INFO - [diffusion][Epoch 9367] diffusion training Loss: 0.061578234657645226
2024-11-05 02:00:09,777 - INFO - [diffusion][Epoch 9367] diffusion learning rate: 0.001
2024-11-05 02:00:09,779 - INFO - [diffusion][Epoch 9367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:09,781 - INFO - [diffusion][Epoch 9368] Epoch 9369/12000
2024-11-05 02:00:13,920 - INFO - [diffusion][Epoch 9368] diffusion training Loss: 0.06002281792461872
2024-11-05 02:00:13,923 - INFO - [diffusion][Epoch 9368] diffusion learning rate: 0.001
2024-11-05 02:00:13,926 - INFO - [diffusion][Epoch 9368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:13,928 - INFO - [diffusion][Epoch 9369] Epoch 9370/12000
2024-11-05 02:00:17,909 - INFO - [diffusion][Epoch 9369] diffusion training Loss: 0.05939271114766598
2024-11-05 02:00:17,911 - INFO - [diffusion][Epoch 9369] diffusion learning rate: 0.001
2024-11-05 02:00:17,913 - INFO - [diffusion][Epoch 9369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:17,914 - INFO - [diffusion][Epoch 9370] Epoch 9371/12000
2024-11-05 02:00:22,188 - INFO - [diffusion][Epoch 9370] diffusion training Loss: 0.06657206825911999
2024-11-05 02:00:22,190 - INFO - [diffusion][Epoch 9370] diffusion learning rate: 0.001
2024-11-05 02:00:22,192 - INFO - [diffusion][Epoch 9370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:22,193 - INFO - [diffusion][Epoch 9371] Epoch 9372/12000
2024-11-05 02:00:26,508 - INFO - [diffusion][Epoch 9371] diffusion training Loss: 0.06637444719672203
2024-11-05 02:00:26,510 - INFO - [diffusion][Epoch 9371] diffusion learning rate: 0.001
2024-11-05 02:00:26,512 - INFO - [diffusion][Epoch 9371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:26,513 - INFO - [diffusion][Epoch 9372] Epoch 9373/12000
2024-11-05 02:00:30,716 - INFO - [diffusion][Epoch 9372] diffusion training Loss: 0.06123854033648968
2024-11-05 02:00:30,718 - INFO - [diffusion][Epoch 9372] diffusion learning rate: 0.001
2024-11-05 02:00:30,720 - INFO - [diffusion][Epoch 9372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:30,722 - INFO - [diffusion][Epoch 9373] Epoch 9374/12000
2024-11-05 02:00:35,025 - INFO - [diffusion][Epoch 9373] diffusion training Loss: 0.06472979113459587
2024-11-05 02:00:35,027 - INFO - [diffusion][Epoch 9373] diffusion learning rate: 0.001
2024-11-05 02:00:35,030 - INFO - [diffusion][Epoch 9373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:35,031 - INFO - [diffusion][Epoch 9374] Epoch 9375/12000
2024-11-05 02:00:39,264 - INFO - [diffusion][Epoch 9374] diffusion training Loss: 0.061527201905846596
2024-11-05 02:00:39,267 - INFO - [diffusion][Epoch 9374] diffusion learning rate: 0.001
2024-11-05 02:00:39,269 - INFO - [diffusion][Epoch 9374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:39,271 - INFO - [diffusion][Epoch 9375] Epoch 9376/12000
2024-11-05 02:00:43,401 - INFO - [diffusion][Epoch 9375] diffusion training Loss: 0.06355327647179365
2024-11-05 02:00:43,403 - INFO - [diffusion][Epoch 9375] diffusion learning rate: 0.001
2024-11-05 02:00:43,405 - INFO - [diffusion][Epoch 9375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:43,407 - INFO - [diffusion][Epoch 9376] Epoch 9377/12000
2024-11-05 02:00:48,308 - INFO - [diffusion][Epoch 9376] diffusion training Loss: 0.06320787500590086
2024-11-05 02:00:48,310 - INFO - [diffusion][Epoch 9376] diffusion learning rate: 0.001
2024-11-05 02:00:48,312 - INFO - [diffusion][Epoch 9376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:48,313 - INFO - [diffusion][Epoch 9377] Epoch 9378/12000
2024-11-05 02:00:52,492 - INFO - [diffusion][Epoch 9377] diffusion training Loss: 0.0609297938644886
2024-11-05 02:00:52,494 - INFO - [diffusion][Epoch 9377] diffusion learning rate: 0.001
2024-11-05 02:00:52,496 - INFO - [diffusion][Epoch 9377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:52,497 - INFO - [diffusion][Epoch 9378] Epoch 9379/12000
2024-11-05 02:00:56,777 - INFO - [diffusion][Epoch 9378] diffusion training Loss: 0.06749513186514378
2024-11-05 02:00:56,780 - INFO - [diffusion][Epoch 9378] diffusion learning rate: 0.001
2024-11-05 02:00:57,009 - INFO - [diffusion][Epoch 9378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:57,010 - INFO - [diffusion][Epoch 9379] Epoch 9380/12000
2024-11-05 02:01:01,273 - INFO - [diffusion][Epoch 9379] diffusion training Loss: 0.06214805319905281
2024-11-05 02:01:01,276 - INFO - [diffusion][Epoch 9379] diffusion learning rate: 0.001
2024-11-05 02:01:01,278 - INFO - [diffusion][Epoch 9379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:01,280 - INFO - [diffusion][Epoch 9380] Epoch 9381/12000
2024-11-05 02:01:05,498 - INFO - [diffusion][Epoch 9380] diffusion training Loss: 0.06425390858203173
2024-11-05 02:01:05,500 - INFO - [diffusion][Epoch 9380] diffusion learning rate: 0.001
2024-11-05 02:01:05,502 - INFO - [diffusion][Epoch 9380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:05,503 - INFO - [diffusion][Epoch 9381] Epoch 9382/12000
2024-11-05 02:01:09,704 - INFO - [diffusion][Epoch 9381] diffusion training Loss: 0.0614422457292676
2024-11-05 02:01:09,706 - INFO - [diffusion][Epoch 9381] diffusion learning rate: 0.001
2024-11-05 02:01:09,708 - INFO - [diffusion][Epoch 9381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:09,709 - INFO - [diffusion][Epoch 9382] Epoch 9383/12000
2024-11-05 02:01:13,943 - INFO - [diffusion][Epoch 9382] diffusion training Loss: 0.06836014520376921
2024-11-05 02:01:13,945 - INFO - [diffusion][Epoch 9382] diffusion learning rate: 0.001
2024-11-05 02:01:13,947 - INFO - [diffusion][Epoch 9382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:13,948 - INFO - [diffusion][Epoch 9383] Epoch 9384/12000
2024-11-05 02:01:18,216 - INFO - [diffusion][Epoch 9383] diffusion training Loss: 0.05918240547180176
2024-11-05 02:01:18,218 - INFO - [diffusion][Epoch 9383] diffusion learning rate: 0.001
2024-11-05 02:01:18,220 - INFO - [diffusion][Epoch 9383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:18,221 - INFO - [diffusion][Epoch 9384] Epoch 9385/12000
2024-11-05 02:01:22,477 - INFO - [diffusion][Epoch 9384] diffusion training Loss: 0.06607314851135015
2024-11-05 02:01:22,479 - INFO - [diffusion][Epoch 9384] diffusion learning rate: 0.001
2024-11-05 02:01:22,481 - INFO - [diffusion][Epoch 9384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:22,482 - INFO - [diffusion][Epoch 9385] Epoch 9386/12000
2024-11-05 02:01:26,621 - INFO - [diffusion][Epoch 9385] diffusion training Loss: 0.0603201650083065
2024-11-05 02:01:26,623 - INFO - [diffusion][Epoch 9385] diffusion learning rate: 0.001
2024-11-05 02:01:26,625 - INFO - [diffusion][Epoch 9385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:26,626 - INFO - [diffusion][Epoch 9386] Epoch 9387/12000
2024-11-05 02:01:30,858 - INFO - [diffusion][Epoch 9386] diffusion training Loss: 0.057434420101344585
2024-11-05 02:01:30,861 - INFO - [diffusion][Epoch 9386] diffusion learning rate: 0.001
2024-11-05 02:01:30,862 - INFO - [diffusion][Epoch 9386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:30,864 - INFO - [diffusion][Epoch 9387] Epoch 9388/12000
2024-11-05 02:01:35,076 - INFO - [diffusion][Epoch 9387] diffusion training Loss: 0.06002189964056015
2024-11-05 02:01:35,078 - INFO - [diffusion][Epoch 9387] diffusion learning rate: 0.001
2024-11-05 02:01:35,080 - INFO - [diffusion][Epoch 9387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:35,082 - INFO - [diffusion][Epoch 9388] Epoch 9389/12000
2024-11-05 02:01:39,279 - INFO - [diffusion][Epoch 9388] diffusion training Loss: 0.06347121391445398
2024-11-05 02:01:39,281 - INFO - [diffusion][Epoch 9388] diffusion learning rate: 0.001
2024-11-05 02:01:39,283 - INFO - [diffusion][Epoch 9388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:39,284 - INFO - [diffusion][Epoch 9389] Epoch 9390/12000
2024-11-05 02:01:43,554 - INFO - [diffusion][Epoch 9389] diffusion training Loss: 0.05823127739131451
2024-11-05 02:01:43,556 - INFO - [diffusion][Epoch 9389] diffusion learning rate: 0.001
2024-11-05 02:01:43,558 - INFO - [diffusion][Epoch 9389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:43,559 - INFO - [diffusion][Epoch 9390] Epoch 9391/12000
2024-11-05 02:01:47,810 - INFO - [diffusion][Epoch 9390] diffusion training Loss: 0.06816541496664286
2024-11-05 02:01:47,812 - INFO - [diffusion][Epoch 9390] diffusion learning rate: 0.001
2024-11-05 02:01:47,814 - INFO - [diffusion][Epoch 9390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:47,816 - INFO - [diffusion][Epoch 9391] Epoch 9392/12000
2024-11-05 02:01:52,030 - INFO - [diffusion][Epoch 9391] diffusion training Loss: 0.06574816070497036
2024-11-05 02:01:52,033 - INFO - [diffusion][Epoch 9391] diffusion learning rate: 0.001
2024-11-05 02:01:52,034 - INFO - [diffusion][Epoch 9391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:52,036 - INFO - [diffusion][Epoch 9392] Epoch 9393/12000
2024-11-05 02:01:56,186 - INFO - [diffusion][Epoch 9392] diffusion training Loss: 0.061107734218239784
2024-11-05 02:01:56,188 - INFO - [diffusion][Epoch 9392] diffusion learning rate: 0.001
2024-11-05 02:01:56,190 - INFO - [diffusion][Epoch 9392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:56,191 - INFO - [diffusion][Epoch 9393] Epoch 9394/12000
2024-11-05 02:02:00,365 - INFO - [diffusion][Epoch 9393] diffusion training Loss: 0.066226608119905
2024-11-05 02:02:00,367 - INFO - [diffusion][Epoch 9393] diffusion learning rate: 0.001
2024-11-05 02:02:00,369 - INFO - [diffusion][Epoch 9393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:00,370 - INFO - [diffusion][Epoch 9394] Epoch 9395/12000
2024-11-05 02:02:04,533 - INFO - [diffusion][Epoch 9394] diffusion training Loss: 0.05961923487484455
2024-11-05 02:02:04,536 - INFO - [diffusion][Epoch 9394] diffusion learning rate: 0.001
2024-11-05 02:02:04,539 - INFO - [diffusion][Epoch 9394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:04,540 - INFO - [diffusion][Epoch 9395] Epoch 9396/12000
2024-11-05 02:02:08,482 - INFO - [diffusion][Epoch 9395] diffusion training Loss: 0.0636355010792613
2024-11-05 02:02:08,484 - INFO - [diffusion][Epoch 9395] diffusion learning rate: 0.001
2024-11-05 02:02:08,486 - INFO - [diffusion][Epoch 9395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:08,487 - INFO - [diffusion][Epoch 9396] Epoch 9397/12000
2024-11-05 02:02:12,279 - INFO - [diffusion][Epoch 9396] diffusion training Loss: 0.06500497832894325
2024-11-05 02:02:12,401 - INFO - [diffusion][Epoch 9396] diffusion learning rate: 0.001
2024-11-05 02:02:12,403 - INFO - [diffusion][Epoch 9396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:12,404 - INFO - [diffusion][Epoch 9397] Epoch 9398/12000
2024-11-05 02:02:16,619 - INFO - [diffusion][Epoch 9397] diffusion training Loss: 0.06339533440768719
2024-11-05 02:02:16,621 - INFO - [diffusion][Epoch 9397] diffusion learning rate: 0.001
2024-11-05 02:02:16,623 - INFO - [diffusion][Epoch 9397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:16,624 - INFO - [diffusion][Epoch 9398] Epoch 9399/12000
2024-11-05 02:02:20,707 - INFO - [diffusion][Epoch 9398] diffusion training Loss: 0.06357201188802719
2024-11-05 02:02:20,709 - INFO - [diffusion][Epoch 9398] diffusion learning rate: 0.001
2024-11-05 02:02:20,711 - INFO - [diffusion][Epoch 9398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:20,712 - INFO - [diffusion][Epoch 9399] Epoch 9400/12000
2024-11-05 02:02:24,900 - INFO - [diffusion][Epoch 9399] diffusion training Loss: 0.06291415356099606
2024-11-05 02:02:24,901 - INFO - [diffusion][Epoch 9399] diffusion learning rate: 0.001
2024-11-05 02:02:24,903 - INFO - [diffusion][Epoch 9399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:24,904 - INFO - [diffusion][Epoch 9400] Epoch 9401/12000
2024-11-05 02:02:28,994 - INFO - [diffusion][Epoch 9400] diffusion training Loss: 0.06173595506697893
2024-11-05 02:02:28,996 - INFO - [diffusion][Epoch 9400] diffusion learning rate: 0.001
2024-11-05 02:02:28,998 - INFO - [diffusion][Epoch 9400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:28,999 - INFO - [diffusion][Epoch 9401] Epoch 9402/12000
2024-11-05 02:02:33,148 - INFO - [diffusion][Epoch 9401] diffusion training Loss: 0.06121345702558756
2024-11-05 02:02:33,150 - INFO - [diffusion][Epoch 9401] diffusion learning rate: 0.001
2024-11-05 02:02:33,152 - INFO - [diffusion][Epoch 9401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:33,153 - INFO - [diffusion][Epoch 9402] Epoch 9403/12000
2024-11-05 02:02:37,104 - INFO - [diffusion][Epoch 9402] diffusion training Loss: 0.060934475623071194
2024-11-05 02:02:37,106 - INFO - [diffusion][Epoch 9402] diffusion learning rate: 0.001
2024-11-05 02:02:37,108 - INFO - [diffusion][Epoch 9402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:37,109 - INFO - [diffusion][Epoch 9403] Epoch 9404/12000
2024-11-05 02:02:41,345 - INFO - [diffusion][Epoch 9403] diffusion training Loss: 0.06339044589549303
2024-11-05 02:02:41,347 - INFO - [diffusion][Epoch 9403] diffusion learning rate: 0.001
2024-11-05 02:02:41,349 - INFO - [diffusion][Epoch 9403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:41,350 - INFO - [diffusion][Epoch 9404] Epoch 9405/12000
2024-11-05 02:02:45,535 - INFO - [diffusion][Epoch 9404] diffusion training Loss: 0.06648107711225748
2024-11-05 02:02:45,537 - INFO - [diffusion][Epoch 9404] diffusion learning rate: 0.001
2024-11-05 02:02:45,539 - INFO - [diffusion][Epoch 9404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:45,540 - INFO - [diffusion][Epoch 9405] Epoch 9406/12000
2024-11-05 02:02:49,799 - INFO - [diffusion][Epoch 9405] diffusion training Loss: 0.0636645220220089
2024-11-05 02:02:49,801 - INFO - [diffusion][Epoch 9405] diffusion learning rate: 0.001
2024-11-05 02:02:49,866 - INFO - [diffusion][Epoch 9405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:49,867 - INFO - [diffusion][Epoch 9406] Epoch 9407/12000
2024-11-05 02:02:54,049 - INFO - [diffusion][Epoch 9406] diffusion training Loss: 0.06301871594041586
2024-11-05 02:02:54,051 - INFO - [diffusion][Epoch 9406] diffusion learning rate: 0.001
2024-11-05 02:02:54,053 - INFO - [diffusion][Epoch 9406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:54,054 - INFO - [diffusion][Epoch 9407] Epoch 9408/12000
2024-11-05 02:02:58,207 - INFO - [diffusion][Epoch 9407] diffusion training Loss: 0.06196464225649834
2024-11-05 02:02:58,209 - INFO - [diffusion][Epoch 9407] diffusion learning rate: 0.001
2024-11-05 02:02:58,211 - INFO - [diffusion][Epoch 9407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:58,212 - INFO - [diffusion][Epoch 9408] Epoch 9409/12000
2024-11-05 02:03:02,405 - INFO - [diffusion][Epoch 9408] diffusion training Loss: 0.06535928975790739
2024-11-05 02:03:02,407 - INFO - [diffusion][Epoch 9408] diffusion learning rate: 0.001
2024-11-05 02:03:02,409 - INFO - [diffusion][Epoch 9408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:02,410 - INFO - [diffusion][Epoch 9409] Epoch 9410/12000
2024-11-05 02:03:06,463 - INFO - [diffusion][Epoch 9409] diffusion training Loss: 0.06297810189425945
2024-11-05 02:03:06,465 - INFO - [diffusion][Epoch 9409] diffusion learning rate: 0.001
2024-11-05 02:03:06,513 - INFO - [diffusion][Epoch 9409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:06,515 - INFO - [diffusion][Epoch 9410] Epoch 9411/12000
2024-11-05 02:03:10,651 - INFO - [diffusion][Epoch 9410] diffusion training Loss: 0.06264138966798782
2024-11-05 02:03:10,653 - INFO - [diffusion][Epoch 9410] diffusion learning rate: 0.001
2024-11-05 02:03:10,655 - INFO - [diffusion][Epoch 9410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:10,656 - INFO - [diffusion][Epoch 9411] Epoch 9412/12000
2024-11-05 02:03:14,902 - INFO - [diffusion][Epoch 9411] diffusion training Loss: 0.056215007789433
2024-11-05 02:03:14,904 - INFO - [diffusion][Epoch 9411] diffusion learning rate: 0.001
2024-11-05 02:03:14,906 - INFO - [diffusion][Epoch 9411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:14,907 - INFO - [diffusion][Epoch 9412] Epoch 9413/12000
2024-11-05 02:03:19,073 - INFO - [diffusion][Epoch 9412] diffusion training Loss: 0.058568958193063736
2024-11-05 02:03:19,075 - INFO - [diffusion][Epoch 9412] diffusion learning rate: 0.001
2024-11-05 02:03:19,076 - INFO - [diffusion][Epoch 9412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:19,078 - INFO - [diffusion][Epoch 9413] Epoch 9414/12000
2024-11-05 02:03:23,262 - INFO - [diffusion][Epoch 9413] diffusion training Loss: 0.06570173427462578
2024-11-05 02:03:23,264 - INFO - [diffusion][Epoch 9413] diffusion learning rate: 0.001
2024-11-05 02:03:23,266 - INFO - [diffusion][Epoch 9413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:23,267 - INFO - [diffusion][Epoch 9414] Epoch 9415/12000
2024-11-05 02:03:27,450 - INFO - [diffusion][Epoch 9414] diffusion training Loss: 0.06161753088235855
2024-11-05 02:03:27,452 - INFO - [diffusion][Epoch 9414] diffusion learning rate: 0.001
2024-11-05 02:03:27,454 - INFO - [diffusion][Epoch 9414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:27,455 - INFO - [diffusion][Epoch 9415] Epoch 9416/12000
2024-11-05 02:03:31,682 - INFO - [diffusion][Epoch 9415] diffusion training Loss: 0.06611335091292858
2024-11-05 02:03:31,685 - INFO - [diffusion][Epoch 9415] diffusion learning rate: 0.001
2024-11-05 02:03:31,686 - INFO - [diffusion][Epoch 9415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:31,688 - INFO - [diffusion][Epoch 9416] Epoch 9417/12000
2024-11-05 02:03:35,928 - INFO - [diffusion][Epoch 9416] diffusion training Loss: 0.059422857128083706
2024-11-05 02:03:35,931 - INFO - [diffusion][Epoch 9416] diffusion learning rate: 0.001
2024-11-05 02:03:35,933 - INFO - [diffusion][Epoch 9416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:35,934 - INFO - [diffusion][Epoch 9417] Epoch 9418/12000
2024-11-05 02:03:40,153 - INFO - [diffusion][Epoch 9417] diffusion training Loss: 0.06487612053751945
2024-11-05 02:03:40,155 - INFO - [diffusion][Epoch 9417] diffusion learning rate: 0.001
2024-11-05 02:03:40,158 - INFO - [diffusion][Epoch 9417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:40,159 - INFO - [diffusion][Epoch 9418] Epoch 9419/12000
2024-11-05 02:03:44,400 - INFO - [diffusion][Epoch 9418] diffusion training Loss: 0.06523379776626825
2024-11-05 02:03:44,402 - INFO - [diffusion][Epoch 9418] diffusion learning rate: 0.001
2024-11-05 02:03:44,403 - INFO - [diffusion][Epoch 9418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:44,405 - INFO - [diffusion][Epoch 9419] Epoch 9420/12000
2024-11-05 02:03:48,713 - INFO - [diffusion][Epoch 9419] diffusion training Loss: 0.06777390837669373
2024-11-05 02:03:48,716 - INFO - [diffusion][Epoch 9419] diffusion learning rate: 0.001
2024-11-05 02:03:48,718 - INFO - [diffusion][Epoch 9419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:48,720 - INFO - [diffusion][Epoch 9420] Epoch 9421/12000
2024-11-05 02:03:52,942 - INFO - [diffusion][Epoch 9420] diffusion training Loss: 0.062044452875852585
2024-11-05 02:03:52,944 - INFO - [diffusion][Epoch 9420] diffusion learning rate: 0.001
2024-11-05 02:03:52,946 - INFO - [diffusion][Epoch 9420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:52,947 - INFO - [diffusion][Epoch 9421] Epoch 9422/12000
2024-11-05 02:03:57,165 - INFO - [diffusion][Epoch 9421] diffusion training Loss: 0.06592828501015902
2024-11-05 02:03:57,167 - INFO - [diffusion][Epoch 9421] diffusion learning rate: 0.001
2024-11-05 02:03:57,169 - INFO - [diffusion][Epoch 9421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:57,170 - INFO - [diffusion][Epoch 9422] Epoch 9423/12000
2024-11-05 02:04:01,180 - INFO - [diffusion][Epoch 9422] diffusion training Loss: 0.06480455584824085
2024-11-05 02:04:01,182 - INFO - [diffusion][Epoch 9422] diffusion learning rate: 0.001
2024-11-05 02:04:01,184 - INFO - [diffusion][Epoch 9422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:01,185 - INFO - [diffusion][Epoch 9423] Epoch 9424/12000
2024-11-05 02:04:05,422 - INFO - [diffusion][Epoch 9423] diffusion training Loss: 0.0662966649979353
2024-11-05 02:04:05,424 - INFO - [diffusion][Epoch 9423] diffusion learning rate: 0.001
2024-11-05 02:04:05,425 - INFO - [diffusion][Epoch 9423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:05,426 - INFO - [diffusion][Epoch 9424] Epoch 9425/12000
2024-11-05 02:04:09,786 - INFO - [diffusion][Epoch 9424] diffusion training Loss: 0.06298461090773344
2024-11-05 02:04:09,788 - INFO - [diffusion][Epoch 9424] diffusion learning rate: 0.001
2024-11-05 02:04:09,790 - INFO - [diffusion][Epoch 9424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:09,791 - INFO - [diffusion][Epoch 9425] Epoch 9426/12000
2024-11-05 02:04:14,067 - INFO - [diffusion][Epoch 9425] diffusion training Loss: 0.06287130899727345
2024-11-05 02:04:14,069 - INFO - [diffusion][Epoch 9425] diffusion learning rate: 0.001
2024-11-05 02:04:14,071 - INFO - [diffusion][Epoch 9425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:14,072 - INFO - [diffusion][Epoch 9426] Epoch 9427/12000
2024-11-05 02:04:18,305 - INFO - [diffusion][Epoch 9426] diffusion training Loss: 0.06087967474013567
2024-11-05 02:04:18,307 - INFO - [diffusion][Epoch 9426] diffusion learning rate: 0.001
2024-11-05 02:04:18,326 - INFO - [diffusion][Epoch 9426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:18,327 - INFO - [diffusion][Epoch 9427] Epoch 9428/12000
2024-11-05 02:04:22,597 - INFO - [diffusion][Epoch 9427] diffusion training Loss: 0.06204292457550764
2024-11-05 02:04:22,598 - INFO - [diffusion][Epoch 9427] diffusion learning rate: 0.001
2024-11-05 02:04:22,600 - INFO - [diffusion][Epoch 9427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:22,601 - INFO - [diffusion][Epoch 9428] Epoch 9429/12000
2024-11-05 02:04:26,883 - INFO - [diffusion][Epoch 9428] diffusion training Loss: 0.06697838753461838
2024-11-05 02:04:26,885 - INFO - [diffusion][Epoch 9428] diffusion learning rate: 0.001
2024-11-05 02:04:26,888 - INFO - [diffusion][Epoch 9428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:26,889 - INFO - [diffusion][Epoch 9429] Epoch 9430/12000
2024-11-05 02:04:30,979 - INFO - [diffusion][Epoch 9429] diffusion training Loss: 0.05882726050913334
2024-11-05 02:04:30,981 - INFO - [diffusion][Epoch 9429] diffusion learning rate: 0.001
2024-11-05 02:04:30,983 - INFO - [diffusion][Epoch 9429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:30,984 - INFO - [diffusion][Epoch 9430] Epoch 9431/12000
2024-11-05 02:04:34,990 - INFO - [diffusion][Epoch 9430] diffusion training Loss: 0.06025645975023508
2024-11-05 02:04:34,995 - INFO - [diffusion][Epoch 9430] diffusion learning rate: 0.001
2024-11-05 02:04:34,997 - INFO - [diffusion][Epoch 9430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:34,999 - INFO - [diffusion][Epoch 9431] Epoch 9432/12000
2024-11-05 02:04:39,130 - INFO - [diffusion][Epoch 9431] diffusion training Loss: 0.06108685303479433
2024-11-05 02:04:39,132 - INFO - [diffusion][Epoch 9431] diffusion learning rate: 0.001
2024-11-05 02:04:39,133 - INFO - [diffusion][Epoch 9431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:39,135 - INFO - [diffusion][Epoch 9432] Epoch 9433/12000
2024-11-05 02:04:43,399 - INFO - [diffusion][Epoch 9432] diffusion training Loss: 0.061455026268959045
2024-11-05 02:04:43,400 - INFO - [diffusion][Epoch 9432] diffusion learning rate: 0.001
2024-11-05 02:04:43,402 - INFO - [diffusion][Epoch 9432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:43,404 - INFO - [diffusion][Epoch 9433] Epoch 9434/12000
2024-11-05 02:04:47,731 - INFO - [diffusion][Epoch 9433] diffusion training Loss: 0.06480533443391323
2024-11-05 02:04:47,733 - INFO - [diffusion][Epoch 9433] diffusion learning rate: 0.001
2024-11-05 02:04:47,734 - INFO - [diffusion][Epoch 9433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:47,736 - INFO - [diffusion][Epoch 9434] Epoch 9435/12000
2024-11-05 02:04:51,856 - INFO - [diffusion][Epoch 9434] diffusion training Loss: 0.06158660352230072
2024-11-05 02:04:51,858 - INFO - [diffusion][Epoch 9434] diffusion learning rate: 0.001
2024-11-05 02:04:51,860 - INFO - [diffusion][Epoch 9434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:51,861 - INFO - [diffusion][Epoch 9435] Epoch 9436/12000
2024-11-05 02:04:55,943 - INFO - [diffusion][Epoch 9435] diffusion training Loss: 0.05733271688222885
2024-11-05 02:04:55,945 - INFO - [diffusion][Epoch 9435] diffusion learning rate: 0.001
2024-11-05 02:04:55,947 - INFO - [diffusion][Epoch 9435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:55,948 - INFO - [diffusion][Epoch 9436] Epoch 9437/12000
2024-11-05 02:05:00,256 - INFO - [diffusion][Epoch 9436] diffusion training Loss: 0.05914402287453413
2024-11-05 02:05:00,258 - INFO - [diffusion][Epoch 9436] diffusion learning rate: 0.001
2024-11-05 02:05:00,260 - INFO - [diffusion][Epoch 9436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:00,262 - INFO - [diffusion][Epoch 9437] Epoch 9438/12000
2024-11-05 02:05:04,503 - INFO - [diffusion][Epoch 9437] diffusion training Loss: 0.0660891467705369
2024-11-05 02:05:04,504 - INFO - [diffusion][Epoch 9437] diffusion learning rate: 0.001
2024-11-05 02:05:04,506 - INFO - [diffusion][Epoch 9437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:04,507 - INFO - [diffusion][Epoch 9438] Epoch 9439/12000
2024-11-05 02:05:08,837 - INFO - [diffusion][Epoch 9438] diffusion training Loss: 0.05734676495194435
2024-11-05 02:05:08,838 - INFO - [diffusion][Epoch 9438] diffusion learning rate: 0.001
2024-11-05 02:05:08,840 - INFO - [diffusion][Epoch 9438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:08,841 - INFO - [diffusion][Epoch 9439] Epoch 9440/12000
2024-11-05 02:05:13,579 - INFO - [diffusion][Epoch 9439] diffusion training Loss: 0.06620829179883003
2024-11-05 02:05:13,581 - INFO - [diffusion][Epoch 9439] diffusion learning rate: 0.001
2024-11-05 02:05:13,583 - INFO - [diffusion][Epoch 9439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:13,584 - INFO - [diffusion][Epoch 9440] Epoch 9441/12000
2024-11-05 02:05:17,781 - INFO - [diffusion][Epoch 9440] diffusion training Loss: 0.06476094573736191
2024-11-05 02:05:17,784 - INFO - [diffusion][Epoch 9440] diffusion learning rate: 0.001
2024-11-05 02:05:17,785 - INFO - [diffusion][Epoch 9440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:17,787 - INFO - [diffusion][Epoch 9441] Epoch 9442/12000
2024-11-05 02:05:21,943 - INFO - [diffusion][Epoch 9441] diffusion training Loss: 0.058343629352748394
2024-11-05 02:05:21,945 - INFO - [diffusion][Epoch 9441] diffusion learning rate: 0.001
2024-11-05 02:05:21,946 - INFO - [diffusion][Epoch 9441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:21,948 - INFO - [diffusion][Epoch 9442] Epoch 9443/12000
2024-11-05 02:05:26,070 - INFO - [diffusion][Epoch 9442] diffusion training Loss: 0.06732781138271093
2024-11-05 02:05:26,072 - INFO - [diffusion][Epoch 9442] diffusion learning rate: 0.001
2024-11-05 02:05:26,074 - INFO - [diffusion][Epoch 9442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:26,075 - INFO - [diffusion][Epoch 9443] Epoch 9444/12000
2024-11-05 02:05:30,221 - INFO - [diffusion][Epoch 9443] diffusion training Loss: 0.07173153944313526
2024-11-05 02:05:30,223 - INFO - [diffusion][Epoch 9443] diffusion learning rate: 0.001
2024-11-05 02:05:30,225 - INFO - [diffusion][Epoch 9443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:30,227 - INFO - [diffusion][Epoch 9444] Epoch 9445/12000
2024-11-05 02:05:34,290 - INFO - [diffusion][Epoch 9444] diffusion training Loss: 0.07244448736310005
2024-11-05 02:05:34,292 - INFO - [diffusion][Epoch 9444] diffusion learning rate: 0.001
2024-11-05 02:05:34,294 - INFO - [diffusion][Epoch 9444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:34,295 - INFO - [diffusion][Epoch 9445] Epoch 9446/12000
2024-11-05 02:05:38,434 - INFO - [diffusion][Epoch 9445] diffusion training Loss: 0.06547826435416937
2024-11-05 02:05:38,436 - INFO - [diffusion][Epoch 9445] diffusion learning rate: 0.001
2024-11-05 02:05:38,438 - INFO - [diffusion][Epoch 9445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:38,439 - INFO - [diffusion][Epoch 9446] Epoch 9447/12000
2024-11-05 02:05:42,560 - INFO - [diffusion][Epoch 9446] diffusion training Loss: 0.06497372314333916
2024-11-05 02:05:42,562 - INFO - [diffusion][Epoch 9446] diffusion learning rate: 0.001
2024-11-05 02:05:42,564 - INFO - [diffusion][Epoch 9446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:42,565 - INFO - [diffusion][Epoch 9447] Epoch 9448/12000
2024-11-05 02:05:46,575 - INFO - [diffusion][Epoch 9447] diffusion training Loss: 0.06438468396663666
2024-11-05 02:05:46,577 - INFO - [diffusion][Epoch 9447] diffusion learning rate: 0.001
2024-11-05 02:05:46,579 - INFO - [diffusion][Epoch 9447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:46,580 - INFO - [diffusion][Epoch 9448] Epoch 9449/12000
2024-11-05 02:05:50,550 - INFO - [diffusion][Epoch 9448] diffusion training Loss: 0.061517142690718174
2024-11-05 02:05:50,552 - INFO - [diffusion][Epoch 9448] diffusion learning rate: 0.001
2024-11-05 02:05:50,554 - INFO - [diffusion][Epoch 9448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:50,555 - INFO - [diffusion][Epoch 9449] Epoch 9450/12000
2024-11-05 02:05:54,579 - INFO - [diffusion][Epoch 9449] diffusion training Loss: 0.06351018510758877
2024-11-05 02:05:54,581 - INFO - [diffusion][Epoch 9449] diffusion learning rate: 0.001
2024-11-05 02:05:54,583 - INFO - [diffusion][Epoch 9449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:54,584 - INFO - [diffusion][Epoch 9450] Epoch 9451/12000
2024-11-05 02:05:58,579 - INFO - [diffusion][Epoch 9450] diffusion training Loss: 0.06473252363502979
2024-11-05 02:05:58,581 - INFO - [diffusion][Epoch 9450] diffusion learning rate: 0.001
2024-11-05 02:05:58,584 - INFO - [diffusion][Epoch 9450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:58,585 - INFO - [diffusion][Epoch 9451] Epoch 9452/12000
2024-11-05 02:06:02,691 - INFO - [diffusion][Epoch 9451] diffusion training Loss: 0.06282057985663414
2024-11-05 02:06:02,693 - INFO - [diffusion][Epoch 9451] diffusion learning rate: 0.001
2024-11-05 02:06:02,700 - INFO - [diffusion][Epoch 9451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:02,701 - INFO - [diffusion][Epoch 9452] Epoch 9453/12000
2024-11-05 02:06:07,059 - INFO - [diffusion][Epoch 9452] diffusion training Loss: 0.06322571821510792
2024-11-05 02:06:07,061 - INFO - [diffusion][Epoch 9452] diffusion learning rate: 0.001
2024-11-05 02:06:07,062 - INFO - [diffusion][Epoch 9452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:07,064 - INFO - [diffusion][Epoch 9453] Epoch 9454/12000
2024-11-05 02:06:11,398 - INFO - [diffusion][Epoch 9453] diffusion training Loss: 0.062967867590487
2024-11-05 02:06:11,400 - INFO - [diffusion][Epoch 9453] diffusion learning rate: 0.001
2024-11-05 02:06:11,402 - INFO - [diffusion][Epoch 9453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:11,404 - INFO - [diffusion][Epoch 9454] Epoch 9455/12000
2024-11-05 02:06:15,436 - INFO - [diffusion][Epoch 9454] diffusion training Loss: 0.058954750187695026
2024-11-05 02:06:15,438 - INFO - [diffusion][Epoch 9454] diffusion learning rate: 0.001
2024-11-05 02:06:15,440 - INFO - [diffusion][Epoch 9454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:15,441 - INFO - [diffusion][Epoch 9455] Epoch 9456/12000
2024-11-05 02:06:19,616 - INFO - [diffusion][Epoch 9455] diffusion training Loss: 0.0598743399605155
2024-11-05 02:06:19,619 - INFO - [diffusion][Epoch 9455] diffusion learning rate: 0.001
2024-11-05 02:06:19,621 - INFO - [diffusion][Epoch 9455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:19,623 - INFO - [diffusion][Epoch 9456] Epoch 9457/12000
2024-11-05 02:06:23,641 - INFO - [diffusion][Epoch 9456] diffusion training Loss: 0.06877636164426804
2024-11-05 02:06:23,643 - INFO - [diffusion][Epoch 9456] diffusion learning rate: 0.001
2024-11-05 02:06:23,645 - INFO - [diffusion][Epoch 9456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:23,646 - INFO - [diffusion][Epoch 9457] Epoch 9458/12000
2024-11-05 02:06:27,613 - INFO - [diffusion][Epoch 9457] diffusion training Loss: 0.06946569122374058
2024-11-05 02:06:27,615 - INFO - [diffusion][Epoch 9457] diffusion learning rate: 0.001
2024-11-05 02:06:27,617 - INFO - [diffusion][Epoch 9457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:27,618 - INFO - [diffusion][Epoch 9458] Epoch 9459/12000
2024-11-05 02:06:31,653 - INFO - [diffusion][Epoch 9458] diffusion training Loss: 0.06262636743485928
2024-11-05 02:06:31,655 - INFO - [diffusion][Epoch 9458] diffusion learning rate: 0.001
2024-11-05 02:06:31,657 - INFO - [diffusion][Epoch 9458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:31,658 - INFO - [diffusion][Epoch 9459] Epoch 9460/12000
2024-11-05 02:06:36,235 - INFO - [diffusion][Epoch 9459] diffusion training Loss: 0.06045104004442692
2024-11-05 02:06:36,237 - INFO - [diffusion][Epoch 9459] diffusion learning rate: 0.001
2024-11-05 02:06:36,239 - INFO - [diffusion][Epoch 9459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:36,240 - INFO - [diffusion][Epoch 9460] Epoch 9461/12000
2024-11-05 02:06:40,320 - INFO - [diffusion][Epoch 9460] diffusion training Loss: 0.06624456588178873
2024-11-05 02:06:40,323 - INFO - [diffusion][Epoch 9460] diffusion learning rate: 0.001
2024-11-05 02:06:40,325 - INFO - [diffusion][Epoch 9460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:40,327 - INFO - [diffusion][Epoch 9461] Epoch 9462/12000
2024-11-05 02:06:44,332 - INFO - [diffusion][Epoch 9461] diffusion training Loss: 0.06216626800596714
2024-11-05 02:06:44,333 - INFO - [diffusion][Epoch 9461] diffusion learning rate: 0.001
2024-11-05 02:06:44,335 - INFO - [diffusion][Epoch 9461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:44,337 - INFO - [diffusion][Epoch 9462] Epoch 9463/12000
2024-11-05 02:06:48,584 - INFO - [diffusion][Epoch 9462] diffusion training Loss: 0.06442350056022406
2024-11-05 02:06:48,587 - INFO - [diffusion][Epoch 9462] diffusion learning rate: 0.001
2024-11-05 02:06:48,588 - INFO - [diffusion][Epoch 9462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:48,590 - INFO - [diffusion][Epoch 9463] Epoch 9464/12000
2024-11-05 02:06:52,680 - INFO - [diffusion][Epoch 9463] diffusion training Loss: 0.06250623613595963
2024-11-05 02:06:52,682 - INFO - [diffusion][Epoch 9463] diffusion learning rate: 0.001
2024-11-05 02:06:52,684 - INFO - [diffusion][Epoch 9463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:52,685 - INFO - [diffusion][Epoch 9464] Epoch 9465/12000
2024-11-05 02:06:56,898 - INFO - [diffusion][Epoch 9464] diffusion training Loss: 0.05965202674269676
2024-11-05 02:06:56,900 - INFO - [diffusion][Epoch 9464] diffusion learning rate: 0.001
2024-11-05 02:06:56,902 - INFO - [diffusion][Epoch 9464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:56,903 - INFO - [diffusion][Epoch 9465] Epoch 9466/12000
2024-11-05 02:07:00,745 - INFO - [diffusion][Epoch 9465] diffusion training Loss: 0.06686767749488354
2024-11-05 02:07:00,747 - INFO - [diffusion][Epoch 9465] diffusion learning rate: 0.001
2024-11-05 02:07:00,749 - INFO - [diffusion][Epoch 9465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:00,750 - INFO - [diffusion][Epoch 9466] Epoch 9467/12000
2024-11-05 02:07:04,937 - INFO - [diffusion][Epoch 9466] diffusion training Loss: 0.06286469660699368
2024-11-05 02:07:04,939 - INFO - [diffusion][Epoch 9466] diffusion learning rate: 0.001
2024-11-05 02:07:04,941 - INFO - [diffusion][Epoch 9466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:04,942 - INFO - [diffusion][Epoch 9467] Epoch 9468/12000
2024-11-05 02:07:09,120 - INFO - [diffusion][Epoch 9467] diffusion training Loss: 0.06837599072605371
2024-11-05 02:07:09,123 - INFO - [diffusion][Epoch 9467] diffusion learning rate: 0.001
2024-11-05 02:07:09,124 - INFO - [diffusion][Epoch 9467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:09,127 - INFO - [diffusion][Epoch 9468] Epoch 9469/12000
2024-11-05 02:07:13,184 - INFO - [diffusion][Epoch 9468] diffusion training Loss: 0.06173052731901407
2024-11-05 02:07:13,187 - INFO - [diffusion][Epoch 9468] diffusion learning rate: 0.001
2024-11-05 02:07:13,189 - INFO - [diffusion][Epoch 9468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:13,190 - INFO - [diffusion][Epoch 9469] Epoch 9470/12000
2024-11-05 02:07:17,405 - INFO - [diffusion][Epoch 9469] diffusion training Loss: 0.06415457837283611
2024-11-05 02:07:17,408 - INFO - [diffusion][Epoch 9469] diffusion learning rate: 0.001
2024-11-05 02:07:17,410 - INFO - [diffusion][Epoch 9469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:17,411 - INFO - [diffusion][Epoch 9470] Epoch 9471/12000
2024-11-05 02:07:21,502 - INFO - [diffusion][Epoch 9470] diffusion training Loss: 0.06275182496756315
2024-11-05 02:07:21,505 - INFO - [diffusion][Epoch 9470] diffusion learning rate: 0.001
2024-11-05 02:07:21,506 - INFO - [diffusion][Epoch 9470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:21,508 - INFO - [diffusion][Epoch 9471] Epoch 9472/12000
2024-11-05 02:07:25,606 - INFO - [diffusion][Epoch 9471] diffusion training Loss: 0.06111238710582256
2024-11-05 02:07:25,607 - INFO - [diffusion][Epoch 9471] diffusion learning rate: 0.001
2024-11-05 02:07:25,609 - INFO - [diffusion][Epoch 9471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:25,610 - INFO - [diffusion][Epoch 9472] Epoch 9473/12000
2024-11-05 02:07:29,759 - INFO - [diffusion][Epoch 9472] diffusion training Loss: 0.05827623512595892
2024-11-05 02:07:29,762 - INFO - [diffusion][Epoch 9472] diffusion learning rate: 0.001
2024-11-05 02:07:29,764 - INFO - [diffusion][Epoch 9472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:29,765 - INFO - [diffusion][Epoch 9473] Epoch 9474/12000
2024-11-05 02:07:33,987 - INFO - [diffusion][Epoch 9473] diffusion training Loss: 0.06314253807067871
2024-11-05 02:07:33,989 - INFO - [diffusion][Epoch 9473] diffusion learning rate: 0.001
2024-11-05 02:07:33,991 - INFO - [diffusion][Epoch 9473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:33,992 - INFO - [diffusion][Epoch 9474] Epoch 9475/12000
2024-11-05 02:07:38,133 - INFO - [diffusion][Epoch 9474] diffusion training Loss: 0.05992663744837046
2024-11-05 02:07:38,135 - INFO - [diffusion][Epoch 9474] diffusion learning rate: 0.001
2024-11-05 02:07:38,138 - INFO - [diffusion][Epoch 9474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:38,139 - INFO - [diffusion][Epoch 9475] Epoch 9476/12000
2024-11-05 02:07:42,334 - INFO - [diffusion][Epoch 9475] diffusion training Loss: 0.06678951997309923
2024-11-05 02:07:42,337 - INFO - [diffusion][Epoch 9475] diffusion learning rate: 0.001
2024-11-05 02:07:42,339 - INFO - [diffusion][Epoch 9475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:42,340 - INFO - [diffusion][Epoch 9476] Epoch 9477/12000
2024-11-05 02:07:46,543 - INFO - [diffusion][Epoch 9476] diffusion training Loss: 0.06392092071473598
2024-11-05 02:07:46,545 - INFO - [diffusion][Epoch 9476] diffusion learning rate: 0.001
2024-11-05 02:07:46,547 - INFO - [diffusion][Epoch 9476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:46,549 - INFO - [diffusion][Epoch 9477] Epoch 9478/12000
2024-11-05 02:07:50,719 - INFO - [diffusion][Epoch 9477] diffusion training Loss: 0.06794450245797634
2024-11-05 02:07:50,722 - INFO - [diffusion][Epoch 9477] diffusion learning rate: 0.001
2024-11-05 02:07:50,724 - INFO - [diffusion][Epoch 9477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:50,726 - INFO - [diffusion][Epoch 9478] Epoch 9479/12000
2024-11-05 02:07:54,998 - INFO - [diffusion][Epoch 9478] diffusion training Loss: 0.061274648644030094
2024-11-05 02:07:55,000 - INFO - [diffusion][Epoch 9478] diffusion learning rate: 0.001
2024-11-05 02:07:55,002 - INFO - [diffusion][Epoch 9478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:55,004 - INFO - [diffusion][Epoch 9479] Epoch 9480/12000
2024-11-05 02:07:59,272 - INFO - [diffusion][Epoch 9479] diffusion training Loss: 0.06378686521202326
2024-11-05 02:07:59,274 - INFO - [diffusion][Epoch 9479] diffusion learning rate: 0.001
2024-11-05 02:07:59,276 - INFO - [diffusion][Epoch 9479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:59,278 - INFO - [diffusion][Epoch 9480] Epoch 9481/12000
2024-11-05 02:08:03,675 - INFO - [diffusion][Epoch 9480] diffusion training Loss: 0.06574752647429705
2024-11-05 02:08:03,677 - INFO - [diffusion][Epoch 9480] diffusion learning rate: 0.001
2024-11-05 02:08:03,679 - INFO - [diffusion][Epoch 9480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:03,681 - INFO - [diffusion][Epoch 9481] Epoch 9482/12000
2024-11-05 02:08:07,888 - INFO - [diffusion][Epoch 9481] diffusion training Loss: 0.05946100503206253
2024-11-05 02:08:07,890 - INFO - [diffusion][Epoch 9481] diffusion learning rate: 0.001
2024-11-05 02:08:07,892 - INFO - [diffusion][Epoch 9481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:07,893 - INFO - [diffusion][Epoch 9482] Epoch 9483/12000
2024-11-05 02:08:12,117 - INFO - [diffusion][Epoch 9482] diffusion training Loss: 0.06264005601406097
2024-11-05 02:08:12,119 - INFO - [diffusion][Epoch 9482] diffusion learning rate: 0.001
2024-11-05 02:08:12,121 - INFO - [diffusion][Epoch 9482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:12,122 - INFO - [diffusion][Epoch 9483] Epoch 9484/12000
2024-11-05 02:08:16,374 - INFO - [diffusion][Epoch 9483] diffusion training Loss: 0.06655607558786869
2024-11-05 02:08:16,376 - INFO - [diffusion][Epoch 9483] diffusion learning rate: 0.001
2024-11-05 02:08:16,378 - INFO - [diffusion][Epoch 9483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:16,379 - INFO - [diffusion][Epoch 9484] Epoch 9485/12000
2024-11-05 02:08:20,435 - INFO - [diffusion][Epoch 9484] diffusion training Loss: 0.05900085810571909
2024-11-05 02:08:20,437 - INFO - [diffusion][Epoch 9484] diffusion learning rate: 0.001
2024-11-05 02:08:20,439 - INFO - [diffusion][Epoch 9484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:20,440 - INFO - [diffusion][Epoch 9485] Epoch 9486/12000
2024-11-05 02:08:24,687 - INFO - [diffusion][Epoch 9485] diffusion training Loss: 0.06348594464361668
2024-11-05 02:08:24,689 - INFO - [diffusion][Epoch 9485] diffusion learning rate: 0.001
2024-11-05 02:08:24,691 - INFO - [diffusion][Epoch 9485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:24,692 - INFO - [diffusion][Epoch 9486] Epoch 9487/12000
2024-11-05 02:08:28,884 - INFO - [diffusion][Epoch 9486] diffusion training Loss: 0.06349264085292816
2024-11-05 02:08:28,886 - INFO - [diffusion][Epoch 9486] diffusion learning rate: 0.001
2024-11-05 02:08:28,888 - INFO - [diffusion][Epoch 9486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:28,889 - INFO - [diffusion][Epoch 9487] Epoch 9488/12000
2024-11-05 02:08:33,240 - INFO - [diffusion][Epoch 9487] diffusion training Loss: 0.060334919951856136
2024-11-05 02:08:33,243 - INFO - [diffusion][Epoch 9487] diffusion learning rate: 0.001
2024-11-05 02:08:33,245 - INFO - [diffusion][Epoch 9487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:33,246 - INFO - [diffusion][Epoch 9488] Epoch 9489/12000
2024-11-05 02:08:37,554 - INFO - [diffusion][Epoch 9488] diffusion training Loss: 0.059755945578217506
2024-11-05 02:08:37,556 - INFO - [diffusion][Epoch 9488] diffusion learning rate: 0.001
2024-11-05 02:08:37,558 - INFO - [diffusion][Epoch 9488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:37,559 - INFO - [diffusion][Epoch 9489] Epoch 9490/12000
2024-11-05 02:08:41,868 - INFO - [diffusion][Epoch 9489] diffusion training Loss: 0.06420655734837055
2024-11-05 02:08:41,870 - INFO - [diffusion][Epoch 9489] diffusion learning rate: 0.001
2024-11-05 02:08:41,872 - INFO - [diffusion][Epoch 9489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:41,874 - INFO - [diffusion][Epoch 9490] Epoch 9491/12000
2024-11-05 02:08:46,110 - INFO - [diffusion][Epoch 9490] diffusion training Loss: 0.05883120000362396
2024-11-05 02:08:46,112 - INFO - [diffusion][Epoch 9490] diffusion learning rate: 0.001
2024-11-05 02:08:46,114 - INFO - [diffusion][Epoch 9490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:46,115 - INFO - [diffusion][Epoch 9491] Epoch 9492/12000
2024-11-05 02:08:50,464 - INFO - [diffusion][Epoch 9491] diffusion training Loss: 0.0628228448331356
2024-11-05 02:08:50,467 - INFO - [diffusion][Epoch 9491] diffusion learning rate: 0.001
2024-11-05 02:08:50,469 - INFO - [diffusion][Epoch 9491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:50,470 - INFO - [diffusion][Epoch 9492] Epoch 9493/12000
2024-11-05 02:08:54,731 - INFO - [diffusion][Epoch 9492] diffusion training Loss: 0.06258852127939463
2024-11-05 02:08:54,734 - INFO - [diffusion][Epoch 9492] diffusion learning rate: 0.001
2024-11-05 02:08:54,736 - INFO - [diffusion][Epoch 9492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:54,737 - INFO - [diffusion][Epoch 9493] Epoch 9494/12000
2024-11-05 02:08:58,982 - INFO - [diffusion][Epoch 9493] diffusion training Loss: 0.06460186094045639
2024-11-05 02:08:58,984 - INFO - [diffusion][Epoch 9493] diffusion learning rate: 0.001
2024-11-05 02:08:58,986 - INFO - [diffusion][Epoch 9493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:58,987 - INFO - [diffusion][Epoch 9494] Epoch 9495/12000
2024-11-05 02:09:03,277 - INFO - [diffusion][Epoch 9494] diffusion training Loss: 0.06494651734828949
2024-11-05 02:09:03,279 - INFO - [diffusion][Epoch 9494] diffusion learning rate: 0.001
2024-11-05 02:09:03,281 - INFO - [diffusion][Epoch 9494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:03,282 - INFO - [diffusion][Epoch 9495] Epoch 9496/12000
2024-11-05 02:09:07,575 - INFO - [diffusion][Epoch 9495] diffusion training Loss: 0.05860185716301203
2024-11-05 02:09:07,577 - INFO - [diffusion][Epoch 9495] diffusion learning rate: 0.001
2024-11-05 02:09:07,579 - INFO - [diffusion][Epoch 9495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:07,581 - INFO - [diffusion][Epoch 9496] Epoch 9497/12000
2024-11-05 02:09:11,652 - INFO - [diffusion][Epoch 9496] diffusion training Loss: 0.06598607078194618
2024-11-05 02:09:11,657 - INFO - [diffusion][Epoch 9496] diffusion learning rate: 0.001
2024-11-05 02:09:11,659 - INFO - [diffusion][Epoch 9496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:11,660 - INFO - [diffusion][Epoch 9497] Epoch 9498/12000
2024-11-05 02:09:16,004 - INFO - [diffusion][Epoch 9497] diffusion training Loss: 0.06316554173827171
2024-11-05 02:09:16,008 - INFO - [diffusion][Epoch 9497] diffusion learning rate: 0.001
2024-11-05 02:09:16,047 - INFO - [diffusion][Epoch 9497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:16,049 - INFO - [diffusion][Epoch 9498] Epoch 9499/12000
2024-11-05 02:09:20,248 - INFO - [diffusion][Epoch 9498] diffusion training Loss: 0.06487120036035776
2024-11-05 02:09:20,250 - INFO - [diffusion][Epoch 9498] diffusion learning rate: 0.001
2024-11-05 02:09:20,252 - INFO - [diffusion][Epoch 9498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:20,253 - INFO - [diffusion][Epoch 9499] Epoch 9500/12000
2024-11-05 02:09:24,265 - INFO - [diffusion][Epoch 9499] diffusion training Loss: 0.05950168892741203
2024-11-05 02:09:24,267 - INFO - [diffusion][Epoch 9499] diffusion learning rate: 0.001
2024-11-05 02:09:24,269 - INFO - [diffusion][Epoch 9499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:24,270 - INFO - [diffusion][Epoch 9500] Epoch 9501/12000
2024-11-05 02:09:28,448 - INFO - [diffusion][Epoch 9500] diffusion training Loss: 0.06630938313901424
2024-11-05 02:09:28,451 - INFO - [diffusion][Epoch 9500] diffusion learning rate: 0.001
2024-11-05 02:09:28,453 - INFO - [diffusion][Epoch 9500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:28,455 - INFO - [diffusion][Epoch 9501] Epoch 9502/12000
2024-11-05 02:09:33,187 - INFO - [diffusion][Epoch 9501] diffusion training Loss: 0.06546354945749044
2024-11-05 02:09:33,189 - INFO - [diffusion][Epoch 9501] diffusion learning rate: 0.001
2024-11-05 02:09:33,208 - INFO - [diffusion][Epoch 9501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:33,210 - INFO - [diffusion][Epoch 9502] Epoch 9503/12000
2024-11-05 02:09:37,334 - INFO - [diffusion][Epoch 9502] diffusion training Loss: 0.061692554503679276
2024-11-05 02:09:37,336 - INFO - [diffusion][Epoch 9502] diffusion learning rate: 0.001
2024-11-05 02:09:37,337 - INFO - [diffusion][Epoch 9502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:37,339 - INFO - [diffusion][Epoch 9503] Epoch 9504/12000
2024-11-05 02:09:41,433 - INFO - [diffusion][Epoch 9503] diffusion training Loss: 0.06182399205863476
2024-11-05 02:09:41,436 - INFO - [diffusion][Epoch 9503] diffusion learning rate: 0.001
2024-11-05 02:09:41,438 - INFO - [diffusion][Epoch 9503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:41,439 - INFO - [diffusion][Epoch 9504] Epoch 9505/12000
2024-11-05 02:09:45,664 - INFO - [diffusion][Epoch 9504] diffusion training Loss: 0.06127477902919054
2024-11-05 02:09:45,666 - INFO - [diffusion][Epoch 9504] diffusion learning rate: 0.001
2024-11-05 02:09:45,668 - INFO - [diffusion][Epoch 9504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:45,669 - INFO - [diffusion][Epoch 9505] Epoch 9506/12000
2024-11-05 02:09:49,883 - INFO - [diffusion][Epoch 9505] diffusion training Loss: 0.057662367820739746
2024-11-05 02:09:49,885 - INFO - [diffusion][Epoch 9505] diffusion learning rate: 0.001
2024-11-05 02:09:49,887 - INFO - [diffusion][Epoch 9505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:49,888 - INFO - [diffusion][Epoch 9506] Epoch 9507/12000
2024-11-05 02:09:54,146 - INFO - [diffusion][Epoch 9506] diffusion training Loss: 0.06432938389480114
2024-11-05 02:09:54,148 - INFO - [diffusion][Epoch 9506] diffusion learning rate: 0.001
2024-11-05 02:09:54,150 - INFO - [diffusion][Epoch 9506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:54,151 - INFO - [diffusion][Epoch 9507] Epoch 9508/12000
2024-11-05 02:09:58,327 - INFO - [diffusion][Epoch 9507] diffusion training Loss: 0.06065479386597872
2024-11-05 02:09:58,329 - INFO - [diffusion][Epoch 9507] diffusion learning rate: 0.001
2024-11-05 02:09:58,331 - INFO - [diffusion][Epoch 9507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:58,333 - INFO - [diffusion][Epoch 9508] Epoch 9509/12000
2024-11-05 02:10:02,618 - INFO - [diffusion][Epoch 9508] diffusion training Loss: 0.06369738560169935
2024-11-05 02:10:02,620 - INFO - [diffusion][Epoch 9508] diffusion learning rate: 0.001
2024-11-05 02:10:02,622 - INFO - [diffusion][Epoch 9508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:02,623 - INFO - [diffusion][Epoch 9509] Epoch 9510/12000
2024-11-05 02:10:06,865 - INFO - [diffusion][Epoch 9509] diffusion training Loss: 0.0656174710020423
2024-11-05 02:10:06,867 - INFO - [diffusion][Epoch 9509] diffusion learning rate: 0.001
2024-11-05 02:10:06,869 - INFO - [diffusion][Epoch 9509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:06,870 - INFO - [diffusion][Epoch 9510] Epoch 9511/12000
2024-11-05 02:10:11,333 - INFO - [diffusion][Epoch 9510] diffusion training Loss: 0.061065918765962124
2024-11-05 02:10:11,335 - INFO - [diffusion][Epoch 9510] diffusion learning rate: 0.001
2024-11-05 02:10:11,337 - INFO - [diffusion][Epoch 9510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:11,339 - INFO - [diffusion][Epoch 9511] Epoch 9512/12000
2024-11-05 02:10:15,578 - INFO - [diffusion][Epoch 9511] diffusion training Loss: 0.06069858744740486
2024-11-05 02:10:15,580 - INFO - [diffusion][Epoch 9511] diffusion learning rate: 0.001
2024-11-05 02:10:15,582 - INFO - [diffusion][Epoch 9511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:15,583 - INFO - [diffusion][Epoch 9512] Epoch 9513/12000
2024-11-05 02:10:19,742 - INFO - [diffusion][Epoch 9512] diffusion training Loss: 0.06448634713888168
2024-11-05 02:10:19,743 - INFO - [diffusion][Epoch 9512] diffusion learning rate: 0.001
2024-11-05 02:10:19,745 - INFO - [diffusion][Epoch 9512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:19,746 - INFO - [diffusion][Epoch 9513] Epoch 9514/12000
2024-11-05 02:10:24,008 - INFO - [diffusion][Epoch 9513] diffusion training Loss: 0.05920737609267235
2024-11-05 02:10:24,010 - INFO - [diffusion][Epoch 9513] diffusion learning rate: 0.001
2024-11-05 02:10:24,012 - INFO - [diffusion][Epoch 9513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:24,013 - INFO - [diffusion][Epoch 9514] Epoch 9515/12000
2024-11-05 02:10:28,409 - INFO - [diffusion][Epoch 9514] diffusion training Loss: 0.06228841468691826
2024-11-05 02:10:28,411 - INFO - [diffusion][Epoch 9514] diffusion learning rate: 0.001
2024-11-05 02:10:28,413 - INFO - [diffusion][Epoch 9514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:28,414 - INFO - [diffusion][Epoch 9515] Epoch 9516/12000
2024-11-05 02:10:32,637 - INFO - [diffusion][Epoch 9515] diffusion training Loss: 0.05518225394189358
2024-11-05 02:10:32,639 - INFO - [diffusion][Epoch 9515] diffusion learning rate: 0.001
2024-11-05 02:10:32,641 - INFO - [diffusion][Epoch 9515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:32,643 - INFO - [diffusion][Epoch 9516] Epoch 9517/12000
2024-11-05 02:10:36,847 - INFO - [diffusion][Epoch 9516] diffusion training Loss: 0.06531879398971796
2024-11-05 02:10:36,849 - INFO - [diffusion][Epoch 9516] diffusion learning rate: 0.001
2024-11-05 02:10:36,851 - INFO - [diffusion][Epoch 9516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:36,853 - INFO - [diffusion][Epoch 9517] Epoch 9518/12000
2024-11-05 02:10:40,860 - INFO - [diffusion][Epoch 9517] diffusion training Loss: 0.06235954165458679
2024-11-05 02:10:40,863 - INFO - [diffusion][Epoch 9517] diffusion learning rate: 0.001
2024-11-05 02:10:40,864 - INFO - [diffusion][Epoch 9517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:40,866 - INFO - [diffusion][Epoch 9518] Epoch 9519/12000
2024-11-05 02:10:45,121 - INFO - [diffusion][Epoch 9518] diffusion training Loss: 0.06869140174239874
2024-11-05 02:10:45,123 - INFO - [diffusion][Epoch 9518] diffusion learning rate: 0.001
2024-11-05 02:10:45,125 - INFO - [diffusion][Epoch 9518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:45,127 - INFO - [diffusion][Epoch 9519] Epoch 9520/12000
2024-11-05 02:10:49,331 - INFO - [diffusion][Epoch 9519] diffusion training Loss: 0.06690853275358677
2024-11-05 02:10:49,333 - INFO - [diffusion][Epoch 9519] diffusion learning rate: 0.001
2024-11-05 02:10:49,335 - INFO - [diffusion][Epoch 9519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:49,336 - INFO - [diffusion][Epoch 9520] Epoch 9521/12000
2024-11-05 02:10:53,579 - INFO - [diffusion][Epoch 9520] diffusion training Loss: 0.06296065356582403
2024-11-05 02:10:53,581 - INFO - [diffusion][Epoch 9520] diffusion learning rate: 0.001
2024-11-05 02:10:53,583 - INFO - [diffusion][Epoch 9520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:53,584 - INFO - [diffusion][Epoch 9521] Epoch 9522/12000
2024-11-05 02:10:57,921 - INFO - [diffusion][Epoch 9521] diffusion training Loss: 0.057930500246584415
2024-11-05 02:10:57,924 - INFO - [diffusion][Epoch 9521] diffusion learning rate: 0.001
2024-11-05 02:10:57,926 - INFO - [diffusion][Epoch 9521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:57,927 - INFO - [diffusion][Epoch 9522] Epoch 9523/12000
2024-11-05 02:11:02,542 - INFO - [diffusion][Epoch 9522] diffusion training Loss: 0.06009729020297527
2024-11-05 02:11:02,544 - INFO - [diffusion][Epoch 9522] diffusion learning rate: 0.001
2024-11-05 02:11:02,546 - INFO - [diffusion][Epoch 9522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:02,547 - INFO - [diffusion][Epoch 9523] Epoch 9524/12000
2024-11-05 02:11:06,856 - INFO - [diffusion][Epoch 9523] diffusion training Loss: 0.066017746925354
2024-11-05 02:11:06,858 - INFO - [diffusion][Epoch 9523] diffusion learning rate: 0.001
2024-11-05 02:11:06,860 - INFO - [diffusion][Epoch 9523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:06,861 - INFO - [diffusion][Epoch 9524] Epoch 9525/12000
2024-11-05 02:11:10,947 - INFO - [diffusion][Epoch 9524] diffusion training Loss: 0.06208282150328159
2024-11-05 02:11:10,949 - INFO - [diffusion][Epoch 9524] diffusion learning rate: 0.001
2024-11-05 02:11:10,951 - INFO - [diffusion][Epoch 9524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:10,953 - INFO - [diffusion][Epoch 9525] Epoch 9526/12000
2024-11-05 02:11:15,226 - INFO - [diffusion][Epoch 9525] diffusion training Loss: 0.06081553641706705
2024-11-05 02:11:15,228 - INFO - [diffusion][Epoch 9525] diffusion learning rate: 0.001
2024-11-05 02:11:15,230 - INFO - [diffusion][Epoch 9525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:15,231 - INFO - [diffusion][Epoch 9526] Epoch 9527/12000
2024-11-05 02:11:19,603 - INFO - [diffusion][Epoch 9526] diffusion training Loss: 0.059527584351599216
2024-11-05 02:11:19,606 - INFO - [diffusion][Epoch 9526] diffusion learning rate: 0.001
2024-11-05 02:11:19,608 - INFO - [diffusion][Epoch 9526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:19,609 - INFO - [diffusion][Epoch 9527] Epoch 9528/12000
2024-11-05 02:11:23,937 - INFO - [diffusion][Epoch 9527] diffusion training Loss: 0.0621663611382246
2024-11-05 02:11:23,939 - INFO - [diffusion][Epoch 9527] diffusion learning rate: 0.001
2024-11-05 02:11:23,941 - INFO - [diffusion][Epoch 9527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:23,942 - INFO - [diffusion][Epoch 9528] Epoch 9529/12000
2024-11-05 02:11:28,139 - INFO - [diffusion][Epoch 9528] diffusion training Loss: 0.06735658086836338
2024-11-05 02:11:28,142 - INFO - [diffusion][Epoch 9528] diffusion learning rate: 0.001
2024-11-05 02:11:28,144 - INFO - [diffusion][Epoch 9528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:28,145 - INFO - [diffusion][Epoch 9529] Epoch 9530/12000
2024-11-05 02:11:32,396 - INFO - [diffusion][Epoch 9529] diffusion training Loss: 0.06565742939710617
2024-11-05 02:11:32,398 - INFO - [diffusion][Epoch 9529] diffusion learning rate: 0.001
2024-11-05 02:11:32,426 - INFO - [diffusion][Epoch 9529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:32,427 - INFO - [diffusion][Epoch 9530] Epoch 9531/12000
2024-11-05 02:11:36,739 - INFO - [diffusion][Epoch 9530] diffusion training Loss: 0.06072411499917507
2024-11-05 02:11:36,741 - INFO - [diffusion][Epoch 9530] diffusion learning rate: 0.001
2024-11-05 02:11:36,743 - INFO - [diffusion][Epoch 9530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:36,744 - INFO - [diffusion][Epoch 9531] Epoch 9532/12000
2024-11-05 02:11:40,949 - INFO - [diffusion][Epoch 9531] diffusion training Loss: 0.06613590382039547
2024-11-05 02:11:40,951 - INFO - [diffusion][Epoch 9531] diffusion learning rate: 0.001
2024-11-05 02:11:40,953 - INFO - [diffusion][Epoch 9531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:40,955 - INFO - [diffusion][Epoch 9532] Epoch 9533/12000
2024-11-05 02:11:45,270 - INFO - [diffusion][Epoch 9532] diffusion training Loss: 0.06361525598913431
2024-11-05 02:11:45,272 - INFO - [diffusion][Epoch 9532] diffusion learning rate: 0.001
2024-11-05 02:11:45,274 - INFO - [diffusion][Epoch 9532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:45,275 - INFO - [diffusion][Epoch 9533] Epoch 9534/12000
2024-11-05 02:11:49,504 - INFO - [diffusion][Epoch 9533] diffusion training Loss: 0.06383933313190937
2024-11-05 02:11:49,507 - INFO - [diffusion][Epoch 9533] diffusion learning rate: 0.001
2024-11-05 02:11:49,508 - INFO - [diffusion][Epoch 9533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:49,510 - INFO - [diffusion][Epoch 9534] Epoch 9535/12000
2024-11-05 02:11:53,774 - INFO - [diffusion][Epoch 9534] diffusion training Loss: 0.062450665049254894
2024-11-05 02:11:53,776 - INFO - [diffusion][Epoch 9534] diffusion learning rate: 0.001
2024-11-05 02:11:53,815 - INFO - [diffusion][Epoch 9534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:53,816 - INFO - [diffusion][Epoch 9535] Epoch 9536/12000
2024-11-05 02:11:58,152 - INFO - [diffusion][Epoch 9535] diffusion training Loss: 0.06119977869093418
2024-11-05 02:11:58,154 - INFO - [diffusion][Epoch 9535] diffusion learning rate: 0.001
2024-11-05 02:11:58,156 - INFO - [diffusion][Epoch 9535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:58,157 - INFO - [diffusion][Epoch 9536] Epoch 9537/12000
2024-11-05 02:12:02,298 - INFO - [diffusion][Epoch 9536] diffusion training Loss: 0.05818070936948061
2024-11-05 02:12:02,300 - INFO - [diffusion][Epoch 9536] diffusion learning rate: 0.001
2024-11-05 02:12:02,302 - INFO - [diffusion][Epoch 9536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:02,304 - INFO - [diffusion][Epoch 9537] Epoch 9538/12000
2024-11-05 02:12:06,474 - INFO - [diffusion][Epoch 9537] diffusion training Loss: 0.06310967728495598
2024-11-05 02:12:06,476 - INFO - [diffusion][Epoch 9537] diffusion learning rate: 0.001
2024-11-05 02:12:06,478 - INFO - [diffusion][Epoch 9537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:06,479 - INFO - [diffusion][Epoch 9538] Epoch 9539/12000
2024-11-05 02:12:10,727 - INFO - [diffusion][Epoch 9538] diffusion training Loss: 0.06589809991419315
2024-11-05 02:12:10,729 - INFO - [diffusion][Epoch 9538] diffusion learning rate: 0.001
2024-11-05 02:12:10,730 - INFO - [diffusion][Epoch 9538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:10,732 - INFO - [diffusion][Epoch 9539] Epoch 9540/12000
2024-11-05 02:12:14,846 - INFO - [diffusion][Epoch 9539] diffusion training Loss: 0.06490745674818754
2024-11-05 02:12:14,848 - INFO - [diffusion][Epoch 9539] diffusion learning rate: 0.001
2024-11-05 02:12:14,850 - INFO - [diffusion][Epoch 9539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:14,851 - INFO - [diffusion][Epoch 9540] Epoch 9541/12000
2024-11-05 02:12:18,991 - INFO - [diffusion][Epoch 9540] diffusion training Loss: 0.06069433782249689
2024-11-05 02:12:18,993 - INFO - [diffusion][Epoch 9540] diffusion learning rate: 0.001
2024-11-05 02:12:18,994 - INFO - [diffusion][Epoch 9540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:18,996 - INFO - [diffusion][Epoch 9541] Epoch 9542/12000
2024-11-05 02:12:23,232 - INFO - [diffusion][Epoch 9541] diffusion training Loss: 0.06462659128010273
2024-11-05 02:12:23,234 - INFO - [diffusion][Epoch 9541] diffusion learning rate: 0.001
2024-11-05 02:12:23,236 - INFO - [diffusion][Epoch 9541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:23,238 - INFO - [diffusion][Epoch 9542] Epoch 9543/12000
2024-11-05 02:12:27,466 - INFO - [diffusion][Epoch 9542] diffusion training Loss: 0.06275511719286442
2024-11-05 02:12:27,469 - INFO - [diffusion][Epoch 9542] diffusion learning rate: 0.001
2024-11-05 02:12:27,471 - INFO - [diffusion][Epoch 9542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:27,472 - INFO - [diffusion][Epoch 9543] Epoch 9544/12000
2024-11-05 02:12:31,265 - INFO - [diffusion][Epoch 9543] diffusion training Loss: 0.06278180796653032
2024-11-05 02:12:31,268 - INFO - [diffusion][Epoch 9543] diffusion learning rate: 0.001
2024-11-05 02:12:31,270 - INFO - [diffusion][Epoch 9543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:31,271 - INFO - [diffusion][Epoch 9544] Epoch 9545/12000
2024-11-05 02:12:35,849 - INFO - [diffusion][Epoch 9544] diffusion training Loss: 0.06397846713662148
2024-11-05 02:12:35,851 - INFO - [diffusion][Epoch 9544] diffusion learning rate: 0.001
2024-11-05 02:12:35,853 - INFO - [diffusion][Epoch 9544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:35,854 - INFO - [diffusion][Epoch 9545] Epoch 9546/12000
2024-11-05 02:12:40,019 - INFO - [diffusion][Epoch 9545] diffusion training Loss: 0.06522017624229193
2024-11-05 02:12:40,049 - INFO - [diffusion][Epoch 9545] diffusion learning rate: 0.001
2024-11-05 02:12:40,052 - INFO - [diffusion][Epoch 9545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:40,053 - INFO - [diffusion][Epoch 9546] Epoch 9547/12000
2024-11-05 02:12:44,326 - INFO - [diffusion][Epoch 9546] diffusion training Loss: 0.06549149006605148
2024-11-05 02:12:44,329 - INFO - [diffusion][Epoch 9546] diffusion learning rate: 0.001
2024-11-05 02:12:44,330 - INFO - [diffusion][Epoch 9546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:44,332 - INFO - [diffusion][Epoch 9547] Epoch 9548/12000
2024-11-05 02:12:48,413 - INFO - [diffusion][Epoch 9547] diffusion training Loss: 0.06529821362346411
2024-11-05 02:12:48,415 - INFO - [diffusion][Epoch 9547] diffusion learning rate: 0.001
2024-11-05 02:12:48,417 - INFO - [diffusion][Epoch 9547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:48,418 - INFO - [diffusion][Epoch 9548] Epoch 9549/12000
2024-11-05 02:12:52,668 - INFO - [diffusion][Epoch 9548] diffusion training Loss: 0.06204485613852739
2024-11-05 02:12:52,670 - INFO - [diffusion][Epoch 9548] diffusion learning rate: 0.001
2024-11-05 02:12:52,672 - INFO - [diffusion][Epoch 9548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:52,673 - INFO - [diffusion][Epoch 9549] Epoch 9550/12000
2024-11-05 02:12:56,982 - INFO - [diffusion][Epoch 9549] diffusion training Loss: 0.06374814361333847
2024-11-05 02:12:56,985 - INFO - [diffusion][Epoch 9549] diffusion learning rate: 0.001
2024-11-05 02:12:56,987 - INFO - [diffusion][Epoch 9549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:56,989 - INFO - [diffusion][Epoch 9550] Epoch 9551/12000
2024-11-05 02:13:01,055 - INFO - [diffusion][Epoch 9550] diffusion training Loss: 0.06335290148854256
2024-11-05 02:13:01,057 - INFO - [diffusion][Epoch 9550] diffusion learning rate: 0.001
2024-11-05 02:13:01,059 - INFO - [diffusion][Epoch 9550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:01,061 - INFO - [diffusion][Epoch 9551] Epoch 9552/12000
2024-11-05 02:13:05,412 - INFO - [diffusion][Epoch 9551] diffusion training Loss: 0.057358636520802975
2024-11-05 02:13:05,414 - INFO - [diffusion][Epoch 9551] diffusion learning rate: 0.001
2024-11-05 02:13:05,416 - INFO - [diffusion][Epoch 9551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:05,418 - INFO - [diffusion][Epoch 9552] Epoch 9553/12000
2024-11-05 02:13:09,498 - INFO - [diffusion][Epoch 9552] diffusion training Loss: 0.06416326947510242
2024-11-05 02:13:09,500 - INFO - [diffusion][Epoch 9552] diffusion learning rate: 0.001
2024-11-05 02:13:09,502 - INFO - [diffusion][Epoch 9552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:09,503 - INFO - [diffusion][Epoch 9553] Epoch 9554/12000
2024-11-05 02:13:13,645 - INFO - [diffusion][Epoch 9553] diffusion training Loss: 0.06789924204349518
2024-11-05 02:13:13,647 - INFO - [diffusion][Epoch 9553] diffusion learning rate: 0.001
2024-11-05 02:13:13,648 - INFO - [diffusion][Epoch 9553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:13,650 - INFO - [diffusion][Epoch 9554] Epoch 9555/12000
2024-11-05 02:13:17,846 - INFO - [diffusion][Epoch 9554] diffusion training Loss: 0.06127663794904947
2024-11-05 02:13:17,848 - INFO - [diffusion][Epoch 9554] diffusion learning rate: 0.001
2024-11-05 02:13:17,850 - INFO - [diffusion][Epoch 9554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:17,851 - INFO - [diffusion][Epoch 9555] Epoch 9556/12000
2024-11-05 02:13:22,110 - INFO - [diffusion][Epoch 9555] diffusion training Loss: 0.059455789625644684
2024-11-05 02:13:22,112 - INFO - [diffusion][Epoch 9555] diffusion learning rate: 0.001
2024-11-05 02:13:22,114 - INFO - [diffusion][Epoch 9555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:22,115 - INFO - [diffusion][Epoch 9556] Epoch 9557/12000
2024-11-05 02:13:26,387 - INFO - [diffusion][Epoch 9556] diffusion training Loss: 0.06398978922516108
2024-11-05 02:13:26,390 - INFO - [diffusion][Epoch 9556] diffusion learning rate: 0.001
2024-11-05 02:13:26,392 - INFO - [diffusion][Epoch 9556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:26,393 - INFO - [diffusion][Epoch 9557] Epoch 9558/12000
2024-11-05 02:13:30,677 - INFO - [diffusion][Epoch 9557] diffusion training Loss: 0.060713399201631546
2024-11-05 02:13:30,680 - INFO - [diffusion][Epoch 9557] diffusion learning rate: 0.001
2024-11-05 02:13:30,761 - INFO - [diffusion][Epoch 9557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:30,762 - INFO - [diffusion][Epoch 9558] Epoch 9559/12000
2024-11-05 02:13:34,889 - INFO - [diffusion][Epoch 9558] diffusion training Loss: 0.06863530725240707
2024-11-05 02:13:34,891 - INFO - [diffusion][Epoch 9558] diffusion learning rate: 0.001
2024-11-05 02:13:34,893 - INFO - [diffusion][Epoch 9558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:34,894 - INFO - [diffusion][Epoch 9559] Epoch 9560/12000
2024-11-05 02:13:39,204 - INFO - [diffusion][Epoch 9559] diffusion training Loss: 0.057798849418759346
2024-11-05 02:13:39,206 - INFO - [diffusion][Epoch 9559] diffusion learning rate: 0.001
2024-11-05 02:13:39,208 - INFO - [diffusion][Epoch 9559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:39,209 - INFO - [diffusion][Epoch 9560] Epoch 9561/12000
2024-11-05 02:13:43,483 - INFO - [diffusion][Epoch 9560] diffusion training Loss: 0.06136040296405554
2024-11-05 02:13:43,485 - INFO - [diffusion][Epoch 9560] diffusion learning rate: 0.001
2024-11-05 02:13:43,487 - INFO - [diffusion][Epoch 9560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:43,488 - INFO - [diffusion][Epoch 9561] Epoch 9562/12000
2024-11-05 02:13:47,843 - INFO - [diffusion][Epoch 9561] diffusion training Loss: 0.0663980208337307
2024-11-05 02:13:47,845 - INFO - [diffusion][Epoch 9561] diffusion learning rate: 0.001
2024-11-05 02:13:47,866 - INFO - [diffusion][Epoch 9561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:47,867 - INFO - [diffusion][Epoch 9562] Epoch 9563/12000
2024-11-05 02:13:52,120 - INFO - [diffusion][Epoch 9562] diffusion training Loss: 0.05988742597401142
2024-11-05 02:13:52,122 - INFO - [diffusion][Epoch 9562] diffusion learning rate: 0.001
2024-11-05 02:13:52,124 - INFO - [diffusion][Epoch 9562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:52,126 - INFO - [diffusion][Epoch 9563] Epoch 9564/12000
2024-11-05 02:13:56,405 - INFO - [diffusion][Epoch 9563] diffusion training Loss: 0.06588825210928917
2024-11-05 02:13:56,407 - INFO - [diffusion][Epoch 9563] diffusion learning rate: 0.001
2024-11-05 02:13:56,409 - INFO - [diffusion][Epoch 9563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:56,410 - INFO - [diffusion][Epoch 9564] Epoch 9565/12000
2024-11-05 02:14:00,825 - INFO - [diffusion][Epoch 9564] diffusion training Loss: 0.061359439976513386
2024-11-05 02:14:00,827 - INFO - [diffusion][Epoch 9564] diffusion learning rate: 0.001
2024-11-05 02:14:00,829 - INFO - [diffusion][Epoch 9564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:00,831 - INFO - [diffusion][Epoch 9565] Epoch 9566/12000
2024-11-05 02:14:05,131 - INFO - [diffusion][Epoch 9565] diffusion training Loss: 0.06593186967074871
2024-11-05 02:14:05,133 - INFO - [diffusion][Epoch 9565] diffusion learning rate: 0.001
2024-11-05 02:14:05,134 - INFO - [diffusion][Epoch 9565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:05,136 - INFO - [diffusion][Epoch 9566] Epoch 9567/12000
2024-11-05 02:14:09,420 - INFO - [diffusion][Epoch 9566] diffusion training Loss: 0.06774766463786364
2024-11-05 02:14:09,422 - INFO - [diffusion][Epoch 9566] diffusion learning rate: 0.001
2024-11-05 02:14:09,424 - INFO - [diffusion][Epoch 9566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:09,425 - INFO - [diffusion][Epoch 9567] Epoch 9568/12000
2024-11-05 02:14:13,593 - INFO - [diffusion][Epoch 9567] diffusion training Loss: 0.06122376583516598
2024-11-05 02:14:13,596 - INFO - [diffusion][Epoch 9567] diffusion learning rate: 0.001
2024-11-05 02:14:13,598 - INFO - [diffusion][Epoch 9567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:13,599 - INFO - [diffusion][Epoch 9568] Epoch 9569/12000
2024-11-05 02:14:17,637 - INFO - [diffusion][Epoch 9568] diffusion training Loss: 0.06084800884127617
2024-11-05 02:14:17,640 - INFO - [diffusion][Epoch 9568] diffusion learning rate: 0.001
2024-11-05 02:14:17,642 - INFO - [diffusion][Epoch 9568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:17,643 - INFO - [diffusion][Epoch 9569] Epoch 9570/12000
2024-11-05 02:14:21,805 - INFO - [diffusion][Epoch 9569] diffusion training Loss: 0.06160602439194918
2024-11-05 02:14:21,807 - INFO - [diffusion][Epoch 9569] diffusion learning rate: 0.001
2024-11-05 02:14:21,809 - INFO - [diffusion][Epoch 9569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:21,811 - INFO - [diffusion][Epoch 9570] Epoch 9571/12000
2024-11-05 02:14:25,923 - INFO - [diffusion][Epoch 9570] diffusion training Loss: 0.05397514160722494
2024-11-05 02:14:25,925 - INFO - [diffusion][Epoch 9570] diffusion learning rate: 0.001
2024-11-05 02:14:25,928 - INFO - [diffusion][Epoch 9570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:25,929 - INFO - [diffusion][Epoch 9571] Epoch 9572/12000
2024-11-05 02:14:30,104 - INFO - [diffusion][Epoch 9571] diffusion training Loss: 0.059600054286420345
2024-11-05 02:14:30,107 - INFO - [diffusion][Epoch 9571] diffusion learning rate: 0.001
2024-11-05 02:14:30,109 - INFO - [diffusion][Epoch 9571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:30,110 - INFO - [diffusion][Epoch 9572] Epoch 9573/12000
2024-11-05 02:14:34,343 - INFO - [diffusion][Epoch 9572] diffusion training Loss: 0.06312650255858898
2024-11-05 02:14:34,346 - INFO - [diffusion][Epoch 9572] diffusion learning rate: 0.001
2024-11-05 02:14:34,349 - INFO - [diffusion][Epoch 9572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:34,350 - INFO - [diffusion][Epoch 9573] Epoch 9574/12000
2024-11-05 02:14:38,655 - INFO - [diffusion][Epoch 9573] diffusion training Loss: 0.07081943564116955
2024-11-05 02:14:38,657 - INFO - [diffusion][Epoch 9573] diffusion learning rate: 0.001
2024-11-05 02:14:38,659 - INFO - [diffusion][Epoch 9573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:38,661 - INFO - [diffusion][Epoch 9574] Epoch 9575/12000
2024-11-05 02:14:42,565 - INFO - [diffusion][Epoch 9574] diffusion training Loss: 0.06104751583188772
2024-11-05 02:14:42,567 - INFO - [diffusion][Epoch 9574] diffusion learning rate: 0.001
2024-11-05 02:14:42,569 - INFO - [diffusion][Epoch 9574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:42,570 - INFO - [diffusion][Epoch 9575] Epoch 9576/12000
2024-11-05 02:14:46,585 - INFO - [diffusion][Epoch 9575] diffusion training Loss: 0.06331198662519455
2024-11-05 02:14:46,588 - INFO - [diffusion][Epoch 9575] diffusion learning rate: 0.001
2024-11-05 02:14:46,590 - INFO - [diffusion][Epoch 9575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:46,592 - INFO - [diffusion][Epoch 9576] Epoch 9577/12000
2024-11-05 02:14:50,728 - INFO - [diffusion][Epoch 9576] diffusion training Loss: 0.06413245387375355
2024-11-05 02:14:50,730 - INFO - [diffusion][Epoch 9576] diffusion learning rate: 0.001
2024-11-05 02:14:50,732 - INFO - [diffusion][Epoch 9576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:50,733 - INFO - [diffusion][Epoch 9577] Epoch 9578/12000
2024-11-05 02:14:54,921 - INFO - [diffusion][Epoch 9577] diffusion training Loss: 0.06609835475683212
2024-11-05 02:14:54,923 - INFO - [diffusion][Epoch 9577] diffusion learning rate: 0.001
2024-11-05 02:14:54,925 - INFO - [diffusion][Epoch 9577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:54,926 - INFO - [diffusion][Epoch 9578] Epoch 9579/12000
2024-11-05 02:14:59,118 - INFO - [diffusion][Epoch 9578] diffusion training Loss: 0.059442903846502304
2024-11-05 02:14:59,120 - INFO - [diffusion][Epoch 9578] diffusion learning rate: 0.001
2024-11-05 02:14:59,122 - INFO - [diffusion][Epoch 9578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:59,123 - INFO - [diffusion][Epoch 9579] Epoch 9580/12000
2024-11-05 02:15:03,315 - INFO - [diffusion][Epoch 9579] diffusion training Loss: 0.06185612827539444
2024-11-05 02:15:03,317 - INFO - [diffusion][Epoch 9579] diffusion learning rate: 0.001
2024-11-05 02:15:03,319 - INFO - [diffusion][Epoch 9579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:03,320 - INFO - [diffusion][Epoch 9580] Epoch 9581/12000
2024-11-05 02:15:07,469 - INFO - [diffusion][Epoch 9580] diffusion training Loss: 0.06414436548948288
2024-11-05 02:15:07,471 - INFO - [diffusion][Epoch 9580] diffusion learning rate: 0.001
2024-11-05 02:15:07,473 - INFO - [diffusion][Epoch 9580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:07,475 - INFO - [diffusion][Epoch 9581] Epoch 9582/12000
2024-11-05 02:15:11,789 - INFO - [diffusion][Epoch 9581] diffusion training Loss: 0.06590440683066845
2024-11-05 02:15:11,791 - INFO - [diffusion][Epoch 9581] diffusion learning rate: 0.001
2024-11-05 02:15:11,793 - INFO - [diffusion][Epoch 9581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:11,795 - INFO - [diffusion][Epoch 9582] Epoch 9583/12000
2024-11-05 02:15:16,113 - INFO - [diffusion][Epoch 9582] diffusion training Loss: 0.0627783453091979
2024-11-05 02:15:16,115 - INFO - [diffusion][Epoch 9582] diffusion learning rate: 0.001
2024-11-05 02:15:16,117 - INFO - [diffusion][Epoch 9582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:16,118 - INFO - [diffusion][Epoch 9583] Epoch 9584/12000
2024-11-05 02:15:20,346 - INFO - [diffusion][Epoch 9583] diffusion training Loss: 0.06287366151809692
2024-11-05 02:15:20,348 - INFO - [diffusion][Epoch 9583] diffusion learning rate: 0.001
2024-11-05 02:15:20,350 - INFO - [diffusion][Epoch 9583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:20,351 - INFO - [diffusion][Epoch 9584] Epoch 9585/12000
2024-11-05 02:15:24,548 - INFO - [diffusion][Epoch 9584] diffusion training Loss: 0.06332838162779808
2024-11-05 02:15:24,550 - INFO - [diffusion][Epoch 9584] diffusion learning rate: 0.001
2024-11-05 02:15:24,552 - INFO - [diffusion][Epoch 9584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:24,553 - INFO - [diffusion][Epoch 9585] Epoch 9586/12000
2024-11-05 02:15:28,801 - INFO - [diffusion][Epoch 9585] diffusion training Loss: 0.06572654098272324
2024-11-05 02:15:28,803 - INFO - [diffusion][Epoch 9585] diffusion learning rate: 0.001
2024-11-05 02:15:28,805 - INFO - [diffusion][Epoch 9585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:28,806 - INFO - [diffusion][Epoch 9586] Epoch 9587/12000
2024-11-05 02:15:32,931 - INFO - [diffusion][Epoch 9586] diffusion training Loss: 0.06513328291475773
2024-11-05 02:15:32,933 - INFO - [diffusion][Epoch 9586] diffusion learning rate: 0.001
2024-11-05 02:15:32,934 - INFO - [diffusion][Epoch 9586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:32,936 - INFO - [diffusion][Epoch 9587] Epoch 9588/12000
2024-11-05 02:15:37,039 - INFO - [diffusion][Epoch 9587] diffusion training Loss: 0.06630517169833183
2024-11-05 02:15:37,041 - INFO - [diffusion][Epoch 9587] diffusion learning rate: 0.001
2024-11-05 02:15:37,042 - INFO - [diffusion][Epoch 9587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:37,044 - INFO - [diffusion][Epoch 9588] Epoch 9589/12000
2024-11-05 02:15:41,295 - INFO - [diffusion][Epoch 9588] diffusion training Loss: 0.05935512203723192
2024-11-05 02:15:41,297 - INFO - [diffusion][Epoch 9588] diffusion learning rate: 0.001
2024-11-05 02:15:41,299 - INFO - [diffusion][Epoch 9588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:41,300 - INFO - [diffusion][Epoch 9589] Epoch 9590/12000
2024-11-05 02:15:45,598 - INFO - [diffusion][Epoch 9589] diffusion training Loss: 0.061290948651731014
2024-11-05 02:15:45,600 - INFO - [diffusion][Epoch 9589] diffusion learning rate: 0.001
2024-11-05 02:15:45,602 - INFO - [diffusion][Epoch 9589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:45,603 - INFO - [diffusion][Epoch 9590] Epoch 9591/12000
2024-11-05 02:15:49,773 - INFO - [diffusion][Epoch 9590] diffusion training Loss: 0.05838983319699764
2024-11-05 02:15:49,775 - INFO - [diffusion][Epoch 9590] diffusion learning rate: 0.001
2024-11-05 02:15:49,778 - INFO - [diffusion][Epoch 9590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:49,779 - INFO - [diffusion][Epoch 9591] Epoch 9592/12000
2024-11-05 02:15:53,839 - INFO - [diffusion][Epoch 9591] diffusion training Loss: 0.06342804338783026
2024-11-05 02:15:53,841 - INFO - [diffusion][Epoch 9591] diffusion learning rate: 0.001
2024-11-05 02:15:53,893 - INFO - [diffusion][Epoch 9591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:53,894 - INFO - [diffusion][Epoch 9592] Epoch 9593/12000
2024-11-05 02:15:58,216 - INFO - [diffusion][Epoch 9592] diffusion training Loss: 0.0648029139265418
2024-11-05 02:15:58,220 - INFO - [diffusion][Epoch 9592] diffusion learning rate: 0.001
2024-11-05 02:15:58,221 - INFO - [diffusion][Epoch 9592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:58,223 - INFO - [diffusion][Epoch 9593] Epoch 9594/12000
2024-11-05 02:16:02,403 - INFO - [diffusion][Epoch 9593] diffusion training Loss: 0.06607470475137234
2024-11-05 02:16:02,405 - INFO - [diffusion][Epoch 9593] diffusion learning rate: 0.001
2024-11-05 02:16:02,407 - INFO - [diffusion][Epoch 9593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:02,408 - INFO - [diffusion][Epoch 9594] Epoch 9595/12000
2024-11-05 02:16:06,491 - INFO - [diffusion][Epoch 9594] diffusion training Loss: 0.06268846616148949
2024-11-05 02:16:06,494 - INFO - [diffusion][Epoch 9594] diffusion learning rate: 0.001
2024-11-05 02:16:06,497 - INFO - [diffusion][Epoch 9594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:06,499 - INFO - [diffusion][Epoch 9595] Epoch 9596/12000
2024-11-05 02:16:10,744 - INFO - [diffusion][Epoch 9595] diffusion training Loss: 0.06141929607838392
2024-11-05 02:16:10,746 - INFO - [diffusion][Epoch 9595] diffusion learning rate: 0.001
2024-11-05 02:16:10,773 - INFO - [diffusion][Epoch 9595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:10,774 - INFO - [diffusion][Epoch 9596] Epoch 9597/12000
2024-11-05 02:16:14,767 - INFO - [diffusion][Epoch 9596] diffusion training Loss: 0.062139988876879215
2024-11-05 02:16:14,769 - INFO - [diffusion][Epoch 9596] diffusion learning rate: 0.001
2024-11-05 02:16:14,771 - INFO - [diffusion][Epoch 9596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:14,772 - INFO - [diffusion][Epoch 9597] Epoch 9598/12000
2024-11-05 02:16:18,974 - INFO - [diffusion][Epoch 9597] diffusion training Loss: 0.06341813132166862
2024-11-05 02:16:18,976 - INFO - [diffusion][Epoch 9597] diffusion learning rate: 0.001
2024-11-05 02:16:18,978 - INFO - [diffusion][Epoch 9597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:18,980 - INFO - [diffusion][Epoch 9598] Epoch 9599/12000
2024-11-05 02:16:23,205 - INFO - [diffusion][Epoch 9598] diffusion training Loss: 0.06276763323694468
2024-11-05 02:16:23,208 - INFO - [diffusion][Epoch 9598] diffusion learning rate: 0.001
2024-11-05 02:16:23,210 - INFO - [diffusion][Epoch 9598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:23,211 - INFO - [diffusion][Epoch 9599] Epoch 9600/12000
2024-11-05 02:16:27,416 - INFO - [diffusion][Epoch 9599] diffusion training Loss: 0.06554547045379877
2024-11-05 02:16:27,419 - INFO - [diffusion][Epoch 9599] diffusion learning rate: 0.001
2024-11-05 02:16:27,421 - INFO - [diffusion][Epoch 9599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:27,422 - INFO - [diffusion][Epoch 9600] Epoch 9601/12000
2024-11-05 02:16:31,655 - INFO - [diffusion][Epoch 9600] diffusion training Loss: 0.05931359343230724
2024-11-05 02:16:31,657 - INFO - [diffusion][Epoch 9600] diffusion learning rate: 0.001
2024-11-05 02:16:31,659 - INFO - [diffusion][Epoch 9600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:31,660 - INFO - [diffusion][Epoch 9601] Epoch 9602/12000
2024-11-05 02:16:35,805 - INFO - [diffusion][Epoch 9601] diffusion training Loss: 0.0644729258492589
2024-11-05 02:16:35,807 - INFO - [diffusion][Epoch 9601] diffusion learning rate: 0.001
2024-11-05 02:16:35,809 - INFO - [diffusion][Epoch 9601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:35,810 - INFO - [diffusion][Epoch 9602] Epoch 9603/12000
2024-11-05 02:16:40,059 - INFO - [diffusion][Epoch 9602] diffusion training Loss: 0.05936254654079676
2024-11-05 02:16:40,061 - INFO - [diffusion][Epoch 9602] diffusion learning rate: 0.001
2024-11-05 02:16:40,063 - INFO - [diffusion][Epoch 9602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:40,064 - INFO - [diffusion][Epoch 9603] Epoch 9604/12000
2024-11-05 02:16:44,294 - INFO - [diffusion][Epoch 9603] diffusion training Loss: 0.058770651929080486
2024-11-05 02:16:44,296 - INFO - [diffusion][Epoch 9603] diffusion learning rate: 0.001
2024-11-05 02:16:44,298 - INFO - [diffusion][Epoch 9603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:44,300 - INFO - [diffusion][Epoch 9604] Epoch 9605/12000
2024-11-05 02:16:48,540 - INFO - [diffusion][Epoch 9604] diffusion training Loss: 0.05501350201666355
2024-11-05 02:16:48,542 - INFO - [diffusion][Epoch 9604] diffusion learning rate: 0.001
2024-11-05 02:16:48,544 - INFO - [diffusion][Epoch 9604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:48,546 - INFO - [diffusion][Epoch 9605] Epoch 9606/12000
2024-11-05 02:16:53,336 - INFO - [diffusion][Epoch 9605] diffusion training Loss: 0.062135711312294006
2024-11-05 02:16:53,338 - INFO - [diffusion][Epoch 9605] diffusion learning rate: 0.001
2024-11-05 02:16:53,340 - INFO - [diffusion][Epoch 9605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:53,343 - INFO - [diffusion][Epoch 9606] Epoch 9607/12000
2024-11-05 02:16:57,417 - INFO - [diffusion][Epoch 9606] diffusion training Loss: 0.06651891395449638
2024-11-05 02:16:57,419 - INFO - [diffusion][Epoch 9606] diffusion learning rate: 0.001
2024-11-05 02:16:57,421 - INFO - [diffusion][Epoch 9606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:57,422 - INFO - [diffusion][Epoch 9607] Epoch 9608/12000
2024-11-05 02:17:01,658 - INFO - [diffusion][Epoch 9607] diffusion training Loss: 0.06710739806294441
2024-11-05 02:17:01,660 - INFO - [diffusion][Epoch 9607] diffusion learning rate: 0.001
2024-11-05 02:17:01,661 - INFO - [diffusion][Epoch 9607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:01,663 - INFO - [diffusion][Epoch 9608] Epoch 9609/12000
2024-11-05 02:17:05,785 - INFO - [diffusion][Epoch 9608] diffusion training Loss: 0.06493900530040264
2024-11-05 02:17:05,787 - INFO - [diffusion][Epoch 9608] diffusion learning rate: 0.001
2024-11-05 02:17:05,789 - INFO - [diffusion][Epoch 9608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:05,790 - INFO - [diffusion][Epoch 9609] Epoch 9610/12000
2024-11-05 02:17:09,829 - INFO - [diffusion][Epoch 9609] diffusion training Loss: 0.0639387983828783
2024-11-05 02:17:09,831 - INFO - [diffusion][Epoch 9609] diffusion learning rate: 0.001
2024-11-05 02:17:09,833 - INFO - [diffusion][Epoch 9609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:09,834 - INFO - [diffusion][Epoch 9610] Epoch 9611/12000
2024-11-05 02:17:13,988 - INFO - [diffusion][Epoch 9610] diffusion training Loss: 0.06208250857889652
2024-11-05 02:17:13,990 - INFO - [diffusion][Epoch 9610] diffusion learning rate: 0.001
2024-11-05 02:17:13,992 - INFO - [diffusion][Epoch 9610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:13,993 - INFO - [diffusion][Epoch 9611] Epoch 9612/12000
2024-11-05 02:17:18,262 - INFO - [diffusion][Epoch 9611] diffusion training Loss: 0.0662336926907301
2024-11-05 02:17:18,264 - INFO - [diffusion][Epoch 9611] diffusion learning rate: 0.001
2024-11-05 02:17:18,472 - INFO - [diffusion][Epoch 9611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:18,474 - INFO - [diffusion][Epoch 9612] Epoch 9613/12000
2024-11-05 02:17:22,711 - INFO - [diffusion][Epoch 9612] diffusion training Loss: 0.06149590574204922
2024-11-05 02:17:22,713 - INFO - [diffusion][Epoch 9612] diffusion learning rate: 0.001
2024-11-05 02:17:22,714 - INFO - [diffusion][Epoch 9612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:22,716 - INFO - [diffusion][Epoch 9613] Epoch 9614/12000
2024-11-05 02:17:26,879 - INFO - [diffusion][Epoch 9613] diffusion training Loss: 0.0644581038504839
2024-11-05 02:17:26,881 - INFO - [diffusion][Epoch 9613] diffusion learning rate: 0.001
2024-11-05 02:17:26,882 - INFO - [diffusion][Epoch 9613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:26,884 - INFO - [diffusion][Epoch 9614] Epoch 9615/12000
2024-11-05 02:17:31,018 - INFO - [diffusion][Epoch 9614] diffusion training Loss: 0.06806726567447186
2024-11-05 02:17:31,020 - INFO - [diffusion][Epoch 9614] diffusion learning rate: 0.001
2024-11-05 02:17:31,022 - INFO - [diffusion][Epoch 9614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:31,023 - INFO - [diffusion][Epoch 9615] Epoch 9616/12000
2024-11-05 02:17:35,101 - INFO - [diffusion][Epoch 9615] diffusion training Loss: 0.06792846787720919
2024-11-05 02:17:35,104 - INFO - [diffusion][Epoch 9615] diffusion learning rate: 0.001
2024-11-05 02:17:35,108 - INFO - [diffusion][Epoch 9615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:35,110 - INFO - [diffusion][Epoch 9616] Epoch 9617/12000
2024-11-05 02:17:39,410 - INFO - [diffusion][Epoch 9616] diffusion training Loss: 0.058110764250159264
2024-11-05 02:17:39,413 - INFO - [diffusion][Epoch 9616] diffusion learning rate: 0.001
2024-11-05 02:17:39,414 - INFO - [diffusion][Epoch 9616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:39,416 - INFO - [diffusion][Epoch 9617] Epoch 9618/12000
2024-11-05 02:17:43,615 - INFO - [diffusion][Epoch 9617] diffusion training Loss: 0.0671166330575943
2024-11-05 02:17:43,617 - INFO - [diffusion][Epoch 9617] diffusion learning rate: 0.001
2024-11-05 02:17:43,619 - INFO - [diffusion][Epoch 9617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:43,620 - INFO - [diffusion][Epoch 9618] Epoch 9619/12000
2024-11-05 02:17:47,974 - INFO - [diffusion][Epoch 9618] diffusion training Loss: 0.06519303657114506
2024-11-05 02:17:47,977 - INFO - [diffusion][Epoch 9618] diffusion learning rate: 0.001
2024-11-05 02:17:47,979 - INFO - [diffusion][Epoch 9618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:47,981 - INFO - [diffusion][Epoch 9619] Epoch 9620/12000
2024-11-05 02:17:52,029 - INFO - [diffusion][Epoch 9619] diffusion training Loss: 0.06508158054202795
2024-11-05 02:17:52,031 - INFO - [diffusion][Epoch 9619] diffusion learning rate: 0.001
2024-11-05 02:17:52,032 - INFO - [diffusion][Epoch 9619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:52,034 - INFO - [diffusion][Epoch 9620] Epoch 9621/12000
2024-11-05 02:17:56,146 - INFO - [diffusion][Epoch 9620] diffusion training Loss: 0.06510462705045938
2024-11-05 02:17:56,155 - INFO - [diffusion][Epoch 9620] diffusion learning rate: 0.001
2024-11-05 02:17:56,157 - INFO - [diffusion][Epoch 9620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:56,159 - INFO - [diffusion][Epoch 9621] Epoch 9622/12000
2024-11-05 02:18:00,566 - INFO - [diffusion][Epoch 9621] diffusion training Loss: 0.06153290066868067
2024-11-05 02:18:00,568 - INFO - [diffusion][Epoch 9621] diffusion learning rate: 0.001
2024-11-05 02:18:00,570 - INFO - [diffusion][Epoch 9621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:00,571 - INFO - [diffusion][Epoch 9622] Epoch 9623/12000
2024-11-05 02:18:04,620 - INFO - [diffusion][Epoch 9622] diffusion training Loss: 0.05952196754515171
2024-11-05 02:18:04,622 - INFO - [diffusion][Epoch 9622] diffusion learning rate: 0.001
2024-11-05 02:18:04,623 - INFO - [diffusion][Epoch 9622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:04,625 - INFO - [diffusion][Epoch 9623] Epoch 9624/12000
2024-11-05 02:18:08,495 - INFO - [diffusion][Epoch 9623] diffusion training Loss: 0.06236032582819462
2024-11-05 02:18:08,496 - INFO - [diffusion][Epoch 9623] diffusion learning rate: 0.001
2024-11-05 02:18:08,498 - INFO - [diffusion][Epoch 9623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:08,500 - INFO - [diffusion][Epoch 9624] Epoch 9625/12000
2024-11-05 02:18:12,511 - INFO - [diffusion][Epoch 9624] diffusion training Loss: 0.06053342763334513
2024-11-05 02:18:12,513 - INFO - [diffusion][Epoch 9624] diffusion learning rate: 0.001
2024-11-05 02:18:12,515 - INFO - [diffusion][Epoch 9624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:12,516 - INFO - [diffusion][Epoch 9625] Epoch 9626/12000
2024-11-05 02:18:16,635 - INFO - [diffusion][Epoch 9625] diffusion training Loss: 0.06191741395741701
2024-11-05 02:18:16,637 - INFO - [diffusion][Epoch 9625] diffusion learning rate: 0.001
2024-11-05 02:18:16,638 - INFO - [diffusion][Epoch 9625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:16,640 - INFO - [diffusion][Epoch 9626] Epoch 9627/12000
2024-11-05 02:18:21,367 - INFO - [diffusion][Epoch 9626] diffusion training Loss: 0.06759841274470091
2024-11-05 02:18:21,369 - INFO - [diffusion][Epoch 9626] diffusion learning rate: 0.001
2024-11-05 02:18:21,371 - INFO - [diffusion][Epoch 9626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:21,373 - INFO - [diffusion][Epoch 9627] Epoch 9628/12000
2024-11-05 02:18:25,546 - INFO - [diffusion][Epoch 9627] diffusion training Loss: 0.06523688416928053
2024-11-05 02:18:25,548 - INFO - [diffusion][Epoch 9627] diffusion learning rate: 0.001
2024-11-05 02:18:25,550 - INFO - [diffusion][Epoch 9627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:25,551 - INFO - [diffusion][Epoch 9628] Epoch 9629/12000
2024-11-05 02:18:29,579 - INFO - [diffusion][Epoch 9628] diffusion training Loss: 0.058093114756047726
2024-11-05 02:18:29,581 - INFO - [diffusion][Epoch 9628] diffusion learning rate: 0.001
2024-11-05 02:18:29,583 - INFO - [diffusion][Epoch 9628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:29,584 - INFO - [diffusion][Epoch 9629] Epoch 9630/12000
2024-11-05 02:18:33,827 - INFO - [diffusion][Epoch 9629] diffusion training Loss: 0.06741831637918949
2024-11-05 02:18:33,829 - INFO - [diffusion][Epoch 9629] diffusion learning rate: 0.001
2024-11-05 02:18:33,831 - INFO - [diffusion][Epoch 9629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:33,832 - INFO - [diffusion][Epoch 9630] Epoch 9631/12000
2024-11-05 02:18:38,076 - INFO - [diffusion][Epoch 9630] diffusion training Loss: 0.06290176138281822
2024-11-05 02:18:38,078 - INFO - [diffusion][Epoch 9630] diffusion learning rate: 0.001
2024-11-05 02:18:38,079 - INFO - [diffusion][Epoch 9630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:38,080 - INFO - [diffusion][Epoch 9631] Epoch 9632/12000
2024-11-05 02:18:42,295 - INFO - [diffusion][Epoch 9631] diffusion training Loss: 0.06424710247665644
2024-11-05 02:18:42,298 - INFO - [diffusion][Epoch 9631] diffusion learning rate: 0.001
2024-11-05 02:18:42,300 - INFO - [diffusion][Epoch 9631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:42,301 - INFO - [diffusion][Epoch 9632] Epoch 9633/12000
2024-11-05 02:18:46,497 - INFO - [diffusion][Epoch 9632] diffusion training Loss: 0.06717861630022526
2024-11-05 02:18:46,499 - INFO - [diffusion][Epoch 9632] diffusion learning rate: 0.001
2024-11-05 02:18:46,501 - INFO - [diffusion][Epoch 9632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:46,503 - INFO - [diffusion][Epoch 9633] Epoch 9634/12000
2024-11-05 02:18:50,775 - INFO - [diffusion][Epoch 9633] diffusion training Loss: 0.06461671181023121
2024-11-05 02:18:50,777 - INFO - [diffusion][Epoch 9633] diffusion learning rate: 0.001
2024-11-05 02:18:50,779 - INFO - [diffusion][Epoch 9633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:50,781 - INFO - [diffusion][Epoch 9634] Epoch 9635/12000
2024-11-05 02:18:54,796 - INFO - [diffusion][Epoch 9634] diffusion training Loss: 0.06152147985994816
2024-11-05 02:18:54,799 - INFO - [diffusion][Epoch 9634] diffusion learning rate: 0.001
2024-11-05 02:18:54,802 - INFO - [diffusion][Epoch 9634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:54,804 - INFO - [diffusion][Epoch 9635] Epoch 9636/12000
2024-11-05 02:18:58,917 - INFO - [diffusion][Epoch 9635] diffusion training Loss: 0.06169716455042362
2024-11-05 02:18:58,919 - INFO - [diffusion][Epoch 9635] diffusion learning rate: 0.001
2024-11-05 02:18:58,921 - INFO - [diffusion][Epoch 9635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:58,922 - INFO - [diffusion][Epoch 9636] Epoch 9637/12000
2024-11-05 02:19:03,103 - INFO - [diffusion][Epoch 9636] diffusion training Loss: 0.06266347039490938
2024-11-05 02:19:03,105 - INFO - [diffusion][Epoch 9636] diffusion learning rate: 0.001
2024-11-05 02:19:03,107 - INFO - [diffusion][Epoch 9636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:03,109 - INFO - [diffusion][Epoch 9637] Epoch 9638/12000
2024-11-05 02:19:07,390 - INFO - [diffusion][Epoch 9637] diffusion training Loss: 0.060664102435112
2024-11-05 02:19:07,392 - INFO - [diffusion][Epoch 9637] diffusion learning rate: 0.001
2024-11-05 02:19:07,393 - INFO - [diffusion][Epoch 9637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:07,395 - INFO - [diffusion][Epoch 9638] Epoch 9639/12000
2024-11-05 02:19:11,644 - INFO - [diffusion][Epoch 9638] diffusion training Loss: 0.06367341335862875
2024-11-05 02:19:11,646 - INFO - [diffusion][Epoch 9638] diffusion learning rate: 0.001
2024-11-05 02:19:11,648 - INFO - [diffusion][Epoch 9638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:11,650 - INFO - [diffusion][Epoch 9639] Epoch 9640/12000
2024-11-05 02:19:15,472 - INFO - [diffusion][Epoch 9639] diffusion training Loss: 0.06135937757790089
2024-11-05 02:19:15,474 - INFO - [diffusion][Epoch 9639] diffusion learning rate: 0.001
2024-11-05 02:19:15,476 - INFO - [diffusion][Epoch 9639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:15,477 - INFO - [diffusion][Epoch 9640] Epoch 9641/12000
2024-11-05 02:19:19,556 - INFO - [diffusion][Epoch 9640] diffusion training Loss: 0.06596438959240913
2024-11-05 02:19:19,559 - INFO - [diffusion][Epoch 9640] diffusion learning rate: 0.001
2024-11-05 02:19:19,560 - INFO - [diffusion][Epoch 9640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:19,561 - INFO - [diffusion][Epoch 9641] Epoch 9642/12000
2024-11-05 02:19:23,597 - INFO - [diffusion][Epoch 9641] diffusion training Loss: 0.05976232513785362
2024-11-05 02:19:23,599 - INFO - [diffusion][Epoch 9641] diffusion learning rate: 0.001
2024-11-05 02:19:23,600 - INFO - [diffusion][Epoch 9641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:23,602 - INFO - [diffusion][Epoch 9642] Epoch 9643/12000
2024-11-05 02:19:27,877 - INFO - [diffusion][Epoch 9642] diffusion training Loss: 0.06651083752512932
2024-11-05 02:19:27,879 - INFO - [diffusion][Epoch 9642] diffusion learning rate: 0.001
2024-11-05 02:19:27,881 - INFO - [diffusion][Epoch 9642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:27,882 - INFO - [diffusion][Epoch 9643] Epoch 9644/12000
2024-11-05 02:19:31,940 - INFO - [diffusion][Epoch 9643] diffusion training Loss: 0.06266124173998833
2024-11-05 02:19:31,941 - INFO - [diffusion][Epoch 9643] diffusion learning rate: 0.001
2024-11-05 02:19:31,943 - INFO - [diffusion][Epoch 9643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:31,944 - INFO - [diffusion][Epoch 9644] Epoch 9645/12000
2024-11-05 02:19:36,285 - INFO - [diffusion][Epoch 9644] diffusion training Loss: 0.06822072993963957
2024-11-05 02:19:36,287 - INFO - [diffusion][Epoch 9644] diffusion learning rate: 0.001
2024-11-05 02:19:36,289 - INFO - [diffusion][Epoch 9644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:36,292 - INFO - [diffusion][Epoch 9645] Epoch 9646/12000
2024-11-05 02:19:40,513 - INFO - [diffusion][Epoch 9645] diffusion training Loss: 0.05971895530819893
2024-11-05 02:19:40,515 - INFO - [diffusion][Epoch 9645] diffusion learning rate: 0.001
2024-11-05 02:19:40,517 - INFO - [diffusion][Epoch 9645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:40,519 - INFO - [diffusion][Epoch 9646] Epoch 9647/12000
2024-11-05 02:19:44,936 - INFO - [diffusion][Epoch 9646] diffusion training Loss: 0.06452061235904694
2024-11-05 02:19:44,938 - INFO - [diffusion][Epoch 9646] diffusion learning rate: 0.001
2024-11-05 02:19:44,940 - INFO - [diffusion][Epoch 9646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:44,941 - INFO - [diffusion][Epoch 9647] Epoch 9648/12000
2024-11-05 02:19:49,276 - INFO - [diffusion][Epoch 9647] diffusion training Loss: 0.0665601259097457
2024-11-05 02:19:49,278 - INFO - [diffusion][Epoch 9647] diffusion learning rate: 0.001
2024-11-05 02:19:49,279 - INFO - [diffusion][Epoch 9647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:49,281 - INFO - [diffusion][Epoch 9648] Epoch 9649/12000
2024-11-05 02:19:53,306 - INFO - [diffusion][Epoch 9648] diffusion training Loss: 0.06468311510980129
2024-11-05 02:19:53,308 - INFO - [diffusion][Epoch 9648] diffusion learning rate: 0.001
2024-11-05 02:19:53,309 - INFO - [diffusion][Epoch 9648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:53,310 - INFO - [diffusion][Epoch 9649] Epoch 9650/12000
2024-11-05 02:19:57,581 - INFO - [diffusion][Epoch 9649] diffusion training Loss: 0.06362087838351727
2024-11-05 02:19:57,583 - INFO - [diffusion][Epoch 9649] diffusion learning rate: 0.001
2024-11-05 02:19:57,585 - INFO - [diffusion][Epoch 9649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:57,586 - INFO - [diffusion][Epoch 9650] Epoch 9651/12000
2024-11-05 02:20:01,822 - INFO - [diffusion][Epoch 9650] diffusion training Loss: 0.05981086939573288
2024-11-05 02:20:01,824 - INFO - [diffusion][Epoch 9650] diffusion learning rate: 0.001
2024-11-05 02:20:01,826 - INFO - [diffusion][Epoch 9650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:01,828 - INFO - [diffusion][Epoch 9651] Epoch 9652/12000
2024-11-05 02:20:05,895 - INFO - [diffusion][Epoch 9651] diffusion training Loss: 0.06434674467891455
2024-11-05 02:20:05,898 - INFO - [diffusion][Epoch 9651] diffusion learning rate: 0.001
2024-11-05 02:20:05,900 - INFO - [diffusion][Epoch 9651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:05,901 - INFO - [diffusion][Epoch 9652] Epoch 9653/12000
2024-11-05 02:20:10,113 - INFO - [diffusion][Epoch 9652] diffusion training Loss: 0.06121625658124685
2024-11-05 02:20:10,115 - INFO - [diffusion][Epoch 9652] diffusion learning rate: 0.001
2024-11-05 02:20:10,117 - INFO - [diffusion][Epoch 9652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:10,118 - INFO - [diffusion][Epoch 9653] Epoch 9654/12000
2024-11-05 02:20:14,388 - INFO - [diffusion][Epoch 9653] diffusion training Loss: 0.05552161019295454
2024-11-05 02:20:14,390 - INFO - [diffusion][Epoch 9653] diffusion learning rate: 0.001
2024-11-05 02:20:14,392 - INFO - [diffusion][Epoch 9653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:14,393 - INFO - [diffusion][Epoch 9654] Epoch 9655/12000
2024-11-05 02:20:18,302 - INFO - [diffusion][Epoch 9654] diffusion training Loss: 0.05277062952518463
2024-11-05 02:20:18,305 - INFO - [diffusion][Epoch 9654] diffusion learning rate: 0.001
2024-11-05 02:20:18,307 - INFO - [diffusion][Epoch 9654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:18,309 - INFO - [diffusion][Epoch 9655] Epoch 9656/12000
2024-11-05 02:20:22,390 - INFO - [diffusion][Epoch 9655] diffusion training Loss: 0.06420367490500212
2024-11-05 02:20:22,393 - INFO - [diffusion][Epoch 9655] diffusion learning rate: 0.001
2024-11-05 02:20:22,395 - INFO - [diffusion][Epoch 9655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:22,396 - INFO - [diffusion][Epoch 9656] Epoch 9657/12000
2024-11-05 02:20:26,482 - INFO - [diffusion][Epoch 9656] diffusion training Loss: 0.05633616168051958
2024-11-05 02:20:26,484 - INFO - [diffusion][Epoch 9656] diffusion learning rate: 0.001
2024-11-05 02:20:26,485 - INFO - [diffusion][Epoch 9656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:26,487 - INFO - [diffusion][Epoch 9657] Epoch 9658/12000
2024-11-05 02:20:30,584 - INFO - [diffusion][Epoch 9657] diffusion training Loss: 0.06041590590029955
2024-11-05 02:20:30,586 - INFO - [diffusion][Epoch 9657] diffusion learning rate: 0.001
2024-11-05 02:20:30,638 - INFO - [diffusion][Epoch 9657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:30,640 - INFO - [diffusion][Epoch 9658] Epoch 9659/12000
2024-11-05 02:20:34,807 - INFO - [diffusion][Epoch 9658] diffusion training Loss: 0.06276058778166771
2024-11-05 02:20:34,808 - INFO - [diffusion][Epoch 9658] diffusion learning rate: 0.001
2024-11-05 02:20:34,810 - INFO - [diffusion][Epoch 9658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:34,811 - INFO - [diffusion][Epoch 9659] Epoch 9660/12000
2024-11-05 02:20:38,979 - INFO - [diffusion][Epoch 9659] diffusion training Loss: 0.06402579136192799
2024-11-05 02:20:38,981 - INFO - [diffusion][Epoch 9659] diffusion learning rate: 0.001
2024-11-05 02:20:38,983 - INFO - [diffusion][Epoch 9659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:38,984 - INFO - [diffusion][Epoch 9660] Epoch 9661/12000
2024-11-05 02:20:43,121 - INFO - [diffusion][Epoch 9660] diffusion training Loss: 0.0629063593223691
2024-11-05 02:20:43,123 - INFO - [diffusion][Epoch 9660] diffusion learning rate: 0.001
2024-11-05 02:20:43,124 - INFO - [diffusion][Epoch 9660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:43,126 - INFO - [diffusion][Epoch 9661] Epoch 9662/12000
2024-11-05 02:20:47,214 - INFO - [diffusion][Epoch 9661] diffusion training Loss: 0.06412476766854525
2024-11-05 02:20:47,216 - INFO - [diffusion][Epoch 9661] diffusion learning rate: 0.001
2024-11-05 02:20:47,218 - INFO - [diffusion][Epoch 9661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:47,219 - INFO - [diffusion][Epoch 9662] Epoch 9663/12000
2024-11-05 02:20:51,327 - INFO - [diffusion][Epoch 9662] diffusion training Loss: 0.06967150047421455
2024-11-05 02:20:51,329 - INFO - [diffusion][Epoch 9662] diffusion learning rate: 0.001
2024-11-05 02:20:51,331 - INFO - [diffusion][Epoch 9662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:51,332 - INFO - [diffusion][Epoch 9663] Epoch 9664/12000
2024-11-05 02:20:55,507 - INFO - [diffusion][Epoch 9663] diffusion training Loss: 0.06935907527804375
2024-11-05 02:20:55,509 - INFO - [diffusion][Epoch 9663] diffusion learning rate: 0.001
2024-11-05 02:20:55,511 - INFO - [diffusion][Epoch 9663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:55,513 - INFO - [diffusion][Epoch 9664] Epoch 9665/12000
2024-11-05 02:20:59,511 - INFO - [diffusion][Epoch 9664] diffusion training Loss: 0.06487845722585917
2024-11-05 02:20:59,513 - INFO - [diffusion][Epoch 9664] diffusion learning rate: 0.001
2024-11-05 02:20:59,515 - INFO - [diffusion][Epoch 9664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:59,516 - INFO - [diffusion][Epoch 9665] Epoch 9666/12000
2024-11-05 02:21:03,643 - INFO - [diffusion][Epoch 9665] diffusion training Loss: 0.0633594412356615
2024-11-05 02:21:03,645 - INFO - [diffusion][Epoch 9665] diffusion learning rate: 0.001
2024-11-05 02:21:03,647 - INFO - [diffusion][Epoch 9665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:03,648 - INFO - [diffusion][Epoch 9666] Epoch 9667/12000
2024-11-05 02:21:07,871 - INFO - [diffusion][Epoch 9666] diffusion training Loss: 0.06129772588610649
2024-11-05 02:21:07,872 - INFO - [diffusion][Epoch 9666] diffusion learning rate: 0.001
2024-11-05 02:21:07,874 - INFO - [diffusion][Epoch 9666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:07,876 - INFO - [diffusion][Epoch 9667] Epoch 9668/12000
2024-11-05 02:21:12,128 - INFO - [diffusion][Epoch 9667] diffusion training Loss: 0.06301780603826046
2024-11-05 02:21:12,130 - INFO - [diffusion][Epoch 9667] diffusion learning rate: 0.001
2024-11-05 02:21:12,132 - INFO - [diffusion][Epoch 9667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:12,133 - INFO - [diffusion][Epoch 9668] Epoch 9669/12000
2024-11-05 02:21:16,333 - INFO - [diffusion][Epoch 9668] diffusion training Loss: 0.06700544897466898
2024-11-05 02:21:16,335 - INFO - [diffusion][Epoch 9668] diffusion learning rate: 0.001
2024-11-05 02:21:16,337 - INFO - [diffusion][Epoch 9668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:16,338 - INFO - [diffusion][Epoch 9669] Epoch 9670/12000
2024-11-05 02:21:20,579 - INFO - [diffusion][Epoch 9669] diffusion training Loss: 0.06428665854036808
2024-11-05 02:21:20,582 - INFO - [diffusion][Epoch 9669] diffusion learning rate: 0.001
2024-11-05 02:21:20,584 - INFO - [diffusion][Epoch 9669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:20,585 - INFO - [diffusion][Epoch 9670] Epoch 9671/12000
2024-11-05 02:21:24,850 - INFO - [diffusion][Epoch 9670] diffusion training Loss: 0.06163046136498451
2024-11-05 02:21:24,852 - INFO - [diffusion][Epoch 9670] diffusion learning rate: 0.001
2024-11-05 02:21:24,854 - INFO - [diffusion][Epoch 9670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:24,855 - INFO - [diffusion][Epoch 9671] Epoch 9672/12000
2024-11-05 02:21:29,013 - INFO - [diffusion][Epoch 9671] diffusion training Loss: 0.06680524349212646
2024-11-05 02:21:29,015 - INFO - [diffusion][Epoch 9671] diffusion learning rate: 0.001
2024-11-05 02:21:29,017 - INFO - [diffusion][Epoch 9671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:29,019 - INFO - [diffusion][Epoch 9672] Epoch 9673/12000
2024-11-05 02:21:33,313 - INFO - [diffusion][Epoch 9672] diffusion training Loss: 0.05734280217438936
2024-11-05 02:21:33,315 - INFO - [diffusion][Epoch 9672] diffusion learning rate: 0.001
2024-11-05 02:21:33,316 - INFO - [diffusion][Epoch 9672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:33,318 - INFO - [diffusion][Epoch 9673] Epoch 9674/12000
2024-11-05 02:21:37,551 - INFO - [diffusion][Epoch 9673] diffusion training Loss: 0.0639956770464778
2024-11-05 02:21:37,553 - INFO - [diffusion][Epoch 9673] diffusion learning rate: 0.001
2024-11-05 02:21:37,555 - INFO - [diffusion][Epoch 9673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:37,556 - INFO - [diffusion][Epoch 9674] Epoch 9675/12000
2024-11-05 02:21:41,694 - INFO - [diffusion][Epoch 9674] diffusion training Loss: 0.0605743695050478
2024-11-05 02:21:41,696 - INFO - [diffusion][Epoch 9674] diffusion learning rate: 0.001
2024-11-05 02:21:41,698 - INFO - [diffusion][Epoch 9674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:41,699 - INFO - [diffusion][Epoch 9675] Epoch 9676/12000
2024-11-05 02:21:45,888 - INFO - [diffusion][Epoch 9675] diffusion training Loss: 0.06490431074053049
2024-11-05 02:21:45,890 - INFO - [diffusion][Epoch 9675] diffusion learning rate: 0.001
2024-11-05 02:21:45,892 - INFO - [diffusion][Epoch 9675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:45,893 - INFO - [diffusion][Epoch 9676] Epoch 9677/12000
2024-11-05 02:21:50,155 - INFO - [diffusion][Epoch 9676] diffusion training Loss: 0.060340382158756256
2024-11-05 02:21:50,157 - INFO - [diffusion][Epoch 9676] diffusion learning rate: 0.001
2024-11-05 02:21:50,159 - INFO - [diffusion][Epoch 9676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:50,160 - INFO - [diffusion][Epoch 9677] Epoch 9678/12000
2024-11-05 02:21:54,270 - INFO - [diffusion][Epoch 9677] diffusion training Loss: 0.06476924102753401
2024-11-05 02:21:54,273 - INFO - [diffusion][Epoch 9677] diffusion learning rate: 0.001
2024-11-05 02:21:54,275 - INFO - [diffusion][Epoch 9677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:54,276 - INFO - [diffusion][Epoch 9678] Epoch 9679/12000
2024-11-05 02:21:58,561 - INFO - [diffusion][Epoch 9678] diffusion training Loss: 0.061817554756999016
2024-11-05 02:21:58,563 - INFO - [diffusion][Epoch 9678] diffusion learning rate: 0.001
2024-11-05 02:21:58,611 - INFO - [diffusion][Epoch 9678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:58,613 - INFO - [diffusion][Epoch 9679] Epoch 9680/12000
2024-11-05 02:22:02,852 - INFO - [diffusion][Epoch 9679] diffusion training Loss: 0.0673151295632124
2024-11-05 02:22:02,854 - INFO - [diffusion][Epoch 9679] diffusion learning rate: 0.001
2024-11-05 02:22:02,856 - INFO - [diffusion][Epoch 9679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:02,857 - INFO - [diffusion][Epoch 9680] Epoch 9681/12000
2024-11-05 02:22:07,155 - INFO - [diffusion][Epoch 9680] diffusion training Loss: 0.06444555148482323
2024-11-05 02:22:07,157 - INFO - [diffusion][Epoch 9680] diffusion learning rate: 0.001
2024-11-05 02:22:07,159 - INFO - [diffusion][Epoch 9680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:07,160 - INFO - [diffusion][Epoch 9681] Epoch 9682/12000
2024-11-05 02:22:11,422 - INFO - [diffusion][Epoch 9681] diffusion training Loss: 0.06369788944721222
2024-11-05 02:22:11,424 - INFO - [diffusion][Epoch 9681] diffusion learning rate: 0.001
2024-11-05 02:22:11,426 - INFO - [diffusion][Epoch 9681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:11,427 - INFO - [diffusion][Epoch 9682] Epoch 9683/12000
2024-11-05 02:22:15,607 - INFO - [diffusion][Epoch 9682] diffusion training Loss: 0.05652187671512365
2024-11-05 02:22:15,609 - INFO - [diffusion][Epoch 9682] diffusion learning rate: 0.001
2024-11-05 02:22:15,611 - INFO - [diffusion][Epoch 9682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:15,612 - INFO - [diffusion][Epoch 9683] Epoch 9684/12000
2024-11-05 02:22:19,888 - INFO - [diffusion][Epoch 9683] diffusion training Loss: 0.062077101320028305
2024-11-05 02:22:19,889 - INFO - [diffusion][Epoch 9683] diffusion learning rate: 0.001
2024-11-05 02:22:19,891 - INFO - [diffusion][Epoch 9683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:19,892 - INFO - [diffusion][Epoch 9684] Epoch 9685/12000
2024-11-05 02:22:24,135 - INFO - [diffusion][Epoch 9684] diffusion training Loss: 0.06506840884685516
2024-11-05 02:22:24,138 - INFO - [diffusion][Epoch 9684] diffusion learning rate: 0.001
2024-11-05 02:22:24,140 - INFO - [diffusion][Epoch 9684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:24,141 - INFO - [diffusion][Epoch 9685] Epoch 9686/12000
2024-11-05 02:22:28,265 - INFO - [diffusion][Epoch 9685] diffusion training Loss: 0.054530332796275616
2024-11-05 02:22:28,267 - INFO - [diffusion][Epoch 9685] diffusion learning rate: 0.001
2024-11-05 02:22:28,268 - INFO - [diffusion][Epoch 9685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:28,270 - INFO - [diffusion][Epoch 9686] Epoch 9687/12000
2024-11-05 02:22:32,441 - INFO - [diffusion][Epoch 9686] diffusion training Loss: 0.06090687960386276
2024-11-05 02:22:32,444 - INFO - [diffusion][Epoch 9686] diffusion learning rate: 0.001
2024-11-05 02:22:32,445 - INFO - [diffusion][Epoch 9686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:32,447 - INFO - [diffusion][Epoch 9687] Epoch 9688/12000
2024-11-05 02:22:37,145 - INFO - [diffusion][Epoch 9687] diffusion training Loss: 0.05896069947630167
2024-11-05 02:22:37,147 - INFO - [diffusion][Epoch 9687] diffusion learning rate: 0.001
2024-11-05 02:22:37,149 - INFO - [diffusion][Epoch 9687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:37,150 - INFO - [diffusion][Epoch 9688] Epoch 9689/12000
2024-11-05 02:22:41,018 - INFO - [diffusion][Epoch 9688] diffusion training Loss: 0.06262924429029226
2024-11-05 02:22:41,020 - INFO - [diffusion][Epoch 9688] diffusion learning rate: 0.001
2024-11-05 02:22:41,022 - INFO - [diffusion][Epoch 9688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:41,023 - INFO - [diffusion][Epoch 9689] Epoch 9690/12000
2024-11-05 02:22:45,270 - INFO - [diffusion][Epoch 9689] diffusion training Loss: 0.05926715210080147
2024-11-05 02:22:45,272 - INFO - [diffusion][Epoch 9689] diffusion learning rate: 0.001
2024-11-05 02:22:45,274 - INFO - [diffusion][Epoch 9689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:45,275 - INFO - [diffusion][Epoch 9690] Epoch 9691/12000
2024-11-05 02:22:49,441 - INFO - [diffusion][Epoch 9690] diffusion training Loss: 0.06512704957276583
2024-11-05 02:22:49,443 - INFO - [diffusion][Epoch 9690] diffusion learning rate: 0.001
2024-11-05 02:22:49,445 - INFO - [diffusion][Epoch 9690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:49,446 - INFO - [diffusion][Epoch 9691] Epoch 9692/12000
2024-11-05 02:22:53,649 - INFO - [diffusion][Epoch 9691] diffusion training Loss: 0.05708467401564121
2024-11-05 02:22:53,651 - INFO - [diffusion][Epoch 9691] diffusion learning rate: 0.001
2024-11-05 02:22:53,653 - INFO - [diffusion][Epoch 9691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:53,654 - INFO - [diffusion][Epoch 9692] Epoch 9693/12000
2024-11-05 02:22:57,763 - INFO - [diffusion][Epoch 9692] diffusion training Loss: 0.06243153661489487
2024-11-05 02:22:57,765 - INFO - [diffusion][Epoch 9692] diffusion learning rate: 0.001
2024-11-05 02:22:57,767 - INFO - [diffusion][Epoch 9692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:57,768 - INFO - [diffusion][Epoch 9693] Epoch 9694/12000
2024-11-05 02:23:01,905 - INFO - [diffusion][Epoch 9693] diffusion training Loss: 0.06114125996828079
2024-11-05 02:23:01,907 - INFO - [diffusion][Epoch 9693] diffusion learning rate: 0.001
2024-11-05 02:23:01,909 - INFO - [diffusion][Epoch 9693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:01,910 - INFO - [diffusion][Epoch 9694] Epoch 9695/12000
2024-11-05 02:23:06,102 - INFO - [diffusion][Epoch 9694] diffusion training Loss: 0.061564164236187935
2024-11-05 02:23:06,104 - INFO - [diffusion][Epoch 9694] diffusion learning rate: 0.001
2024-11-05 02:23:06,106 - INFO - [diffusion][Epoch 9694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:06,107 - INFO - [diffusion][Epoch 9695] Epoch 9696/12000
2024-11-05 02:23:10,281 - INFO - [diffusion][Epoch 9695] diffusion training Loss: 0.06248206086456776
2024-11-05 02:23:10,283 - INFO - [diffusion][Epoch 9695] diffusion learning rate: 0.001
2024-11-05 02:23:10,285 - INFO - [diffusion][Epoch 9695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:10,287 - INFO - [diffusion][Epoch 9696] Epoch 9697/12000
2024-11-05 02:23:14,352 - INFO - [diffusion][Epoch 9696] diffusion training Loss: 0.05986648704856634
2024-11-05 02:23:14,357 - INFO - [diffusion][Epoch 9696] diffusion learning rate: 0.001
2024-11-05 02:23:14,363 - INFO - [diffusion][Epoch 9696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:14,365 - INFO - [diffusion][Epoch 9697] Epoch 9698/12000
2024-11-05 02:23:18,483 - INFO - [diffusion][Epoch 9697] diffusion training Loss: 0.06640239804983139
2024-11-05 02:23:18,485 - INFO - [diffusion][Epoch 9697] diffusion learning rate: 0.001
2024-11-05 02:23:18,487 - INFO - [diffusion][Epoch 9697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:18,488 - INFO - [diffusion][Epoch 9698] Epoch 9699/12000
2024-11-05 02:23:22,551 - INFO - [diffusion][Epoch 9698] diffusion training Loss: 0.061273674480617046
2024-11-05 02:23:22,553 - INFO - [diffusion][Epoch 9698] diffusion learning rate: 0.001
2024-11-05 02:23:22,555 - INFO - [diffusion][Epoch 9698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:22,556 - INFO - [diffusion][Epoch 9699] Epoch 9700/12000
2024-11-05 02:23:26,798 - INFO - [diffusion][Epoch 9699] diffusion training Loss: 0.06247180514037609
2024-11-05 02:23:26,800 - INFO - [diffusion][Epoch 9699] diffusion learning rate: 0.001
2024-11-05 02:23:26,802 - INFO - [diffusion][Epoch 9699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:26,804 - INFO - [diffusion][Epoch 9700] Epoch 9701/12000
2024-11-05 02:23:30,816 - INFO - [diffusion][Epoch 9700] diffusion training Loss: 0.06569401174783707
2024-11-05 02:23:30,818 - INFO - [diffusion][Epoch 9700] diffusion learning rate: 0.001
2024-11-05 02:23:30,820 - INFO - [diffusion][Epoch 9700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:30,821 - INFO - [diffusion][Epoch 9701] Epoch 9702/12000
2024-11-05 02:23:35,016 - INFO - [diffusion][Epoch 9701] diffusion training Loss: 0.06405192241072655
2024-11-05 02:23:35,018 - INFO - [diffusion][Epoch 9701] diffusion learning rate: 0.001
2024-11-05 02:23:35,019 - INFO - [diffusion][Epoch 9701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:35,021 - INFO - [diffusion][Epoch 9702] Epoch 9703/12000
2024-11-05 02:23:39,394 - INFO - [diffusion][Epoch 9702] diffusion training Loss: 0.06744403764605522
2024-11-05 02:23:39,396 - INFO - [diffusion][Epoch 9702] diffusion learning rate: 0.001
2024-11-05 02:23:39,397 - INFO - [diffusion][Epoch 9702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:39,399 - INFO - [diffusion][Epoch 9703] Epoch 9704/12000
2024-11-05 02:23:43,554 - INFO - [diffusion][Epoch 9703] diffusion training Loss: 0.06303737591952085
2024-11-05 02:23:43,556 - INFO - [diffusion][Epoch 9703] diffusion learning rate: 0.001
2024-11-05 02:23:43,559 - INFO - [diffusion][Epoch 9703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:43,560 - INFO - [diffusion][Epoch 9704] Epoch 9705/12000
2024-11-05 02:23:47,456 - INFO - [diffusion][Epoch 9704] diffusion training Loss: 0.05975430738180876
2024-11-05 02:23:47,458 - INFO - [diffusion][Epoch 9704] diffusion learning rate: 0.001
2024-11-05 02:23:47,460 - INFO - [diffusion][Epoch 9704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:47,461 - INFO - [diffusion][Epoch 9705] Epoch 9706/12000
2024-11-05 02:23:51,653 - INFO - [diffusion][Epoch 9705] diffusion training Loss: 0.06402228213846684
2024-11-05 02:23:51,655 - INFO - [diffusion][Epoch 9705] diffusion learning rate: 0.001
2024-11-05 02:23:51,657 - INFO - [diffusion][Epoch 9705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:51,658 - INFO - [diffusion][Epoch 9706] Epoch 9707/12000
2024-11-05 02:23:55,903 - INFO - [diffusion][Epoch 9706] diffusion training Loss: 0.06576558854430914
2024-11-05 02:23:55,905 - INFO - [diffusion][Epoch 9706] diffusion learning rate: 0.001
2024-11-05 02:23:55,907 - INFO - [diffusion][Epoch 9706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:55,908 - INFO - [diffusion][Epoch 9707] Epoch 9708/12000
2024-11-05 02:24:00,185 - INFO - [diffusion][Epoch 9707] diffusion training Loss: 0.06161177158355713
2024-11-05 02:24:00,187 - INFO - [diffusion][Epoch 9707] diffusion learning rate: 0.001
2024-11-05 02:24:00,189 - INFO - [diffusion][Epoch 9707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:00,190 - INFO - [diffusion][Epoch 9708] Epoch 9709/12000
2024-11-05 02:24:04,298 - INFO - [diffusion][Epoch 9708] diffusion training Loss: 0.06003654841333628
2024-11-05 02:24:04,300 - INFO - [diffusion][Epoch 9708] diffusion learning rate: 0.001
2024-11-05 02:24:04,303 - INFO - [diffusion][Epoch 9708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:04,304 - INFO - [diffusion][Epoch 9709] Epoch 9710/12000
2024-11-05 02:24:08,639 - INFO - [diffusion][Epoch 9709] diffusion training Loss: 0.06450852192938328
2024-11-05 02:24:08,641 - INFO - [diffusion][Epoch 9709] diffusion learning rate: 0.001
2024-11-05 02:24:08,643 - INFO - [diffusion][Epoch 9709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:08,644 - INFO - [diffusion][Epoch 9710] Epoch 9711/12000
2024-11-05 02:24:12,910 - INFO - [diffusion][Epoch 9710] diffusion training Loss: 0.0632065599784255
2024-11-05 02:24:12,912 - INFO - [diffusion][Epoch 9710] diffusion learning rate: 0.001
2024-11-05 02:24:12,914 - INFO - [diffusion][Epoch 9710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:12,915 - INFO - [diffusion][Epoch 9711] Epoch 9712/12000
2024-11-05 02:24:17,082 - INFO - [diffusion][Epoch 9711] diffusion training Loss: 0.06316523719578981
2024-11-05 02:24:17,084 - INFO - [diffusion][Epoch 9711] diffusion learning rate: 0.001
2024-11-05 02:24:17,086 - INFO - [diffusion][Epoch 9711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:17,087 - INFO - [diffusion][Epoch 9712] Epoch 9713/12000
2024-11-05 02:24:21,239 - INFO - [diffusion][Epoch 9712] diffusion training Loss: 0.05796330887824297
2024-11-05 02:24:21,242 - INFO - [diffusion][Epoch 9712] diffusion learning rate: 0.001
2024-11-05 02:24:21,243 - INFO - [diffusion][Epoch 9712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:21,245 - INFO - [diffusion][Epoch 9713] Epoch 9714/12000
2024-11-05 02:24:25,505 - INFO - [diffusion][Epoch 9713] diffusion training Loss: 0.06467480398714542
2024-11-05 02:24:25,507 - INFO - [diffusion][Epoch 9713] diffusion learning rate: 0.001
2024-11-05 02:24:25,509 - INFO - [diffusion][Epoch 9713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:25,510 - INFO - [diffusion][Epoch 9714] Epoch 9715/12000
2024-11-05 02:24:29,735 - INFO - [diffusion][Epoch 9714] diffusion training Loss: 0.06428449228405952
2024-11-05 02:24:29,737 - INFO - [diffusion][Epoch 9714] diffusion learning rate: 0.001
2024-11-05 02:24:29,739 - INFO - [diffusion][Epoch 9714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:29,740 - INFO - [diffusion][Epoch 9715] Epoch 9716/12000
2024-11-05 02:24:34,033 - INFO - [diffusion][Epoch 9715] diffusion training Loss: 0.05841910187155008
2024-11-05 02:24:34,035 - INFO - [diffusion][Epoch 9715] diffusion learning rate: 0.001
2024-11-05 02:24:34,036 - INFO - [diffusion][Epoch 9715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:34,038 - INFO - [diffusion][Epoch 9716] Epoch 9717/12000
2024-11-05 02:24:38,341 - INFO - [diffusion][Epoch 9716] diffusion training Loss: 0.06960194185376167
2024-11-05 02:24:38,343 - INFO - [diffusion][Epoch 9716] diffusion learning rate: 0.001
2024-11-05 02:24:38,345 - INFO - [diffusion][Epoch 9716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:38,347 - INFO - [diffusion][Epoch 9717] Epoch 9718/12000
2024-11-05 02:24:42,553 - INFO - [diffusion][Epoch 9717] diffusion training Loss: 0.06304278038442135
2024-11-05 02:24:42,555 - INFO - [diffusion][Epoch 9717] diffusion learning rate: 0.001
2024-11-05 02:24:42,557 - INFO - [diffusion][Epoch 9717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:42,558 - INFO - [diffusion][Epoch 9718] Epoch 9719/12000
2024-11-05 02:24:46,791 - INFO - [diffusion][Epoch 9718] diffusion training Loss: 0.06393559090793133
2024-11-05 02:24:46,793 - INFO - [diffusion][Epoch 9718] diffusion learning rate: 0.001
2024-11-05 02:24:46,842 - INFO - [diffusion][Epoch 9718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:46,844 - INFO - [diffusion][Epoch 9719] Epoch 9720/12000
2024-11-05 02:24:50,899 - INFO - [diffusion][Epoch 9719] diffusion training Loss: 0.06948664039373398
2024-11-05 02:24:50,901 - INFO - [diffusion][Epoch 9719] diffusion learning rate: 0.001
2024-11-05 02:24:50,903 - INFO - [diffusion][Epoch 9719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:50,905 - INFO - [diffusion][Epoch 9720] Epoch 9721/12000
2024-11-05 02:24:55,249 - INFO - [diffusion][Epoch 9720] diffusion training Loss: 0.06044558994472027
2024-11-05 02:24:55,251 - INFO - [diffusion][Epoch 9720] diffusion learning rate: 0.001
2024-11-05 02:24:55,253 - INFO - [diffusion][Epoch 9720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:55,255 - INFO - [diffusion][Epoch 9721] Epoch 9722/12000
2024-11-05 02:24:59,515 - INFO - [diffusion][Epoch 9721] diffusion training Loss: 0.05743008945137262
2024-11-05 02:24:59,518 - INFO - [diffusion][Epoch 9721] diffusion learning rate: 0.001
2024-11-05 02:24:59,519 - INFO - [diffusion][Epoch 9721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:59,520 - INFO - [diffusion][Epoch 9722] Epoch 9723/12000
2024-11-05 02:25:03,685 - INFO - [diffusion][Epoch 9722] diffusion training Loss: 0.05796296335756779
2024-11-05 02:25:03,687 - INFO - [diffusion][Epoch 9722] diffusion learning rate: 0.001
2024-11-05 02:25:03,689 - INFO - [diffusion][Epoch 9722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:03,690 - INFO - [diffusion][Epoch 9723] Epoch 9724/12000
2024-11-05 02:25:07,895 - INFO - [diffusion][Epoch 9723] diffusion training Loss: 0.05910517368465662
2024-11-05 02:25:07,897 - INFO - [diffusion][Epoch 9723] diffusion learning rate: 0.001
2024-11-05 02:25:07,899 - INFO - [diffusion][Epoch 9723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:07,901 - INFO - [diffusion][Epoch 9724] Epoch 9725/12000
2024-11-05 02:25:12,055 - INFO - [diffusion][Epoch 9724] diffusion training Loss: 0.05851475428789854
2024-11-05 02:25:12,057 - INFO - [diffusion][Epoch 9724] diffusion learning rate: 0.001
2024-11-05 02:25:12,059 - INFO - [diffusion][Epoch 9724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:12,060 - INFO - [diffusion][Epoch 9725] Epoch 9726/12000
2024-11-05 02:25:16,321 - INFO - [diffusion][Epoch 9725] diffusion training Loss: 0.06112110521644354
2024-11-05 02:25:16,323 - INFO - [diffusion][Epoch 9725] diffusion learning rate: 0.001
2024-11-05 02:25:16,325 - INFO - [diffusion][Epoch 9725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:16,326 - INFO - [diffusion][Epoch 9726] Epoch 9727/12000
2024-11-05 02:25:20,698 - INFO - [diffusion][Epoch 9726] diffusion training Loss: 0.06125384662300348
2024-11-05 02:25:20,701 - INFO - [diffusion][Epoch 9726] diffusion learning rate: 0.001
2024-11-05 02:25:20,703 - INFO - [diffusion][Epoch 9726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:20,705 - INFO - [diffusion][Epoch 9727] Epoch 9728/12000
2024-11-05 02:25:24,877 - INFO - [diffusion][Epoch 9727] diffusion training Loss: 0.05857381969690323
2024-11-05 02:25:24,879 - INFO - [diffusion][Epoch 9727] diffusion learning rate: 0.001
2024-11-05 02:25:24,911 - INFO - [diffusion][Epoch 9727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:24,912 - INFO - [diffusion][Epoch 9728] Epoch 9729/12000
2024-11-05 02:25:29,687 - INFO - [diffusion][Epoch 9728] diffusion training Loss: 0.06243709847331047
2024-11-05 02:25:29,689 - INFO - [diffusion][Epoch 9728] diffusion learning rate: 0.001
2024-11-05 02:25:29,692 - INFO - [diffusion][Epoch 9728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:29,693 - INFO - [diffusion][Epoch 9729] Epoch 9730/12000
2024-11-05 02:25:33,891 - INFO - [diffusion][Epoch 9729] diffusion training Loss: 0.06444358546286821
2024-11-05 02:25:33,894 - INFO - [diffusion][Epoch 9729] diffusion learning rate: 0.001
2024-11-05 02:25:33,896 - INFO - [diffusion][Epoch 9729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:33,897 - INFO - [diffusion][Epoch 9730] Epoch 9731/12000
2024-11-05 02:25:38,133 - INFO - [diffusion][Epoch 9730] diffusion training Loss: 0.0626843310892582
2024-11-05 02:25:38,135 - INFO - [diffusion][Epoch 9730] diffusion learning rate: 0.001
2024-11-05 02:25:38,137 - INFO - [diffusion][Epoch 9730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:38,139 - INFO - [diffusion][Epoch 9731] Epoch 9732/12000
2024-11-05 02:25:42,343 - INFO - [diffusion][Epoch 9731] diffusion training Loss: 0.06244084518402815
2024-11-05 02:25:42,345 - INFO - [diffusion][Epoch 9731] diffusion learning rate: 0.001
2024-11-05 02:25:42,347 - INFO - [diffusion][Epoch 9731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:42,348 - INFO - [diffusion][Epoch 9732] Epoch 9733/12000
2024-11-05 02:25:46,556 - INFO - [diffusion][Epoch 9732] diffusion training Loss: 0.058954961597919464
2024-11-05 02:25:46,558 - INFO - [diffusion][Epoch 9732] diffusion learning rate: 0.001
2024-11-05 02:25:46,560 - INFO - [diffusion][Epoch 9732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:46,561 - INFO - [diffusion][Epoch 9733] Epoch 9734/12000
2024-11-05 02:25:50,786 - INFO - [diffusion][Epoch 9733] diffusion training Loss: 0.06518156826496124
2024-11-05 02:25:50,789 - INFO - [diffusion][Epoch 9733] diffusion learning rate: 0.001
2024-11-05 02:25:50,792 - INFO - [diffusion][Epoch 9733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:50,793 - INFO - [diffusion][Epoch 9734] Epoch 9735/12000
2024-11-05 02:25:54,965 - INFO - [diffusion][Epoch 9734] diffusion training Loss: 0.06255193054676056
2024-11-05 02:25:54,968 - INFO - [diffusion][Epoch 9734] diffusion learning rate: 0.001
2024-11-05 02:25:54,970 - INFO - [diffusion][Epoch 9734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:54,972 - INFO - [diffusion][Epoch 9735] Epoch 9736/12000
2024-11-05 02:25:59,196 - INFO - [diffusion][Epoch 9735] diffusion training Loss: 0.06409412808716297
2024-11-05 02:25:59,199 - INFO - [diffusion][Epoch 9735] diffusion learning rate: 0.001
2024-11-05 02:25:59,201 - INFO - [diffusion][Epoch 9735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:59,202 - INFO - [diffusion][Epoch 9736] Epoch 9737/12000
2024-11-05 02:26:03,431 - INFO - [diffusion][Epoch 9736] diffusion training Loss: 0.06532261148095131
2024-11-05 02:26:03,433 - INFO - [diffusion][Epoch 9736] diffusion learning rate: 0.001
2024-11-05 02:26:03,435 - INFO - [diffusion][Epoch 9736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:03,436 - INFO - [diffusion][Epoch 9737] Epoch 9738/12000
2024-11-05 02:26:07,596 - INFO - [diffusion][Epoch 9737] diffusion training Loss: 0.06250278931111097
2024-11-05 02:26:07,598 - INFO - [diffusion][Epoch 9737] diffusion learning rate: 0.001
2024-11-05 02:26:07,600 - INFO - [diffusion][Epoch 9737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:07,601 - INFO - [diffusion][Epoch 9738] Epoch 9739/12000
2024-11-05 02:26:11,854 - INFO - [diffusion][Epoch 9738] diffusion training Loss: 0.06387676298618317
2024-11-05 02:26:11,855 - INFO - [diffusion][Epoch 9738] diffusion learning rate: 0.001
2024-11-05 02:26:11,857 - INFO - [diffusion][Epoch 9738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:11,858 - INFO - [diffusion][Epoch 9739] Epoch 9740/12000
2024-11-05 02:26:15,948 - INFO - [diffusion][Epoch 9739] diffusion training Loss: 0.06513674464076757
2024-11-05 02:26:15,951 - INFO - [diffusion][Epoch 9739] diffusion learning rate: 0.001
2024-11-05 02:26:15,953 - INFO - [diffusion][Epoch 9739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:15,955 - INFO - [diffusion][Epoch 9740] Epoch 9741/12000
2024-11-05 02:26:20,111 - INFO - [diffusion][Epoch 9740] diffusion training Loss: 0.05902617797255516
2024-11-05 02:26:20,113 - INFO - [diffusion][Epoch 9740] diffusion learning rate: 0.001
2024-11-05 02:26:20,115 - INFO - [diffusion][Epoch 9740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:20,116 - INFO - [diffusion][Epoch 9741] Epoch 9742/12000
2024-11-05 02:26:24,334 - INFO - [diffusion][Epoch 9741] diffusion training Loss: 0.06356599181890488
2024-11-05 02:26:24,370 - INFO - [diffusion][Epoch 9741] diffusion learning rate: 0.001
2024-11-05 02:26:24,372 - INFO - [diffusion][Epoch 9741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:24,373 - INFO - [diffusion][Epoch 9742] Epoch 9743/12000
2024-11-05 02:26:28,581 - INFO - [diffusion][Epoch 9742] diffusion training Loss: 0.06363850459456444
2024-11-05 02:26:28,583 - INFO - [diffusion][Epoch 9742] diffusion learning rate: 0.001
2024-11-05 02:26:28,585 - INFO - [diffusion][Epoch 9742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:28,586 - INFO - [diffusion][Epoch 9743] Epoch 9744/12000
2024-11-05 02:26:32,864 - INFO - [diffusion][Epoch 9743] diffusion training Loss: 0.06685729324817657
2024-11-05 02:26:32,866 - INFO - [diffusion][Epoch 9743] diffusion learning rate: 0.001
2024-11-05 02:26:32,868 - INFO - [diffusion][Epoch 9743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:32,869 - INFO - [diffusion][Epoch 9744] Epoch 9745/12000
2024-11-05 02:26:37,047 - INFO - [diffusion][Epoch 9744] diffusion training Loss: 0.0621133279055357
2024-11-05 02:26:37,049 - INFO - [diffusion][Epoch 9744] diffusion learning rate: 0.001
2024-11-05 02:26:37,051 - INFO - [diffusion][Epoch 9744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:37,052 - INFO - [diffusion][Epoch 9745] Epoch 9746/12000
2024-11-05 02:26:41,038 - INFO - [diffusion][Epoch 9745] diffusion training Loss: 0.06681826058775187
2024-11-05 02:26:41,040 - INFO - [diffusion][Epoch 9745] diffusion learning rate: 0.001
2024-11-05 02:26:41,041 - INFO - [diffusion][Epoch 9745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:41,042 - INFO - [diffusion][Epoch 9746] Epoch 9747/12000
2024-11-05 02:26:44,979 - INFO - [diffusion][Epoch 9746] diffusion training Loss: 0.06134154740720987
2024-11-05 02:26:44,981 - INFO - [diffusion][Epoch 9746] diffusion learning rate: 0.001
2024-11-05 02:26:44,983 - INFO - [diffusion][Epoch 9746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:44,985 - INFO - [diffusion][Epoch 9747] Epoch 9748/12000
2024-11-05 02:26:49,216 - INFO - [diffusion][Epoch 9747] diffusion training Loss: 0.06257440336048603
2024-11-05 02:26:49,218 - INFO - [diffusion][Epoch 9747] diffusion learning rate: 0.001
2024-11-05 02:26:49,220 - INFO - [diffusion][Epoch 9747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:49,221 - INFO - [diffusion][Epoch 9748] Epoch 9749/12000
2024-11-05 02:26:53,240 - INFO - [diffusion][Epoch 9748] diffusion training Loss: 0.06494880933314562
2024-11-05 02:26:53,242 - INFO - [diffusion][Epoch 9748] diffusion learning rate: 0.001
2024-11-05 02:26:53,244 - INFO - [diffusion][Epoch 9748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:53,245 - INFO - [diffusion][Epoch 9749] Epoch 9750/12000
2024-11-05 02:26:58,100 - INFO - [diffusion][Epoch 9749] diffusion training Loss: 0.06048347055912018
2024-11-05 02:26:58,102 - INFO - [diffusion][Epoch 9749] diffusion learning rate: 0.001
2024-11-05 02:26:58,104 - INFO - [diffusion][Epoch 9749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:58,105 - INFO - [diffusion][Epoch 9750] Epoch 9751/12000
2024-11-05 02:27:01,775 - INFO - [diffusion][Epoch 9750] diffusion training Loss: 0.05878632329404354
2024-11-05 02:27:01,778 - INFO - [diffusion][Epoch 9750] diffusion learning rate: 0.001
2024-11-05 02:27:01,817 - INFO - [diffusion][Epoch 9750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:01,819 - INFO - [diffusion][Epoch 9751] Epoch 9752/12000
2024-11-05 02:27:05,871 - INFO - [diffusion][Epoch 9751] diffusion training Loss: 0.06459370721131563
2024-11-05 02:27:05,874 - INFO - [diffusion][Epoch 9751] diffusion learning rate: 0.001
2024-11-05 02:27:05,876 - INFO - [diffusion][Epoch 9751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:05,877 - INFO - [diffusion][Epoch 9752] Epoch 9753/12000
2024-11-05 02:27:10,055 - INFO - [diffusion][Epoch 9752] diffusion training Loss: 0.059133727103471756
2024-11-05 02:27:10,057 - INFO - [diffusion][Epoch 9752] diffusion learning rate: 0.001
2024-11-05 02:27:10,059 - INFO - [diffusion][Epoch 9752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:10,061 - INFO - [diffusion][Epoch 9753] Epoch 9754/12000
2024-11-05 02:27:14,334 - INFO - [diffusion][Epoch 9753] diffusion training Loss: 0.06323464680463076
2024-11-05 02:27:14,337 - INFO - [diffusion][Epoch 9753] diffusion learning rate: 0.001
2024-11-05 02:27:14,338 - INFO - [diffusion][Epoch 9753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:14,340 - INFO - [diffusion][Epoch 9754] Epoch 9755/12000
2024-11-05 02:27:18,534 - INFO - [diffusion][Epoch 9754] diffusion training Loss: 0.06023036688566208
2024-11-05 02:27:18,535 - INFO - [diffusion][Epoch 9754] diffusion learning rate: 0.001
2024-11-05 02:27:18,537 - INFO - [diffusion][Epoch 9754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:18,540 - INFO - [diffusion][Epoch 9755] Epoch 9756/12000
2024-11-05 02:27:22,677 - INFO - [diffusion][Epoch 9755] diffusion training Loss: 0.06231035944074392
2024-11-05 02:27:22,680 - INFO - [diffusion][Epoch 9755] diffusion learning rate: 0.001
2024-11-05 02:27:22,682 - INFO - [diffusion][Epoch 9755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:22,683 - INFO - [diffusion][Epoch 9756] Epoch 9757/12000
2024-11-05 02:27:26,762 - INFO - [diffusion][Epoch 9756] diffusion training Loss: 0.06020418740808964
2024-11-05 02:27:26,764 - INFO - [diffusion][Epoch 9756] diffusion learning rate: 0.001
2024-11-05 02:27:26,765 - INFO - [diffusion][Epoch 9756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:26,767 - INFO - [diffusion][Epoch 9757] Epoch 9758/12000
2024-11-05 02:27:31,029 - INFO - [diffusion][Epoch 9757] diffusion training Loss: 0.059528579004108906
2024-11-05 02:27:31,031 - INFO - [diffusion][Epoch 9757] diffusion learning rate: 0.001
2024-11-05 02:27:31,032 - INFO - [diffusion][Epoch 9757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:31,034 - INFO - [diffusion][Epoch 9758] Epoch 9759/12000
2024-11-05 02:27:35,133 - INFO - [diffusion][Epoch 9758] diffusion training Loss: 0.06284778472036123
2024-11-05 02:27:35,135 - INFO - [diffusion][Epoch 9758] diffusion learning rate: 0.001
2024-11-05 02:27:35,137 - INFO - [diffusion][Epoch 9758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:35,138 - INFO - [diffusion][Epoch 9759] Epoch 9760/12000
2024-11-05 02:27:39,294 - INFO - [diffusion][Epoch 9759] diffusion training Loss: 0.0640994580462575
2024-11-05 02:27:39,296 - INFO - [diffusion][Epoch 9759] diffusion learning rate: 0.001
2024-11-05 02:27:39,298 - INFO - [diffusion][Epoch 9759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:39,299 - INFO - [diffusion][Epoch 9760] Epoch 9761/12000
2024-11-05 02:27:43,550 - INFO - [diffusion][Epoch 9760] diffusion training Loss: 0.061284517869353294
2024-11-05 02:27:43,551 - INFO - [diffusion][Epoch 9760] diffusion learning rate: 0.001
2024-11-05 02:27:43,553 - INFO - [diffusion][Epoch 9760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:43,554 - INFO - [diffusion][Epoch 9761] Epoch 9762/12000
2024-11-05 02:27:47,784 - INFO - [diffusion][Epoch 9761] diffusion training Loss: 0.06566584296524525
2024-11-05 02:27:47,796 - INFO - [diffusion][Epoch 9761] diffusion learning rate: 0.001
2024-11-05 02:27:47,798 - INFO - [diffusion][Epoch 9761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:47,800 - INFO - [diffusion][Epoch 9762] Epoch 9763/12000
2024-11-05 02:27:51,994 - INFO - [diffusion][Epoch 9762] diffusion training Loss: 0.06219521723687649
2024-11-05 02:27:51,996 - INFO - [diffusion][Epoch 9762] diffusion learning rate: 0.001
2024-11-05 02:27:51,998 - INFO - [diffusion][Epoch 9762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:51,999 - INFO - [diffusion][Epoch 9763] Epoch 9764/12000
2024-11-05 02:27:56,266 - INFO - [diffusion][Epoch 9763] diffusion training Loss: 0.056488181464374065
2024-11-05 02:27:56,268 - INFO - [diffusion][Epoch 9763] diffusion learning rate: 0.001
2024-11-05 02:27:56,270 - INFO - [diffusion][Epoch 9763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:56,271 - INFO - [diffusion][Epoch 9764] Epoch 9765/12000
2024-11-05 02:28:00,571 - INFO - [diffusion][Epoch 9764] diffusion training Loss: 0.06275778356939554
2024-11-05 02:28:00,573 - INFO - [diffusion][Epoch 9764] diffusion learning rate: 0.001
2024-11-05 02:28:00,617 - INFO - [diffusion][Epoch 9764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:00,619 - INFO - [diffusion][Epoch 9765] Epoch 9766/12000
2024-11-05 02:28:04,764 - INFO - [diffusion][Epoch 9765] diffusion training Loss: 0.05839020386338234
2024-11-05 02:28:04,766 - INFO - [diffusion][Epoch 9765] diffusion learning rate: 0.001
2024-11-05 02:28:04,768 - INFO - [diffusion][Epoch 9765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:04,769 - INFO - [diffusion][Epoch 9766] Epoch 9767/12000
2024-11-05 02:28:08,968 - INFO - [diffusion][Epoch 9766] diffusion training Loss: 0.062382680363953114
2024-11-05 02:28:08,970 - INFO - [diffusion][Epoch 9766] diffusion learning rate: 0.001
2024-11-05 02:28:08,972 - INFO - [diffusion][Epoch 9766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:08,973 - INFO - [diffusion][Epoch 9767] Epoch 9768/12000
2024-11-05 02:28:13,012 - INFO - [diffusion][Epoch 9767] diffusion training Loss: 0.0668233698233962
2024-11-05 02:28:13,014 - INFO - [diffusion][Epoch 9767] diffusion learning rate: 0.001
2024-11-05 02:28:13,015 - INFO - [diffusion][Epoch 9767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:13,017 - INFO - [diffusion][Epoch 9768] Epoch 9769/12000
2024-11-05 02:28:17,273 - INFO - [diffusion][Epoch 9768] diffusion training Loss: 0.05757107678800821
2024-11-05 02:28:17,276 - INFO - [diffusion][Epoch 9768] diffusion learning rate: 0.001
2024-11-05 02:28:17,277 - INFO - [diffusion][Epoch 9768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:17,279 - INFO - [diffusion][Epoch 9769] Epoch 9770/12000
2024-11-05 02:28:21,511 - INFO - [diffusion][Epoch 9769] diffusion training Loss: 0.05754558276385069
2024-11-05 02:28:21,513 - INFO - [diffusion][Epoch 9769] diffusion learning rate: 0.001
2024-11-05 02:28:21,515 - INFO - [diffusion][Epoch 9769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:21,516 - INFO - [diffusion][Epoch 9770] Epoch 9771/12000
2024-11-05 02:28:25,769 - INFO - [diffusion][Epoch 9770] diffusion training Loss: 0.06609049625694752
2024-11-05 02:28:25,771 - INFO - [diffusion][Epoch 9770] diffusion learning rate: 0.001
2024-11-05 02:28:25,773 - INFO - [diffusion][Epoch 9770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:25,774 - INFO - [diffusion][Epoch 9771] Epoch 9772/12000
2024-11-05 02:28:29,986 - INFO - [diffusion][Epoch 9771] diffusion training Loss: 0.06334054004400969
2024-11-05 02:28:29,988 - INFO - [diffusion][Epoch 9771] diffusion learning rate: 0.001
2024-11-05 02:28:29,990 - INFO - [diffusion][Epoch 9771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:29,991 - INFO - [diffusion][Epoch 9772] Epoch 9773/12000
2024-11-05 02:28:34,185 - INFO - [diffusion][Epoch 9772] diffusion training Loss: 0.057850745506584644
2024-11-05 02:28:34,187 - INFO - [diffusion][Epoch 9772] diffusion learning rate: 0.001
2024-11-05 02:28:34,189 - INFO - [diffusion][Epoch 9772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:34,191 - INFO - [diffusion][Epoch 9773] Epoch 9774/12000
2024-11-05 02:28:38,400 - INFO - [diffusion][Epoch 9773] diffusion training Loss: 0.06853720359504223
2024-11-05 02:28:38,402 - INFO - [diffusion][Epoch 9773] diffusion learning rate: 0.001
2024-11-05 02:28:38,404 - INFO - [diffusion][Epoch 9773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:38,405 - INFO - [diffusion][Epoch 9774] Epoch 9775/12000
2024-11-05 02:28:42,574 - INFO - [diffusion][Epoch 9774] diffusion training Loss: 0.060375019907951355
2024-11-05 02:28:42,576 - INFO - [diffusion][Epoch 9774] diffusion learning rate: 0.001
2024-11-05 02:28:42,578 - INFO - [diffusion][Epoch 9774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:42,579 - INFO - [diffusion][Epoch 9775] Epoch 9776/12000
2024-11-05 02:28:46,768 - INFO - [diffusion][Epoch 9775] diffusion training Loss: 0.06301439739763737
2024-11-05 02:28:46,770 - INFO - [diffusion][Epoch 9775] diffusion learning rate: 0.001
2024-11-05 02:28:46,772 - INFO - [diffusion][Epoch 9775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:46,773 - INFO - [diffusion][Epoch 9776] Epoch 9777/12000
2024-11-05 02:28:50,955 - INFO - [diffusion][Epoch 9776] diffusion training Loss: 0.06283973157405853
2024-11-05 02:28:50,958 - INFO - [diffusion][Epoch 9776] diffusion learning rate: 0.001
2024-11-05 02:28:50,960 - INFO - [diffusion][Epoch 9776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:50,962 - INFO - [diffusion][Epoch 9777] Epoch 9778/12000
2024-11-05 02:28:55,018 - INFO - [diffusion][Epoch 9777] diffusion training Loss: 0.06409375835210085
2024-11-05 02:28:55,020 - INFO - [diffusion][Epoch 9777] diffusion learning rate: 0.001
2024-11-05 02:28:55,022 - INFO - [diffusion][Epoch 9777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:55,024 - INFO - [diffusion][Epoch 9778] Epoch 9779/12000
2024-11-05 02:28:59,320 - INFO - [diffusion][Epoch 9778] diffusion training Loss: 0.06181105598807335
2024-11-05 02:28:59,322 - INFO - [diffusion][Epoch 9778] diffusion learning rate: 0.001
2024-11-05 02:28:59,324 - INFO - [diffusion][Epoch 9778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:59,326 - INFO - [diffusion][Epoch 9779] Epoch 9780/12000
2024-11-05 02:29:03,600 - INFO - [diffusion][Epoch 9779] diffusion training Loss: 0.06779417395591736
2024-11-05 02:29:03,603 - INFO - [diffusion][Epoch 9779] diffusion learning rate: 0.001
2024-11-05 02:29:03,605 - INFO - [diffusion][Epoch 9779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:03,607 - INFO - [diffusion][Epoch 9780] Epoch 9781/12000
2024-11-05 02:29:07,748 - INFO - [diffusion][Epoch 9780] diffusion training Loss: 0.05747482739388943
2024-11-05 02:29:07,750 - INFO - [diffusion][Epoch 9780] diffusion learning rate: 0.001
2024-11-05 02:29:07,751 - INFO - [diffusion][Epoch 9780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:07,753 - INFO - [diffusion][Epoch 9781] Epoch 9782/12000
2024-11-05 02:29:11,848 - INFO - [diffusion][Epoch 9781] diffusion training Loss: 0.05774167086929083
2024-11-05 02:29:11,850 - INFO - [diffusion][Epoch 9781] diffusion learning rate: 0.001
2024-11-05 02:29:11,852 - INFO - [diffusion][Epoch 9781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:11,853 - INFO - [diffusion][Epoch 9782] Epoch 9783/12000
2024-11-05 02:29:16,117 - INFO - [diffusion][Epoch 9782] diffusion training Loss: 0.06398866884410381
2024-11-05 02:29:16,119 - INFO - [diffusion][Epoch 9782] diffusion learning rate: 0.001
2024-11-05 02:29:16,121 - INFO - [diffusion][Epoch 9782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:16,122 - INFO - [diffusion][Epoch 9783] Epoch 9784/12000
2024-11-05 02:29:20,329 - INFO - [diffusion][Epoch 9783] diffusion training Loss: 0.06606857292354107
2024-11-05 02:29:20,331 - INFO - [diffusion][Epoch 9783] diffusion learning rate: 0.001
2024-11-05 02:29:20,333 - INFO - [diffusion][Epoch 9783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:20,334 - INFO - [diffusion][Epoch 9784] Epoch 9785/12000
2024-11-05 02:29:24,444 - INFO - [diffusion][Epoch 9784] diffusion training Loss: 0.0639395285397768
2024-11-05 02:29:24,447 - INFO - [diffusion][Epoch 9784] diffusion learning rate: 0.001
2024-11-05 02:29:24,449 - INFO - [diffusion][Epoch 9784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:24,450 - INFO - [diffusion][Epoch 9785] Epoch 9786/12000
2024-11-05 02:29:28,558 - INFO - [diffusion][Epoch 9785] diffusion training Loss: 0.06349575333297253
2024-11-05 02:29:28,560 - INFO - [diffusion][Epoch 9785] diffusion learning rate: 0.001
2024-11-05 02:29:28,562 - INFO - [diffusion][Epoch 9785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:28,563 - INFO - [diffusion][Epoch 9786] Epoch 9787/12000
2024-11-05 02:29:32,659 - INFO - [diffusion][Epoch 9786] diffusion training Loss: 0.06456139869987965
2024-11-05 02:29:32,661 - INFO - [diffusion][Epoch 9786] diffusion learning rate: 0.001
2024-11-05 02:29:32,663 - INFO - [diffusion][Epoch 9786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:32,664 - INFO - [diffusion][Epoch 9787] Epoch 9788/12000
2024-11-05 02:29:36,770 - INFO - [diffusion][Epoch 9787] diffusion training Loss: 0.05964398104697466
2024-11-05 02:29:36,772 - INFO - [diffusion][Epoch 9787] diffusion learning rate: 0.001
2024-11-05 02:29:36,774 - INFO - [diffusion][Epoch 9787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:36,775 - INFO - [diffusion][Epoch 9788] Epoch 9789/12000
2024-11-05 02:29:40,887 - INFO - [diffusion][Epoch 9788] diffusion training Loss: 0.0644899345934391
2024-11-05 02:29:40,889 - INFO - [diffusion][Epoch 9788] diffusion learning rate: 0.001
2024-11-05 02:29:40,890 - INFO - [diffusion][Epoch 9788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:40,892 - INFO - [diffusion][Epoch 9789] Epoch 9790/12000
2024-11-05 02:29:45,221 - INFO - [diffusion][Epoch 9789] diffusion training Loss: 0.06745855510234833
2024-11-05 02:29:45,224 - INFO - [diffusion][Epoch 9789] diffusion learning rate: 0.001
2024-11-05 02:29:45,262 - INFO - [diffusion][Epoch 9789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:45,264 - INFO - [diffusion][Epoch 9790] Epoch 9791/12000
2024-11-05 02:29:49,261 - INFO - [diffusion][Epoch 9790] diffusion training Loss: 0.06844822410494089
2024-11-05 02:29:49,263 - INFO - [diffusion][Epoch 9790] diffusion learning rate: 0.001
2024-11-05 02:29:49,265 - INFO - [diffusion][Epoch 9790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:49,266 - INFO - [diffusion][Epoch 9791] Epoch 9792/12000
2024-11-05 02:29:53,929 - INFO - [diffusion][Epoch 9791] diffusion training Loss: 0.06150724086910486
2024-11-05 02:29:53,931 - INFO - [diffusion][Epoch 9791] diffusion learning rate: 0.001
2024-11-05 02:29:53,932 - INFO - [diffusion][Epoch 9791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:53,934 - INFO - [diffusion][Epoch 9792] Epoch 9793/12000
2024-11-05 02:29:57,923 - INFO - [diffusion][Epoch 9792] diffusion training Loss: 0.05885620880872011
2024-11-05 02:29:57,925 - INFO - [diffusion][Epoch 9792] diffusion learning rate: 0.001
2024-11-05 02:29:57,926 - INFO - [diffusion][Epoch 9792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:57,928 - INFO - [diffusion][Epoch 9793] Epoch 9794/12000
2024-11-05 02:30:02,038 - INFO - [diffusion][Epoch 9793] diffusion training Loss: 0.05841905251145363
2024-11-05 02:30:02,040 - INFO - [diffusion][Epoch 9793] diffusion learning rate: 0.001
2024-11-05 02:30:02,042 - INFO - [diffusion][Epoch 9793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:02,043 - INFO - [diffusion][Epoch 9794] Epoch 9795/12000
2024-11-05 02:30:06,219 - INFO - [diffusion][Epoch 9794] diffusion training Loss: 0.0609710905700922
2024-11-05 02:30:06,221 - INFO - [diffusion][Epoch 9794] diffusion learning rate: 0.001
2024-11-05 02:30:06,224 - INFO - [diffusion][Epoch 9794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:06,225 - INFO - [diffusion][Epoch 9795] Epoch 9796/12000
2024-11-05 02:30:10,440 - INFO - [diffusion][Epoch 9795] diffusion training Loss: 0.059704478830099106
2024-11-05 02:30:10,442 - INFO - [diffusion][Epoch 9795] diffusion learning rate: 0.001
2024-11-05 02:30:10,445 - INFO - [diffusion][Epoch 9795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:10,446 - INFO - [diffusion][Epoch 9796] Epoch 9797/12000
2024-11-05 02:30:14,740 - INFO - [diffusion][Epoch 9796] diffusion training Loss: 0.06046037841588259
2024-11-05 02:30:14,742 - INFO - [diffusion][Epoch 9796] diffusion learning rate: 0.001
2024-11-05 02:30:14,744 - INFO - [diffusion][Epoch 9796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:14,745 - INFO - [diffusion][Epoch 9797] Epoch 9798/12000
2024-11-05 02:30:18,930 - INFO - [diffusion][Epoch 9797] diffusion training Loss: 0.06576485838741064
2024-11-05 02:30:18,932 - INFO - [diffusion][Epoch 9797] diffusion learning rate: 0.001
2024-11-05 02:30:18,934 - INFO - [diffusion][Epoch 9797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:18,935 - INFO - [diffusion][Epoch 9798] Epoch 9799/12000
2024-11-05 02:30:23,051 - INFO - [diffusion][Epoch 9798] diffusion training Loss: 0.06357967853546143
2024-11-05 02:30:23,053 - INFO - [diffusion][Epoch 9798] diffusion learning rate: 0.001
2024-11-05 02:30:23,055 - INFO - [diffusion][Epoch 9798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:23,056 - INFO - [diffusion][Epoch 9799] Epoch 9800/12000
2024-11-05 02:30:26,639 - INFO - [diffusion][Epoch 9799] diffusion training Loss: 0.06491620931774378
2024-11-05 02:30:26,642 - INFO - [diffusion][Epoch 9799] diffusion learning rate: 0.001
2024-11-05 02:30:26,643 - INFO - [diffusion][Epoch 9799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:26,645 - INFO - [diffusion][Epoch 9800] Epoch 9801/12000
2024-11-05 02:30:30,807 - INFO - [diffusion][Epoch 9800] diffusion training Loss: 0.0626601055264473
2024-11-05 02:30:30,810 - INFO - [diffusion][Epoch 9800] diffusion learning rate: 0.001
2024-11-05 02:30:30,851 - INFO - [diffusion][Epoch 9800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:30,852 - INFO - [diffusion][Epoch 9801] Epoch 9802/12000
2024-11-05 02:30:35,011 - INFO - [diffusion][Epoch 9801] diffusion training Loss: 0.06221786793321371
2024-11-05 02:30:35,013 - INFO - [diffusion][Epoch 9801] diffusion learning rate: 0.001
2024-11-05 02:30:35,015 - INFO - [diffusion][Epoch 9801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:35,016 - INFO - [diffusion][Epoch 9802] Epoch 9803/12000
2024-11-05 02:30:39,249 - INFO - [diffusion][Epoch 9802] diffusion training Loss: 0.06650278344750404
2024-11-05 02:30:39,251 - INFO - [diffusion][Epoch 9802] diffusion learning rate: 0.001
2024-11-05 02:30:39,253 - INFO - [diffusion][Epoch 9802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:39,254 - INFO - [diffusion][Epoch 9803] Epoch 9804/12000
2024-11-05 02:30:43,363 - INFO - [diffusion][Epoch 9803] diffusion training Loss: 0.0588781563565135
2024-11-05 02:30:43,365 - INFO - [diffusion][Epoch 9803] diffusion learning rate: 0.001
2024-11-05 02:30:43,367 - INFO - [diffusion][Epoch 9803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:43,368 - INFO - [diffusion][Epoch 9804] Epoch 9805/12000
2024-11-05 02:30:47,377 - INFO - [diffusion][Epoch 9804] diffusion training Loss: 0.06169304437935352
2024-11-05 02:30:47,379 - INFO - [diffusion][Epoch 9804] diffusion learning rate: 0.001
2024-11-05 02:30:47,381 - INFO - [diffusion][Epoch 9804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:47,382 - INFO - [diffusion][Epoch 9805] Epoch 9806/12000
2024-11-05 02:30:51,495 - INFO - [diffusion][Epoch 9805] diffusion training Loss: 0.06556642428040504
2024-11-05 02:30:51,497 - INFO - [diffusion][Epoch 9805] diffusion learning rate: 0.001
2024-11-05 02:30:51,499 - INFO - [diffusion][Epoch 9805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:51,500 - INFO - [diffusion][Epoch 9806] Epoch 9807/12000
2024-11-05 02:30:55,669 - INFO - [diffusion][Epoch 9806] diffusion training Loss: 0.06742449197918177
2024-11-05 02:30:55,671 - INFO - [diffusion][Epoch 9806] diffusion learning rate: 0.001
2024-11-05 02:30:55,673 - INFO - [diffusion][Epoch 9806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:55,674 - INFO - [diffusion][Epoch 9807] Epoch 9808/12000
2024-11-05 02:31:00,004 - INFO - [diffusion][Epoch 9807] diffusion training Loss: 0.05844190530478954
2024-11-05 02:31:00,006 - INFO - [diffusion][Epoch 9807] diffusion learning rate: 0.001
2024-11-05 02:31:00,008 - INFO - [diffusion][Epoch 9807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:00,009 - INFO - [diffusion][Epoch 9808] Epoch 9809/12000
2024-11-05 02:31:04,200 - INFO - [diffusion][Epoch 9808] diffusion training Loss: 0.062101373448967934
2024-11-05 02:31:04,202 - INFO - [diffusion][Epoch 9808] diffusion learning rate: 0.001
2024-11-05 02:31:04,204 - INFO - [diffusion][Epoch 9808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:04,205 - INFO - [diffusion][Epoch 9809] Epoch 9810/12000
2024-11-05 02:31:08,442 - INFO - [diffusion][Epoch 9809] diffusion training Loss: 0.06628416664898396
2024-11-05 02:31:08,444 - INFO - [diffusion][Epoch 9809] diffusion learning rate: 0.001
2024-11-05 02:31:08,446 - INFO - [diffusion][Epoch 9809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:08,447 - INFO - [diffusion][Epoch 9810] Epoch 9811/12000
2024-11-05 02:31:12,724 - INFO - [diffusion][Epoch 9810] diffusion training Loss: 0.05985056050121784
2024-11-05 02:31:12,727 - INFO - [diffusion][Epoch 9810] diffusion learning rate: 0.001
2024-11-05 02:31:12,729 - INFO - [diffusion][Epoch 9810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:12,730 - INFO - [diffusion][Epoch 9811] Epoch 9812/12000
2024-11-05 02:31:16,944 - INFO - [diffusion][Epoch 9811] diffusion training Loss: 0.058372835628688335
2024-11-05 02:31:16,946 - INFO - [diffusion][Epoch 9811] diffusion learning rate: 0.001
2024-11-05 02:31:16,948 - INFO - [diffusion][Epoch 9811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:16,949 - INFO - [diffusion][Epoch 9812] Epoch 9813/12000
2024-11-05 02:31:21,290 - INFO - [diffusion][Epoch 9812] diffusion training Loss: 0.07022873125970364
2024-11-05 02:31:21,293 - INFO - [diffusion][Epoch 9812] diffusion learning rate: 0.001
2024-11-05 02:31:21,295 - INFO - [diffusion][Epoch 9812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:21,296 - INFO - [diffusion][Epoch 9813] Epoch 9814/12000
2024-11-05 02:31:25,549 - INFO - [diffusion][Epoch 9813] diffusion training Loss: 0.06161509081721306
2024-11-05 02:31:25,551 - INFO - [diffusion][Epoch 9813] diffusion learning rate: 0.001
2024-11-05 02:31:25,554 - INFO - [diffusion][Epoch 9813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:25,555 - INFO - [diffusion][Epoch 9814] Epoch 9815/12000
2024-11-05 02:31:29,784 - INFO - [diffusion][Epoch 9814] diffusion training Loss: 0.06726072542369366
2024-11-05 02:31:29,786 - INFO - [diffusion][Epoch 9814] diffusion learning rate: 0.001
2024-11-05 02:31:29,788 - INFO - [diffusion][Epoch 9814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:29,789 - INFO - [diffusion][Epoch 9815] Epoch 9816/12000
2024-11-05 02:31:34,055 - INFO - [diffusion][Epoch 9815] diffusion training Loss: 0.0581353735178709
2024-11-05 02:31:34,058 - INFO - [diffusion][Epoch 9815] diffusion learning rate: 0.001
2024-11-05 02:31:34,060 - INFO - [diffusion][Epoch 9815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:34,061 - INFO - [diffusion][Epoch 9816] Epoch 9817/12000
2024-11-05 02:31:38,326 - INFO - [diffusion][Epoch 9816] diffusion training Loss: 0.05832439195364714
2024-11-05 02:31:38,328 - INFO - [diffusion][Epoch 9816] diffusion learning rate: 0.001
2024-11-05 02:31:38,330 - INFO - [diffusion][Epoch 9816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:38,332 - INFO - [diffusion][Epoch 9817] Epoch 9818/12000
2024-11-05 02:31:42,619 - INFO - [diffusion][Epoch 9817] diffusion training Loss: 0.0599375581368804
2024-11-05 02:31:42,621 - INFO - [diffusion][Epoch 9817] diffusion learning rate: 0.001
2024-11-05 02:31:42,623 - INFO - [diffusion][Epoch 9817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:42,625 - INFO - [diffusion][Epoch 9818] Epoch 9819/12000
2024-11-05 02:31:46,948 - INFO - [diffusion][Epoch 9818] diffusion training Loss: 0.061307936906814575
2024-11-05 02:31:46,951 - INFO - [diffusion][Epoch 9818] diffusion learning rate: 0.001
2024-11-05 02:31:46,953 - INFO - [diffusion][Epoch 9818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:46,954 - INFO - [diffusion][Epoch 9819] Epoch 9820/12000
2024-11-05 02:31:51,364 - INFO - [diffusion][Epoch 9819] diffusion training Loss: 0.06271957419812679
2024-11-05 02:31:51,366 - INFO - [diffusion][Epoch 9819] diffusion learning rate: 0.001
2024-11-05 02:31:51,368 - INFO - [diffusion][Epoch 9819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:51,369 - INFO - [diffusion][Epoch 9820] Epoch 9821/12000
2024-11-05 02:31:55,543 - INFO - [diffusion][Epoch 9820] diffusion training Loss: 0.06147078704088926
2024-11-05 02:31:55,545 - INFO - [diffusion][Epoch 9820] diffusion learning rate: 0.001
2024-11-05 02:31:55,547 - INFO - [diffusion][Epoch 9820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:55,549 - INFO - [diffusion][Epoch 9821] Epoch 9822/12000
2024-11-05 02:31:59,878 - INFO - [diffusion][Epoch 9821] diffusion training Loss: 0.06617729552090168
2024-11-05 02:31:59,881 - INFO - [diffusion][Epoch 9821] diffusion learning rate: 0.001
2024-11-05 02:31:59,884 - INFO - [diffusion][Epoch 9821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:59,885 - INFO - [diffusion][Epoch 9822] Epoch 9823/12000
2024-11-05 02:32:04,176 - INFO - [diffusion][Epoch 9822] diffusion training Loss: 0.06169081386178732
2024-11-05 02:32:04,178 - INFO - [diffusion][Epoch 9822] diffusion learning rate: 0.001
2024-11-05 02:32:04,210 - INFO - [diffusion][Epoch 9822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:04,212 - INFO - [diffusion][Epoch 9823] Epoch 9824/12000
2024-11-05 02:32:08,469 - INFO - [diffusion][Epoch 9823] diffusion training Loss: 0.06672660261392593
2024-11-05 02:32:08,471 - INFO - [diffusion][Epoch 9823] diffusion learning rate: 0.001
2024-11-05 02:32:08,473 - INFO - [diffusion][Epoch 9823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:08,475 - INFO - [diffusion][Epoch 9824] Epoch 9825/12000
2024-11-05 02:32:12,779 - INFO - [diffusion][Epoch 9824] diffusion training Loss: 0.07017253246158361
2024-11-05 02:32:12,781 - INFO - [diffusion][Epoch 9824] diffusion learning rate: 0.001
2024-11-05 02:32:12,783 - INFO - [diffusion][Epoch 9824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:12,786 - INFO - [diffusion][Epoch 9825] Epoch 9826/12000
2024-11-05 02:32:16,975 - INFO - [diffusion][Epoch 9825] diffusion training Loss: 0.061900075525045395
2024-11-05 02:32:16,977 - INFO - [diffusion][Epoch 9825] diffusion learning rate: 0.001
2024-11-05 02:32:16,979 - INFO - [diffusion][Epoch 9825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:16,980 - INFO - [diffusion][Epoch 9826] Epoch 9827/12000
2024-11-05 02:32:21,211 - INFO - [diffusion][Epoch 9826] diffusion training Loss: 0.06505435705184937
2024-11-05 02:32:21,213 - INFO - [diffusion][Epoch 9826] diffusion learning rate: 0.001
2024-11-05 02:32:21,215 - INFO - [diffusion][Epoch 9826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:21,216 - INFO - [diffusion][Epoch 9827] Epoch 9828/12000
2024-11-05 02:32:25,562 - INFO - [diffusion][Epoch 9827] diffusion training Loss: 0.0634724535048008
2024-11-05 02:32:25,564 - INFO - [diffusion][Epoch 9827] diffusion learning rate: 0.001
2024-11-05 02:32:25,566 - INFO - [diffusion][Epoch 9827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:25,568 - INFO - [diffusion][Epoch 9828] Epoch 9829/12000
2024-11-05 02:32:29,816 - INFO - [diffusion][Epoch 9828] diffusion training Loss: 0.061146458610892296
2024-11-05 02:32:29,818 - INFO - [diffusion][Epoch 9828] diffusion learning rate: 0.001
2024-11-05 02:32:29,820 - INFO - [diffusion][Epoch 9828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:29,821 - INFO - [diffusion][Epoch 9829] Epoch 9830/12000
2024-11-05 02:32:34,058 - INFO - [diffusion][Epoch 9829] diffusion training Loss: 0.06078411918133497
2024-11-05 02:32:34,062 - INFO - [diffusion][Epoch 9829] diffusion learning rate: 0.001
2024-11-05 02:32:34,064 - INFO - [diffusion][Epoch 9829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:34,065 - INFO - [diffusion][Epoch 9830] Epoch 9831/12000
2024-11-05 02:32:38,359 - INFO - [diffusion][Epoch 9830] diffusion training Loss: 0.06244917772710323
2024-11-05 02:32:38,361 - INFO - [diffusion][Epoch 9830] diffusion learning rate: 0.001
2024-11-05 02:32:38,363 - INFO - [diffusion][Epoch 9830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:38,365 - INFO - [diffusion][Epoch 9831] Epoch 9832/12000
2024-11-05 02:32:42,647 - INFO - [diffusion][Epoch 9831] diffusion training Loss: 0.058487892150878906
2024-11-05 02:32:42,650 - INFO - [diffusion][Epoch 9831] diffusion learning rate: 0.001
2024-11-05 02:32:42,651 - INFO - [diffusion][Epoch 9831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:42,653 - INFO - [diffusion][Epoch 9832] Epoch 9833/12000
2024-11-05 02:32:46,860 - INFO - [diffusion][Epoch 9832] diffusion training Loss: 0.06097471620887518
2024-11-05 02:32:46,862 - INFO - [diffusion][Epoch 9832] diffusion learning rate: 0.001
2024-11-05 02:32:46,864 - INFO - [diffusion][Epoch 9832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:46,865 - INFO - [diffusion][Epoch 9833] Epoch 9834/12000
2024-11-05 02:32:51,111 - INFO - [diffusion][Epoch 9833] diffusion training Loss: 0.06525358557701111
2024-11-05 02:32:51,113 - INFO - [diffusion][Epoch 9833] diffusion learning rate: 0.001
2024-11-05 02:32:51,114 - INFO - [diffusion][Epoch 9833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:51,116 - INFO - [diffusion][Epoch 9834] Epoch 9835/12000
2024-11-05 02:32:55,482 - INFO - [diffusion][Epoch 9834] diffusion training Loss: 0.05639389343559742
2024-11-05 02:32:55,485 - INFO - [diffusion][Epoch 9834] diffusion learning rate: 0.001
2024-11-05 02:32:55,487 - INFO - [diffusion][Epoch 9834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:55,488 - INFO - [diffusion][Epoch 9835] Epoch 9836/12000
2024-11-05 02:32:59,772 - INFO - [diffusion][Epoch 9835] diffusion training Loss: 0.06192182470113039
2024-11-05 02:32:59,774 - INFO - [diffusion][Epoch 9835] diffusion learning rate: 0.001
2024-11-05 02:32:59,776 - INFO - [diffusion][Epoch 9835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:59,777 - INFO - [diffusion][Epoch 9836] Epoch 9837/12000
2024-11-05 02:33:03,911 - INFO - [diffusion][Epoch 9836] diffusion training Loss: 0.05741144251078367
2024-11-05 02:33:03,913 - INFO - [diffusion][Epoch 9836] diffusion learning rate: 0.001
2024-11-05 02:33:03,914 - INFO - [diffusion][Epoch 9836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:03,916 - INFO - [diffusion][Epoch 9837] Epoch 9838/12000
2024-11-05 02:33:08,198 - INFO - [diffusion][Epoch 9837] diffusion training Loss: 0.06468862481415272
2024-11-05 02:33:08,200 - INFO - [diffusion][Epoch 9837] diffusion learning rate: 0.001
2024-11-05 02:33:08,202 - INFO - [diffusion][Epoch 9837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:08,203 - INFO - [diffusion][Epoch 9838] Epoch 9839/12000
2024-11-05 02:33:12,455 - INFO - [diffusion][Epoch 9838] diffusion training Loss: 0.05612809117883444
2024-11-05 02:33:12,456 - INFO - [diffusion][Epoch 9838] diffusion learning rate: 0.001
2024-11-05 02:33:12,458 - INFO - [diffusion][Epoch 9838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:12,459 - INFO - [diffusion][Epoch 9839] Epoch 9840/12000
2024-11-05 02:33:16,789 - INFO - [diffusion][Epoch 9839] diffusion training Loss: 0.06455983594059944
2024-11-05 02:33:16,791 - INFO - [diffusion][Epoch 9839] diffusion learning rate: 0.001
2024-11-05 02:33:16,793 - INFO - [diffusion][Epoch 9839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:16,795 - INFO - [diffusion][Epoch 9840] Epoch 9841/12000
2024-11-05 02:33:21,097 - INFO - [diffusion][Epoch 9840] diffusion training Loss: 0.053086746484041214
2024-11-05 02:33:21,099 - INFO - [diffusion][Epoch 9840] diffusion learning rate: 0.001
2024-11-05 02:33:21,101 - INFO - [diffusion][Epoch 9840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:21,103 - INFO - [diffusion][Epoch 9841] Epoch 9842/12000
2024-11-05 02:33:25,257 - INFO - [diffusion][Epoch 9841] diffusion training Loss: 0.06184650305658579
2024-11-05 02:33:25,259 - INFO - [diffusion][Epoch 9841] diffusion learning rate: 0.001
2024-11-05 02:33:25,261 - INFO - [diffusion][Epoch 9841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:25,262 - INFO - [diffusion][Epoch 9842] Epoch 9843/12000
2024-11-05 02:33:29,490 - INFO - [diffusion][Epoch 9842] diffusion training Loss: 0.060747332870960236
2024-11-05 02:33:29,493 - INFO - [diffusion][Epoch 9842] diffusion learning rate: 0.001
2024-11-05 02:33:29,495 - INFO - [diffusion][Epoch 9842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:29,496 - INFO - [diffusion][Epoch 9843] Epoch 9844/12000
2024-11-05 02:33:33,811 - INFO - [diffusion][Epoch 9843] diffusion training Loss: 0.06069173198193312
2024-11-05 02:33:33,813 - INFO - [diffusion][Epoch 9843] diffusion learning rate: 0.001
2024-11-05 02:33:33,815 - INFO - [diffusion][Epoch 9843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:33,816 - INFO - [diffusion][Epoch 9844] Epoch 9845/12000
2024-11-05 02:33:37,690 - INFO - [diffusion][Epoch 9844] diffusion training Loss: 0.06026623863726854
2024-11-05 02:33:37,693 - INFO - [diffusion][Epoch 9844] diffusion learning rate: 0.001
2024-11-05 02:33:37,694 - INFO - [diffusion][Epoch 9844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:37,696 - INFO - [diffusion][Epoch 9845] Epoch 9846/12000
2024-11-05 02:33:41,870 - INFO - [diffusion][Epoch 9845] diffusion training Loss: 0.06717582605779171
2024-11-05 02:33:41,872 - INFO - [diffusion][Epoch 9845] diffusion learning rate: 0.001
2024-11-05 02:33:41,874 - INFO - [diffusion][Epoch 9845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:41,876 - INFO - [diffusion][Epoch 9846] Epoch 9847/12000
2024-11-05 02:33:46,265 - INFO - [diffusion][Epoch 9846] diffusion training Loss: 0.07047210168093443
2024-11-05 02:33:46,267 - INFO - [diffusion][Epoch 9846] diffusion learning rate: 0.001
2024-11-05 02:33:46,269 - INFO - [diffusion][Epoch 9846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:46,270 - INFO - [diffusion][Epoch 9847] Epoch 9848/12000
2024-11-05 02:33:50,523 - INFO - [diffusion][Epoch 9847] diffusion training Loss: 0.06373324431478977
2024-11-05 02:33:50,526 - INFO - [diffusion][Epoch 9847] diffusion learning rate: 0.001
2024-11-05 02:33:50,527 - INFO - [diffusion][Epoch 9847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:50,529 - INFO - [diffusion][Epoch 9848] Epoch 9849/12000
2024-11-05 02:33:54,719 - INFO - [diffusion][Epoch 9848] diffusion training Loss: 0.06269899103790522
2024-11-05 02:33:54,721 - INFO - [diffusion][Epoch 9848] diffusion learning rate: 0.001
2024-11-05 02:33:54,723 - INFO - [diffusion][Epoch 9848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:54,725 - INFO - [diffusion][Epoch 9849] Epoch 9850/12000
2024-11-05 02:33:58,898 - INFO - [diffusion][Epoch 9849] diffusion training Loss: 0.06406820099800825
2024-11-05 02:33:58,900 - INFO - [diffusion][Epoch 9849] diffusion learning rate: 0.001
2024-11-05 02:33:58,902 - INFO - [diffusion][Epoch 9849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:58,903 - INFO - [diffusion][Epoch 9850] Epoch 9851/12000
2024-11-05 02:34:03,170 - INFO - [diffusion][Epoch 9850] diffusion training Loss: 0.058262161910533905
2024-11-05 02:34:03,172 - INFO - [diffusion][Epoch 9850] diffusion learning rate: 0.001
2024-11-05 02:34:03,174 - INFO - [diffusion][Epoch 9850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:03,175 - INFO - [diffusion][Epoch 9851] Epoch 9852/12000
2024-11-05 02:34:07,393 - INFO - [diffusion][Epoch 9851] diffusion training Loss: 0.06504734884947538
2024-11-05 02:34:07,395 - INFO - [diffusion][Epoch 9851] diffusion learning rate: 0.001
2024-11-05 02:34:07,397 - INFO - [diffusion][Epoch 9851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:07,399 - INFO - [diffusion][Epoch 9852] Epoch 9853/12000
2024-11-05 02:34:12,118 - INFO - [diffusion][Epoch 9852] diffusion training Loss: 0.06380430608987808
2024-11-05 02:34:12,313 - INFO - [diffusion][Epoch 9852] diffusion learning rate: 0.001
2024-11-05 02:34:12,315 - INFO - [diffusion][Epoch 9852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:12,316 - INFO - [diffusion][Epoch 9853] Epoch 9854/12000
2024-11-05 02:34:16,542 - INFO - [diffusion][Epoch 9853] diffusion training Loss: 0.06809842586517334
2024-11-05 02:34:16,544 - INFO - [diffusion][Epoch 9853] diffusion learning rate: 0.001
2024-11-05 02:34:16,546 - INFO - [diffusion][Epoch 9853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:16,547 - INFO - [diffusion][Epoch 9854] Epoch 9855/12000
2024-11-05 02:34:20,844 - INFO - [diffusion][Epoch 9854] diffusion training Loss: 0.06462367810308933
2024-11-05 02:34:20,846 - INFO - [diffusion][Epoch 9854] diffusion learning rate: 0.001
2024-11-05 02:34:20,848 - INFO - [diffusion][Epoch 9854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:20,849 - INFO - [diffusion][Epoch 9855] Epoch 9856/12000
2024-11-05 02:34:25,086 - INFO - [diffusion][Epoch 9855] diffusion training Loss: 0.06338364537805319
2024-11-05 02:34:25,088 - INFO - [diffusion][Epoch 9855] diffusion learning rate: 0.001
2024-11-05 02:34:25,090 - INFO - [diffusion][Epoch 9855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:25,092 - INFO - [diffusion][Epoch 9856] Epoch 9857/12000
2024-11-05 02:34:29,358 - INFO - [diffusion][Epoch 9856] diffusion training Loss: 0.06103803962469101
2024-11-05 02:34:29,360 - INFO - [diffusion][Epoch 9856] diffusion learning rate: 0.001
2024-11-05 02:34:29,362 - INFO - [diffusion][Epoch 9856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:29,363 - INFO - [diffusion][Epoch 9857] Epoch 9858/12000
2024-11-05 02:34:33,484 - INFO - [diffusion][Epoch 9857] diffusion training Loss: 0.06442936323583126
2024-11-05 02:34:33,486 - INFO - [diffusion][Epoch 9857] diffusion learning rate: 0.001
2024-11-05 02:34:33,488 - INFO - [diffusion][Epoch 9857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:33,489 - INFO - [diffusion][Epoch 9858] Epoch 9859/12000
2024-11-05 02:34:37,716 - INFO - [diffusion][Epoch 9858] diffusion training Loss: 0.06270155869424343
2024-11-05 02:34:37,719 - INFO - [diffusion][Epoch 9858] diffusion learning rate: 0.001
2024-11-05 02:34:37,721 - INFO - [diffusion][Epoch 9858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:37,722 - INFO - [diffusion][Epoch 9859] Epoch 9860/12000
2024-11-05 02:34:41,995 - INFO - [diffusion][Epoch 9859] diffusion training Loss: 0.06725054234266281
2024-11-05 02:34:41,998 - INFO - [diffusion][Epoch 9859] diffusion learning rate: 0.001
2024-11-05 02:34:42,030 - INFO - [diffusion][Epoch 9859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:42,031 - INFO - [diffusion][Epoch 9860] Epoch 9861/12000
2024-11-05 02:34:46,206 - INFO - [diffusion][Epoch 9860] diffusion training Loss: 0.0636983523145318
2024-11-05 02:34:46,208 - INFO - [diffusion][Epoch 9860] diffusion learning rate: 0.001
2024-11-05 02:34:46,212 - INFO - [diffusion][Epoch 9860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:46,214 - INFO - [diffusion][Epoch 9861] Epoch 9862/12000
2024-11-05 02:34:50,260 - INFO - [diffusion][Epoch 9861] diffusion training Loss: 0.05619495362043381
2024-11-05 02:34:50,262 - INFO - [diffusion][Epoch 9861] diffusion learning rate: 0.001
2024-11-05 02:34:50,264 - INFO - [diffusion][Epoch 9861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:50,265 - INFO - [diffusion][Epoch 9862] Epoch 9863/12000
2024-11-05 02:34:54,223 - INFO - [diffusion][Epoch 9862] diffusion training Loss: 0.059367118403315544
2024-11-05 02:34:54,225 - INFO - [diffusion][Epoch 9862] diffusion learning rate: 0.001
2024-11-05 02:34:54,227 - INFO - [diffusion][Epoch 9862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:54,229 - INFO - [diffusion][Epoch 9863] Epoch 9864/12000
2024-11-05 02:34:58,453 - INFO - [diffusion][Epoch 9863] diffusion training Loss: 0.06256752274930477
2024-11-05 02:34:58,455 - INFO - [diffusion][Epoch 9863] diffusion learning rate: 0.001
2024-11-05 02:34:58,457 - INFO - [diffusion][Epoch 9863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:58,458 - INFO - [diffusion][Epoch 9864] Epoch 9865/12000
2024-11-05 02:35:02,742 - INFO - [diffusion][Epoch 9864] diffusion training Loss: 0.06411655712872744
2024-11-05 02:35:02,744 - INFO - [diffusion][Epoch 9864] diffusion learning rate: 0.001
2024-11-05 02:35:02,746 - INFO - [diffusion][Epoch 9864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:02,747 - INFO - [diffusion][Epoch 9865] Epoch 9866/12000
2024-11-05 02:35:06,988 - INFO - [diffusion][Epoch 9865] diffusion training Loss: 0.06514297984540462
2024-11-05 02:35:06,990 - INFO - [diffusion][Epoch 9865] diffusion learning rate: 0.001
2024-11-05 02:35:06,992 - INFO - [diffusion][Epoch 9865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:06,993 - INFO - [diffusion][Epoch 9866] Epoch 9867/12000
2024-11-05 02:35:11,270 - INFO - [diffusion][Epoch 9866] diffusion training Loss: 0.06279056891798973
2024-11-05 02:35:11,272 - INFO - [diffusion][Epoch 9866] diffusion learning rate: 0.001
2024-11-05 02:35:11,274 - INFO - [diffusion][Epoch 9866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:11,275 - INFO - [diffusion][Epoch 9867] Epoch 9868/12000
2024-11-05 02:35:15,524 - INFO - [diffusion][Epoch 9867] diffusion training Loss: 0.06916516646742821
2024-11-05 02:35:15,526 - INFO - [diffusion][Epoch 9867] diffusion learning rate: 0.001
2024-11-05 02:35:15,528 - INFO - [diffusion][Epoch 9867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:15,529 - INFO - [diffusion][Epoch 9868] Epoch 9869/12000
2024-11-05 02:35:19,733 - INFO - [diffusion][Epoch 9868] diffusion training Loss: 0.06831040605902672
2024-11-05 02:35:19,735 - INFO - [diffusion][Epoch 9868] diffusion learning rate: 0.001
2024-11-05 02:35:19,737 - INFO - [diffusion][Epoch 9868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:19,738 - INFO - [diffusion][Epoch 9869] Epoch 9870/12000
2024-11-05 02:35:24,011 - INFO - [diffusion][Epoch 9869] diffusion training Loss: 0.06276045739650726
2024-11-05 02:35:24,013 - INFO - [diffusion][Epoch 9869] diffusion learning rate: 0.001
2024-11-05 02:35:24,015 - INFO - [diffusion][Epoch 9869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:24,016 - INFO - [diffusion][Epoch 9870] Epoch 9871/12000
2024-11-05 02:35:28,302 - INFO - [diffusion][Epoch 9870] diffusion training Loss: 0.06905571557581425
2024-11-05 02:35:28,304 - INFO - [diffusion][Epoch 9870] diffusion learning rate: 0.001
2024-11-05 02:35:28,306 - INFO - [diffusion][Epoch 9870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:28,307 - INFO - [diffusion][Epoch 9871] Epoch 9872/12000
2024-11-05 02:35:32,481 - INFO - [diffusion][Epoch 9871] diffusion training Loss: 0.06453073490411043
2024-11-05 02:35:32,483 - INFO - [diffusion][Epoch 9871] diffusion learning rate: 0.001
2024-11-05 02:35:32,485 - INFO - [diffusion][Epoch 9871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:32,486 - INFO - [diffusion][Epoch 9872] Epoch 9873/12000
2024-11-05 02:35:37,125 - INFO - [diffusion][Epoch 9872] diffusion training Loss: 0.0646891575306654
2024-11-05 02:35:37,127 - INFO - [diffusion][Epoch 9872] diffusion learning rate: 0.001
2024-11-05 02:35:37,129 - INFO - [diffusion][Epoch 9872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:37,130 - INFO - [diffusion][Epoch 9873] Epoch 9874/12000
2024-11-05 02:35:41,208 - INFO - [diffusion][Epoch 9873] diffusion training Loss: 0.05778096243739128
2024-11-05 02:35:41,211 - INFO - [diffusion][Epoch 9873] diffusion learning rate: 0.001
2024-11-05 02:35:41,212 - INFO - [diffusion][Epoch 9873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:41,214 - INFO - [diffusion][Epoch 9874] Epoch 9875/12000
2024-11-05 02:35:45,629 - INFO - [diffusion][Epoch 9874] diffusion training Loss: 0.05342518538236618
2024-11-05 02:35:45,631 - INFO - [diffusion][Epoch 9874] diffusion learning rate: 0.001
2024-11-05 02:35:45,633 - INFO - [diffusion][Epoch 9874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:45,634 - INFO - [diffusion][Epoch 9875] Epoch 9876/12000
2024-11-05 02:35:49,918 - INFO - [diffusion][Epoch 9875] diffusion training Loss: 0.059243121184408665
2024-11-05 02:35:49,920 - INFO - [diffusion][Epoch 9875] diffusion learning rate: 0.001
2024-11-05 02:35:49,922 - INFO - [diffusion][Epoch 9875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:49,923 - INFO - [diffusion][Epoch 9876] Epoch 9877/12000
2024-11-05 02:35:54,190 - INFO - [diffusion][Epoch 9876] diffusion training Loss: 0.06099137105047703
2024-11-05 02:35:54,192 - INFO - [diffusion][Epoch 9876] diffusion learning rate: 0.001
2024-11-05 02:35:54,196 - INFO - [diffusion][Epoch 9876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:54,197 - INFO - [diffusion][Epoch 9877] Epoch 9878/12000
2024-11-05 02:35:58,340 - INFO - [diffusion][Epoch 9877] diffusion training Loss: 0.06393595319241285
2024-11-05 02:35:58,343 - INFO - [diffusion][Epoch 9877] diffusion learning rate: 0.001
2024-11-05 02:35:58,344 - INFO - [diffusion][Epoch 9877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:58,346 - INFO - [diffusion][Epoch 9878] Epoch 9879/12000
2024-11-05 02:36:02,686 - INFO - [diffusion][Epoch 9878] diffusion training Loss: 0.06335189286619425
2024-11-05 02:36:02,689 - INFO - [diffusion][Epoch 9878] diffusion learning rate: 0.001
2024-11-05 02:36:02,690 - INFO - [diffusion][Epoch 9878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:02,692 - INFO - [diffusion][Epoch 9879] Epoch 9880/12000
2024-11-05 02:36:07,015 - INFO - [diffusion][Epoch 9879] diffusion training Loss: 0.06293535884469748
2024-11-05 02:36:07,017 - INFO - [diffusion][Epoch 9879] diffusion learning rate: 0.001
2024-11-05 02:36:07,019 - INFO - [diffusion][Epoch 9879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:07,020 - INFO - [diffusion][Epoch 9880] Epoch 9881/12000
2024-11-05 02:36:11,227 - INFO - [diffusion][Epoch 9880] diffusion training Loss: 0.05986163020133972
2024-11-05 02:36:11,230 - INFO - [diffusion][Epoch 9880] diffusion learning rate: 0.001
2024-11-05 02:36:11,231 - INFO - [diffusion][Epoch 9880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:11,233 - INFO - [diffusion][Epoch 9881] Epoch 9882/12000
2024-11-05 02:36:15,457 - INFO - [diffusion][Epoch 9881] diffusion training Loss: 0.059794528409838676
2024-11-05 02:36:15,459 - INFO - [diffusion][Epoch 9881] diffusion learning rate: 0.001
2024-11-05 02:36:15,461 - INFO - [diffusion][Epoch 9881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:15,462 - INFO - [diffusion][Epoch 9882] Epoch 9883/12000
2024-11-05 02:36:19,636 - INFO - [diffusion][Epoch 9882] diffusion training Loss: 0.05753623880445957
2024-11-05 02:36:19,638 - INFO - [diffusion][Epoch 9882] diffusion learning rate: 0.001
2024-11-05 02:36:19,640 - INFO - [diffusion][Epoch 9882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:19,641 - INFO - [diffusion][Epoch 9883] Epoch 9884/12000
2024-11-05 02:36:23,827 - INFO - [diffusion][Epoch 9883] diffusion training Loss: 0.06558905448764563
2024-11-05 02:36:23,828 - INFO - [diffusion][Epoch 9883] diffusion learning rate: 0.001
2024-11-05 02:36:23,830 - INFO - [diffusion][Epoch 9883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:23,831 - INFO - [diffusion][Epoch 9884] Epoch 9885/12000
2024-11-05 02:36:28,098 - INFO - [diffusion][Epoch 9884] diffusion training Loss: 0.062022360041737556
2024-11-05 02:36:28,100 - INFO - [diffusion][Epoch 9884] diffusion learning rate: 0.001
2024-11-05 02:36:28,102 - INFO - [diffusion][Epoch 9884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:28,103 - INFO - [diffusion][Epoch 9885] Epoch 9886/12000
2024-11-05 02:36:32,431 - INFO - [diffusion][Epoch 9885] diffusion training Loss: 0.059731004759669304
2024-11-05 02:36:32,433 - INFO - [diffusion][Epoch 9885] diffusion learning rate: 0.001
2024-11-05 02:36:32,453 - INFO - [diffusion][Epoch 9885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:32,455 - INFO - [diffusion][Epoch 9886] Epoch 9887/12000
2024-11-05 02:36:36,768 - INFO - [diffusion][Epoch 9886] diffusion training Loss: 0.06483146548271179
2024-11-05 02:36:36,770 - INFO - [diffusion][Epoch 9886] diffusion learning rate: 0.001
2024-11-05 02:36:36,772 - INFO - [diffusion][Epoch 9886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:36,773 - INFO - [diffusion][Epoch 9887] Epoch 9888/12000
2024-11-05 02:36:41,058 - INFO - [diffusion][Epoch 9887] diffusion training Loss: 0.060708063654601574
2024-11-05 02:36:41,060 - INFO - [diffusion][Epoch 9887] diffusion learning rate: 0.001
2024-11-05 02:36:41,062 - INFO - [diffusion][Epoch 9887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:41,064 - INFO - [diffusion][Epoch 9888] Epoch 9889/12000
2024-11-05 02:36:45,349 - INFO - [diffusion][Epoch 9888] diffusion training Loss: 0.06875332351773977
2024-11-05 02:36:45,351 - INFO - [diffusion][Epoch 9888] diffusion learning rate: 0.001
2024-11-05 02:36:45,353 - INFO - [diffusion][Epoch 9888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:45,354 - INFO - [diffusion][Epoch 9889] Epoch 9890/12000
2024-11-05 02:36:49,650 - INFO - [diffusion][Epoch 9889] diffusion training Loss: 0.06173415295779705
2024-11-05 02:36:49,652 - INFO - [diffusion][Epoch 9889] diffusion learning rate: 0.001
2024-11-05 02:36:49,654 - INFO - [diffusion][Epoch 9889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:49,655 - INFO - [diffusion][Epoch 9890] Epoch 9891/12000
2024-11-05 02:36:53,837 - INFO - [diffusion][Epoch 9890] diffusion training Loss: 0.06283521372824907
2024-11-05 02:36:53,839 - INFO - [diffusion][Epoch 9890] diffusion learning rate: 0.001
2024-11-05 02:36:53,840 - INFO - [diffusion][Epoch 9890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:53,842 - INFO - [diffusion][Epoch 9891] Epoch 9892/12000
2024-11-05 02:36:58,054 - INFO - [diffusion][Epoch 9891] diffusion training Loss: 0.05760866031050682
2024-11-05 02:36:58,055 - INFO - [diffusion][Epoch 9891] diffusion learning rate: 0.001
2024-11-05 02:36:58,057 - INFO - [diffusion][Epoch 9891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:58,058 - INFO - [diffusion][Epoch 9892] Epoch 9893/12000
2024-11-05 02:37:02,350 - INFO - [diffusion][Epoch 9892] diffusion training Loss: 0.06171799171715975
2024-11-05 02:37:02,353 - INFO - [diffusion][Epoch 9892] diffusion learning rate: 0.001
2024-11-05 02:37:02,355 - INFO - [diffusion][Epoch 9892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:02,356 - INFO - [diffusion][Epoch 9893] Epoch 9894/12000
2024-11-05 02:37:06,687 - INFO - [diffusion][Epoch 9893] diffusion training Loss: 0.061976585537195206
2024-11-05 02:37:06,689 - INFO - [diffusion][Epoch 9893] diffusion learning rate: 0.001
2024-11-05 02:37:06,691 - INFO - [diffusion][Epoch 9893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:06,692 - INFO - [diffusion][Epoch 9894] Epoch 9895/12000
2024-11-05 02:37:10,903 - INFO - [diffusion][Epoch 9894] diffusion training Loss: 0.06346478965133429
2024-11-05 02:37:10,905 - INFO - [diffusion][Epoch 9894] diffusion learning rate: 0.001
2024-11-05 02:37:10,907 - INFO - [diffusion][Epoch 9894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:10,908 - INFO - [diffusion][Epoch 9895] Epoch 9896/12000
2024-11-05 02:37:15,129 - INFO - [diffusion][Epoch 9895] diffusion training Loss: 0.05874521844089031
2024-11-05 02:37:15,131 - INFO - [diffusion][Epoch 9895] diffusion learning rate: 0.001
2024-11-05 02:37:15,133 - INFO - [diffusion][Epoch 9895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:15,134 - INFO - [diffusion][Epoch 9896] Epoch 9897/12000
2024-11-05 02:37:19,375 - INFO - [diffusion][Epoch 9896] diffusion training Loss: 0.06182465795427561
2024-11-05 02:37:19,377 - INFO - [diffusion][Epoch 9896] diffusion learning rate: 0.001
2024-11-05 02:37:19,379 - INFO - [diffusion][Epoch 9896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:19,380 - INFO - [diffusion][Epoch 9897] Epoch 9898/12000
2024-11-05 02:37:23,642 - INFO - [diffusion][Epoch 9897] diffusion training Loss: 0.062178113497793674
2024-11-05 02:37:23,644 - INFO - [diffusion][Epoch 9897] diffusion learning rate: 0.001
2024-11-05 02:37:23,646 - INFO - [diffusion][Epoch 9897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:23,647 - INFO - [diffusion][Epoch 9898] Epoch 9899/12000
2024-11-05 02:37:27,861 - INFO - [diffusion][Epoch 9898] diffusion training Loss: 0.0657312236726284
2024-11-05 02:37:27,863 - INFO - [diffusion][Epoch 9898] diffusion learning rate: 0.001
2024-11-05 02:37:27,865 - INFO - [diffusion][Epoch 9898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:27,867 - INFO - [diffusion][Epoch 9899] Epoch 9900/12000
2024-11-05 02:37:32,060 - INFO - [diffusion][Epoch 9899] diffusion training Loss: 0.061326620168983936
2024-11-05 02:37:32,062 - INFO - [diffusion][Epoch 9899] diffusion learning rate: 0.001
2024-11-05 02:37:32,064 - INFO - [diffusion][Epoch 9899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:32,065 - INFO - [diffusion][Epoch 9900] Epoch 9901/12000
2024-11-05 02:37:36,313 - INFO - [diffusion][Epoch 9900] diffusion training Loss: 0.06069184932857752
2024-11-05 02:37:36,315 - INFO - [diffusion][Epoch 9900] diffusion learning rate: 0.001
2024-11-05 02:37:36,317 - INFO - [diffusion][Epoch 9900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:36,318 - INFO - [diffusion][Epoch 9901] Epoch 9902/12000
2024-11-05 02:37:40,564 - INFO - [diffusion][Epoch 9901] diffusion training Loss: 0.057972267270088196
2024-11-05 02:37:40,566 - INFO - [diffusion][Epoch 9901] diffusion learning rate: 0.001
2024-11-05 02:37:40,568 - INFO - [diffusion][Epoch 9901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:40,569 - INFO - [diffusion][Epoch 9902] Epoch 9903/12000
2024-11-05 02:37:44,853 - INFO - [diffusion][Epoch 9902] diffusion training Loss: 0.06244418490678072
2024-11-05 02:37:44,855 - INFO - [diffusion][Epoch 9902] diffusion learning rate: 0.001
2024-11-05 02:37:44,857 - INFO - [diffusion][Epoch 9902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:44,859 - INFO - [diffusion][Epoch 9903] Epoch 9904/12000
2024-11-05 02:37:49,231 - INFO - [diffusion][Epoch 9903] diffusion training Loss: 0.06744878180325031
2024-11-05 02:37:49,233 - INFO - [diffusion][Epoch 9903] diffusion learning rate: 0.001
2024-11-05 02:37:49,236 - INFO - [diffusion][Epoch 9903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:49,237 - INFO - [diffusion][Epoch 9904] Epoch 9905/12000
2024-11-05 02:37:53,439 - INFO - [diffusion][Epoch 9904] diffusion training Loss: 0.06468470022082329
2024-11-05 02:37:53,441 - INFO - [diffusion][Epoch 9904] diffusion learning rate: 0.001
2024-11-05 02:37:53,443 - INFO - [diffusion][Epoch 9904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:53,444 - INFO - [diffusion][Epoch 9905] Epoch 9906/12000
2024-11-05 02:37:57,648 - INFO - [diffusion][Epoch 9905] diffusion training Loss: 0.06326948385685682
2024-11-05 02:37:57,650 - INFO - [diffusion][Epoch 9905] diffusion learning rate: 0.001
2024-11-05 02:37:57,652 - INFO - [diffusion][Epoch 9905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:57,653 - INFO - [diffusion][Epoch 9906] Epoch 9907/12000
2024-11-05 02:38:01,829 - INFO - [diffusion][Epoch 9906] diffusion training Loss: 0.06287114415317774
2024-11-05 02:38:01,831 - INFO - [diffusion][Epoch 9906] diffusion learning rate: 0.001
2024-11-05 02:38:01,833 - INFO - [diffusion][Epoch 9906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:01,834 - INFO - [diffusion][Epoch 9907] Epoch 9908/12000
2024-11-05 02:38:06,200 - INFO - [diffusion][Epoch 9907] diffusion training Loss: 0.06416682060807943
2024-11-05 02:38:06,202 - INFO - [diffusion][Epoch 9907] diffusion learning rate: 0.001
2024-11-05 02:38:06,204 - INFO - [diffusion][Epoch 9907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:06,205 - INFO - [diffusion][Epoch 9908] Epoch 9909/12000
2024-11-05 02:38:10,445 - INFO - [diffusion][Epoch 9908] diffusion training Loss: 0.06312355399131775
2024-11-05 02:38:10,448 - INFO - [diffusion][Epoch 9908] diffusion learning rate: 0.001
2024-11-05 02:38:10,450 - INFO - [diffusion][Epoch 9908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:10,451 - INFO - [diffusion][Epoch 9909] Epoch 9910/12000
2024-11-05 02:38:14,612 - INFO - [diffusion][Epoch 9909] diffusion training Loss: 0.056113351136446
2024-11-05 02:38:14,615 - INFO - [diffusion][Epoch 9909] diffusion learning rate: 0.001
2024-11-05 02:38:14,616 - INFO - [diffusion][Epoch 9909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:14,618 - INFO - [diffusion][Epoch 9910] Epoch 9911/12000
2024-11-05 02:38:18,905 - INFO - [diffusion][Epoch 9910] diffusion training Loss: 0.06032085884362459
2024-11-05 02:38:18,907 - INFO - [diffusion][Epoch 9910] diffusion learning rate: 0.001
2024-11-05 02:38:18,909 - INFO - [diffusion][Epoch 9910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:18,910 - INFO - [diffusion][Epoch 9911] Epoch 9912/12000
2024-11-05 02:38:23,126 - INFO - [diffusion][Epoch 9911] diffusion training Loss: 0.05923966318368912
2024-11-05 02:38:23,128 - INFO - [diffusion][Epoch 9911] diffusion learning rate: 0.001
2024-11-05 02:38:23,130 - INFO - [diffusion][Epoch 9911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:23,131 - INFO - [diffusion][Epoch 9912] Epoch 9913/12000
2024-11-05 02:38:27,423 - INFO - [diffusion][Epoch 9912] diffusion training Loss: 0.06335873436182737
2024-11-05 02:38:27,425 - INFO - [diffusion][Epoch 9912] diffusion learning rate: 0.001
2024-11-05 02:38:27,427 - INFO - [diffusion][Epoch 9912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:27,428 - INFO - [diffusion][Epoch 9913] Epoch 9914/12000
2024-11-05 02:38:31,750 - INFO - [diffusion][Epoch 9913] diffusion training Loss: 0.06572004407644272
2024-11-05 02:38:31,752 - INFO - [diffusion][Epoch 9913] diffusion learning rate: 0.001
2024-11-05 02:38:31,781 - INFO - [diffusion][Epoch 9913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:31,782 - INFO - [diffusion][Epoch 9914] Epoch 9915/12000
2024-11-05 02:38:35,984 - INFO - [diffusion][Epoch 9914] diffusion training Loss: 0.06947590038180351
2024-11-05 02:38:35,987 - INFO - [diffusion][Epoch 9914] diffusion learning rate: 0.001
2024-11-05 02:38:35,989 - INFO - [diffusion][Epoch 9914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:35,991 - INFO - [diffusion][Epoch 9915] Epoch 9916/12000
2024-11-05 02:38:40,183 - INFO - [diffusion][Epoch 9915] diffusion training Loss: 0.06145288422703743
2024-11-05 02:38:40,185 - INFO - [diffusion][Epoch 9915] diffusion learning rate: 0.001
2024-11-05 02:38:40,187 - INFO - [diffusion][Epoch 9915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:40,189 - INFO - [diffusion][Epoch 9916] Epoch 9917/12000
2024-11-05 02:38:44,528 - INFO - [diffusion][Epoch 9916] diffusion training Loss: 0.058412812650203705
2024-11-05 02:38:44,530 - INFO - [diffusion][Epoch 9916] diffusion learning rate: 0.001
2024-11-05 02:38:44,532 - INFO - [diffusion][Epoch 9916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:44,533 - INFO - [diffusion][Epoch 9917] Epoch 9918/12000
2024-11-05 02:38:48,880 - INFO - [diffusion][Epoch 9917] diffusion training Loss: 0.0625301394611597
2024-11-05 02:38:48,882 - INFO - [diffusion][Epoch 9917] diffusion learning rate: 0.001
2024-11-05 02:38:48,883 - INFO - [diffusion][Epoch 9917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:48,885 - INFO - [diffusion][Epoch 9918] Epoch 9919/12000
2024-11-05 02:38:53,147 - INFO - [diffusion][Epoch 9918] diffusion training Loss: 0.060805633664131165
2024-11-05 02:38:53,149 - INFO - [diffusion][Epoch 9918] diffusion learning rate: 0.001
2024-11-05 02:38:53,151 - INFO - [diffusion][Epoch 9918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:53,152 - INFO - [diffusion][Epoch 9919] Epoch 9920/12000
2024-11-05 02:38:57,416 - INFO - [diffusion][Epoch 9919] diffusion training Loss: 0.0599850844591856
2024-11-05 02:38:57,418 - INFO - [diffusion][Epoch 9919] diffusion learning rate: 0.001
2024-11-05 02:38:57,419 - INFO - [diffusion][Epoch 9919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:57,421 - INFO - [diffusion][Epoch 9920] Epoch 9921/12000
2024-11-05 02:39:01,782 - INFO - [diffusion][Epoch 9920] diffusion training Loss: 0.060566579923033714
2024-11-05 02:39:01,784 - INFO - [diffusion][Epoch 9920] diffusion learning rate: 0.001
2024-11-05 02:39:01,786 - INFO - [diffusion][Epoch 9920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:01,788 - INFO - [diffusion][Epoch 9921] Epoch 9922/12000
2024-11-05 02:39:06,059 - INFO - [diffusion][Epoch 9921] diffusion training Loss: 0.05976573657244444
2024-11-05 02:39:06,061 - INFO - [diffusion][Epoch 9921] diffusion learning rate: 0.001
2024-11-05 02:39:06,064 - INFO - [diffusion][Epoch 9921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:06,065 - INFO - [diffusion][Epoch 9922] Epoch 9923/12000
2024-11-05 02:39:10,261 - INFO - [diffusion][Epoch 9922] diffusion training Loss: 0.06275970116257668
2024-11-05 02:39:10,263 - INFO - [diffusion][Epoch 9922] diffusion learning rate: 0.001
2024-11-05 02:39:10,265 - INFO - [diffusion][Epoch 9922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:10,267 - INFO - [diffusion][Epoch 9923] Epoch 9924/12000
2024-11-05 02:39:14,324 - INFO - [diffusion][Epoch 9923] diffusion training Loss: 0.0621601352468133
2024-11-05 02:39:14,327 - INFO - [diffusion][Epoch 9923] diffusion learning rate: 0.001
2024-11-05 02:39:14,329 - INFO - [diffusion][Epoch 9923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:14,331 - INFO - [diffusion][Epoch 9924] Epoch 9925/12000
2024-11-05 02:39:18,136 - INFO - [diffusion][Epoch 9924] diffusion training Loss: 0.06446787435561419
2024-11-05 02:39:18,138 - INFO - [diffusion][Epoch 9924] diffusion learning rate: 0.001
2024-11-05 02:39:18,140 - INFO - [diffusion][Epoch 9924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:18,141 - INFO - [diffusion][Epoch 9925] Epoch 9926/12000
2024-11-05 02:39:22,280 - INFO - [diffusion][Epoch 9925] diffusion training Loss: 0.05813301261514425
2024-11-05 02:39:22,282 - INFO - [diffusion][Epoch 9925] diffusion learning rate: 0.001
2024-11-05 02:39:22,306 - INFO - [diffusion][Epoch 9925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:22,307 - INFO - [diffusion][Epoch 9926] Epoch 9927/12000
2024-11-05 02:39:26,479 - INFO - [diffusion][Epoch 9926] diffusion training Loss: 0.06132178381085396
2024-11-05 02:39:26,481 - INFO - [diffusion][Epoch 9926] diffusion learning rate: 0.001
2024-11-05 02:39:26,483 - INFO - [diffusion][Epoch 9926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:26,485 - INFO - [diffusion][Epoch 9927] Epoch 9928/12000
2024-11-05 02:39:30,570 - INFO - [diffusion][Epoch 9927] diffusion training Loss: 0.06661429442465305
2024-11-05 02:39:30,572 - INFO - [diffusion][Epoch 9927] diffusion learning rate: 0.001
2024-11-05 02:39:30,575 - INFO - [diffusion][Epoch 9927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:30,576 - INFO - [diffusion][Epoch 9928] Epoch 9929/12000
2024-11-05 02:39:34,509 - INFO - [diffusion][Epoch 9928] diffusion training Loss: 0.061660719104111195
2024-11-05 02:39:34,511 - INFO - [diffusion][Epoch 9928] diffusion learning rate: 0.001
2024-11-05 02:39:34,512 - INFO - [diffusion][Epoch 9928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:34,514 - INFO - [diffusion][Epoch 9929] Epoch 9930/12000
2024-11-05 02:39:38,717 - INFO - [diffusion][Epoch 9929] diffusion training Loss: 0.057363515719771385
2024-11-05 02:39:38,719 - INFO - [diffusion][Epoch 9929] diffusion learning rate: 0.001
2024-11-05 02:39:38,721 - INFO - [diffusion][Epoch 9929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:38,723 - INFO - [diffusion][Epoch 9930] Epoch 9931/12000
2024-11-05 02:39:42,857 - INFO - [diffusion][Epoch 9930] diffusion training Loss: 0.05839275661855936
2024-11-05 02:39:42,859 - INFO - [diffusion][Epoch 9930] diffusion learning rate: 0.001
2024-11-05 02:39:42,861 - INFO - [diffusion][Epoch 9930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:42,863 - INFO - [diffusion][Epoch 9931] Epoch 9932/12000
2024-11-05 02:39:47,076 - INFO - [diffusion][Epoch 9931] diffusion training Loss: 0.05993238929659128
2024-11-05 02:39:47,079 - INFO - [diffusion][Epoch 9931] diffusion learning rate: 0.001
2024-11-05 02:39:47,080 - INFO - [diffusion][Epoch 9931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:47,082 - INFO - [diffusion][Epoch 9932] Epoch 9933/12000
2024-11-05 02:39:51,109 - INFO - [diffusion][Epoch 9932] diffusion training Loss: 0.06541142240166664
2024-11-05 02:39:51,111 - INFO - [diffusion][Epoch 9932] diffusion learning rate: 0.001
2024-11-05 02:39:51,113 - INFO - [diffusion][Epoch 9932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:51,114 - INFO - [diffusion][Epoch 9933] Epoch 9934/12000
2024-11-05 02:39:55,008 - INFO - [diffusion][Epoch 9933] diffusion training Loss: 0.06401356682181358
2024-11-05 02:39:55,012 - INFO - [diffusion][Epoch 9933] diffusion learning rate: 0.001
2024-11-05 02:39:55,014 - INFO - [diffusion][Epoch 9933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:55,016 - INFO - [diffusion][Epoch 9934] Epoch 9935/12000
2024-11-05 02:39:59,271 - INFO - [diffusion][Epoch 9934] diffusion training Loss: 0.06653554458171129
2024-11-05 02:39:59,273 - INFO - [diffusion][Epoch 9934] diffusion learning rate: 0.001
2024-11-05 02:39:59,275 - INFO - [diffusion][Epoch 9934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:59,277 - INFO - [diffusion][Epoch 9935] Epoch 9936/12000
2024-11-05 02:40:03,652 - INFO - [diffusion][Epoch 9935] diffusion training Loss: 0.06325260736048222
2024-11-05 02:40:03,655 - INFO - [diffusion][Epoch 9935] diffusion learning rate: 0.001
2024-11-05 02:40:03,696 - INFO - [diffusion][Epoch 9935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:03,697 - INFO - [diffusion][Epoch 9936] Epoch 9937/12000
2024-11-05 02:40:07,774 - INFO - [diffusion][Epoch 9936] diffusion training Loss: 0.06565729808062315
2024-11-05 02:40:07,777 - INFO - [diffusion][Epoch 9936] diffusion learning rate: 0.001
2024-11-05 02:40:07,779 - INFO - [diffusion][Epoch 9936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:07,780 - INFO - [diffusion][Epoch 9937] Epoch 9938/12000
2024-11-05 02:40:11,843 - INFO - [diffusion][Epoch 9937] diffusion training Loss: 0.06202036887407303
2024-11-05 02:40:11,846 - INFO - [diffusion][Epoch 9937] diffusion learning rate: 0.001
2024-11-05 02:40:11,848 - INFO - [diffusion][Epoch 9937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:11,849 - INFO - [diffusion][Epoch 9938] Epoch 9939/12000
2024-11-05 02:40:15,861 - INFO - [diffusion][Epoch 9938] diffusion training Loss: 0.0635217484086752
2024-11-05 02:40:15,863 - INFO - [diffusion][Epoch 9938] diffusion learning rate: 0.001
2024-11-05 02:40:15,866 - INFO - [diffusion][Epoch 9938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:15,867 - INFO - [diffusion][Epoch 9939] Epoch 9940/12000
2024-11-05 02:40:20,020 - INFO - [diffusion][Epoch 9939] diffusion training Loss: 0.05873417388647795
2024-11-05 02:40:20,023 - INFO - [diffusion][Epoch 9939] diffusion learning rate: 0.001
2024-11-05 02:40:20,025 - INFO - [diffusion][Epoch 9939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:20,026 - INFO - [diffusion][Epoch 9940] Epoch 9941/12000
2024-11-05 02:40:24,275 - INFO - [diffusion][Epoch 9940] diffusion training Loss: 0.05858621187508106
2024-11-05 02:40:24,277 - INFO - [diffusion][Epoch 9940] diffusion learning rate: 0.001
2024-11-05 02:40:24,327 - INFO - [diffusion][Epoch 9940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:24,329 - INFO - [diffusion][Epoch 9941] Epoch 9942/12000
2024-11-05 02:40:28,655 - INFO - [diffusion][Epoch 9941] diffusion training Loss: 0.05604194663465023
2024-11-05 02:40:28,657 - INFO - [diffusion][Epoch 9941] diffusion learning rate: 0.001
2024-11-05 02:40:28,659 - INFO - [diffusion][Epoch 9941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:28,660 - INFO - [diffusion][Epoch 9942] Epoch 9943/12000
2024-11-05 02:40:32,578 - INFO - [diffusion][Epoch 9942] diffusion training Loss: 0.0654690582305193
2024-11-05 02:40:32,583 - INFO - [diffusion][Epoch 9942] diffusion learning rate: 0.001
2024-11-05 02:40:32,585 - INFO - [diffusion][Epoch 9942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:32,586 - INFO - [diffusion][Epoch 9943] Epoch 9944/12000
2024-11-05 02:40:36,736 - INFO - [diffusion][Epoch 9943] diffusion training Loss: 0.06117249187082052
2024-11-05 02:40:36,738 - INFO - [diffusion][Epoch 9943] diffusion learning rate: 0.001
2024-11-05 02:40:36,740 - INFO - [diffusion][Epoch 9943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:36,741 - INFO - [diffusion][Epoch 9944] Epoch 9945/12000
2024-11-05 02:40:41,017 - INFO - [diffusion][Epoch 9944] diffusion training Loss: 0.06420840322971344
2024-11-05 02:40:41,019 - INFO - [diffusion][Epoch 9944] diffusion learning rate: 0.001
2024-11-05 02:40:41,021 - INFO - [diffusion][Epoch 9944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:41,023 - INFO - [diffusion][Epoch 9945] Epoch 9946/12000
2024-11-05 02:40:45,196 - INFO - [diffusion][Epoch 9945] diffusion training Loss: 0.06292376946657896
2024-11-05 02:40:45,199 - INFO - [diffusion][Epoch 9945] diffusion learning rate: 0.001
2024-11-05 02:40:45,200 - INFO - [diffusion][Epoch 9945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:45,202 - INFO - [diffusion][Epoch 9946] Epoch 9947/12000
2024-11-05 02:40:49,362 - INFO - [diffusion][Epoch 9946] diffusion training Loss: 0.05862638633698225
2024-11-05 02:40:49,364 - INFO - [diffusion][Epoch 9946] diffusion learning rate: 0.001
2024-11-05 02:40:49,366 - INFO - [diffusion][Epoch 9946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:49,368 - INFO - [diffusion][Epoch 9947] Epoch 9948/12000
2024-11-05 02:40:53,631 - INFO - [diffusion][Epoch 9947] diffusion training Loss: 0.0672133956104517
2024-11-05 02:40:53,634 - INFO - [diffusion][Epoch 9947] diffusion learning rate: 0.001
2024-11-05 02:40:53,635 - INFO - [diffusion][Epoch 9947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:53,637 - INFO - [diffusion][Epoch 9948] Epoch 9949/12000
2024-11-05 02:40:57,685 - INFO - [diffusion][Epoch 9948] diffusion training Loss: 0.062189809046685696
2024-11-05 02:40:57,687 - INFO - [diffusion][Epoch 9948] diffusion learning rate: 0.001
2024-11-05 02:40:57,688 - INFO - [diffusion][Epoch 9948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:57,690 - INFO - [diffusion][Epoch 9949] Epoch 9950/12000
2024-11-05 02:41:01,934 - INFO - [diffusion][Epoch 9949] diffusion training Loss: 0.06401568092405796
2024-11-05 02:41:01,979 - INFO - [diffusion][Epoch 9949] diffusion learning rate: 0.001
2024-11-05 02:41:01,981 - INFO - [diffusion][Epoch 9949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:01,983 - INFO - [diffusion][Epoch 9950] Epoch 9951/12000
2024-11-05 02:41:06,162 - INFO - [diffusion][Epoch 9950] diffusion training Loss: 0.058526162058115005
2024-11-05 02:41:06,164 - INFO - [diffusion][Epoch 9950] diffusion learning rate: 0.001
2024-11-05 02:41:06,166 - INFO - [diffusion][Epoch 9950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:06,167 - INFO - [diffusion][Epoch 9951] Epoch 9952/12000
2024-11-05 02:41:10,430 - INFO - [diffusion][Epoch 9951] diffusion training Loss: 0.06342273857444525
2024-11-05 02:41:10,432 - INFO - [diffusion][Epoch 9951] diffusion learning rate: 0.001
2024-11-05 02:41:10,434 - INFO - [diffusion][Epoch 9951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:10,435 - INFO - [diffusion][Epoch 9952] Epoch 9953/12000
2024-11-05 02:41:14,570 - INFO - [diffusion][Epoch 9952] diffusion training Loss: 0.0613179923966527
2024-11-05 02:41:14,572 - INFO - [diffusion][Epoch 9952] diffusion learning rate: 0.001
2024-11-05 02:41:14,573 - INFO - [diffusion][Epoch 9952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:14,575 - INFO - [diffusion][Epoch 9953] Epoch 9954/12000
2024-11-05 02:41:18,960 - INFO - [diffusion][Epoch 9953] diffusion training Loss: 0.06343499198555946
2024-11-05 02:41:18,962 - INFO - [diffusion][Epoch 9953] diffusion learning rate: 0.001
2024-11-05 02:41:18,963 - INFO - [diffusion][Epoch 9953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:18,965 - INFO - [diffusion][Epoch 9954] Epoch 9955/12000
2024-11-05 02:41:23,100 - INFO - [diffusion][Epoch 9954] diffusion training Loss: 0.05875017400830984
2024-11-05 02:41:23,103 - INFO - [diffusion][Epoch 9954] diffusion learning rate: 0.001
2024-11-05 02:41:23,105 - INFO - [diffusion][Epoch 9954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:23,106 - INFO - [diffusion][Epoch 9955] Epoch 9956/12000
2024-11-05 02:41:27,220 - INFO - [diffusion][Epoch 9955] diffusion training Loss: 0.05507004726678133
2024-11-05 02:41:27,222 - INFO - [diffusion][Epoch 9955] diffusion learning rate: 0.001
2024-11-05 02:41:27,223 - INFO - [diffusion][Epoch 9955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:27,225 - INFO - [diffusion][Epoch 9956] Epoch 9957/12000
2024-11-05 02:41:31,360 - INFO - [diffusion][Epoch 9956] diffusion training Loss: 0.06560355611145496
2024-11-05 02:41:31,362 - INFO - [diffusion][Epoch 9956] diffusion learning rate: 0.001
2024-11-05 02:41:31,364 - INFO - [diffusion][Epoch 9956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:31,365 - INFO - [diffusion][Epoch 9957] Epoch 9958/12000
2024-11-05 02:41:35,333 - INFO - [diffusion][Epoch 9957] diffusion training Loss: 0.06361236423254013
2024-11-05 02:41:35,335 - INFO - [diffusion][Epoch 9957] diffusion learning rate: 0.001
2024-11-05 02:41:35,372 - INFO - [diffusion][Epoch 9957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:35,374 - INFO - [diffusion][Epoch 9958] Epoch 9959/12000
2024-11-05 02:41:39,543 - INFO - [diffusion][Epoch 9958] diffusion training Loss: 0.0604677926748991
2024-11-05 02:41:39,544 - INFO - [diffusion][Epoch 9958] diffusion learning rate: 0.001
2024-11-05 02:41:39,546 - INFO - [diffusion][Epoch 9958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:39,548 - INFO - [diffusion][Epoch 9959] Epoch 9960/12000
2024-11-05 02:41:43,839 - INFO - [diffusion][Epoch 9959] diffusion training Loss: 0.06983655504882336
2024-11-05 02:41:43,841 - INFO - [diffusion][Epoch 9959] diffusion learning rate: 0.001
2024-11-05 02:41:43,843 - INFO - [diffusion][Epoch 9959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:43,844 - INFO - [diffusion][Epoch 9960] Epoch 9961/12000
2024-11-05 02:41:48,067 - INFO - [diffusion][Epoch 9960] diffusion training Loss: 0.06417297758162022
2024-11-05 02:41:48,070 - INFO - [diffusion][Epoch 9960] diffusion learning rate: 0.001
2024-11-05 02:41:48,072 - INFO - [diffusion][Epoch 9960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:48,073 - INFO - [diffusion][Epoch 9961] Epoch 9962/12000
2024-11-05 02:41:52,500 - INFO - [diffusion][Epoch 9961] diffusion training Loss: 0.057723985984921455
2024-11-05 02:41:52,503 - INFO - [diffusion][Epoch 9961] diffusion learning rate: 0.001
2024-11-05 02:41:52,505 - INFO - [diffusion][Epoch 9961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:52,506 - INFO - [diffusion][Epoch 9962] Epoch 9963/12000
2024-11-05 02:41:56,788 - INFO - [diffusion][Epoch 9962] diffusion training Loss: 0.06205444410443306
2024-11-05 02:41:56,790 - INFO - [diffusion][Epoch 9962] diffusion learning rate: 0.001
2024-11-05 02:41:56,843 - INFO - [diffusion][Epoch 9962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:56,844 - INFO - [diffusion][Epoch 9963] Epoch 9964/12000
2024-11-05 02:42:01,030 - INFO - [diffusion][Epoch 9963] diffusion training Loss: 0.06317048147320747
2024-11-05 02:42:01,033 - INFO - [diffusion][Epoch 9963] diffusion learning rate: 0.001
2024-11-05 02:42:01,035 - INFO - [diffusion][Epoch 9963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:01,036 - INFO - [diffusion][Epoch 9964] Epoch 9965/12000
2024-11-05 02:42:05,163 - INFO - [diffusion][Epoch 9964] diffusion training Loss: 0.06589584145694971
2024-11-05 02:42:05,165 - INFO - [diffusion][Epoch 9964] diffusion learning rate: 0.001
2024-11-05 02:42:05,166 - INFO - [diffusion][Epoch 9964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:05,168 - INFO - [diffusion][Epoch 9965] Epoch 9966/12000
2024-11-05 02:42:09,381 - INFO - [diffusion][Epoch 9965] diffusion training Loss: 0.061581024900078773
2024-11-05 02:42:09,382 - INFO - [diffusion][Epoch 9965] diffusion learning rate: 0.001
2024-11-05 02:42:09,384 - INFO - [diffusion][Epoch 9965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:09,385 - INFO - [diffusion][Epoch 9966] Epoch 9967/12000
2024-11-05 02:42:13,437 - INFO - [diffusion][Epoch 9966] diffusion training Loss: 0.07152844779193401
2024-11-05 02:42:13,439 - INFO - [diffusion][Epoch 9966] diffusion learning rate: 0.001
2024-11-05 02:42:13,441 - INFO - [diffusion][Epoch 9966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:13,442 - INFO - [diffusion][Epoch 9967] Epoch 9968/12000
2024-11-05 02:42:17,497 - INFO - [diffusion][Epoch 9967] diffusion training Loss: 0.059004765935242176
2024-11-05 02:42:17,499 - INFO - [diffusion][Epoch 9967] diffusion learning rate: 0.001
2024-11-05 02:42:17,501 - INFO - [diffusion][Epoch 9967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:17,502 - INFO - [diffusion][Epoch 9968] Epoch 9969/12000
2024-11-05 02:42:21,725 - INFO - [diffusion][Epoch 9968] diffusion training Loss: 0.06420017685741186
2024-11-05 02:42:21,728 - INFO - [diffusion][Epoch 9968] diffusion learning rate: 0.001
2024-11-05 02:42:21,730 - INFO - [diffusion][Epoch 9968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:21,731 - INFO - [diffusion][Epoch 9969] Epoch 9970/12000
2024-11-05 02:42:25,863 - INFO - [diffusion][Epoch 9969] diffusion training Loss: 0.06070618983358145
2024-11-05 02:42:25,865 - INFO - [diffusion][Epoch 9969] diffusion learning rate: 0.001
2024-11-05 02:42:25,866 - INFO - [diffusion][Epoch 9969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:25,868 - INFO - [diffusion][Epoch 9970] Epoch 9971/12000
2024-11-05 02:42:29,878 - INFO - [diffusion][Epoch 9970] diffusion training Loss: 0.06436687521636486
2024-11-05 02:42:29,880 - INFO - [diffusion][Epoch 9970] diffusion learning rate: 0.001
2024-11-05 02:42:29,881 - INFO - [diffusion][Epoch 9970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:29,882 - INFO - [diffusion][Epoch 9971] Epoch 9972/12000
2024-11-05 02:42:34,056 - INFO - [diffusion][Epoch 9971] diffusion training Loss: 0.0586017956957221
2024-11-05 02:42:34,058 - INFO - [diffusion][Epoch 9971] diffusion learning rate: 0.001
2024-11-05 02:42:34,060 - INFO - [diffusion][Epoch 9971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:34,061 - INFO - [diffusion][Epoch 9972] Epoch 9973/12000
2024-11-05 02:42:38,310 - INFO - [diffusion][Epoch 9972] diffusion training Loss: 0.06745048053562641
2024-11-05 02:42:38,311 - INFO - [diffusion][Epoch 9972] diffusion learning rate: 0.001
2024-11-05 02:42:38,313 - INFO - [diffusion][Epoch 9972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:38,314 - INFO - [diffusion][Epoch 9973] Epoch 9974/12000
2024-11-05 02:42:42,564 - INFO - [diffusion][Epoch 9973] diffusion training Loss: 0.06492078024893999
2024-11-05 02:42:42,567 - INFO - [diffusion][Epoch 9973] diffusion learning rate: 0.001
2024-11-05 02:42:42,568 - INFO - [diffusion][Epoch 9973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:42,570 - INFO - [diffusion][Epoch 9974] Epoch 9975/12000
2024-11-05 02:42:47,648 - INFO - [diffusion][Epoch 9974] diffusion training Loss: 0.056622520089149475
2024-11-05 02:42:47,651 - INFO - [diffusion][Epoch 9974] diffusion learning rate: 0.001
2024-11-05 02:42:47,652 - INFO - [diffusion][Epoch 9974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:47,654 - INFO - [diffusion][Epoch 9975] Epoch 9976/12000
2024-11-05 02:42:51,720 - INFO - [diffusion][Epoch 9975] diffusion training Loss: 0.06202789396047592
2024-11-05 02:42:51,722 - INFO - [diffusion][Epoch 9975] diffusion learning rate: 0.001
2024-11-05 02:42:51,750 - INFO - [diffusion][Epoch 9975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:51,751 - INFO - [diffusion][Epoch 9976] Epoch 9977/12000
2024-11-05 02:42:55,932 - INFO - [diffusion][Epoch 9976] diffusion training Loss: 0.06730310712009668
2024-11-05 02:42:55,935 - INFO - [diffusion][Epoch 9976] diffusion learning rate: 0.001
2024-11-05 02:42:55,937 - INFO - [diffusion][Epoch 9976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:55,939 - INFO - [diffusion][Epoch 9977] Epoch 9978/12000
2024-11-05 02:43:00,021 - INFO - [diffusion][Epoch 9977] diffusion training Loss: 0.06285271421074867
2024-11-05 02:43:00,024 - INFO - [diffusion][Epoch 9977] diffusion learning rate: 0.001
2024-11-05 02:43:00,026 - INFO - [diffusion][Epoch 9977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:00,027 - INFO - [diffusion][Epoch 9978] Epoch 9979/12000
2024-11-05 02:43:04,262 - INFO - [diffusion][Epoch 9978] diffusion training Loss: 0.06224354822188616
2024-11-05 02:43:04,264 - INFO - [diffusion][Epoch 9978] diffusion learning rate: 0.001
2024-11-05 02:43:04,266 - INFO - [diffusion][Epoch 9978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:04,267 - INFO - [diffusion][Epoch 9979] Epoch 9980/12000
2024-11-05 02:43:08,504 - INFO - [diffusion][Epoch 9979] diffusion training Loss: 0.0637670960277319
2024-11-05 02:43:08,506 - INFO - [diffusion][Epoch 9979] diffusion learning rate: 0.001
2024-11-05 02:43:08,507 - INFO - [diffusion][Epoch 9979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:08,509 - INFO - [diffusion][Epoch 9980] Epoch 9981/12000
2024-11-05 02:43:12,719 - INFO - [diffusion][Epoch 9980] diffusion training Loss: 0.06205848231911659
2024-11-05 02:43:12,722 - INFO - [diffusion][Epoch 9980] diffusion learning rate: 0.001
2024-11-05 02:43:12,724 - INFO - [diffusion][Epoch 9980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:12,726 - INFO - [diffusion][Epoch 9981] Epoch 9982/12000
2024-11-05 02:43:16,977 - INFO - [diffusion][Epoch 9981] diffusion training Loss: 0.061984190717339516
2024-11-05 02:43:16,979 - INFO - [diffusion][Epoch 9981] diffusion learning rate: 0.001
2024-11-05 02:43:16,980 - INFO - [diffusion][Epoch 9981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:16,982 - INFO - [diffusion][Epoch 9982] Epoch 9983/12000
2024-11-05 02:43:21,268 - INFO - [diffusion][Epoch 9982] diffusion training Loss: 0.058524468913674355
2024-11-05 02:43:21,270 - INFO - [diffusion][Epoch 9982] diffusion learning rate: 0.001
2024-11-05 02:43:21,272 - INFO - [diffusion][Epoch 9982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:21,273 - INFO - [diffusion][Epoch 9983] Epoch 9984/12000
2024-11-05 02:43:25,597 - INFO - [diffusion][Epoch 9983] diffusion training Loss: 0.05387443583458662
2024-11-05 02:43:25,599 - INFO - [diffusion][Epoch 9983] diffusion learning rate: 0.001
2024-11-05 02:43:25,601 - INFO - [diffusion][Epoch 9983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:25,602 - INFO - [diffusion][Epoch 9984] Epoch 9985/12000
2024-11-05 02:43:29,931 - INFO - [diffusion][Epoch 9984] diffusion training Loss: 0.05944641772657633
2024-11-05 02:43:29,933 - INFO - [diffusion][Epoch 9984] diffusion learning rate: 0.001
2024-11-05 02:43:29,935 - INFO - [diffusion][Epoch 9984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:29,937 - INFO - [diffusion][Epoch 9985] Epoch 9986/12000
2024-11-05 02:43:34,170 - INFO - [diffusion][Epoch 9985] diffusion training Loss: 0.0603177510201931
2024-11-05 02:43:34,172 - INFO - [diffusion][Epoch 9985] diffusion learning rate: 0.001
2024-11-05 02:43:34,173 - INFO - [diffusion][Epoch 9985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:34,175 - INFO - [diffusion][Epoch 9986] Epoch 9987/12000
2024-11-05 02:43:38,373 - INFO - [diffusion][Epoch 9986] diffusion training Loss: 0.06289353966712952
2024-11-05 02:43:38,375 - INFO - [diffusion][Epoch 9986] diffusion learning rate: 0.001
2024-11-05 02:43:38,377 - INFO - [diffusion][Epoch 9986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:38,378 - INFO - [diffusion][Epoch 9987] Epoch 9988/12000
2024-11-05 02:43:42,174 - INFO - [diffusion][Epoch 9987] diffusion training Loss: 0.057432075031101704
2024-11-05 02:43:42,176 - INFO - [diffusion][Epoch 9987] diffusion learning rate: 0.001
2024-11-05 02:43:42,177 - INFO - [diffusion][Epoch 9987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:42,179 - INFO - [diffusion][Epoch 9988] Epoch 9989/12000
2024-11-05 02:43:46,448 - INFO - [diffusion][Epoch 9988] diffusion training Loss: 0.05864883121103048
2024-11-05 02:43:46,451 - INFO - [diffusion][Epoch 9988] diffusion learning rate: 0.001
2024-11-05 02:43:46,453 - INFO - [diffusion][Epoch 9988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:46,454 - INFO - [diffusion][Epoch 9989] Epoch 9990/12000
2024-11-05 02:43:50,433 - INFO - [diffusion][Epoch 9989] diffusion training Loss: 0.057998950593173504
2024-11-05 02:43:50,436 - INFO - [diffusion][Epoch 9989] diffusion learning rate: 0.001
2024-11-05 02:43:50,438 - INFO - [diffusion][Epoch 9989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:50,439 - INFO - [diffusion][Epoch 9990] Epoch 9991/12000
2024-11-05 02:43:54,706 - INFO - [diffusion][Epoch 9990] diffusion training Loss: 0.05948862247169018
2024-11-05 02:43:54,708 - INFO - [diffusion][Epoch 9990] diffusion learning rate: 0.001
2024-11-05 02:43:54,710 - INFO - [diffusion][Epoch 9990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:54,711 - INFO - [diffusion][Epoch 9991] Epoch 9992/12000
2024-11-05 02:43:58,909 - INFO - [diffusion][Epoch 9991] diffusion training Loss: 0.05953659303486347
2024-11-05 02:43:58,911 - INFO - [diffusion][Epoch 9991] diffusion learning rate: 0.001
2024-11-05 02:43:58,913 - INFO - [diffusion][Epoch 9991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:58,914 - INFO - [diffusion][Epoch 9992] Epoch 9993/12000
2024-11-05 02:44:03,214 - INFO - [diffusion][Epoch 9992] diffusion training Loss: 0.0644275601953268
2024-11-05 02:44:03,217 - INFO - [diffusion][Epoch 9992] diffusion learning rate: 0.001
2024-11-05 02:44:03,222 - INFO - [diffusion][Epoch 9992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:03,223 - INFO - [diffusion][Epoch 9993] Epoch 9994/12000
2024-11-05 02:44:07,498 - INFO - [diffusion][Epoch 9993] diffusion training Loss: 0.06156041566282511
2024-11-05 02:44:07,500 - INFO - [diffusion][Epoch 9993] diffusion learning rate: 0.001
2024-11-05 02:44:07,502 - INFO - [diffusion][Epoch 9993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:07,504 - INFO - [diffusion][Epoch 9994] Epoch 9995/12000
2024-11-05 02:44:11,779 - INFO - [diffusion][Epoch 9994] diffusion training Loss: 0.06083599850535393
2024-11-05 02:44:11,781 - INFO - [diffusion][Epoch 9994] diffusion learning rate: 0.001
2024-11-05 02:44:11,783 - INFO - [diffusion][Epoch 9994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:11,784 - INFO - [diffusion][Epoch 9995] Epoch 9996/12000
2024-11-05 02:44:16,063 - INFO - [diffusion][Epoch 9995] diffusion training Loss: 0.060909016989171505
2024-11-05 02:44:16,065 - INFO - [diffusion][Epoch 9995] diffusion learning rate: 0.001
2024-11-05 02:44:16,067 - INFO - [diffusion][Epoch 9995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:16,068 - INFO - [diffusion][Epoch 9996] Epoch 9997/12000
2024-11-05 02:44:20,261 - INFO - [diffusion][Epoch 9996] diffusion training Loss: 0.06876163743436337
2024-11-05 02:44:20,263 - INFO - [diffusion][Epoch 9996] diffusion learning rate: 0.001
2024-11-05 02:44:20,265 - INFO - [diffusion][Epoch 9996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:20,266 - INFO - [diffusion][Epoch 9997] Epoch 9998/12000
2024-11-05 02:44:24,533 - INFO - [diffusion][Epoch 9997] diffusion training Loss: 0.06139224302023649
2024-11-05 02:44:24,535 - INFO - [diffusion][Epoch 9997] diffusion learning rate: 0.001
2024-11-05 02:44:24,564 - INFO - [diffusion][Epoch 9997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:24,565 - INFO - [diffusion][Epoch 9998] Epoch 9999/12000
2024-11-05 02:44:28,815 - INFO - [diffusion][Epoch 9998] diffusion training Loss: 0.06046663410961628
2024-11-05 02:44:28,817 - INFO - [diffusion][Epoch 9998] diffusion learning rate: 0.001
2024-11-05 02:44:28,819 - INFO - [diffusion][Epoch 9998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:28,820 - INFO - [diffusion][Epoch 9999] Epoch 10000/12000
2024-11-05 02:44:33,055 - INFO - [diffusion][Epoch 9999] diffusion training Loss: 0.05680472031235695
2024-11-05 02:44:33,057 - INFO - [diffusion][Epoch 9999] diffusion learning rate: 0.001
2024-11-05 02:44:33,171 - INFO - [diffusion][Epoch 9999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:33,172 - INFO - [diffusion][Epoch 10000] Epoch 10001/12000
2024-11-05 02:44:37,462 - INFO - [diffusion][Epoch 10000] diffusion training Loss: 0.061490194872021675
2024-11-05 02:44:37,465 - INFO - [diffusion][Epoch 10000] diffusion learning rate: 0.001
2024-11-05 02:44:37,467 - INFO - [diffusion][Epoch 10000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:37,468 - INFO - [diffusion][Epoch 10001] Epoch 10002/12000
2024-11-05 02:44:41,672 - INFO - [diffusion][Epoch 10001] diffusion training Loss: 0.06467159651219845
2024-11-05 02:44:41,674 - INFO - [diffusion][Epoch 10001] diffusion learning rate: 0.001
2024-11-05 02:44:41,676 - INFO - [diffusion][Epoch 10001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:41,678 - INFO - [diffusion][Epoch 10002] Epoch 10003/12000
2024-11-05 02:44:45,905 - INFO - [diffusion][Epoch 10002] diffusion training Loss: 0.060393547639250755
2024-11-05 02:44:45,907 - INFO - [diffusion][Epoch 10002] diffusion learning rate: 0.001
2024-11-05 02:44:45,909 - INFO - [diffusion][Epoch 10002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:45,910 - INFO - [diffusion][Epoch 10003] Epoch 10004/12000
2024-11-05 02:44:49,975 - INFO - [diffusion][Epoch 10003] diffusion training Loss: 0.06559509504586458
2024-11-05 02:44:49,977 - INFO - [diffusion][Epoch 10003] diffusion learning rate: 0.001
2024-11-05 02:44:49,979 - INFO - [diffusion][Epoch 10003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:49,980 - INFO - [diffusion][Epoch 10004] Epoch 10005/12000
2024-11-05 02:44:54,250 - INFO - [diffusion][Epoch 10004] diffusion training Loss: 0.06114300340414047
2024-11-05 02:44:54,252 - INFO - [diffusion][Epoch 10004] diffusion learning rate: 0.001
2024-11-05 02:44:54,255 - INFO - [diffusion][Epoch 10004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:54,256 - INFO - [diffusion][Epoch 10005] Epoch 10006/12000
2024-11-05 02:44:58,336 - INFO - [diffusion][Epoch 10005] diffusion training Loss: 0.06345605477690697
2024-11-05 02:44:58,338 - INFO - [diffusion][Epoch 10005] diffusion learning rate: 0.001
2024-11-05 02:44:58,340 - INFO - [diffusion][Epoch 10005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:58,342 - INFO - [diffusion][Epoch 10006] Epoch 10007/12000
2024-11-05 02:45:02,574 - INFO - [diffusion][Epoch 10006] diffusion training Loss: 0.06280631199479103
2024-11-05 02:45:02,576 - INFO - [diffusion][Epoch 10006] diffusion learning rate: 0.001
2024-11-05 02:45:02,606 - INFO - [diffusion][Epoch 10006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:02,607 - INFO - [diffusion][Epoch 10007] Epoch 10008/12000
2024-11-05 02:45:06,873 - INFO - [diffusion][Epoch 10007] diffusion training Loss: 0.06203954108059406
2024-11-05 02:45:06,875 - INFO - [diffusion][Epoch 10007] diffusion learning rate: 0.001
2024-11-05 02:45:06,877 - INFO - [diffusion][Epoch 10007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:06,878 - INFO - [diffusion][Epoch 10008] Epoch 10009/12000
2024-11-05 02:45:11,015 - INFO - [diffusion][Epoch 10008] diffusion training Loss: 0.06271294970065355
2024-11-05 02:45:11,018 - INFO - [diffusion][Epoch 10008] diffusion learning rate: 0.001
2024-11-05 02:45:11,020 - INFO - [diffusion][Epoch 10008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:11,021 - INFO - [diffusion][Epoch 10009] Epoch 10010/12000
2024-11-05 02:45:15,280 - INFO - [diffusion][Epoch 10009] diffusion training Loss: 0.06284021213650703
2024-11-05 02:45:15,282 - INFO - [diffusion][Epoch 10009] diffusion learning rate: 0.001
2024-11-05 02:45:15,284 - INFO - [diffusion][Epoch 10009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:15,285 - INFO - [diffusion][Epoch 10010] Epoch 10011/12000
2024-11-05 02:45:19,689 - INFO - [diffusion][Epoch 10010] diffusion training Loss: 0.061455393210053444
2024-11-05 02:45:19,691 - INFO - [diffusion][Epoch 10010] diffusion learning rate: 0.001
2024-11-05 02:45:19,693 - INFO - [diffusion][Epoch 10010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:19,695 - INFO - [diffusion][Epoch 10011] Epoch 10012/12000
2024-11-05 02:45:23,903 - INFO - [diffusion][Epoch 10011] diffusion training Loss: 0.06308718025684357
2024-11-05 02:45:23,905 - INFO - [diffusion][Epoch 10011] diffusion learning rate: 0.001
2024-11-05 02:45:23,907 - INFO - [diffusion][Epoch 10011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:23,908 - INFO - [diffusion][Epoch 10012] Epoch 10013/12000
2024-11-05 02:45:28,203 - INFO - [diffusion][Epoch 10012] diffusion training Loss: 0.06434114370495081
2024-11-05 02:45:28,205 - INFO - [diffusion][Epoch 10012] diffusion learning rate: 0.001
2024-11-05 02:45:28,206 - INFO - [diffusion][Epoch 10012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:28,208 - INFO - [diffusion][Epoch 10013] Epoch 10014/12000
2024-11-05 02:45:32,382 - INFO - [diffusion][Epoch 10013] diffusion training Loss: 0.06043406389653683
2024-11-05 02:45:32,384 - INFO - [diffusion][Epoch 10013] diffusion learning rate: 0.001
2024-11-05 02:45:32,386 - INFO - [diffusion][Epoch 10013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:32,388 - INFO - [diffusion][Epoch 10014] Epoch 10015/12000
2024-11-05 02:45:36,622 - INFO - [diffusion][Epoch 10014] diffusion training Loss: 0.06296611949801445
2024-11-05 02:45:36,625 - INFO - [diffusion][Epoch 10014] diffusion learning rate: 0.001
2024-11-05 02:45:36,627 - INFO - [diffusion][Epoch 10014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:36,628 - INFO - [diffusion][Epoch 10015] Epoch 10016/12000
2024-11-05 02:45:40,929 - INFO - [diffusion][Epoch 10015] diffusion training Loss: 0.06642720568925142
2024-11-05 02:45:40,931 - INFO - [diffusion][Epoch 10015] diffusion learning rate: 0.001
2024-11-05 02:45:40,933 - INFO - [diffusion][Epoch 10015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:40,934 - INFO - [diffusion][Epoch 10016] Epoch 10017/12000
2024-11-05 02:45:45,198 - INFO - [diffusion][Epoch 10016] diffusion training Loss: 0.05969575885683298
2024-11-05 02:45:45,201 - INFO - [diffusion][Epoch 10016] diffusion learning rate: 0.001
2024-11-05 02:45:45,203 - INFO - [diffusion][Epoch 10016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:45,204 - INFO - [diffusion][Epoch 10017] Epoch 10018/12000
2024-11-05 02:45:49,479 - INFO - [diffusion][Epoch 10017] diffusion training Loss: 0.06345074996352196
2024-11-05 02:45:49,481 - INFO - [diffusion][Epoch 10017] diffusion learning rate: 0.001
2024-11-05 02:45:49,483 - INFO - [diffusion][Epoch 10017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:49,484 - INFO - [diffusion][Epoch 10018] Epoch 10019/12000
2024-11-05 02:45:53,906 - INFO - [diffusion][Epoch 10018] diffusion training Loss: 0.05593294557183981
2024-11-05 02:45:53,908 - INFO - [diffusion][Epoch 10018] diffusion learning rate: 0.001
2024-11-05 02:45:53,909 - INFO - [diffusion][Epoch 10018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:53,911 - INFO - [diffusion][Epoch 10019] Epoch 10020/12000
2024-11-05 02:45:57,844 - INFO - [diffusion][Epoch 10019] diffusion training Loss: 0.06014575995504856
2024-11-05 02:45:57,846 - INFO - [diffusion][Epoch 10019] diffusion learning rate: 0.001
2024-11-05 02:45:57,848 - INFO - [diffusion][Epoch 10019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:57,850 - INFO - [diffusion][Epoch 10020] Epoch 10021/12000
2024-11-05 02:46:01,808 - INFO - [diffusion][Epoch 10020] diffusion training Loss: 0.06334315799176693
2024-11-05 02:46:01,810 - INFO - [diffusion][Epoch 10020] diffusion learning rate: 0.001
2024-11-05 02:46:01,812 - INFO - [diffusion][Epoch 10020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:01,813 - INFO - [diffusion][Epoch 10021] Epoch 10022/12000
2024-11-05 02:46:05,975 - INFO - [diffusion][Epoch 10021] diffusion training Loss: 0.051354444585740566
2024-11-05 02:46:05,977 - INFO - [diffusion][Epoch 10021] diffusion learning rate: 0.001
2024-11-05 02:46:06,009 - INFO - [diffusion][Epoch 10021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:06,010 - INFO - [diffusion][Epoch 10022] Epoch 10023/12000
2024-11-05 02:46:10,180 - INFO - [diffusion][Epoch 10022] diffusion training Loss: 0.05857578385621309
2024-11-05 02:46:10,183 - INFO - [diffusion][Epoch 10022] diffusion learning rate: 0.001
2024-11-05 02:46:10,185 - INFO - [diffusion][Epoch 10022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:10,186 - INFO - [diffusion][Epoch 10023] Epoch 10024/12000
2024-11-05 02:46:14,348 - INFO - [diffusion][Epoch 10023] diffusion training Loss: 0.06040373910218477
2024-11-05 02:46:14,350 - INFO - [diffusion][Epoch 10023] diffusion learning rate: 0.001
2024-11-05 02:46:14,351 - INFO - [diffusion][Epoch 10023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:14,353 - INFO - [diffusion][Epoch 10024] Epoch 10025/12000
2024-11-05 02:46:18,596 - INFO - [diffusion][Epoch 10024] diffusion training Loss: 0.06433379091322422
2024-11-05 02:46:18,598 - INFO - [diffusion][Epoch 10024] diffusion learning rate: 0.001
2024-11-05 02:46:18,600 - INFO - [diffusion][Epoch 10024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:18,601 - INFO - [diffusion][Epoch 10025] Epoch 10026/12000
2024-11-05 02:46:22,910 - INFO - [diffusion][Epoch 10025] diffusion training Loss: 0.059371999464929104
2024-11-05 02:46:22,912 - INFO - [diffusion][Epoch 10025] diffusion learning rate: 0.001
2024-11-05 02:46:22,914 - INFO - [diffusion][Epoch 10025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:22,917 - INFO - [diffusion][Epoch 10026] Epoch 10027/12000
2024-11-05 02:46:27,091 - INFO - [diffusion][Epoch 10026] diffusion training Loss: 0.060300273820757866
2024-11-05 02:46:27,092 - INFO - [diffusion][Epoch 10026] diffusion learning rate: 0.001
2024-11-05 02:46:27,094 - INFO - [diffusion][Epoch 10026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:27,095 - INFO - [diffusion][Epoch 10027] Epoch 10028/12000
2024-11-05 02:46:31,367 - INFO - [diffusion][Epoch 10027] diffusion training Loss: 0.0671605160459876
2024-11-05 02:46:31,369 - INFO - [diffusion][Epoch 10027] diffusion learning rate: 0.001
2024-11-05 02:46:31,370 - INFO - [diffusion][Epoch 10027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:31,372 - INFO - [diffusion][Epoch 10028] Epoch 10029/12000
2024-11-05 02:46:35,524 - INFO - [diffusion][Epoch 10028] diffusion training Loss: 0.05886299908161163
2024-11-05 02:46:35,526 - INFO - [diffusion][Epoch 10028] diffusion learning rate: 0.001
2024-11-05 02:46:35,528 - INFO - [diffusion][Epoch 10028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:35,529 - INFO - [diffusion][Epoch 10029] Epoch 10030/12000
2024-11-05 02:46:39,715 - INFO - [diffusion][Epoch 10029] diffusion training Loss: 0.06282733660191298
2024-11-05 02:46:39,717 - INFO - [diffusion][Epoch 10029] diffusion learning rate: 0.001
2024-11-05 02:46:39,720 - INFO - [diffusion][Epoch 10029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:39,721 - INFO - [diffusion][Epoch 10030] Epoch 10031/12000
2024-11-05 02:46:44,012 - INFO - [diffusion][Epoch 10030] diffusion training Loss: 0.06491667591035366
2024-11-05 02:46:44,014 - INFO - [diffusion][Epoch 10030] diffusion learning rate: 0.001
2024-11-05 02:46:44,015 - INFO - [diffusion][Epoch 10030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:44,017 - INFO - [diffusion][Epoch 10031] Epoch 10032/12000
2024-11-05 02:46:48,183 - INFO - [diffusion][Epoch 10031] diffusion training Loss: 0.05833264719694853
2024-11-05 02:46:48,185 - INFO - [diffusion][Epoch 10031] diffusion learning rate: 0.001
2024-11-05 02:46:48,187 - INFO - [diffusion][Epoch 10031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:48,189 - INFO - [diffusion][Epoch 10032] Epoch 10033/12000
2024-11-05 02:46:52,335 - INFO - [diffusion][Epoch 10032] diffusion training Loss: 0.058479717932641506
2024-11-05 02:46:52,337 - INFO - [diffusion][Epoch 10032] diffusion learning rate: 0.001
2024-11-05 02:46:52,339 - INFO - [diffusion][Epoch 10032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:52,341 - INFO - [diffusion][Epoch 10033] Epoch 10034/12000
2024-11-05 02:46:56,650 - INFO - [diffusion][Epoch 10033] diffusion training Loss: 0.06633764877915382
2024-11-05 02:46:56,652 - INFO - [diffusion][Epoch 10033] diffusion learning rate: 0.001
2024-11-05 02:46:56,654 - INFO - [diffusion][Epoch 10033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:56,655 - INFO - [diffusion][Epoch 10034] Epoch 10035/12000
2024-11-05 02:47:00,882 - INFO - [diffusion][Epoch 10034] diffusion training Loss: 0.060066716745495796
2024-11-05 02:47:00,884 - INFO - [diffusion][Epoch 10034] diffusion learning rate: 0.001
2024-11-05 02:47:00,886 - INFO - [diffusion][Epoch 10034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:00,888 - INFO - [diffusion][Epoch 10035] Epoch 10036/12000
2024-11-05 02:47:04,742 - INFO - [diffusion][Epoch 10035] diffusion training Loss: 0.062257444486021996
2024-11-05 02:47:04,745 - INFO - [diffusion][Epoch 10035] diffusion learning rate: 0.001
2024-11-05 02:47:04,746 - INFO - [diffusion][Epoch 10035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:04,748 - INFO - [diffusion][Epoch 10036] Epoch 10037/12000
2024-11-05 02:47:09,604 - INFO - [diffusion][Epoch 10036] diffusion training Loss: 0.06555809546262026
2024-11-05 02:47:09,606 - INFO - [diffusion][Epoch 10036] diffusion learning rate: 0.001
2024-11-05 02:47:09,608 - INFO - [diffusion][Epoch 10036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:09,609 - INFO - [diffusion][Epoch 10037] Epoch 10038/12000
2024-11-05 02:47:13,673 - INFO - [diffusion][Epoch 10037] diffusion training Loss: 0.058115470223128796
2024-11-05 02:47:13,675 - INFO - [diffusion][Epoch 10037] diffusion learning rate: 0.001
2024-11-05 02:47:13,695 - INFO - [diffusion][Epoch 10037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:13,697 - INFO - [diffusion][Epoch 10038] Epoch 10039/12000
2024-11-05 02:47:17,744 - INFO - [diffusion][Epoch 10038] diffusion training Loss: 0.056704046204686165
2024-11-05 02:47:17,747 - INFO - [diffusion][Epoch 10038] diffusion learning rate: 0.001
2024-11-05 02:47:17,749 - INFO - [diffusion][Epoch 10038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:17,751 - INFO - [diffusion][Epoch 10039] Epoch 10040/12000
2024-11-05 02:47:22,002 - INFO - [diffusion][Epoch 10039] diffusion training Loss: 0.06274692341685295
2024-11-05 02:47:22,004 - INFO - [diffusion][Epoch 10039] diffusion learning rate: 0.001
2024-11-05 02:47:22,006 - INFO - [diffusion][Epoch 10039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:22,008 - INFO - [diffusion][Epoch 10040] Epoch 10041/12000
2024-11-05 02:47:26,270 - INFO - [diffusion][Epoch 10040] diffusion training Loss: 0.060898324474692345
2024-11-05 02:47:26,272 - INFO - [diffusion][Epoch 10040] diffusion learning rate: 0.001
2024-11-05 02:47:26,275 - INFO - [diffusion][Epoch 10040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:26,277 - INFO - [diffusion][Epoch 10041] Epoch 10042/12000
2024-11-05 02:47:30,479 - INFO - [diffusion][Epoch 10041] diffusion training Loss: 0.06553954351693392
2024-11-05 02:47:30,481 - INFO - [diffusion][Epoch 10041] diffusion learning rate: 0.001
2024-11-05 02:47:30,482 - INFO - [diffusion][Epoch 10041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:30,484 - INFO - [diffusion][Epoch 10042] Epoch 10043/12000
2024-11-05 02:47:34,655 - INFO - [diffusion][Epoch 10042] diffusion training Loss: 0.0594186270609498
2024-11-05 02:47:34,658 - INFO - [diffusion][Epoch 10042] diffusion learning rate: 0.001
2024-11-05 02:47:34,660 - INFO - [diffusion][Epoch 10042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:34,661 - INFO - [diffusion][Epoch 10043] Epoch 10044/12000
2024-11-05 02:47:38,772 - INFO - [diffusion][Epoch 10043] diffusion training Loss: 0.06136211007833481
2024-11-05 02:47:38,774 - INFO - [diffusion][Epoch 10043] diffusion learning rate: 0.001
2024-11-05 02:47:38,776 - INFO - [diffusion][Epoch 10043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:38,778 - INFO - [diffusion][Epoch 10044] Epoch 10045/12000
2024-11-05 02:47:42,962 - INFO - [diffusion][Epoch 10044] diffusion training Loss: 0.0651638824492693
2024-11-05 02:47:42,964 - INFO - [diffusion][Epoch 10044] diffusion learning rate: 0.001
2024-11-05 02:47:42,966 - INFO - [diffusion][Epoch 10044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:42,967 - INFO - [diffusion][Epoch 10045] Epoch 10046/12000
2024-11-05 02:47:47,170 - INFO - [diffusion][Epoch 10045] diffusion training Loss: 0.060219443403184414
2024-11-05 02:47:47,172 - INFO - [diffusion][Epoch 10045] diffusion learning rate: 0.001
2024-11-05 02:47:47,173 - INFO - [diffusion][Epoch 10045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:47,174 - INFO - [diffusion][Epoch 10046] Epoch 10047/12000
2024-11-05 02:47:51,321 - INFO - [diffusion][Epoch 10046] diffusion training Loss: 0.062395064160227776
2024-11-05 02:47:51,323 - INFO - [diffusion][Epoch 10046] diffusion learning rate: 0.001
2024-11-05 02:47:51,325 - INFO - [diffusion][Epoch 10046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:51,326 - INFO - [diffusion][Epoch 10047] Epoch 10048/12000
2024-11-05 02:47:55,586 - INFO - [diffusion][Epoch 10047] diffusion training Loss: 0.06060726195573807
2024-11-05 02:47:55,588 - INFO - [diffusion][Epoch 10047] diffusion learning rate: 0.001
2024-11-05 02:47:55,590 - INFO - [diffusion][Epoch 10047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:55,591 - INFO - [diffusion][Epoch 10048] Epoch 10049/12000
2024-11-05 02:47:59,877 - INFO - [diffusion][Epoch 10048] diffusion training Loss: 0.06193080358207226
2024-11-05 02:47:59,879 - INFO - [diffusion][Epoch 10048] diffusion learning rate: 0.001
2024-11-05 02:47:59,881 - INFO - [diffusion][Epoch 10048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:59,882 - INFO - [diffusion][Epoch 10049] Epoch 10050/12000
2024-11-05 02:48:04,071 - INFO - [diffusion][Epoch 10049] diffusion training Loss: 0.05861820746213198
2024-11-05 02:48:04,073 - INFO - [diffusion][Epoch 10049] diffusion learning rate: 0.001
2024-11-05 02:48:04,074 - INFO - [diffusion][Epoch 10049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:04,076 - INFO - [diffusion][Epoch 10050] Epoch 10051/12000
2024-11-05 02:48:08,362 - INFO - [diffusion][Epoch 10050] diffusion training Loss: 0.062271746806800365
2024-11-05 02:48:08,364 - INFO - [diffusion][Epoch 10050] diffusion learning rate: 0.001
2024-11-05 02:48:08,366 - INFO - [diffusion][Epoch 10050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:08,367 - INFO - [diffusion][Epoch 10051] Epoch 10052/12000
2024-11-05 02:48:12,599 - INFO - [diffusion][Epoch 10051] diffusion training Loss: 0.06326257437467575
2024-11-05 02:48:12,601 - INFO - [diffusion][Epoch 10051] diffusion learning rate: 0.001
2024-11-05 02:48:12,603 - INFO - [diffusion][Epoch 10051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:12,604 - INFO - [diffusion][Epoch 10052] Epoch 10053/12000
2024-11-05 02:48:16,900 - INFO - [diffusion][Epoch 10052] diffusion training Loss: 0.06068520527333021
2024-11-05 02:48:16,902 - INFO - [diffusion][Epoch 10052] diffusion learning rate: 0.001
2024-11-05 02:48:16,904 - INFO - [diffusion][Epoch 10052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:16,905 - INFO - [diffusion][Epoch 10053] Epoch 10054/12000
2024-11-05 02:48:21,136 - INFO - [diffusion][Epoch 10053] diffusion training Loss: 0.06000000424683094
2024-11-05 02:48:21,138 - INFO - [diffusion][Epoch 10053] diffusion learning rate: 0.001
2024-11-05 02:48:21,158 - INFO - [diffusion][Epoch 10053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:21,160 - INFO - [diffusion][Epoch 10054] Epoch 10055/12000
2024-11-05 02:48:25,349 - INFO - [diffusion][Epoch 10054] diffusion training Loss: 0.06236464250832796
2024-11-05 02:48:25,351 - INFO - [diffusion][Epoch 10054] diffusion learning rate: 0.001
2024-11-05 02:48:25,353 - INFO - [diffusion][Epoch 10054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:25,354 - INFO - [diffusion][Epoch 10055] Epoch 10056/12000
2024-11-05 02:48:29,554 - INFO - [diffusion][Epoch 10055] diffusion training Loss: 0.05835344083607197
2024-11-05 02:48:29,556 - INFO - [diffusion][Epoch 10055] diffusion learning rate: 0.001
2024-11-05 02:48:29,558 - INFO - [diffusion][Epoch 10055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:29,560 - INFO - [diffusion][Epoch 10056] Epoch 10057/12000
2024-11-05 02:48:33,723 - INFO - [diffusion][Epoch 10056] diffusion training Loss: 0.057934779673814774
2024-11-05 02:48:33,725 - INFO - [diffusion][Epoch 10056] diffusion learning rate: 0.001
2024-11-05 02:48:33,727 - INFO - [diffusion][Epoch 10056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:33,728 - INFO - [diffusion][Epoch 10057] Epoch 10058/12000
2024-11-05 02:48:38,653 - INFO - [diffusion][Epoch 10057] diffusion training Loss: 0.06271221581846476
2024-11-05 02:48:38,655 - INFO - [diffusion][Epoch 10057] diffusion learning rate: 0.001
2024-11-05 02:48:38,657 - INFO - [diffusion][Epoch 10057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:38,658 - INFO - [diffusion][Epoch 10058] Epoch 10059/12000
2024-11-05 02:48:42,934 - INFO - [diffusion][Epoch 10058] diffusion training Loss: 0.06314829550683498
2024-11-05 02:48:42,936 - INFO - [diffusion][Epoch 10058] diffusion learning rate: 0.001
2024-11-05 02:48:42,937 - INFO - [diffusion][Epoch 10058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:42,939 - INFO - [diffusion][Epoch 10059] Epoch 10060/12000
2024-11-05 02:48:47,166 - INFO - [diffusion][Epoch 10059] diffusion training Loss: 0.056925288401544094
2024-11-05 02:48:47,168 - INFO - [diffusion][Epoch 10059] diffusion learning rate: 0.001
2024-11-05 02:48:47,170 - INFO - [diffusion][Epoch 10059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:47,171 - INFO - [diffusion][Epoch 10060] Epoch 10061/12000
2024-11-05 02:48:51,354 - INFO - [diffusion][Epoch 10060] diffusion training Loss: 0.06856068782508373
2024-11-05 02:48:51,356 - INFO - [diffusion][Epoch 10060] diffusion learning rate: 0.001
2024-11-05 02:48:51,357 - INFO - [diffusion][Epoch 10060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:51,359 - INFO - [diffusion][Epoch 10061] Epoch 10062/12000
2024-11-05 02:48:55,554 - INFO - [diffusion][Epoch 10061] diffusion training Loss: 0.06613990850746632
2024-11-05 02:48:55,556 - INFO - [diffusion][Epoch 10061] diffusion learning rate: 0.001
2024-11-05 02:48:55,557 - INFO - [diffusion][Epoch 10061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:55,559 - INFO - [diffusion][Epoch 10062] Epoch 10063/12000
2024-11-05 02:48:59,739 - INFO - [diffusion][Epoch 10062] diffusion training Loss: 0.06373266130685806
2024-11-05 02:48:59,740 - INFO - [diffusion][Epoch 10062] diffusion learning rate: 0.001
2024-11-05 02:48:59,742 - INFO - [diffusion][Epoch 10062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:59,744 - INFO - [diffusion][Epoch 10063] Epoch 10064/12000
2024-11-05 02:49:04,083 - INFO - [diffusion][Epoch 10063] diffusion training Loss: 0.05656718276441097
2024-11-05 02:49:04,085 - INFO - [diffusion][Epoch 10063] diffusion learning rate: 0.001
2024-11-05 02:49:04,087 - INFO - [diffusion][Epoch 10063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:04,088 - INFO - [diffusion][Epoch 10064] Epoch 10065/12000
2024-11-05 02:49:08,162 - INFO - [diffusion][Epoch 10064] diffusion training Loss: 0.06351045332849026
2024-11-05 02:49:08,165 - INFO - [diffusion][Epoch 10064] diffusion learning rate: 0.001
2024-11-05 02:49:08,166 - INFO - [diffusion][Epoch 10064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:08,168 - INFO - [diffusion][Epoch 10065] Epoch 10066/12000
2024-11-05 02:49:12,372 - INFO - [diffusion][Epoch 10065] diffusion training Loss: 0.06025737710297108
2024-11-05 02:49:12,374 - INFO - [diffusion][Epoch 10065] diffusion learning rate: 0.001
2024-11-05 02:49:12,376 - INFO - [diffusion][Epoch 10065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:12,377 - INFO - [diffusion][Epoch 10066] Epoch 10067/12000
2024-11-05 02:49:16,651 - INFO - [diffusion][Epoch 10066] diffusion training Loss: 0.0612163171172142
2024-11-05 02:49:16,653 - INFO - [diffusion][Epoch 10066] diffusion learning rate: 0.001
2024-11-05 02:49:16,680 - INFO - [diffusion][Epoch 10066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:16,681 - INFO - [diffusion][Epoch 10067] Epoch 10068/12000
2024-11-05 02:49:20,889 - INFO - [diffusion][Epoch 10067] diffusion training Loss: 0.05920253600925207
2024-11-05 02:49:20,891 - INFO - [diffusion][Epoch 10067] diffusion learning rate: 0.001
2024-11-05 02:49:20,893 - INFO - [diffusion][Epoch 10067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:20,894 - INFO - [diffusion][Epoch 10068] Epoch 10069/12000
2024-11-05 02:49:25,071 - INFO - [diffusion][Epoch 10068] diffusion training Loss: 0.06760898139327765
2024-11-05 02:49:25,073 - INFO - [diffusion][Epoch 10068] diffusion learning rate: 0.001
2024-11-05 02:49:25,075 - INFO - [diffusion][Epoch 10068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:25,077 - INFO - [diffusion][Epoch 10069] Epoch 10070/12000
2024-11-05 02:49:29,315 - INFO - [diffusion][Epoch 10069] diffusion training Loss: 0.06061772350221872
2024-11-05 02:49:29,317 - INFO - [diffusion][Epoch 10069] diffusion learning rate: 0.001
2024-11-05 02:49:29,319 - INFO - [diffusion][Epoch 10069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:29,320 - INFO - [diffusion][Epoch 10070] Epoch 10071/12000
2024-11-05 02:49:33,569 - INFO - [diffusion][Epoch 10070] diffusion training Loss: 0.06137574650347233
2024-11-05 02:49:33,572 - INFO - [diffusion][Epoch 10070] diffusion learning rate: 0.001
2024-11-05 02:49:33,612 - INFO - [diffusion][Epoch 10070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:33,613 - INFO - [diffusion][Epoch 10071] Epoch 10072/12000
2024-11-05 02:49:37,880 - INFO - [diffusion][Epoch 10071] diffusion training Loss: 0.06232871301472187
2024-11-05 02:49:37,882 - INFO - [diffusion][Epoch 10071] diffusion learning rate: 0.001
2024-11-05 02:49:37,883 - INFO - [diffusion][Epoch 10071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:37,885 - INFO - [diffusion][Epoch 10072] Epoch 10073/12000
2024-11-05 02:49:42,153 - INFO - [diffusion][Epoch 10072] diffusion training Loss: 0.06644496694207191
2024-11-05 02:49:42,155 - INFO - [diffusion][Epoch 10072] diffusion learning rate: 0.001
2024-11-05 02:49:42,157 - INFO - [diffusion][Epoch 10072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:42,158 - INFO - [diffusion][Epoch 10073] Epoch 10074/12000
2024-11-05 02:49:46,466 - INFO - [diffusion][Epoch 10073] diffusion training Loss: 0.06730367802083492
2024-11-05 02:49:46,469 - INFO - [diffusion][Epoch 10073] diffusion learning rate: 0.001
2024-11-05 02:49:46,470 - INFO - [diffusion][Epoch 10073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:46,472 - INFO - [diffusion][Epoch 10074] Epoch 10075/12000
2024-11-05 02:49:50,729 - INFO - [diffusion][Epoch 10074] diffusion training Loss: 0.059257776476442814
2024-11-05 02:49:50,731 - INFO - [diffusion][Epoch 10074] diffusion learning rate: 0.001
2024-11-05 02:49:50,733 - INFO - [diffusion][Epoch 10074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:50,734 - INFO - [diffusion][Epoch 10075] Epoch 10076/12000
2024-11-05 02:49:54,869 - INFO - [diffusion][Epoch 10075] diffusion training Loss: 0.05911942943930626
2024-11-05 02:49:54,871 - INFO - [diffusion][Epoch 10075] diffusion learning rate: 0.001
2024-11-05 02:49:54,873 - INFO - [diffusion][Epoch 10075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:54,875 - INFO - [diffusion][Epoch 10076] Epoch 10077/12000
2024-11-05 02:49:59,277 - INFO - [diffusion][Epoch 10076] diffusion training Loss: 0.06032218970358372
2024-11-05 02:49:59,279 - INFO - [diffusion][Epoch 10076] diffusion learning rate: 0.001
2024-11-05 02:49:59,281 - INFO - [diffusion][Epoch 10076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:59,283 - INFO - [diffusion][Epoch 10077] Epoch 10078/12000
2024-11-05 02:50:03,466 - INFO - [diffusion][Epoch 10077] diffusion training Loss: 0.05981926154345274
2024-11-05 02:50:03,468 - INFO - [diffusion][Epoch 10077] diffusion learning rate: 0.001
2024-11-05 02:50:03,470 - INFO - [diffusion][Epoch 10077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:03,471 - INFO - [diffusion][Epoch 10078] Epoch 10079/12000
2024-11-05 02:50:07,727 - INFO - [diffusion][Epoch 10078] diffusion training Loss: 0.06302730552852154
2024-11-05 02:50:07,729 - INFO - [diffusion][Epoch 10078] diffusion learning rate: 0.001
2024-11-05 02:50:07,731 - INFO - [diffusion][Epoch 10078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:07,732 - INFO - [diffusion][Epoch 10079] Epoch 10080/12000
2024-11-05 02:50:11,844 - INFO - [diffusion][Epoch 10079] diffusion training Loss: 0.05838876869529486
2024-11-05 02:50:11,846 - INFO - [diffusion][Epoch 10079] diffusion learning rate: 0.001
2024-11-05 02:50:11,848 - INFO - [diffusion][Epoch 10079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:11,849 - INFO - [diffusion][Epoch 10080] Epoch 10081/12000
2024-11-05 02:50:16,097 - INFO - [diffusion][Epoch 10080] diffusion training Loss: 0.06188100576400757
2024-11-05 02:50:16,099 - INFO - [diffusion][Epoch 10080] diffusion learning rate: 0.001
2024-11-05 02:50:16,101 - INFO - [diffusion][Epoch 10080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:16,102 - INFO - [diffusion][Epoch 10081] Epoch 10082/12000
2024-11-05 02:50:20,374 - INFO - [diffusion][Epoch 10081] diffusion training Loss: 0.06001958157867193
2024-11-05 02:50:20,376 - INFO - [diffusion][Epoch 10081] diffusion learning rate: 0.001
2024-11-05 02:50:20,378 - INFO - [diffusion][Epoch 10081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:20,379 - INFO - [diffusion][Epoch 10082] Epoch 10083/12000
2024-11-05 02:50:24,514 - INFO - [diffusion][Epoch 10082] diffusion training Loss: 0.06122496351599693
2024-11-05 02:50:24,517 - INFO - [diffusion][Epoch 10082] diffusion learning rate: 0.001
2024-11-05 02:50:24,544 - INFO - [diffusion][Epoch 10082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:24,546 - INFO - [diffusion][Epoch 10083] Epoch 10084/12000
2024-11-05 02:50:28,821 - INFO - [diffusion][Epoch 10083] diffusion training Loss: 0.0624140864238143
2024-11-05 02:50:28,824 - INFO - [diffusion][Epoch 10083] diffusion learning rate: 0.001
2024-11-05 02:50:28,825 - INFO - [diffusion][Epoch 10083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:28,827 - INFO - [diffusion][Epoch 10084] Epoch 10085/12000
2024-11-05 02:50:33,110 - INFO - [diffusion][Epoch 10084] diffusion training Loss: 0.06379391811788082
2024-11-05 02:50:33,112 - INFO - [diffusion][Epoch 10084] diffusion learning rate: 0.001
2024-11-05 02:50:33,114 - INFO - [diffusion][Epoch 10084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:33,115 - INFO - [diffusion][Epoch 10085] Epoch 10086/12000
2024-11-05 02:50:37,420 - INFO - [diffusion][Epoch 10085] diffusion training Loss: 0.06485736276954412
2024-11-05 02:50:37,422 - INFO - [diffusion][Epoch 10085] diffusion learning rate: 0.001
2024-11-05 02:50:37,423 - INFO - [diffusion][Epoch 10085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:37,425 - INFO - [diffusion][Epoch 10086] Epoch 10087/12000
2024-11-05 02:50:41,721 - INFO - [diffusion][Epoch 10086] diffusion training Loss: 0.06766337156295776
2024-11-05 02:50:41,723 - INFO - [diffusion][Epoch 10086] diffusion learning rate: 0.001
2024-11-05 02:50:41,725 - INFO - [diffusion][Epoch 10086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:41,726 - INFO - [diffusion][Epoch 10087] Epoch 10088/12000
2024-11-05 02:50:45,945 - INFO - [diffusion][Epoch 10087] diffusion training Loss: 0.06204328499734402
2024-11-05 02:50:45,947 - INFO - [diffusion][Epoch 10087] diffusion learning rate: 0.001
2024-11-05 02:50:45,949 - INFO - [diffusion][Epoch 10087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:45,950 - INFO - [diffusion][Epoch 10088] Epoch 10089/12000
2024-11-05 02:50:50,201 - INFO - [diffusion][Epoch 10088] diffusion training Loss: 0.06253350712358952
2024-11-05 02:50:50,203 - INFO - [diffusion][Epoch 10088] diffusion learning rate: 0.001
2024-11-05 02:50:50,204 - INFO - [diffusion][Epoch 10088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:50,206 - INFO - [diffusion][Epoch 10089] Epoch 10090/12000
2024-11-05 02:50:54,458 - INFO - [diffusion][Epoch 10089] diffusion training Loss: 0.06412223726511002
2024-11-05 02:50:54,461 - INFO - [diffusion][Epoch 10089] diffusion learning rate: 0.001
2024-11-05 02:50:54,462 - INFO - [diffusion][Epoch 10089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:54,464 - INFO - [diffusion][Epoch 10090] Epoch 10091/12000
2024-11-05 02:50:58,719 - INFO - [diffusion][Epoch 10090] diffusion training Loss: 0.06298996321856976
2024-11-05 02:50:58,721 - INFO - [diffusion][Epoch 10090] diffusion learning rate: 0.001
2024-11-05 02:50:58,722 - INFO - [diffusion][Epoch 10090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:58,724 - INFO - [diffusion][Epoch 10091] Epoch 10092/12000
2024-11-05 02:51:02,887 - INFO - [diffusion][Epoch 10091] diffusion training Loss: 0.06876835227012634
2024-11-05 02:51:02,889 - INFO - [diffusion][Epoch 10091] diffusion learning rate: 0.001
2024-11-05 02:51:02,891 - INFO - [diffusion][Epoch 10091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:02,892 - INFO - [diffusion][Epoch 10092] Epoch 10093/12000
2024-11-05 02:51:07,082 - INFO - [diffusion][Epoch 10092] diffusion training Loss: 0.06398958247154951
2024-11-05 02:51:07,084 - INFO - [diffusion][Epoch 10092] diffusion learning rate: 0.001
2024-11-05 02:51:07,086 - INFO - [diffusion][Epoch 10092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:07,087 - INFO - [diffusion][Epoch 10093] Epoch 10094/12000
2024-11-05 02:51:11,239 - INFO - [diffusion][Epoch 10093] diffusion training Loss: 0.06141240708529949
2024-11-05 02:51:11,241 - INFO - [diffusion][Epoch 10093] diffusion learning rate: 0.001
2024-11-05 02:51:11,243 - INFO - [diffusion][Epoch 10093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:11,245 - INFO - [diffusion][Epoch 10094] Epoch 10095/12000
2024-11-05 02:51:15,439 - INFO - [diffusion][Epoch 10094] diffusion training Loss: 0.06096759717911482
2024-11-05 02:51:15,441 - INFO - [diffusion][Epoch 10094] diffusion learning rate: 0.001
2024-11-05 02:51:15,443 - INFO - [diffusion][Epoch 10094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:15,444 - INFO - [diffusion][Epoch 10095] Epoch 10096/12000
2024-11-05 02:51:19,609 - INFO - [diffusion][Epoch 10095] diffusion training Loss: 0.06155620142817497
2024-11-05 02:51:19,612 - INFO - [diffusion][Epoch 10095] diffusion learning rate: 0.001
2024-11-05 02:51:19,613 - INFO - [diffusion][Epoch 10095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:19,615 - INFO - [diffusion][Epoch 10096] Epoch 10097/12000
2024-11-05 02:51:23,610 - INFO - [diffusion][Epoch 10096] diffusion training Loss: 0.06600483693182468
2024-11-05 02:51:23,612 - INFO - [diffusion][Epoch 10096] diffusion learning rate: 0.001
2024-11-05 02:51:23,613 - INFO - [diffusion][Epoch 10096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:23,615 - INFO - [diffusion][Epoch 10097] Epoch 10098/12000
2024-11-05 02:51:27,978 - INFO - [diffusion][Epoch 10097] diffusion training Loss: 0.06065244413912296
2024-11-05 02:51:27,981 - INFO - [diffusion][Epoch 10097] diffusion learning rate: 0.001
2024-11-05 02:51:28,009 - INFO - [diffusion][Epoch 10097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:28,011 - INFO - [diffusion][Epoch 10098] Epoch 10099/12000
2024-11-05 02:51:32,295 - INFO - [diffusion][Epoch 10098] diffusion training Loss: 0.06133514456450939
2024-11-05 02:51:32,297 - INFO - [diffusion][Epoch 10098] diffusion learning rate: 0.001
2024-11-05 02:51:32,299 - INFO - [diffusion][Epoch 10098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:32,300 - INFO - [diffusion][Epoch 10099] Epoch 10100/12000
2024-11-05 02:51:36,616 - INFO - [diffusion][Epoch 10099] diffusion training Loss: 0.061137608252465725
2024-11-05 02:51:36,618 - INFO - [diffusion][Epoch 10099] diffusion learning rate: 0.001
2024-11-05 02:51:36,619 - INFO - [diffusion][Epoch 10099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:36,621 - INFO - [diffusion][Epoch 10100] Epoch 10101/12000
2024-11-05 02:51:40,911 - INFO - [diffusion][Epoch 10100] diffusion training Loss: 0.06131230015307665
2024-11-05 02:51:40,913 - INFO - [diffusion][Epoch 10100] diffusion learning rate: 0.001
2024-11-05 02:51:40,914 - INFO - [diffusion][Epoch 10100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:40,916 - INFO - [diffusion][Epoch 10101] Epoch 10102/12000
2024-11-05 02:51:45,078 - INFO - [diffusion][Epoch 10101] diffusion training Loss: 0.05871938448399305
2024-11-05 02:51:45,080 - INFO - [diffusion][Epoch 10101] diffusion learning rate: 0.001
2024-11-05 02:51:45,082 - INFO - [diffusion][Epoch 10101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:45,083 - INFO - [diffusion][Epoch 10102] Epoch 10103/12000
2024-11-05 02:51:49,146 - INFO - [diffusion][Epoch 10102] diffusion training Loss: 0.06231849454343319
2024-11-05 02:51:49,148 - INFO - [diffusion][Epoch 10102] diffusion learning rate: 0.001
2024-11-05 02:51:49,149 - INFO - [diffusion][Epoch 10102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:49,151 - INFO - [diffusion][Epoch 10103] Epoch 10104/12000
2024-11-05 02:51:53,264 - INFO - [diffusion][Epoch 10103] diffusion training Loss: 0.058799910359084606
2024-11-05 02:51:53,266 - INFO - [diffusion][Epoch 10103] diffusion learning rate: 0.001
2024-11-05 02:51:53,268 - INFO - [diffusion][Epoch 10103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:53,269 - INFO - [diffusion][Epoch 10104] Epoch 10105/12000
2024-11-05 02:51:57,447 - INFO - [diffusion][Epoch 10104] diffusion training Loss: 0.06155996583402157
2024-11-05 02:51:57,449 - INFO - [diffusion][Epoch 10104] diffusion learning rate: 0.001
2024-11-05 02:51:57,451 - INFO - [diffusion][Epoch 10104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:57,452 - INFO - [diffusion][Epoch 10105] Epoch 10106/12000
2024-11-05 02:52:01,733 - INFO - [diffusion][Epoch 10105] diffusion training Loss: 0.05936482734978199
2024-11-05 02:52:01,735 - INFO - [diffusion][Epoch 10105] diffusion learning rate: 0.001
2024-11-05 02:52:01,737 - INFO - [diffusion][Epoch 10105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:01,738 - INFO - [diffusion][Epoch 10106] Epoch 10107/12000
2024-11-05 02:52:05,969 - INFO - [diffusion][Epoch 10106] diffusion training Loss: 0.05689104273915291
2024-11-05 02:52:05,971 - INFO - [diffusion][Epoch 10106] diffusion learning rate: 0.001
2024-11-05 02:52:05,973 - INFO - [diffusion][Epoch 10106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:05,974 - INFO - [diffusion][Epoch 10107] Epoch 10108/12000
2024-11-05 02:52:10,079 - INFO - [diffusion][Epoch 10107] diffusion training Loss: 0.05686657316982746
2024-11-05 02:52:10,081 - INFO - [diffusion][Epoch 10107] diffusion learning rate: 0.001
2024-11-05 02:52:10,084 - INFO - [diffusion][Epoch 10107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:10,086 - INFO - [diffusion][Epoch 10108] Epoch 10109/12000
2024-11-05 02:52:14,193 - INFO - [diffusion][Epoch 10108] diffusion training Loss: 0.05859156046062708
2024-11-05 02:52:14,195 - INFO - [diffusion][Epoch 10108] diffusion learning rate: 0.001
2024-11-05 02:52:14,197 - INFO - [diffusion][Epoch 10108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:14,198 - INFO - [diffusion][Epoch 10109] Epoch 10110/12000
2024-11-05 02:52:18,497 - INFO - [diffusion][Epoch 10109] diffusion training Loss: 0.05957106500864029
2024-11-05 02:52:18,499 - INFO - [diffusion][Epoch 10109] diffusion learning rate: 0.001
2024-11-05 02:52:18,501 - INFO - [diffusion][Epoch 10109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:18,502 - INFO - [diffusion][Epoch 10110] Epoch 10111/12000
2024-11-05 02:52:22,602 - INFO - [diffusion][Epoch 10110] diffusion training Loss: 0.058155227452516556
2024-11-05 02:52:22,604 - INFO - [diffusion][Epoch 10110] diffusion learning rate: 0.001
2024-11-05 02:52:22,606 - INFO - [diffusion][Epoch 10110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:22,607 - INFO - [diffusion][Epoch 10111] Epoch 10112/12000
2024-11-05 02:52:26,793 - INFO - [diffusion][Epoch 10111] diffusion training Loss: 0.06728669442236423
2024-11-05 02:52:26,795 - INFO - [diffusion][Epoch 10111] diffusion learning rate: 0.001
2024-11-05 02:52:26,797 - INFO - [diffusion][Epoch 10111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:26,798 - INFO - [diffusion][Epoch 10112] Epoch 10113/12000
2024-11-05 02:52:30,962 - INFO - [diffusion][Epoch 10112] diffusion training Loss: 0.06324389018118382
2024-11-05 02:52:30,964 - INFO - [diffusion][Epoch 10112] diffusion learning rate: 0.001
2024-11-05 02:52:30,966 - INFO - [diffusion][Epoch 10112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:30,967 - INFO - [diffusion][Epoch 10113] Epoch 10114/12000
2024-11-05 02:52:35,242 - INFO - [diffusion][Epoch 10113] diffusion training Loss: 0.0587364686653018
2024-11-05 02:52:35,244 - INFO - [diffusion][Epoch 10113] diffusion learning rate: 0.001
2024-11-05 02:52:35,246 - INFO - [diffusion][Epoch 10113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:35,247 - INFO - [diffusion][Epoch 10114] Epoch 10115/12000
2024-11-05 02:52:39,469 - INFO - [diffusion][Epoch 10114] diffusion training Loss: 0.0579128498211503
2024-11-05 02:52:39,471 - INFO - [diffusion][Epoch 10114] diffusion learning rate: 0.001
2024-11-05 02:52:39,473 - INFO - [diffusion][Epoch 10114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:39,474 - INFO - [diffusion][Epoch 10115] Epoch 10116/12000
2024-11-05 02:52:43,700 - INFO - [diffusion][Epoch 10115] diffusion training Loss: 0.05800705403089523
2024-11-05 02:52:43,702 - INFO - [diffusion][Epoch 10115] diffusion learning rate: 0.001
2024-11-05 02:52:43,704 - INFO - [diffusion][Epoch 10115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:43,705 - INFO - [diffusion][Epoch 10116] Epoch 10117/12000
2024-11-05 02:52:47,940 - INFO - [diffusion][Epoch 10116] diffusion training Loss: 0.06317562237381935
2024-11-05 02:52:47,942 - INFO - [diffusion][Epoch 10116] diffusion learning rate: 0.001
2024-11-05 02:52:47,944 - INFO - [diffusion][Epoch 10116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:47,945 - INFO - [diffusion][Epoch 10117] Epoch 10118/12000
2024-11-05 02:52:52,187 - INFO - [diffusion][Epoch 10117] diffusion training Loss: 0.06062508188188076
2024-11-05 02:52:52,189 - INFO - [diffusion][Epoch 10117] diffusion learning rate: 0.001
2024-11-05 02:52:52,191 - INFO - [diffusion][Epoch 10117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:52,192 - INFO - [diffusion][Epoch 10118] Epoch 10119/12000
2024-11-05 02:52:56,430 - INFO - [diffusion][Epoch 10118] diffusion training Loss: 0.06521808356046677
2024-11-05 02:52:56,432 - INFO - [diffusion][Epoch 10118] diffusion learning rate: 0.001
2024-11-05 02:52:56,434 - INFO - [diffusion][Epoch 10118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:56,435 - INFO - [diffusion][Epoch 10119] Epoch 10120/12000
2024-11-05 02:53:00,789 - INFO - [diffusion][Epoch 10119] diffusion training Loss: 0.06265576835721731
2024-11-05 02:53:00,792 - INFO - [diffusion][Epoch 10119] diffusion learning rate: 0.001
2024-11-05 02:53:00,794 - INFO - [diffusion][Epoch 10119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:00,795 - INFO - [diffusion][Epoch 10120] Epoch 10121/12000
2024-11-05 02:53:04,990 - INFO - [diffusion][Epoch 10120] diffusion training Loss: 0.0667478060349822
2024-11-05 02:53:04,993 - INFO - [diffusion][Epoch 10120] diffusion learning rate: 0.001
2024-11-05 02:53:04,994 - INFO - [diffusion][Epoch 10120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:04,996 - INFO - [diffusion][Epoch 10121] Epoch 10122/12000
2024-11-05 02:53:09,157 - INFO - [diffusion][Epoch 10121] diffusion training Loss: 0.06563667859882116
2024-11-05 02:53:09,159 - INFO - [diffusion][Epoch 10121] diffusion learning rate: 0.001
2024-11-05 02:53:09,161 - INFO - [diffusion][Epoch 10121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:09,162 - INFO - [diffusion][Epoch 10122] Epoch 10123/12000
2024-11-05 02:53:13,382 - INFO - [diffusion][Epoch 10122] diffusion training Loss: 0.057095141150057316
2024-11-05 02:53:13,384 - INFO - [diffusion][Epoch 10122] diffusion learning rate: 0.001
2024-11-05 02:53:13,386 - INFO - [diffusion][Epoch 10122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:13,387 - INFO - [diffusion][Epoch 10123] Epoch 10124/12000
2024-11-05 02:53:17,565 - INFO - [diffusion][Epoch 10123] diffusion training Loss: 0.05787850450724363
2024-11-05 02:53:17,567 - INFO - [diffusion][Epoch 10123] diffusion learning rate: 0.001
2024-11-05 02:53:17,568 - INFO - [diffusion][Epoch 10123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:17,570 - INFO - [diffusion][Epoch 10124] Epoch 10125/12000
2024-11-05 02:53:21,690 - INFO - [diffusion][Epoch 10124] diffusion training Loss: 0.05709036719053984
2024-11-05 02:53:21,692 - INFO - [diffusion][Epoch 10124] diffusion learning rate: 0.001
2024-11-05 02:53:21,693 - INFO - [diffusion][Epoch 10124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:21,695 - INFO - [diffusion][Epoch 10125] Epoch 10126/12000
2024-11-05 02:53:25,938 - INFO - [diffusion][Epoch 10125] diffusion training Loss: 0.05624257307499647
2024-11-05 02:53:25,940 - INFO - [diffusion][Epoch 10125] diffusion learning rate: 0.001
2024-11-05 02:53:25,942 - INFO - [diffusion][Epoch 10125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:25,943 - INFO - [diffusion][Epoch 10126] Epoch 10127/12000
2024-11-05 02:53:30,259 - INFO - [diffusion][Epoch 10126] diffusion training Loss: 0.060214780271053314
2024-11-05 02:53:30,261 - INFO - [diffusion][Epoch 10126] diffusion learning rate: 0.001
2024-11-05 02:53:30,263 - INFO - [diffusion][Epoch 10126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:30,264 - INFO - [diffusion][Epoch 10127] Epoch 10128/12000
2024-11-05 02:53:34,571 - INFO - [diffusion][Epoch 10127] diffusion training Loss: 0.06193139310926199
2024-11-05 02:53:34,573 - INFO - [diffusion][Epoch 10127] diffusion learning rate: 0.001
2024-11-05 02:53:34,575 - INFO - [diffusion][Epoch 10127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:34,576 - INFO - [diffusion][Epoch 10128] Epoch 10129/12000
2024-11-05 02:53:38,831 - INFO - [diffusion][Epoch 10128] diffusion training Loss: 0.0566873736679554
2024-11-05 02:53:38,834 - INFO - [diffusion][Epoch 10128] diffusion learning rate: 0.001
2024-11-05 02:53:38,837 - INFO - [diffusion][Epoch 10128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:38,838 - INFO - [diffusion][Epoch 10129] Epoch 10130/12000
2024-11-05 02:53:43,078 - INFO - [diffusion][Epoch 10129] diffusion training Loss: 0.05691701266914606
2024-11-05 02:53:43,080 - INFO - [diffusion][Epoch 10129] diffusion learning rate: 0.001
2024-11-05 02:53:43,081 - INFO - [diffusion][Epoch 10129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:43,082 - INFO - [diffusion][Epoch 10130] Epoch 10131/12000
2024-11-05 02:53:46,840 - INFO - [diffusion][Epoch 10130] diffusion training Loss: 0.0625251429155469
2024-11-05 02:53:46,843 - INFO - [diffusion][Epoch 10130] diffusion learning rate: 0.001
2024-11-05 02:53:46,844 - INFO - [diffusion][Epoch 10130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:46,846 - INFO - [diffusion][Epoch 10131] Epoch 10132/12000
2024-11-05 02:53:51,033 - INFO - [diffusion][Epoch 10131] diffusion training Loss: 0.05802338197827339
2024-11-05 02:53:51,035 - INFO - [diffusion][Epoch 10131] diffusion learning rate: 0.001
2024-11-05 02:53:51,037 - INFO - [diffusion][Epoch 10131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:51,038 - INFO - [diffusion][Epoch 10132] Epoch 10133/12000
2024-11-05 02:53:55,303 - INFO - [diffusion][Epoch 10132] diffusion training Loss: 0.055811124853789806
2024-11-05 02:53:55,305 - INFO - [diffusion][Epoch 10132] diffusion learning rate: 0.001
2024-11-05 02:53:55,307 - INFO - [diffusion][Epoch 10132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:55,308 - INFO - [diffusion][Epoch 10133] Epoch 10134/12000
2024-11-05 02:53:59,477 - INFO - [diffusion][Epoch 10133] diffusion training Loss: 0.05504226312041283
2024-11-05 02:53:59,479 - INFO - [diffusion][Epoch 10133] diffusion learning rate: 0.001
2024-11-05 02:53:59,481 - INFO - [diffusion][Epoch 10133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:59,482 - INFO - [diffusion][Epoch 10134] Epoch 10135/12000
2024-11-05 02:54:03,570 - INFO - [diffusion][Epoch 10134] diffusion training Loss: 0.060635861940681934
2024-11-05 02:54:03,572 - INFO - [diffusion][Epoch 10134] diffusion learning rate: 0.001
2024-11-05 02:54:03,573 - INFO - [diffusion][Epoch 10134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:03,575 - INFO - [diffusion][Epoch 10135] Epoch 10136/12000
2024-11-05 02:54:07,745 - INFO - [diffusion][Epoch 10135] diffusion training Loss: 0.05687848851084709
2024-11-05 02:54:07,747 - INFO - [diffusion][Epoch 10135] diffusion learning rate: 0.001
2024-11-05 02:54:07,749 - INFO - [diffusion][Epoch 10135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:07,750 - INFO - [diffusion][Epoch 10136] Epoch 10137/12000
2024-11-05 02:54:11,992 - INFO - [diffusion][Epoch 10136] diffusion training Loss: 0.06300350651144981
2024-11-05 02:54:11,994 - INFO - [diffusion][Epoch 10136] diffusion learning rate: 0.001
2024-11-05 02:54:11,996 - INFO - [diffusion][Epoch 10136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:11,997 - INFO - [diffusion][Epoch 10137] Epoch 10138/12000
2024-11-05 02:54:16,229 - INFO - [diffusion][Epoch 10137] diffusion training Loss: 0.05963785294443369
2024-11-05 02:54:16,231 - INFO - [diffusion][Epoch 10137] diffusion learning rate: 0.001
2024-11-05 02:54:16,233 - INFO - [diffusion][Epoch 10137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:16,234 - INFO - [diffusion][Epoch 10138] Epoch 10139/12000
2024-11-05 02:54:20,373 - INFO - [diffusion][Epoch 10138] diffusion training Loss: 0.06243753619492054
2024-11-05 02:54:20,375 - INFO - [diffusion][Epoch 10138] diffusion learning rate: 0.001
2024-11-05 02:54:20,377 - INFO - [diffusion][Epoch 10138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:20,378 - INFO - [diffusion][Epoch 10139] Epoch 10140/12000
2024-11-05 02:54:24,553 - INFO - [diffusion][Epoch 10139] diffusion training Loss: 0.06242125853896141
2024-11-05 02:54:24,555 - INFO - [diffusion][Epoch 10139] diffusion learning rate: 0.001
2024-11-05 02:54:24,557 - INFO - [diffusion][Epoch 10139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:24,558 - INFO - [diffusion][Epoch 10140] Epoch 10141/12000
2024-11-05 02:54:29,145 - INFO - [diffusion][Epoch 10140] diffusion training Loss: 0.06207771599292755
2024-11-05 02:54:29,147 - INFO - [diffusion][Epoch 10140] diffusion learning rate: 0.001
2024-11-05 02:54:29,149 - INFO - [diffusion][Epoch 10140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:29,150 - INFO - [diffusion][Epoch 10141] Epoch 10142/12000
2024-11-05 02:54:33,333 - INFO - [diffusion][Epoch 10141] diffusion training Loss: 0.06240794621407986
2024-11-05 02:54:33,335 - INFO - [diffusion][Epoch 10141] diffusion learning rate: 0.001
2024-11-05 02:54:33,336 - INFO - [diffusion][Epoch 10141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:33,337 - INFO - [diffusion][Epoch 10142] Epoch 10143/12000
2024-11-05 02:54:37,564 - INFO - [diffusion][Epoch 10142] diffusion training Loss: 0.06240703258663416
2024-11-05 02:54:37,566 - INFO - [diffusion][Epoch 10142] diffusion learning rate: 0.001
2024-11-05 02:54:37,568 - INFO - [diffusion][Epoch 10142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:37,569 - INFO - [diffusion][Epoch 10143] Epoch 10144/12000
2024-11-05 02:54:41,799 - INFO - [diffusion][Epoch 10143] diffusion training Loss: 0.05696744844317436
2024-11-05 02:54:41,801 - INFO - [diffusion][Epoch 10143] diffusion learning rate: 0.001
2024-11-05 02:54:41,803 - INFO - [diffusion][Epoch 10143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:41,804 - INFO - [diffusion][Epoch 10144] Epoch 10145/12000
2024-11-05 02:54:46,007 - INFO - [diffusion][Epoch 10144] diffusion training Loss: 0.059296198189258575
2024-11-05 02:54:46,009 - INFO - [diffusion][Epoch 10144] diffusion learning rate: 0.001
2024-11-05 02:54:46,011 - INFO - [diffusion][Epoch 10144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:46,012 - INFO - [diffusion][Epoch 10145] Epoch 10146/12000
2024-11-05 02:54:50,273 - INFO - [diffusion][Epoch 10145] diffusion training Loss: 0.06161140464246273
2024-11-05 02:54:50,275 - INFO - [diffusion][Epoch 10145] diffusion learning rate: 0.001
2024-11-05 02:54:50,295 - INFO - [diffusion][Epoch 10145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:50,297 - INFO - [diffusion][Epoch 10146] Epoch 10147/12000
2024-11-05 02:54:54,518 - INFO - [diffusion][Epoch 10146] diffusion training Loss: 0.05968128889799118
2024-11-05 02:54:54,520 - INFO - [diffusion][Epoch 10146] diffusion learning rate: 0.001
2024-11-05 02:54:54,522 - INFO - [diffusion][Epoch 10146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:54,523 - INFO - [diffusion][Epoch 10147] Epoch 10148/12000
2024-11-05 02:54:58,676 - INFO - [diffusion][Epoch 10147] diffusion training Loss: 0.06091839820146561
2024-11-05 02:54:58,678 - INFO - [diffusion][Epoch 10147] diffusion learning rate: 0.001
2024-11-05 02:54:58,680 - INFO - [diffusion][Epoch 10147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:58,681 - INFO - [diffusion][Epoch 10148] Epoch 10149/12000
2024-11-05 02:55:02,841 - INFO - [diffusion][Epoch 10148] diffusion training Loss: 0.060899097472429276
2024-11-05 02:55:02,843 - INFO - [diffusion][Epoch 10148] diffusion learning rate: 0.001
2024-11-05 02:55:02,845 - INFO - [diffusion][Epoch 10148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:02,847 - INFO - [diffusion][Epoch 10149] Epoch 10150/12000
2024-11-05 02:55:07,019 - INFO - [diffusion][Epoch 10149] diffusion training Loss: 0.060879540629684925
2024-11-05 02:55:07,022 - INFO - [diffusion][Epoch 10149] diffusion learning rate: 0.001
2024-11-05 02:55:07,024 - INFO - [diffusion][Epoch 10149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:07,025 - INFO - [diffusion][Epoch 10150] Epoch 10151/12000
2024-11-05 02:55:11,164 - INFO - [diffusion][Epoch 10150] diffusion training Loss: 0.0612352816388011
2024-11-05 02:55:11,166 - INFO - [diffusion][Epoch 10150] diffusion learning rate: 0.001
2024-11-05 02:55:11,168 - INFO - [diffusion][Epoch 10150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:11,170 - INFO - [diffusion][Epoch 10151] Epoch 10152/12000
2024-11-05 02:55:15,380 - INFO - [diffusion][Epoch 10151] diffusion training Loss: 0.05970307998359203
2024-11-05 02:55:15,381 - INFO - [diffusion][Epoch 10151] diffusion learning rate: 0.001
2024-11-05 02:55:15,384 - INFO - [diffusion][Epoch 10151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:15,385 - INFO - [diffusion][Epoch 10152] Epoch 10153/12000
2024-11-05 02:55:19,617 - INFO - [diffusion][Epoch 10152] diffusion training Loss: 0.06002217158675194
2024-11-05 02:55:19,619 - INFO - [diffusion][Epoch 10152] diffusion learning rate: 0.001
2024-11-05 02:55:19,621 - INFO - [diffusion][Epoch 10152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:19,622 - INFO - [diffusion][Epoch 10153] Epoch 10154/12000
2024-11-05 02:55:23,925 - INFO - [diffusion][Epoch 10153] diffusion training Loss: 0.0670393006876111
2024-11-05 02:55:23,927 - INFO - [diffusion][Epoch 10153] diffusion learning rate: 0.001
2024-11-05 02:55:23,929 - INFO - [diffusion][Epoch 10153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:23,930 - INFO - [diffusion][Epoch 10154] Epoch 10155/12000
2024-11-05 02:55:28,160 - INFO - [diffusion][Epoch 10154] diffusion training Loss: 0.05813338793814182
2024-11-05 02:55:28,162 - INFO - [diffusion][Epoch 10154] diffusion learning rate: 0.001
2024-11-05 02:55:28,164 - INFO - [diffusion][Epoch 10154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:28,165 - INFO - [diffusion][Epoch 10155] Epoch 10156/12000
2024-11-05 02:55:32,445 - INFO - [diffusion][Epoch 10155] diffusion training Loss: 0.05801939405500889
2024-11-05 02:55:32,447 - INFO - [diffusion][Epoch 10155] diffusion learning rate: 0.001
2024-11-05 02:55:32,449 - INFO - [diffusion][Epoch 10155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:32,450 - INFO - [diffusion][Epoch 10156] Epoch 10157/12000
2024-11-05 02:55:36,612 - INFO - [diffusion][Epoch 10156] diffusion training Loss: 0.06888230796903372
2024-11-05 02:55:36,614 - INFO - [diffusion][Epoch 10156] diffusion learning rate: 0.001
2024-11-05 02:55:36,617 - INFO - [diffusion][Epoch 10156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:36,618 - INFO - [diffusion][Epoch 10157] Epoch 10158/12000
2024-11-05 02:55:40,859 - INFO - [diffusion][Epoch 10157] diffusion training Loss: 0.05783820990473032
2024-11-05 02:55:40,862 - INFO - [diffusion][Epoch 10157] diffusion learning rate: 0.001
2024-11-05 02:55:40,864 - INFO - [diffusion][Epoch 10157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:40,865 - INFO - [diffusion][Epoch 10158] Epoch 10159/12000
2024-11-05 02:55:45,191 - INFO - [diffusion][Epoch 10158] diffusion training Loss: 0.05910608544945717
2024-11-05 02:55:45,193 - INFO - [diffusion][Epoch 10158] diffusion learning rate: 0.001
2024-11-05 02:55:45,195 - INFO - [diffusion][Epoch 10158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:45,197 - INFO - [diffusion][Epoch 10159] Epoch 10160/12000
2024-11-05 02:55:49,386 - INFO - [diffusion][Epoch 10159] diffusion training Loss: 0.06540518160909414
2024-11-05 02:55:49,388 - INFO - [diffusion][Epoch 10159] diffusion learning rate: 0.001
2024-11-05 02:55:49,390 - INFO - [diffusion][Epoch 10159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:49,391 - INFO - [diffusion][Epoch 10160] Epoch 10161/12000
2024-11-05 02:55:53,688 - INFO - [diffusion][Epoch 10160] diffusion training Loss: 0.060134866274893284
2024-11-05 02:55:53,691 - INFO - [diffusion][Epoch 10160] diffusion learning rate: 0.001
2024-11-05 02:55:53,724 - INFO - [diffusion][Epoch 10160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:53,725 - INFO - [diffusion][Epoch 10161] Epoch 10162/12000
2024-11-05 02:55:57,965 - INFO - [diffusion][Epoch 10161] diffusion training Loss: 0.05906684882938862
2024-11-05 02:55:57,969 - INFO - [diffusion][Epoch 10161] diffusion learning rate: 0.001
2024-11-05 02:55:57,971 - INFO - [diffusion][Epoch 10161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:57,972 - INFO - [diffusion][Epoch 10162] Epoch 10163/12000
2024-11-05 02:56:02,291 - INFO - [diffusion][Epoch 10162] diffusion training Loss: 0.06363422237336636
2024-11-05 02:56:02,293 - INFO - [diffusion][Epoch 10162] diffusion learning rate: 0.001
2024-11-05 02:56:02,294 - INFO - [diffusion][Epoch 10162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:02,296 - INFO - [diffusion][Epoch 10163] Epoch 10164/12000
2024-11-05 02:56:06,460 - INFO - [diffusion][Epoch 10163] diffusion training Loss: 0.05946745444089174
2024-11-05 02:56:06,462 - INFO - [diffusion][Epoch 10163] diffusion learning rate: 0.001
2024-11-05 02:56:06,464 - INFO - [diffusion][Epoch 10163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:06,465 - INFO - [diffusion][Epoch 10164] Epoch 10165/12000
2024-11-05 02:56:10,665 - INFO - [diffusion][Epoch 10164] diffusion training Loss: 0.06277737487107515
2024-11-05 02:56:10,667 - INFO - [diffusion][Epoch 10164] diffusion learning rate: 0.001
2024-11-05 02:56:10,670 - INFO - [diffusion][Epoch 10164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:10,671 - INFO - [diffusion][Epoch 10165] Epoch 10166/12000
2024-11-05 02:56:14,837 - INFO - [diffusion][Epoch 10165] diffusion training Loss: 0.06233561411499977
2024-11-05 02:56:14,839 - INFO - [diffusion][Epoch 10165] diffusion learning rate: 0.001
2024-11-05 02:56:14,841 - INFO - [diffusion][Epoch 10165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:14,842 - INFO - [diffusion][Epoch 10166] Epoch 10167/12000
2024-11-05 02:56:19,140 - INFO - [diffusion][Epoch 10166] diffusion training Loss: 0.06854848936200142
2024-11-05 02:56:19,142 - INFO - [diffusion][Epoch 10166] diffusion learning rate: 0.001
2024-11-05 02:56:19,144 - INFO - [diffusion][Epoch 10166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:19,145 - INFO - [diffusion][Epoch 10167] Epoch 10168/12000
2024-11-05 02:56:23,360 - INFO - [diffusion][Epoch 10167] diffusion training Loss: 0.06316093727946281
2024-11-05 02:56:23,363 - INFO - [diffusion][Epoch 10167] diffusion learning rate: 0.001
2024-11-05 02:56:23,364 - INFO - [diffusion][Epoch 10167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:23,366 - INFO - [diffusion][Epoch 10168] Epoch 10169/12000
2024-11-05 02:56:27,581 - INFO - [diffusion][Epoch 10168] diffusion training Loss: 0.05691587179899216
2024-11-05 02:56:27,583 - INFO - [diffusion][Epoch 10168] diffusion learning rate: 0.001
2024-11-05 02:56:27,585 - INFO - [diffusion][Epoch 10168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:27,587 - INFO - [diffusion][Epoch 10169] Epoch 10170/12000
2024-11-05 02:56:31,859 - INFO - [diffusion][Epoch 10169] diffusion training Loss: 0.06012274418026209
2024-11-05 02:56:31,861 - INFO - [diffusion][Epoch 10169] diffusion learning rate: 0.001
2024-11-05 02:56:31,863 - INFO - [diffusion][Epoch 10169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:31,865 - INFO - [diffusion][Epoch 10170] Epoch 10171/12000
2024-11-05 02:56:36,149 - INFO - [diffusion][Epoch 10170] diffusion training Loss: 0.06093438249081373
2024-11-05 02:56:36,151 - INFO - [diffusion][Epoch 10170] diffusion learning rate: 0.001
2024-11-05 02:56:36,153 - INFO - [diffusion][Epoch 10170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:36,155 - INFO - [diffusion][Epoch 10171] Epoch 10172/12000
2024-11-05 02:56:40,352 - INFO - [diffusion][Epoch 10171] diffusion training Loss: 0.05905954726040363
2024-11-05 02:56:40,355 - INFO - [diffusion][Epoch 10171] diffusion learning rate: 0.001
2024-11-05 02:56:40,356 - INFO - [diffusion][Epoch 10171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:40,358 - INFO - [diffusion][Epoch 10172] Epoch 10173/12000
2024-11-05 02:56:44,550 - INFO - [diffusion][Epoch 10172] diffusion training Loss: 0.0593568691983819
2024-11-05 02:56:44,552 - INFO - [diffusion][Epoch 10172] diffusion learning rate: 0.001
2024-11-05 02:56:44,554 - INFO - [diffusion][Epoch 10172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:44,555 - INFO - [diffusion][Epoch 10173] Epoch 10174/12000
2024-11-05 02:56:48,711 - INFO - [diffusion][Epoch 10173] diffusion training Loss: 0.05977200996130705
2024-11-05 02:56:48,713 - INFO - [diffusion][Epoch 10173] diffusion learning rate: 0.001
2024-11-05 02:56:48,715 - INFO - [diffusion][Epoch 10173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:48,716 - INFO - [diffusion][Epoch 10174] Epoch 10175/12000
2024-11-05 02:56:52,830 - INFO - [diffusion][Epoch 10174] diffusion training Loss: 0.06435709074139595
2024-11-05 02:56:52,832 - INFO - [diffusion][Epoch 10174] diffusion learning rate: 0.001
2024-11-05 02:56:52,834 - INFO - [diffusion][Epoch 10174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:52,835 - INFO - [diffusion][Epoch 10175] Epoch 10176/12000
2024-11-05 02:56:57,019 - INFO - [diffusion][Epoch 10175] diffusion training Loss: 0.062429098412394524
2024-11-05 02:56:57,021 - INFO - [diffusion][Epoch 10175] diffusion learning rate: 0.001
2024-11-05 02:56:57,023 - INFO - [diffusion][Epoch 10175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:57,024 - INFO - [diffusion][Epoch 10176] Epoch 10177/12000
2024-11-05 02:57:01,193 - INFO - [diffusion][Epoch 10176] diffusion training Loss: 0.05517416913062334
2024-11-05 02:57:01,195 - INFO - [diffusion][Epoch 10176] diffusion learning rate: 0.001
2024-11-05 02:57:01,197 - INFO - [diffusion][Epoch 10176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:01,198 - INFO - [diffusion][Epoch 10177] Epoch 10178/12000
2024-11-05 02:57:05,372 - INFO - [diffusion][Epoch 10177] diffusion training Loss: 0.05918560642749071
2024-11-05 02:57:05,374 - INFO - [diffusion][Epoch 10177] diffusion learning rate: 0.001
2024-11-05 02:57:05,376 - INFO - [diffusion][Epoch 10177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:05,377 - INFO - [diffusion][Epoch 10178] Epoch 10179/12000
2024-11-05 02:57:09,477 - INFO - [diffusion][Epoch 10178] diffusion training Loss: 0.06303796265274286
2024-11-05 02:57:09,480 - INFO - [diffusion][Epoch 10178] diffusion learning rate: 0.001
2024-11-05 02:57:09,482 - INFO - [diffusion][Epoch 10178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:09,483 - INFO - [diffusion][Epoch 10179] Epoch 10180/12000
2024-11-05 02:57:13,643 - INFO - [diffusion][Epoch 10179] diffusion training Loss: 0.05834122747182846
2024-11-05 02:57:13,645 - INFO - [diffusion][Epoch 10179] diffusion learning rate: 0.001
2024-11-05 02:57:13,647 - INFO - [diffusion][Epoch 10179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:13,648 - INFO - [diffusion][Epoch 10180] Epoch 10181/12000
2024-11-05 02:57:17,852 - INFO - [diffusion][Epoch 10180] diffusion training Loss: 0.0550815099850297
2024-11-05 02:57:17,854 - INFO - [diffusion][Epoch 10180] diffusion learning rate: 0.001
2024-11-05 02:57:17,856 - INFO - [diffusion][Epoch 10180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:17,857 - INFO - [diffusion][Epoch 10181] Epoch 10182/12000
2024-11-05 02:57:22,162 - INFO - [diffusion][Epoch 10181] diffusion training Loss: 0.06297040451318026
2024-11-05 02:57:22,164 - INFO - [diffusion][Epoch 10181] diffusion learning rate: 0.001
2024-11-05 02:57:22,165 - INFO - [diffusion][Epoch 10181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:22,167 - INFO - [diffusion][Epoch 10182] Epoch 10183/12000
2024-11-05 02:57:26,509 - INFO - [diffusion][Epoch 10182] diffusion training Loss: 0.05560562293976545
2024-11-05 02:57:26,511 - INFO - [diffusion][Epoch 10182] diffusion learning rate: 0.001
2024-11-05 02:57:26,513 - INFO - [diffusion][Epoch 10182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:26,514 - INFO - [diffusion][Epoch 10183] Epoch 10184/12000
2024-11-05 02:57:30,692 - INFO - [diffusion][Epoch 10183] diffusion training Loss: 0.06129015702754259
2024-11-05 02:57:30,694 - INFO - [diffusion][Epoch 10183] diffusion learning rate: 0.001
2024-11-05 02:57:30,696 - INFO - [diffusion][Epoch 10183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:30,697 - INFO - [diffusion][Epoch 10184] Epoch 10185/12000
2024-11-05 02:57:35,018 - INFO - [diffusion][Epoch 10184] diffusion training Loss: 0.06415238045156002
2024-11-05 02:57:35,020 - INFO - [diffusion][Epoch 10184] diffusion learning rate: 0.001
2024-11-05 02:57:35,022 - INFO - [diffusion][Epoch 10184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:35,023 - INFO - [diffusion][Epoch 10185] Epoch 10186/12000
2024-11-05 02:57:39,174 - INFO - [diffusion][Epoch 10185] diffusion training Loss: 0.0597701882943511
2024-11-05 02:57:39,176 - INFO - [diffusion][Epoch 10185] diffusion learning rate: 0.001
2024-11-05 02:57:39,178 - INFO - [diffusion][Epoch 10185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:39,179 - INFO - [diffusion][Epoch 10186] Epoch 10187/12000
2024-11-05 02:57:43,273 - INFO - [diffusion][Epoch 10186] diffusion training Loss: 0.060476647689938545
2024-11-05 02:57:43,275 - INFO - [diffusion][Epoch 10186] diffusion learning rate: 0.001
2024-11-05 02:57:43,277 - INFO - [diffusion][Epoch 10186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:43,278 - INFO - [diffusion][Epoch 10187] Epoch 10188/12000
2024-11-05 02:57:47,330 - INFO - [diffusion][Epoch 10187] diffusion training Loss: 0.06482126470655203
2024-11-05 02:57:47,332 - INFO - [diffusion][Epoch 10187] diffusion learning rate: 0.001
2024-11-05 02:57:47,334 - INFO - [diffusion][Epoch 10187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:47,336 - INFO - [diffusion][Epoch 10188] Epoch 10189/12000
2024-11-05 02:57:51,571 - INFO - [diffusion][Epoch 10188] diffusion training Loss: 0.05837821774184704
2024-11-05 02:57:51,572 - INFO - [diffusion][Epoch 10188] diffusion learning rate: 0.001
2024-11-05 02:57:51,574 - INFO - [diffusion][Epoch 10188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:51,576 - INFO - [diffusion][Epoch 10189] Epoch 10190/12000
2024-11-05 02:57:55,835 - INFO - [diffusion][Epoch 10189] diffusion training Loss: 0.05974457412958145
2024-11-05 02:57:55,837 - INFO - [diffusion][Epoch 10189] diffusion learning rate: 0.001
2024-11-05 02:57:55,839 - INFO - [diffusion][Epoch 10189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:55,840 - INFO - [diffusion][Epoch 10190] Epoch 10191/12000
2024-11-05 02:58:00,125 - INFO - [diffusion][Epoch 10190] diffusion training Loss: 0.0607894491404295
2024-11-05 02:58:00,128 - INFO - [diffusion][Epoch 10190] diffusion learning rate: 0.001
2024-11-05 02:58:00,130 - INFO - [diffusion][Epoch 10190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:00,131 - INFO - [diffusion][Epoch 10191] Epoch 10192/12000
2024-11-05 02:58:04,332 - INFO - [diffusion][Epoch 10191] diffusion training Loss: 0.0641280971467495
2024-11-05 02:58:04,334 - INFO - [diffusion][Epoch 10191] diffusion learning rate: 0.001
2024-11-05 02:58:04,336 - INFO - [diffusion][Epoch 10191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:04,337 - INFO - [diffusion][Epoch 10192] Epoch 10193/12000
2024-11-05 02:58:08,661 - INFO - [diffusion][Epoch 10192] diffusion training Loss: 0.057397641241550446
2024-11-05 02:58:08,663 - INFO - [diffusion][Epoch 10192] diffusion learning rate: 0.001
2024-11-05 02:58:08,665 - INFO - [diffusion][Epoch 10192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:08,667 - INFO - [diffusion][Epoch 10193] Epoch 10194/12000
2024-11-05 02:58:12,983 - INFO - [diffusion][Epoch 10193] diffusion training Loss: 0.05776953790336847
2024-11-05 02:58:12,985 - INFO - [diffusion][Epoch 10193] diffusion learning rate: 0.001
2024-11-05 02:58:13,034 - INFO - [diffusion][Epoch 10193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:13,036 - INFO - [diffusion][Epoch 10194] Epoch 10195/12000
2024-11-05 02:58:17,249 - INFO - [diffusion][Epoch 10194] diffusion training Loss: 0.06318037398159504
2024-11-05 02:58:17,252 - INFO - [diffusion][Epoch 10194] diffusion learning rate: 0.001
2024-11-05 02:58:17,253 - INFO - [diffusion][Epoch 10194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:17,255 - INFO - [diffusion][Epoch 10195] Epoch 10196/12000
2024-11-05 02:58:21,479 - INFO - [diffusion][Epoch 10195] diffusion training Loss: 0.05544197000563145
2024-11-05 02:58:21,481 - INFO - [diffusion][Epoch 10195] diffusion learning rate: 0.001
2024-11-05 02:58:21,482 - INFO - [diffusion][Epoch 10195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:21,484 - INFO - [diffusion][Epoch 10196] Epoch 10197/12000
2024-11-05 02:58:25,691 - INFO - [diffusion][Epoch 10196] diffusion training Loss: 0.061426364816725254
2024-11-05 02:58:25,692 - INFO - [diffusion][Epoch 10196] diffusion learning rate: 0.001
2024-11-05 02:58:25,694 - INFO - [diffusion][Epoch 10196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:25,696 - INFO - [diffusion][Epoch 10197] Epoch 10198/12000
2024-11-05 02:58:29,866 - INFO - [diffusion][Epoch 10197] diffusion training Loss: 0.05790067370980978
2024-11-05 02:58:29,868 - INFO - [diffusion][Epoch 10197] diffusion learning rate: 0.001
2024-11-05 02:58:29,869 - INFO - [diffusion][Epoch 10197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:29,871 - INFO - [diffusion][Epoch 10198] Epoch 10199/12000
2024-11-05 02:58:34,114 - INFO - [diffusion][Epoch 10198] diffusion training Loss: 0.0630660243332386
2024-11-05 02:58:34,116 - INFO - [diffusion][Epoch 10198] diffusion learning rate: 0.001
2024-11-05 02:58:34,118 - INFO - [diffusion][Epoch 10198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:34,119 - INFO - [diffusion][Epoch 10199] Epoch 10200/12000
2024-11-05 02:58:38,435 - INFO - [diffusion][Epoch 10199] diffusion training Loss: 0.06081706844270229
2024-11-05 02:58:38,437 - INFO - [diffusion][Epoch 10199] diffusion learning rate: 0.001
2024-11-05 02:58:38,439 - INFO - [diffusion][Epoch 10199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:38,440 - INFO - [diffusion][Epoch 10200] Epoch 10201/12000
2024-11-05 02:58:42,679 - INFO - [diffusion][Epoch 10200] diffusion training Loss: 0.05577394273132086
2024-11-05 02:58:42,681 - INFO - [diffusion][Epoch 10200] diffusion learning rate: 0.001
2024-11-05 02:58:42,682 - INFO - [diffusion][Epoch 10200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:42,684 - INFO - [diffusion][Epoch 10201] Epoch 10202/12000
2024-11-05 02:58:47,435 - INFO - [diffusion][Epoch 10201] diffusion training Loss: 0.06314639840275049
2024-11-05 02:58:47,437 - INFO - [diffusion][Epoch 10201] diffusion learning rate: 0.001
2024-11-05 02:58:47,439 - INFO - [diffusion][Epoch 10201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:47,440 - INFO - [diffusion][Epoch 10202] Epoch 10203/12000
2024-11-05 02:58:51,595 - INFO - [diffusion][Epoch 10202] diffusion training Loss: 0.06497674342244864
2024-11-05 02:58:51,598 - INFO - [diffusion][Epoch 10202] diffusion learning rate: 0.001
2024-11-05 02:58:51,600 - INFO - [diffusion][Epoch 10202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:51,601 - INFO - [diffusion][Epoch 10203] Epoch 10204/12000
2024-11-05 02:58:55,870 - INFO - [diffusion][Epoch 10203] diffusion training Loss: 0.0605047857388854
2024-11-05 02:58:55,872 - INFO - [diffusion][Epoch 10203] diffusion learning rate: 0.001
2024-11-05 02:58:55,874 - INFO - [diffusion][Epoch 10203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:55,875 - INFO - [diffusion][Epoch 10204] Epoch 10205/12000
2024-11-05 02:58:59,953 - INFO - [diffusion][Epoch 10204] diffusion training Loss: 0.05724574625492096
2024-11-05 02:58:59,955 - INFO - [diffusion][Epoch 10204] diffusion learning rate: 0.001
2024-11-05 02:58:59,956 - INFO - [diffusion][Epoch 10204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:59,958 - INFO - [diffusion][Epoch 10205] Epoch 10206/12000
2024-11-05 02:59:04,124 - INFO - [diffusion][Epoch 10205] diffusion training Loss: 0.05884512513875961
2024-11-05 02:59:04,126 - INFO - [diffusion][Epoch 10205] diffusion learning rate: 0.001
2024-11-05 02:59:04,128 - INFO - [diffusion][Epoch 10205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:04,129 - INFO - [diffusion][Epoch 10206] Epoch 10207/12000
2024-11-05 02:59:08,355 - INFO - [diffusion][Epoch 10206] diffusion training Loss: 0.05782653857022524
2024-11-05 02:59:08,357 - INFO - [diffusion][Epoch 10206] diffusion learning rate: 0.001
2024-11-05 02:59:08,359 - INFO - [diffusion][Epoch 10206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:08,360 - INFO - [diffusion][Epoch 10207] Epoch 10208/12000
2024-11-05 02:59:12,553 - INFO - [diffusion][Epoch 10207] diffusion training Loss: 0.0614364929497242
2024-11-05 02:59:12,555 - INFO - [diffusion][Epoch 10207] diffusion learning rate: 0.001
2024-11-05 02:59:12,557 - INFO - [diffusion][Epoch 10207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:12,558 - INFO - [diffusion][Epoch 10208] Epoch 10209/12000
2024-11-05 02:59:16,569 - INFO - [diffusion][Epoch 10208] diffusion training Loss: 0.06330645363777876
2024-11-05 02:59:16,571 - INFO - [diffusion][Epoch 10208] diffusion learning rate: 0.001
2024-11-05 02:59:16,572 - INFO - [diffusion][Epoch 10208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:16,574 - INFO - [diffusion][Epoch 10209] Epoch 10210/12000
2024-11-05 02:59:20,693 - INFO - [diffusion][Epoch 10209] diffusion training Loss: 0.05894452054053545
2024-11-05 02:59:20,695 - INFO - [diffusion][Epoch 10209] diffusion learning rate: 0.001
2024-11-05 02:59:20,697 - INFO - [diffusion][Epoch 10209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:20,698 - INFO - [diffusion][Epoch 10210] Epoch 10211/12000
2024-11-05 02:59:24,692 - INFO - [diffusion][Epoch 10210] diffusion training Loss: 0.05342800170183182
2024-11-05 02:59:24,693 - INFO - [diffusion][Epoch 10210] diffusion learning rate: 0.001
2024-11-05 02:59:24,695 - INFO - [diffusion][Epoch 10210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:24,696 - INFO - [diffusion][Epoch 10211] Epoch 10212/12000
2024-11-05 02:59:28,871 - INFO - [diffusion][Epoch 10211] diffusion training Loss: 0.05915294215083122
2024-11-05 02:59:28,873 - INFO - [diffusion][Epoch 10211] diffusion learning rate: 0.001
2024-11-05 02:59:28,876 - INFO - [diffusion][Epoch 10211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:28,877 - INFO - [diffusion][Epoch 10212] Epoch 10213/12000
2024-11-05 02:59:32,921 - INFO - [diffusion][Epoch 10212] diffusion training Loss: 0.06627762317657471
2024-11-05 02:59:32,923 - INFO - [diffusion][Epoch 10212] diffusion learning rate: 0.001
2024-11-05 02:59:32,925 - INFO - [diffusion][Epoch 10212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:32,926 - INFO - [diffusion][Epoch 10213] Epoch 10214/12000
2024-11-05 02:59:37,066 - INFO - [diffusion][Epoch 10213] diffusion training Loss: 0.06056584883481264
2024-11-05 02:59:37,068 - INFO - [diffusion][Epoch 10213] diffusion learning rate: 0.001
2024-11-05 02:59:37,070 - INFO - [diffusion][Epoch 10213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:37,071 - INFO - [diffusion][Epoch 10214] Epoch 10215/12000
2024-11-05 02:59:41,203 - INFO - [diffusion][Epoch 10214] diffusion training Loss: 0.061756432056427
2024-11-05 02:59:41,206 - INFO - [diffusion][Epoch 10214] diffusion learning rate: 0.001
2024-11-05 02:59:41,208 - INFO - [diffusion][Epoch 10214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:41,210 - INFO - [diffusion][Epoch 10215] Epoch 10216/12000
2024-11-05 02:59:45,193 - INFO - [diffusion][Epoch 10215] diffusion training Loss: 0.06477815192192793
2024-11-05 02:59:45,195 - INFO - [diffusion][Epoch 10215] diffusion learning rate: 0.001
2024-11-05 02:59:45,198 - INFO - [diffusion][Epoch 10215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:45,199 - INFO - [diffusion][Epoch 10216] Epoch 10217/12000
2024-11-05 02:59:49,356 - INFO - [diffusion][Epoch 10216] diffusion training Loss: 0.056788066402077675
2024-11-05 02:59:49,358 - INFO - [diffusion][Epoch 10216] diffusion learning rate: 0.001
2024-11-05 02:59:49,360 - INFO - [diffusion][Epoch 10216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:49,361 - INFO - [diffusion][Epoch 10217] Epoch 10218/12000
2024-11-05 02:59:53,417 - INFO - [diffusion][Epoch 10217] diffusion training Loss: 0.06083466485142708
2024-11-05 02:59:53,419 - INFO - [diffusion][Epoch 10217] diffusion learning rate: 0.001
2024-11-05 02:59:53,420 - INFO - [diffusion][Epoch 10217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:53,421 - INFO - [diffusion][Epoch 10218] Epoch 10219/12000
2024-11-05 02:59:57,601 - INFO - [diffusion][Epoch 10218] diffusion training Loss: 0.06358823552727699
2024-11-05 02:59:57,603 - INFO - [diffusion][Epoch 10218] diffusion learning rate: 0.001
2024-11-05 02:59:57,645 - INFO - [diffusion][Epoch 10218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:57,647 - INFO - [diffusion][Epoch 10219] Epoch 10220/12000
2024-11-05 03:00:01,602 - INFO - [diffusion][Epoch 10219] diffusion training Loss: 0.05865028128027916
2024-11-05 03:00:01,604 - INFO - [diffusion][Epoch 10219] diffusion learning rate: 0.001
2024-11-05 03:00:01,605 - INFO - [diffusion][Epoch 10219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:01,606 - INFO - [diffusion][Epoch 10220] Epoch 10221/12000
2024-11-05 03:00:05,529 - INFO - [diffusion][Epoch 10220] diffusion training Loss: 0.06286919862031937
2024-11-05 03:00:05,530 - INFO - [diffusion][Epoch 10220] diffusion learning rate: 0.001
2024-11-05 03:00:05,532 - INFO - [diffusion][Epoch 10220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:05,534 - INFO - [diffusion][Epoch 10221] Epoch 10222/12000
2024-11-05 03:00:09,619 - INFO - [diffusion][Epoch 10221] diffusion training Loss: 0.06241621449589729
2024-11-05 03:00:09,621 - INFO - [diffusion][Epoch 10221] diffusion learning rate: 0.001
2024-11-05 03:00:09,641 - INFO - [diffusion][Epoch 10221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:09,642 - INFO - [diffusion][Epoch 10222] Epoch 10223/12000
2024-11-05 03:00:13,753 - INFO - [diffusion][Epoch 10222] diffusion training Loss: 0.06006881594657898
2024-11-05 03:00:13,755 - INFO - [diffusion][Epoch 10222] diffusion learning rate: 0.001
2024-11-05 03:00:13,757 - INFO - [diffusion][Epoch 10222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:13,758 - INFO - [diffusion][Epoch 10223] Epoch 10224/12000
2024-11-05 03:00:18,020 - INFO - [diffusion][Epoch 10223] diffusion training Loss: 0.06306411232799292
2024-11-05 03:00:18,022 - INFO - [diffusion][Epoch 10223] diffusion learning rate: 0.001
2024-11-05 03:00:18,024 - INFO - [diffusion][Epoch 10223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:18,025 - INFO - [diffusion][Epoch 10224] Epoch 10225/12000
2024-11-05 03:00:22,157 - INFO - [diffusion][Epoch 10224] diffusion training Loss: 0.06275050342082977
2024-11-05 03:00:22,159 - INFO - [diffusion][Epoch 10224] diffusion learning rate: 0.001
2024-11-05 03:00:22,161 - INFO - [diffusion][Epoch 10224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:22,162 - INFO - [diffusion][Epoch 10225] Epoch 10226/12000
2024-11-05 03:00:26,233 - INFO - [diffusion][Epoch 10225] diffusion training Loss: 0.05842029396444559
2024-11-05 03:00:26,235 - INFO - [diffusion][Epoch 10225] diffusion learning rate: 0.001
2024-11-05 03:00:26,237 - INFO - [diffusion][Epoch 10225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:26,238 - INFO - [diffusion][Epoch 10226] Epoch 10227/12000
2024-11-05 03:00:30,248 - INFO - [diffusion][Epoch 10226] diffusion training Loss: 0.0632979366928339
2024-11-05 03:00:30,250 - INFO - [diffusion][Epoch 10226] diffusion learning rate: 0.001
2024-11-05 03:00:30,252 - INFO - [diffusion][Epoch 10226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:30,253 - INFO - [diffusion][Epoch 10227] Epoch 10228/12000
2024-11-05 03:00:34,299 - INFO - [diffusion][Epoch 10227] diffusion training Loss: 0.06346401292830706
2024-11-05 03:00:34,301 - INFO - [diffusion][Epoch 10227] diffusion learning rate: 0.001
2024-11-05 03:00:34,302 - INFO - [diffusion][Epoch 10227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:34,304 - INFO - [diffusion][Epoch 10228] Epoch 10229/12000
2024-11-05 03:00:38,483 - INFO - [diffusion][Epoch 10228] diffusion training Loss: 0.06022170092910528
2024-11-05 03:00:38,484 - INFO - [diffusion][Epoch 10228] diffusion learning rate: 0.001
2024-11-05 03:00:38,486 - INFO - [diffusion][Epoch 10228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:38,487 - INFO - [diffusion][Epoch 10229] Epoch 10230/12000
2024-11-05 03:00:42,773 - INFO - [diffusion][Epoch 10229] diffusion training Loss: 0.057085598818957806
2024-11-05 03:00:42,775 - INFO - [diffusion][Epoch 10229] diffusion learning rate: 0.001
2024-11-05 03:00:42,777 - INFO - [diffusion][Epoch 10229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:42,778 - INFO - [diffusion][Epoch 10230] Epoch 10231/12000
2024-11-05 03:00:46,793 - INFO - [diffusion][Epoch 10230] diffusion training Loss: 0.06185004487633705
2024-11-05 03:00:46,795 - INFO - [diffusion][Epoch 10230] diffusion learning rate: 0.001
2024-11-05 03:00:46,797 - INFO - [diffusion][Epoch 10230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:46,798 - INFO - [diffusion][Epoch 10231] Epoch 10232/12000
2024-11-05 03:00:51,020 - INFO - [diffusion][Epoch 10231] diffusion training Loss: 0.06077194306999445
2024-11-05 03:00:51,022 - INFO - [diffusion][Epoch 10231] diffusion learning rate: 0.001
2024-11-05 03:00:51,024 - INFO - [diffusion][Epoch 10231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:51,025 - INFO - [diffusion][Epoch 10232] Epoch 10233/12000
2024-11-05 03:00:55,172 - INFO - [diffusion][Epoch 10232] diffusion training Loss: 0.06412407848984003
2024-11-05 03:00:55,175 - INFO - [diffusion][Epoch 10232] diffusion learning rate: 0.001
2024-11-05 03:00:55,177 - INFO - [diffusion][Epoch 10232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:55,179 - INFO - [diffusion][Epoch 10233] Epoch 10234/12000
2024-11-05 03:00:59,427 - INFO - [diffusion][Epoch 10233] diffusion training Loss: 0.05967387184500694
2024-11-05 03:00:59,429 - INFO - [diffusion][Epoch 10233] diffusion learning rate: 0.001
2024-11-05 03:00:59,431 - INFO - [diffusion][Epoch 10233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:59,432 - INFO - [diffusion][Epoch 10234] Epoch 10235/12000
2024-11-05 03:01:03,729 - INFO - [diffusion][Epoch 10234] diffusion training Loss: 0.05466074123978615
2024-11-05 03:01:03,731 - INFO - [diffusion][Epoch 10234] diffusion learning rate: 0.001
2024-11-05 03:01:03,732 - INFO - [diffusion][Epoch 10234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:03,733 - INFO - [diffusion][Epoch 10235] Epoch 10236/12000
2024-11-05 03:01:07,786 - INFO - [diffusion][Epoch 10235] diffusion training Loss: 0.0605772789567709
2024-11-05 03:01:07,788 - INFO - [diffusion][Epoch 10235] diffusion learning rate: 0.001
2024-11-05 03:01:07,790 - INFO - [diffusion][Epoch 10235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:07,791 - INFO - [diffusion][Epoch 10236] Epoch 10237/12000
2024-11-05 03:01:11,951 - INFO - [diffusion][Epoch 10236] diffusion training Loss: 0.06550035066902637
2024-11-05 03:01:11,953 - INFO - [diffusion][Epoch 10236] diffusion learning rate: 0.001
2024-11-05 03:01:11,955 - INFO - [diffusion][Epoch 10236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:11,956 - INFO - [diffusion][Epoch 10237] Epoch 10238/12000
2024-11-05 03:01:16,045 - INFO - [diffusion][Epoch 10237] diffusion training Loss: 0.06032464001327753
2024-11-05 03:01:16,047 - INFO - [diffusion][Epoch 10237] diffusion learning rate: 0.001
2024-11-05 03:01:16,048 - INFO - [diffusion][Epoch 10237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:16,050 - INFO - [diffusion][Epoch 10238] Epoch 10239/12000
2024-11-05 03:01:20,026 - INFO - [diffusion][Epoch 10238] diffusion training Loss: 0.058237952180206776
2024-11-05 03:01:20,028 - INFO - [diffusion][Epoch 10238] diffusion learning rate: 0.001
2024-11-05 03:01:20,030 - INFO - [diffusion][Epoch 10238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:20,031 - INFO - [diffusion][Epoch 10239] Epoch 10240/12000
2024-11-05 03:01:24,143 - INFO - [diffusion][Epoch 10239] diffusion training Loss: 0.0574368704110384
2024-11-05 03:01:24,145 - INFO - [diffusion][Epoch 10239] diffusion learning rate: 0.001
2024-11-05 03:01:24,146 - INFO - [diffusion][Epoch 10239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:24,148 - INFO - [diffusion][Epoch 10240] Epoch 10241/12000
2024-11-05 03:01:27,924 - INFO - [diffusion][Epoch 10240] diffusion training Loss: 0.058433386497199535
2024-11-05 03:01:27,926 - INFO - [diffusion][Epoch 10240] diffusion learning rate: 0.001
2024-11-05 03:01:27,928 - INFO - [diffusion][Epoch 10240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:27,929 - INFO - [diffusion][Epoch 10241] Epoch 10242/12000
2024-11-05 03:01:32,079 - INFO - [diffusion][Epoch 10241] diffusion training Loss: 0.0570473400875926
2024-11-05 03:01:32,081 - INFO - [diffusion][Epoch 10241] diffusion learning rate: 0.001
2024-11-05 03:01:32,083 - INFO - [diffusion][Epoch 10241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:32,084 - INFO - [diffusion][Epoch 10242] Epoch 10243/12000
2024-11-05 03:01:36,350 - INFO - [diffusion][Epoch 10242] diffusion training Loss: 0.0574725279584527
2024-11-05 03:01:36,352 - INFO - [diffusion][Epoch 10242] diffusion learning rate: 0.001
2024-11-05 03:01:36,354 - INFO - [diffusion][Epoch 10242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:36,355 - INFO - [diffusion][Epoch 10243] Epoch 10244/12000
2024-11-05 03:01:40,444 - INFO - [diffusion][Epoch 10243] diffusion training Loss: 0.06707893498241901
2024-11-05 03:01:40,446 - INFO - [diffusion][Epoch 10243] diffusion learning rate: 0.001
2024-11-05 03:01:40,448 - INFO - [diffusion][Epoch 10243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:40,449 - INFO - [diffusion][Epoch 10244] Epoch 10245/12000
2024-11-05 03:01:44,685 - INFO - [diffusion][Epoch 10244] diffusion training Loss: 0.054701254703104496
2024-11-05 03:01:44,688 - INFO - [diffusion][Epoch 10244] diffusion learning rate: 0.001
2024-11-05 03:01:44,731 - INFO - [diffusion][Epoch 10244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:44,732 - INFO - [diffusion][Epoch 10245] Epoch 10246/12000
2024-11-05 03:01:48,963 - INFO - [diffusion][Epoch 10245] diffusion training Loss: 0.058533507399261
2024-11-05 03:01:48,964 - INFO - [diffusion][Epoch 10245] diffusion learning rate: 0.001
2024-11-05 03:01:48,966 - INFO - [diffusion][Epoch 10245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:48,967 - INFO - [diffusion][Epoch 10246] Epoch 10247/12000
2024-11-05 03:01:53,316 - INFO - [diffusion][Epoch 10246] diffusion training Loss: 0.0606432156637311
2024-11-05 03:01:53,318 - INFO - [diffusion][Epoch 10246] diffusion learning rate: 0.001
2024-11-05 03:01:53,320 - INFO - [diffusion][Epoch 10246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:53,322 - INFO - [diffusion][Epoch 10247] Epoch 10248/12000
2024-11-05 03:01:57,639 - INFO - [diffusion][Epoch 10247] diffusion training Loss: 0.06045223958790302
2024-11-05 03:01:57,641 - INFO - [diffusion][Epoch 10247] diffusion learning rate: 0.001
2024-11-05 03:01:57,643 - INFO - [diffusion][Epoch 10247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:57,644 - INFO - [diffusion][Epoch 10248] Epoch 10249/12000
2024-11-05 03:02:01,918 - INFO - [diffusion][Epoch 10248] diffusion training Loss: 0.055353627540171146
2024-11-05 03:02:01,920 - INFO - [diffusion][Epoch 10248] diffusion learning rate: 0.001
2024-11-05 03:02:01,923 - INFO - [diffusion][Epoch 10248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:01,924 - INFO - [diffusion][Epoch 10249] Epoch 10250/12000
2024-11-05 03:02:06,224 - INFO - [diffusion][Epoch 10249] diffusion training Loss: 0.06291153188794851
2024-11-05 03:02:06,227 - INFO - [diffusion][Epoch 10249] diffusion learning rate: 0.001
2024-11-05 03:02:06,229 - INFO - [diffusion][Epoch 10249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:06,231 - INFO - [diffusion][Epoch 10250] Epoch 10251/12000
2024-11-05 03:02:10,590 - INFO - [diffusion][Epoch 10250] diffusion training Loss: 0.06276064179837704
2024-11-05 03:02:10,592 - INFO - [diffusion][Epoch 10250] diffusion learning rate: 0.001
2024-11-05 03:02:10,594 - INFO - [diffusion][Epoch 10250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:10,595 - INFO - [diffusion][Epoch 10251] Epoch 10252/12000
2024-11-05 03:02:14,850 - INFO - [diffusion][Epoch 10251] diffusion training Loss: 0.06090850010514259
2024-11-05 03:02:14,852 - INFO - [diffusion][Epoch 10251] diffusion learning rate: 0.001
2024-11-05 03:02:14,854 - INFO - [diffusion][Epoch 10251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:14,856 - INFO - [diffusion][Epoch 10252] Epoch 10253/12000
2024-11-05 03:02:18,997 - INFO - [diffusion][Epoch 10252] diffusion training Loss: 0.057931787334382534
2024-11-05 03:02:18,999 - INFO - [diffusion][Epoch 10252] diffusion learning rate: 0.001
2024-11-05 03:02:19,003 - INFO - [diffusion][Epoch 10252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:19,005 - INFO - [diffusion][Epoch 10253] Epoch 10254/12000
2024-11-05 03:02:23,080 - INFO - [diffusion][Epoch 10253] diffusion training Loss: 0.0606141434982419
2024-11-05 03:02:23,082 - INFO - [diffusion][Epoch 10253] diffusion learning rate: 0.001
2024-11-05 03:02:23,084 - INFO - [diffusion][Epoch 10253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:23,085 - INFO - [diffusion][Epoch 10254] Epoch 10255/12000
2024-11-05 03:02:27,371 - INFO - [diffusion][Epoch 10254] diffusion training Loss: 0.058715359307825565
2024-11-05 03:02:27,374 - INFO - [diffusion][Epoch 10254] diffusion learning rate: 0.001
2024-11-05 03:02:27,376 - INFO - [diffusion][Epoch 10254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:27,378 - INFO - [diffusion][Epoch 10255] Epoch 10256/12000
2024-11-05 03:02:31,406 - INFO - [diffusion][Epoch 10255] diffusion training Loss: 0.05726887937635183
2024-11-05 03:02:31,408 - INFO - [diffusion][Epoch 10255] diffusion learning rate: 0.001
2024-11-05 03:02:31,409 - INFO - [diffusion][Epoch 10255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:31,410 - INFO - [diffusion][Epoch 10256] Epoch 10257/12000
2024-11-05 03:02:35,662 - INFO - [diffusion][Epoch 10256] diffusion training Loss: 0.05932620074599981
2024-11-05 03:02:35,664 - INFO - [diffusion][Epoch 10256] diffusion learning rate: 0.001
2024-11-05 03:02:35,665 - INFO - [diffusion][Epoch 10256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:35,667 - INFO - [diffusion][Epoch 10257] Epoch 10258/12000
2024-11-05 03:02:39,822 - INFO - [diffusion][Epoch 10257] diffusion training Loss: 0.06652048788964748
2024-11-05 03:02:39,824 - INFO - [diffusion][Epoch 10257] diffusion learning rate: 0.001
2024-11-05 03:02:39,826 - INFO - [diffusion][Epoch 10257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:39,827 - INFO - [diffusion][Epoch 10258] Epoch 10259/12000
2024-11-05 03:02:43,945 - INFO - [diffusion][Epoch 10258] diffusion training Loss: 0.06084295082837343
2024-11-05 03:02:43,947 - INFO - [diffusion][Epoch 10258] diffusion learning rate: 0.001
2024-11-05 03:02:43,949 - INFO - [diffusion][Epoch 10258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:43,950 - INFO - [diffusion][Epoch 10259] Epoch 10260/12000
2024-11-05 03:02:48,015 - INFO - [diffusion][Epoch 10259] diffusion training Loss: 0.06047184858471155
2024-11-05 03:02:48,017 - INFO - [diffusion][Epoch 10259] diffusion learning rate: 0.001
2024-11-05 03:02:48,019 - INFO - [diffusion][Epoch 10259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:48,020 - INFO - [diffusion][Epoch 10260] Epoch 10261/12000
2024-11-05 03:02:52,221 - INFO - [diffusion][Epoch 10260] diffusion training Loss: 0.06011374853551388
2024-11-05 03:02:52,223 - INFO - [diffusion][Epoch 10260] diffusion learning rate: 0.001
2024-11-05 03:02:52,225 - INFO - [diffusion][Epoch 10260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:52,227 - INFO - [diffusion][Epoch 10261] Epoch 10262/12000
2024-11-05 03:02:56,293 - INFO - [diffusion][Epoch 10261] diffusion training Loss: 0.060109290294349194
2024-11-05 03:02:56,295 - INFO - [diffusion][Epoch 10261] diffusion learning rate: 0.001
2024-11-05 03:02:56,297 - INFO - [diffusion][Epoch 10261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:56,298 - INFO - [diffusion][Epoch 10262] Epoch 10263/12000
2024-11-05 03:03:00,854 - INFO - [diffusion][Epoch 10262] diffusion training Loss: 0.06098045874387026
2024-11-05 03:03:00,856 - INFO - [diffusion][Epoch 10262] diffusion learning rate: 0.001
2024-11-05 03:03:00,858 - INFO - [diffusion][Epoch 10262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:00,859 - INFO - [diffusion][Epoch 10263] Epoch 10264/12000
2024-11-05 03:03:05,093 - INFO - [diffusion][Epoch 10263] diffusion training Loss: 0.06420136615633965
2024-11-05 03:03:05,095 - INFO - [diffusion][Epoch 10263] diffusion learning rate: 0.001
2024-11-05 03:03:05,097 - INFO - [diffusion][Epoch 10263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:05,098 - INFO - [diffusion][Epoch 10264] Epoch 10265/12000
2024-11-05 03:03:09,150 - INFO - [diffusion][Epoch 10264] diffusion training Loss: 0.060247765854001045
2024-11-05 03:03:09,152 - INFO - [diffusion][Epoch 10264] diffusion learning rate: 0.001
2024-11-05 03:03:09,154 - INFO - [diffusion][Epoch 10264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:09,155 - INFO - [diffusion][Epoch 10265] Epoch 10266/12000
2024-11-05 03:03:13,407 - INFO - [diffusion][Epoch 10265] diffusion training Loss: 0.05937595013529062
2024-11-05 03:03:13,409 - INFO - [diffusion][Epoch 10265] diffusion learning rate: 0.001
2024-11-05 03:03:13,412 - INFO - [diffusion][Epoch 10265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:13,413 - INFO - [diffusion][Epoch 10266] Epoch 10267/12000
2024-11-05 03:03:17,557 - INFO - [diffusion][Epoch 10266] diffusion training Loss: 0.065876011736691
2024-11-05 03:03:17,558 - INFO - [diffusion][Epoch 10266] diffusion learning rate: 0.001
2024-11-05 03:03:17,560 - INFO - [diffusion][Epoch 10266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:17,561 - INFO - [diffusion][Epoch 10267] Epoch 10268/12000
2024-11-05 03:03:21,821 - INFO - [diffusion][Epoch 10267] diffusion training Loss: 0.064065164886415
2024-11-05 03:03:21,823 - INFO - [diffusion][Epoch 10267] diffusion learning rate: 0.001
2024-11-05 03:03:21,824 - INFO - [diffusion][Epoch 10267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:21,826 - INFO - [diffusion][Epoch 10268] Epoch 10269/12000
2024-11-05 03:03:26,125 - INFO - [diffusion][Epoch 10268] diffusion training Loss: 0.06674125324934721
2024-11-05 03:03:26,127 - INFO - [diffusion][Epoch 10268] diffusion learning rate: 0.001
2024-11-05 03:03:26,129 - INFO - [diffusion][Epoch 10268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:26,130 - INFO - [diffusion][Epoch 10269] Epoch 10270/12000
2024-11-05 03:03:30,149 - INFO - [diffusion][Epoch 10269] diffusion training Loss: 0.06363538652658463
2024-11-05 03:03:30,151 - INFO - [diffusion][Epoch 10269] diffusion learning rate: 0.001
2024-11-05 03:03:30,193 - INFO - [diffusion][Epoch 10269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:30,195 - INFO - [diffusion][Epoch 10270] Epoch 10271/12000
2024-11-05 03:03:34,358 - INFO - [diffusion][Epoch 10270] diffusion training Loss: 0.071684954687953
2024-11-05 03:03:34,360 - INFO - [diffusion][Epoch 10270] diffusion learning rate: 0.001
2024-11-05 03:03:34,362 - INFO - [diffusion][Epoch 10270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:34,363 - INFO - [diffusion][Epoch 10271] Epoch 10272/12000
2024-11-05 03:03:38,664 - INFO - [diffusion][Epoch 10271] diffusion training Loss: 0.06311624497175217
2024-11-05 03:03:38,667 - INFO - [diffusion][Epoch 10271] diffusion learning rate: 0.001
2024-11-05 03:03:38,669 - INFO - [diffusion][Epoch 10271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:38,671 - INFO - [diffusion][Epoch 10272] Epoch 10273/12000
2024-11-05 03:03:42,983 - INFO - [diffusion][Epoch 10272] diffusion training Loss: 0.06479601748287678
2024-11-05 03:03:42,985 - INFO - [diffusion][Epoch 10272] diffusion learning rate: 0.001
2024-11-05 03:03:42,987 - INFO - [diffusion][Epoch 10272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:42,989 - INFO - [diffusion][Epoch 10273] Epoch 10274/12000
2024-11-05 03:03:46,983 - INFO - [diffusion][Epoch 10273] diffusion training Loss: 0.0663154311478138
2024-11-05 03:03:46,986 - INFO - [diffusion][Epoch 10273] diffusion learning rate: 0.001
2024-11-05 03:03:46,987 - INFO - [diffusion][Epoch 10273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:46,989 - INFO - [diffusion][Epoch 10274] Epoch 10275/12000
2024-11-05 03:03:51,028 - INFO - [diffusion][Epoch 10274] diffusion training Loss: 0.058729782700538635
2024-11-05 03:03:51,031 - INFO - [diffusion][Epoch 10274] diffusion learning rate: 0.001
2024-11-05 03:03:51,033 - INFO - [diffusion][Epoch 10274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:51,035 - INFO - [diffusion][Epoch 10275] Epoch 10276/12000
2024-11-05 03:03:55,227 - INFO - [diffusion][Epoch 10275] diffusion training Loss: 0.06306986417621374
2024-11-05 03:03:55,230 - INFO - [diffusion][Epoch 10275] diffusion learning rate: 0.001
2024-11-05 03:03:55,232 - INFO - [diffusion][Epoch 10275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:55,234 - INFO - [diffusion][Epoch 10276] Epoch 10277/12000
2024-11-05 03:03:59,272 - INFO - [diffusion][Epoch 10276] diffusion training Loss: 0.05716981738805771
2024-11-05 03:03:59,274 - INFO - [diffusion][Epoch 10276] diffusion learning rate: 0.001
2024-11-05 03:03:59,276 - INFO - [diffusion][Epoch 10276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:59,277 - INFO - [diffusion][Epoch 10277] Epoch 10278/12000
2024-11-05 03:04:03,420 - INFO - [diffusion][Epoch 10277] diffusion training Loss: 0.0633855415508151
2024-11-05 03:04:03,422 - INFO - [diffusion][Epoch 10277] diffusion learning rate: 0.001
2024-11-05 03:04:03,425 - INFO - [diffusion][Epoch 10277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:03,427 - INFO - [diffusion][Epoch 10278] Epoch 10279/12000
2024-11-05 03:04:07,586 - INFO - [diffusion][Epoch 10278] diffusion training Loss: 0.05931536667048931
2024-11-05 03:04:07,588 - INFO - [diffusion][Epoch 10278] diffusion learning rate: 0.001
2024-11-05 03:04:07,589 - INFO - [diffusion][Epoch 10278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:07,591 - INFO - [diffusion][Epoch 10279] Epoch 10280/12000
2024-11-05 03:04:11,735 - INFO - [diffusion][Epoch 10279] diffusion training Loss: 0.059256874956190586
2024-11-05 03:04:11,737 - INFO - [diffusion][Epoch 10279] diffusion learning rate: 0.001
2024-11-05 03:04:11,739 - INFO - [diffusion][Epoch 10279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:11,740 - INFO - [diffusion][Epoch 10280] Epoch 10281/12000
2024-11-05 03:04:15,957 - INFO - [diffusion][Epoch 10280] diffusion training Loss: 0.0647055096924305
2024-11-05 03:04:15,959 - INFO - [diffusion][Epoch 10280] diffusion learning rate: 0.001
2024-11-05 03:04:15,961 - INFO - [diffusion][Epoch 10280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:15,963 - INFO - [diffusion][Epoch 10281] Epoch 10282/12000
2024-11-05 03:04:20,197 - INFO - [diffusion][Epoch 10281] diffusion training Loss: 0.06563385017216206
2024-11-05 03:04:20,199 - INFO - [diffusion][Epoch 10281] diffusion learning rate: 0.001
2024-11-05 03:04:20,201 - INFO - [diffusion][Epoch 10281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:20,203 - INFO - [diffusion][Epoch 10282] Epoch 10283/12000
2024-11-05 03:04:24,269 - INFO - [diffusion][Epoch 10282] diffusion training Loss: 0.06249970942735672
2024-11-05 03:04:24,272 - INFO - [diffusion][Epoch 10282] diffusion learning rate: 0.001
2024-11-05 03:04:24,274 - INFO - [diffusion][Epoch 10282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:24,275 - INFO - [diffusion][Epoch 10283] Epoch 10284/12000
2024-11-05 03:04:28,858 - INFO - [diffusion][Epoch 10283] diffusion training Loss: 0.05500352755188942
2024-11-05 03:04:28,860 - INFO - [diffusion][Epoch 10283] diffusion learning rate: 0.001
2024-11-05 03:04:28,862 - INFO - [diffusion][Epoch 10283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:28,863 - INFO - [diffusion][Epoch 10284] Epoch 10285/12000
2024-11-05 03:04:33,011 - INFO - [diffusion][Epoch 10284] diffusion training Loss: 0.0646081194281578
2024-11-05 03:04:33,014 - INFO - [diffusion][Epoch 10284] diffusion learning rate: 0.001
2024-11-05 03:04:33,016 - INFO - [diffusion][Epoch 10284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:33,017 - INFO - [diffusion][Epoch 10285] Epoch 10286/12000
2024-11-05 03:04:37,136 - INFO - [diffusion][Epoch 10285] diffusion training Loss: 0.05858945567160845
2024-11-05 03:04:37,138 - INFO - [diffusion][Epoch 10285] diffusion learning rate: 0.001
2024-11-05 03:04:37,140 - INFO - [diffusion][Epoch 10285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:37,141 - INFO - [diffusion][Epoch 10286] Epoch 10287/12000
2024-11-05 03:04:41,192 - INFO - [diffusion][Epoch 10286] diffusion training Loss: 0.05438538268208504
2024-11-05 03:04:41,194 - INFO - [diffusion][Epoch 10286] diffusion learning rate: 0.001
2024-11-05 03:04:41,196 - INFO - [diffusion][Epoch 10286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:41,197 - INFO - [diffusion][Epoch 10287] Epoch 10288/12000
2024-11-05 03:04:45,380 - INFO - [diffusion][Epoch 10287] diffusion training Loss: 0.06149324309080839
2024-11-05 03:04:45,383 - INFO - [diffusion][Epoch 10287] diffusion learning rate: 0.001
2024-11-05 03:04:45,385 - INFO - [diffusion][Epoch 10287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:45,386 - INFO - [diffusion][Epoch 10288] Epoch 10289/12000
2024-11-05 03:04:49,328 - INFO - [diffusion][Epoch 10288] diffusion training Loss: 0.062042576260864735
2024-11-05 03:04:49,330 - INFO - [diffusion][Epoch 10288] diffusion learning rate: 0.001
2024-11-05 03:04:49,332 - INFO - [diffusion][Epoch 10288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:49,333 - INFO - [diffusion][Epoch 10289] Epoch 10290/12000
2024-11-05 03:04:53,551 - INFO - [diffusion][Epoch 10289] diffusion training Loss: 0.06428179331123829
2024-11-05 03:04:53,553 - INFO - [diffusion][Epoch 10289] diffusion learning rate: 0.001
2024-11-05 03:04:53,555 - INFO - [diffusion][Epoch 10289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:53,556 - INFO - [diffusion][Epoch 10290] Epoch 10291/12000
2024-11-05 03:04:57,577 - INFO - [diffusion][Epoch 10290] diffusion training Loss: 0.06280978955328465
2024-11-05 03:04:57,579 - INFO - [diffusion][Epoch 10290] diffusion learning rate: 0.001
2024-11-05 03:04:57,581 - INFO - [diffusion][Epoch 10290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:57,582 - INFO - [diffusion][Epoch 10291] Epoch 10292/12000
2024-11-05 03:05:01,797 - INFO - [diffusion][Epoch 10291] diffusion training Loss: 0.06206974759697914
2024-11-05 03:05:01,799 - INFO - [diffusion][Epoch 10291] diffusion learning rate: 0.001
2024-11-05 03:05:01,801 - INFO - [diffusion][Epoch 10291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:01,802 - INFO - [diffusion][Epoch 10292] Epoch 10293/12000
2024-11-05 03:05:05,876 - INFO - [diffusion][Epoch 10292] diffusion training Loss: 0.061973704025149345
2024-11-05 03:05:05,878 - INFO - [diffusion][Epoch 10292] diffusion learning rate: 0.001
2024-11-05 03:05:05,880 - INFO - [diffusion][Epoch 10292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:05,881 - INFO - [diffusion][Epoch 10293] Epoch 10294/12000
2024-11-05 03:05:10,012 - INFO - [diffusion][Epoch 10293] diffusion training Loss: 0.06353862956166267
2024-11-05 03:05:10,014 - INFO - [diffusion][Epoch 10293] diffusion learning rate: 0.001
2024-11-05 03:05:10,016 - INFO - [diffusion][Epoch 10293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:10,017 - INFO - [diffusion][Epoch 10294] Epoch 10295/12000
2024-11-05 03:05:14,298 - INFO - [diffusion][Epoch 10294] diffusion training Loss: 0.06257801689207554
2024-11-05 03:05:14,300 - INFO - [diffusion][Epoch 10294] diffusion learning rate: 0.001
2024-11-05 03:05:14,302 - INFO - [diffusion][Epoch 10294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:14,303 - INFO - [diffusion][Epoch 10295] Epoch 10296/12000
2024-11-05 03:05:18,471 - INFO - [diffusion][Epoch 10295] diffusion training Loss: 0.05847332999110222
2024-11-05 03:05:18,473 - INFO - [diffusion][Epoch 10295] diffusion learning rate: 0.001
2024-11-05 03:05:18,475 - INFO - [diffusion][Epoch 10295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:18,476 - INFO - [diffusion][Epoch 10296] Epoch 10297/12000
2024-11-05 03:05:22,676 - INFO - [diffusion][Epoch 10296] diffusion training Loss: 0.05956902075558901
2024-11-05 03:05:22,678 - INFO - [diffusion][Epoch 10296] diffusion learning rate: 0.001
2024-11-05 03:05:22,680 - INFO - [diffusion][Epoch 10296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:22,681 - INFO - [diffusion][Epoch 10297] Epoch 10298/12000
2024-11-05 03:05:26,809 - INFO - [diffusion][Epoch 10297] diffusion training Loss: 0.061556496657431126
2024-11-05 03:05:26,812 - INFO - [diffusion][Epoch 10297] diffusion learning rate: 0.001
2024-11-05 03:05:26,814 - INFO - [diffusion][Epoch 10297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:26,815 - INFO - [diffusion][Epoch 10298] Epoch 10299/12000
2024-11-05 03:05:30,760 - INFO - [diffusion][Epoch 10298] diffusion training Loss: 0.06292342953383923
2024-11-05 03:05:30,761 - INFO - [diffusion][Epoch 10298] diffusion learning rate: 0.001
2024-11-05 03:05:30,763 - INFO - [diffusion][Epoch 10298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:30,765 - INFO - [diffusion][Epoch 10299] Epoch 10300/12000
2024-11-05 03:05:35,063 - INFO - [diffusion][Epoch 10299] diffusion training Loss: 0.0605340301990509
2024-11-05 03:05:35,066 - INFO - [diffusion][Epoch 10299] diffusion learning rate: 0.001
2024-11-05 03:05:35,107 - INFO - [diffusion][Epoch 10299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:35,109 - INFO - [diffusion][Epoch 10300] Epoch 10301/12000
2024-11-05 03:05:39,433 - INFO - [diffusion][Epoch 10300] diffusion training Loss: 0.05780468322336674
2024-11-05 03:05:39,435 - INFO - [diffusion][Epoch 10300] diffusion learning rate: 0.001
2024-11-05 03:05:39,437 - INFO - [diffusion][Epoch 10300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:39,438 - INFO - [diffusion][Epoch 10301] Epoch 10302/12000
2024-11-05 03:05:43,682 - INFO - [diffusion][Epoch 10301] diffusion training Loss: 0.06125371064990759
2024-11-05 03:05:43,684 - INFO - [diffusion][Epoch 10301] diffusion learning rate: 0.001
2024-11-05 03:05:43,686 - INFO - [diffusion][Epoch 10301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:43,687 - INFO - [diffusion][Epoch 10302] Epoch 10303/12000
2024-11-05 03:05:47,976 - INFO - [diffusion][Epoch 10302] diffusion training Loss: 0.059472065418958664
2024-11-05 03:05:47,980 - INFO - [diffusion][Epoch 10302] diffusion learning rate: 0.001
2024-11-05 03:05:47,982 - INFO - [diffusion][Epoch 10302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:47,984 - INFO - [diffusion][Epoch 10303] Epoch 10304/12000
2024-11-05 03:05:52,043 - INFO - [diffusion][Epoch 10303] diffusion training Loss: 0.06180866248905659
2024-11-05 03:05:52,044 - INFO - [diffusion][Epoch 10303] diffusion learning rate: 0.001
2024-11-05 03:05:52,046 - INFO - [diffusion][Epoch 10303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:52,048 - INFO - [diffusion][Epoch 10304] Epoch 10305/12000
2024-11-05 03:05:56,431 - INFO - [diffusion][Epoch 10304] diffusion training Loss: 0.06282670423388481
2024-11-05 03:05:56,434 - INFO - [diffusion][Epoch 10304] diffusion learning rate: 0.001
2024-11-05 03:05:56,436 - INFO - [diffusion][Epoch 10304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:56,437 - INFO - [diffusion][Epoch 10305] Epoch 10306/12000
2024-11-05 03:06:00,617 - INFO - [diffusion][Epoch 10305] diffusion training Loss: 0.05536236986517906
2024-11-05 03:06:00,619 - INFO - [diffusion][Epoch 10305] diffusion learning rate: 0.001
2024-11-05 03:06:00,621 - INFO - [diffusion][Epoch 10305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:00,623 - INFO - [diffusion][Epoch 10306] Epoch 10307/12000
2024-11-05 03:06:04,872 - INFO - [diffusion][Epoch 10306] diffusion training Loss: 0.06250752042979002
2024-11-05 03:06:04,874 - INFO - [diffusion][Epoch 10306] diffusion learning rate: 0.001
2024-11-05 03:06:04,876 - INFO - [diffusion][Epoch 10306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:04,877 - INFO - [diffusion][Epoch 10307] Epoch 10308/12000
2024-11-05 03:06:09,156 - INFO - [diffusion][Epoch 10307] diffusion training Loss: 0.06475928146392107
2024-11-05 03:06:09,158 - INFO - [diffusion][Epoch 10307] diffusion learning rate: 0.001
2024-11-05 03:06:09,160 - INFO - [diffusion][Epoch 10307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:09,162 - INFO - [diffusion][Epoch 10308] Epoch 10309/12000
2024-11-05 03:06:13,491 - INFO - [diffusion][Epoch 10308] diffusion training Loss: 0.06201784126460552
2024-11-05 03:06:13,493 - INFO - [diffusion][Epoch 10308] diffusion learning rate: 0.001
2024-11-05 03:06:13,495 - INFO - [diffusion][Epoch 10308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:13,497 - INFO - [diffusion][Epoch 10309] Epoch 10310/12000
2024-11-05 03:06:17,776 - INFO - [diffusion][Epoch 10309] diffusion training Loss: 0.06311988644301891
2024-11-05 03:06:17,778 - INFO - [diffusion][Epoch 10309] diffusion learning rate: 0.001
2024-11-05 03:06:17,828 - INFO - [diffusion][Epoch 10309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:17,829 - INFO - [diffusion][Epoch 10310] Epoch 10311/12000
2024-11-05 03:06:22,085 - INFO - [diffusion][Epoch 10310] diffusion training Loss: 0.062069025821983814
2024-11-05 03:06:22,087 - INFO - [diffusion][Epoch 10310] diffusion learning rate: 0.001
2024-11-05 03:06:22,090 - INFO - [diffusion][Epoch 10310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:22,091 - INFO - [diffusion][Epoch 10311] Epoch 10312/12000
2024-11-05 03:06:26,308 - INFO - [diffusion][Epoch 10311] diffusion training Loss: 0.062424156814813614
2024-11-05 03:06:26,310 - INFO - [diffusion][Epoch 10311] diffusion learning rate: 0.001
2024-11-05 03:06:26,311 - INFO - [diffusion][Epoch 10311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:26,313 - INFO - [diffusion][Epoch 10312] Epoch 10313/12000
2024-11-05 03:06:30,466 - INFO - [diffusion][Epoch 10312] diffusion training Loss: 0.061703579500317574
2024-11-05 03:06:30,554 - INFO - [diffusion][Epoch 10312] diffusion learning rate: 0.001
2024-11-05 03:06:30,557 - INFO - [diffusion][Epoch 10312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:30,558 - INFO - [diffusion][Epoch 10313] Epoch 10314/12000
2024-11-05 03:06:34,783 - INFO - [diffusion][Epoch 10313] diffusion training Loss: 0.05747831612825394
2024-11-05 03:06:34,785 - INFO - [diffusion][Epoch 10313] diffusion learning rate: 0.001
2024-11-05 03:06:34,787 - INFO - [diffusion][Epoch 10313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:34,788 - INFO - [diffusion][Epoch 10314] Epoch 10315/12000
2024-11-05 03:06:39,097 - INFO - [diffusion][Epoch 10314] diffusion training Loss: 0.06077626347541809
2024-11-05 03:06:39,100 - INFO - [diffusion][Epoch 10314] diffusion learning rate: 0.001
2024-11-05 03:06:39,102 - INFO - [diffusion][Epoch 10314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:39,103 - INFO - [diffusion][Epoch 10315] Epoch 10316/12000
2024-11-05 03:06:43,270 - INFO - [diffusion][Epoch 10315] diffusion training Loss: 0.057666925713419914
2024-11-05 03:06:43,273 - INFO - [diffusion][Epoch 10315] diffusion learning rate: 0.001
2024-11-05 03:06:43,274 - INFO - [diffusion][Epoch 10315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:43,276 - INFO - [diffusion][Epoch 10316] Epoch 10317/12000
2024-11-05 03:06:47,480 - INFO - [diffusion][Epoch 10316] diffusion training Loss: 0.06549212895333767
2024-11-05 03:06:47,482 - INFO - [diffusion][Epoch 10316] diffusion learning rate: 0.001
2024-11-05 03:06:47,484 - INFO - [diffusion][Epoch 10316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:47,486 - INFO - [diffusion][Epoch 10317] Epoch 10318/12000
2024-11-05 03:06:51,766 - INFO - [diffusion][Epoch 10317] diffusion training Loss: 0.05549297481775284
2024-11-05 03:06:51,768 - INFO - [diffusion][Epoch 10317] diffusion learning rate: 0.001
2024-11-05 03:06:51,770 - INFO - [diffusion][Epoch 10317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:51,772 - INFO - [diffusion][Epoch 10318] Epoch 10319/12000
2024-11-05 03:06:56,008 - INFO - [diffusion][Epoch 10318] diffusion training Loss: 0.06202065106481314
2024-11-05 03:06:56,010 - INFO - [diffusion][Epoch 10318] diffusion learning rate: 0.001
2024-11-05 03:06:56,012 - INFO - [diffusion][Epoch 10318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:56,013 - INFO - [diffusion][Epoch 10319] Epoch 10320/12000
2024-11-05 03:07:00,215 - INFO - [diffusion][Epoch 10319] diffusion training Loss: 0.06636833399534225
2024-11-05 03:07:00,218 - INFO - [diffusion][Epoch 10319] diffusion learning rate: 0.001
2024-11-05 03:07:00,220 - INFO - [diffusion][Epoch 10319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:00,221 - INFO - [diffusion][Epoch 10320] Epoch 10321/12000
2024-11-05 03:07:04,403 - INFO - [diffusion][Epoch 10320] diffusion training Loss: 0.05939516518265009
2024-11-05 03:07:04,405 - INFO - [diffusion][Epoch 10320] diffusion learning rate: 0.001
2024-11-05 03:07:04,407 - INFO - [diffusion][Epoch 10320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:04,409 - INFO - [diffusion][Epoch 10321] Epoch 10322/12000
2024-11-05 03:07:08,578 - INFO - [diffusion][Epoch 10321] diffusion training Loss: 0.05751435365527868
2024-11-05 03:07:08,580 - INFO - [diffusion][Epoch 10321] diffusion learning rate: 0.001
2024-11-05 03:07:08,582 - INFO - [diffusion][Epoch 10321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:08,583 - INFO - [diffusion][Epoch 10322] Epoch 10323/12000
2024-11-05 03:07:12,750 - INFO - [diffusion][Epoch 10322] diffusion training Loss: 0.05787286255508661
2024-11-05 03:07:12,752 - INFO - [diffusion][Epoch 10322] diffusion learning rate: 0.001
2024-11-05 03:07:12,754 - INFO - [diffusion][Epoch 10322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:12,755 - INFO - [diffusion][Epoch 10323] Epoch 10324/12000
2024-11-05 03:07:16,987 - INFO - [diffusion][Epoch 10323] diffusion training Loss: 0.06116276606917381
2024-11-05 03:07:16,989 - INFO - [diffusion][Epoch 10323] diffusion learning rate: 0.001
2024-11-05 03:07:16,991 - INFO - [diffusion][Epoch 10323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:16,993 - INFO - [diffusion][Epoch 10324] Epoch 10325/12000
2024-11-05 03:07:21,838 - INFO - [diffusion][Epoch 10324] diffusion training Loss: 0.05908984690904617
2024-11-05 03:07:21,840 - INFO - [diffusion][Epoch 10324] diffusion learning rate: 0.001
2024-11-05 03:07:21,842 - INFO - [diffusion][Epoch 10324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:21,843 - INFO - [diffusion][Epoch 10325] Epoch 10326/12000
2024-11-05 03:07:26,009 - INFO - [diffusion][Epoch 10325] diffusion training Loss: 0.059218237176537514
2024-11-05 03:07:26,011 - INFO - [diffusion][Epoch 10325] diffusion learning rate: 0.001
2024-11-05 03:07:26,013 - INFO - [diffusion][Epoch 10325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:26,014 - INFO - [diffusion][Epoch 10326] Epoch 10327/12000
2024-11-05 03:07:30,254 - INFO - [diffusion][Epoch 10326] diffusion training Loss: 0.060409845784306526
2024-11-05 03:07:30,256 - INFO - [diffusion][Epoch 10326] diffusion learning rate: 0.001
2024-11-05 03:07:30,258 - INFO - [diffusion][Epoch 10326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:30,260 - INFO - [diffusion][Epoch 10327] Epoch 10328/12000
2024-11-05 03:07:34,530 - INFO - [diffusion][Epoch 10327] diffusion training Loss: 0.05801829416304827
2024-11-05 03:07:34,532 - INFO - [diffusion][Epoch 10327] diffusion learning rate: 0.001
2024-11-05 03:07:34,534 - INFO - [diffusion][Epoch 10327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:34,535 - INFO - [diffusion][Epoch 10328] Epoch 10329/12000
2024-11-05 03:07:38,726 - INFO - [diffusion][Epoch 10328] diffusion training Loss: 0.06651250086724758
2024-11-05 03:07:38,729 - INFO - [diffusion][Epoch 10328] diffusion learning rate: 0.001
2024-11-05 03:07:38,731 - INFO - [diffusion][Epoch 10328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:38,732 - INFO - [diffusion][Epoch 10329] Epoch 10330/12000
2024-11-05 03:07:42,993 - INFO - [diffusion][Epoch 10329] diffusion training Loss: 0.06042786128818989
2024-11-05 03:07:42,995 - INFO - [diffusion][Epoch 10329] diffusion learning rate: 0.001
2024-11-05 03:07:42,997 - INFO - [diffusion][Epoch 10329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:42,999 - INFO - [diffusion][Epoch 10330] Epoch 10331/12000
2024-11-05 03:07:47,235 - INFO - [diffusion][Epoch 10330] diffusion training Loss: 0.06251840200275183
2024-11-05 03:07:47,237 - INFO - [diffusion][Epoch 10330] diffusion learning rate: 0.001
2024-11-05 03:07:47,239 - INFO - [diffusion][Epoch 10330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:47,240 - INFO - [diffusion][Epoch 10331] Epoch 10332/12000
2024-11-05 03:07:51,513 - INFO - [diffusion][Epoch 10331] diffusion training Loss: 0.06623890809714794
2024-11-05 03:07:51,515 - INFO - [diffusion][Epoch 10331] diffusion learning rate: 0.001
2024-11-05 03:07:51,517 - INFO - [diffusion][Epoch 10331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:51,518 - INFO - [diffusion][Epoch 10332] Epoch 10333/12000
2024-11-05 03:07:55,750 - INFO - [diffusion][Epoch 10332] diffusion training Loss: 0.06442956812679768
2024-11-05 03:07:55,752 - INFO - [diffusion][Epoch 10332] diffusion learning rate: 0.001
2024-11-05 03:07:55,754 - INFO - [diffusion][Epoch 10332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:55,756 - INFO - [diffusion][Epoch 10333] Epoch 10334/12000
2024-11-05 03:07:59,966 - INFO - [diffusion][Epoch 10333] diffusion training Loss: 0.06573285162448883
2024-11-05 03:07:59,968 - INFO - [diffusion][Epoch 10333] diffusion learning rate: 0.001
2024-11-05 03:07:59,971 - INFO - [diffusion][Epoch 10333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:59,973 - INFO - [diffusion][Epoch 10334] Epoch 10335/12000
2024-11-05 03:08:04,168 - INFO - [diffusion][Epoch 10334] diffusion training Loss: 0.06276174541562796
2024-11-05 03:08:04,170 - INFO - [diffusion][Epoch 10334] diffusion learning rate: 0.001
2024-11-05 03:08:04,173 - INFO - [diffusion][Epoch 10334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:04,174 - INFO - [diffusion][Epoch 10335] Epoch 10336/12000
2024-11-05 03:08:08,460 - INFO - [diffusion][Epoch 10335] diffusion training Loss: 0.05752363242208958
2024-11-05 03:08:08,463 - INFO - [diffusion][Epoch 10335] diffusion learning rate: 0.001
2024-11-05 03:08:08,465 - INFO - [diffusion][Epoch 10335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:08,466 - INFO - [diffusion][Epoch 10336] Epoch 10337/12000
2024-11-05 03:08:12,623 - INFO - [diffusion][Epoch 10336] diffusion training Loss: 0.06417477875947952
2024-11-05 03:08:12,694 - INFO - [diffusion][Epoch 10336] diffusion learning rate: 0.001
2024-11-05 03:08:12,696 - INFO - [diffusion][Epoch 10336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:12,697 - INFO - [diffusion][Epoch 10337] Epoch 10338/12000
2024-11-05 03:08:16,871 - INFO - [diffusion][Epoch 10337] diffusion training Loss: 0.06403124332427979
2024-11-05 03:08:16,873 - INFO - [diffusion][Epoch 10337] diffusion learning rate: 0.001
2024-11-05 03:08:16,875 - INFO - [diffusion][Epoch 10337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:16,876 - INFO - [diffusion][Epoch 10338] Epoch 10339/12000
2024-11-05 03:08:21,100 - INFO - [diffusion][Epoch 10338] diffusion training Loss: 0.062300718389451504
2024-11-05 03:08:21,102 - INFO - [diffusion][Epoch 10338] diffusion learning rate: 0.001
2024-11-05 03:08:21,104 - INFO - [diffusion][Epoch 10338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:21,105 - INFO - [diffusion][Epoch 10339] Epoch 10340/12000
2024-11-05 03:08:25,191 - INFO - [diffusion][Epoch 10339] diffusion training Loss: 0.05885949358344078
2024-11-05 03:08:25,193 - INFO - [diffusion][Epoch 10339] diffusion learning rate: 0.001
2024-11-05 03:08:25,194 - INFO - [diffusion][Epoch 10339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:25,196 - INFO - [diffusion][Epoch 10340] Epoch 10341/12000
2024-11-05 03:08:29,275 - INFO - [diffusion][Epoch 10340] diffusion training Loss: 0.06603020802140236
2024-11-05 03:08:29,277 - INFO - [diffusion][Epoch 10340] diffusion learning rate: 0.001
2024-11-05 03:08:29,279 - INFO - [diffusion][Epoch 10340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:29,281 - INFO - [diffusion][Epoch 10341] Epoch 10342/12000
2024-11-05 03:08:33,455 - INFO - [diffusion][Epoch 10341] diffusion training Loss: 0.06369285564869642
2024-11-05 03:08:33,457 - INFO - [diffusion][Epoch 10341] diffusion learning rate: 0.001
2024-11-05 03:08:33,459 - INFO - [diffusion][Epoch 10341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:33,460 - INFO - [diffusion][Epoch 10342] Epoch 10343/12000
2024-11-05 03:08:37,616 - INFO - [diffusion][Epoch 10342] diffusion training Loss: 0.05761180259287357
2024-11-05 03:08:37,617 - INFO - [diffusion][Epoch 10342] diffusion learning rate: 0.001
2024-11-05 03:08:37,619 - INFO - [diffusion][Epoch 10342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:37,620 - INFO - [diffusion][Epoch 10343] Epoch 10344/12000
2024-11-05 03:08:41,856 - INFO - [diffusion][Epoch 10343] diffusion training Loss: 0.05848200060427189
2024-11-05 03:08:41,858 - INFO - [diffusion][Epoch 10343] diffusion learning rate: 0.001
2024-11-05 03:08:41,860 - INFO - [diffusion][Epoch 10343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:41,861 - INFO - [diffusion][Epoch 10344] Epoch 10345/12000
2024-11-05 03:08:46,776 - INFO - [diffusion][Epoch 10344] diffusion training Loss: 0.06167521607130766
2024-11-05 03:08:46,778 - INFO - [diffusion][Epoch 10344] diffusion learning rate: 0.001
2024-11-05 03:08:46,780 - INFO - [diffusion][Epoch 10344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:46,781 - INFO - [diffusion][Epoch 10345] Epoch 10346/12000
2024-11-05 03:08:50,861 - INFO - [diffusion][Epoch 10345] diffusion training Loss: 0.0595302302390337
2024-11-05 03:08:50,863 - INFO - [diffusion][Epoch 10345] diffusion learning rate: 0.001
2024-11-05 03:08:50,865 - INFO - [diffusion][Epoch 10345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:50,866 - INFO - [diffusion][Epoch 10346] Epoch 10347/12000
2024-11-05 03:08:55,189 - INFO - [diffusion][Epoch 10346] diffusion training Loss: 0.056710848584771156
2024-11-05 03:08:55,191 - INFO - [diffusion][Epoch 10346] diffusion learning rate: 0.001
2024-11-05 03:08:55,193 - INFO - [diffusion][Epoch 10346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:55,195 - INFO - [diffusion][Epoch 10347] Epoch 10348/12000
2024-11-05 03:08:59,455 - INFO - [diffusion][Epoch 10347] diffusion training Loss: 0.060871695168316364
2024-11-05 03:08:59,457 - INFO - [diffusion][Epoch 10347] diffusion learning rate: 0.001
2024-11-05 03:08:59,459 - INFO - [diffusion][Epoch 10347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:59,460 - INFO - [diffusion][Epoch 10348] Epoch 10349/12000
2024-11-05 03:09:03,647 - INFO - [diffusion][Epoch 10348] diffusion training Loss: 0.05834723636507988
2024-11-05 03:09:03,650 - INFO - [diffusion][Epoch 10348] diffusion learning rate: 0.001
2024-11-05 03:09:03,653 - INFO - [diffusion][Epoch 10348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:03,655 - INFO - [diffusion][Epoch 10349] Epoch 10350/12000
2024-11-05 03:09:07,864 - INFO - [diffusion][Epoch 10349] diffusion training Loss: 0.06376467272639275
2024-11-05 03:09:07,866 - INFO - [diffusion][Epoch 10349] diffusion learning rate: 0.001
2024-11-05 03:09:07,867 - INFO - [diffusion][Epoch 10349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:07,869 - INFO - [diffusion][Epoch 10350] Epoch 10351/12000
2024-11-05 03:09:11,935 - INFO - [diffusion][Epoch 10350] diffusion training Loss: 0.06310136057436466
2024-11-05 03:09:11,937 - INFO - [diffusion][Epoch 10350] diffusion learning rate: 0.001
2024-11-05 03:09:11,978 - INFO - [diffusion][Epoch 10350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:11,980 - INFO - [diffusion][Epoch 10351] Epoch 10352/12000
2024-11-05 03:09:15,688 - INFO - [diffusion][Epoch 10351] diffusion training Loss: 0.05860871355980635
2024-11-05 03:09:15,690 - INFO - [diffusion][Epoch 10351] diffusion learning rate: 0.001
2024-11-05 03:09:15,691 - INFO - [diffusion][Epoch 10351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:15,693 - INFO - [diffusion][Epoch 10352] Epoch 10353/12000
2024-11-05 03:09:19,952 - INFO - [diffusion][Epoch 10352] diffusion training Loss: 0.05581774655729532
2024-11-05 03:09:19,954 - INFO - [diffusion][Epoch 10352] diffusion learning rate: 0.001
2024-11-05 03:09:19,956 - INFO - [diffusion][Epoch 10352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:19,958 - INFO - [diffusion][Epoch 10353] Epoch 10354/12000
2024-11-05 03:09:24,018 - INFO - [diffusion][Epoch 10353] diffusion training Loss: 0.057046590372920036
2024-11-05 03:09:24,020 - INFO - [diffusion][Epoch 10353] diffusion learning rate: 0.001
2024-11-05 03:09:24,022 - INFO - [diffusion][Epoch 10353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:24,023 - INFO - [diffusion][Epoch 10354] Epoch 10355/12000
2024-11-05 03:09:28,043 - INFO - [diffusion][Epoch 10354] diffusion training Loss: 0.06299352087080479
2024-11-05 03:09:28,045 - INFO - [diffusion][Epoch 10354] diffusion learning rate: 0.001
2024-11-05 03:09:28,047 - INFO - [diffusion][Epoch 10354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:28,048 - INFO - [diffusion][Epoch 10355] Epoch 10356/12000
2024-11-05 03:09:32,394 - INFO - [diffusion][Epoch 10355] diffusion training Loss: 0.06202856823801994
2024-11-05 03:09:32,397 - INFO - [diffusion][Epoch 10355] diffusion learning rate: 0.001
2024-11-05 03:09:32,399 - INFO - [diffusion][Epoch 10355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:32,400 - INFO - [diffusion][Epoch 10356] Epoch 10357/12000
2024-11-05 03:09:36,560 - INFO - [diffusion][Epoch 10356] diffusion training Loss: 0.059004951268434525
2024-11-05 03:09:36,562 - INFO - [diffusion][Epoch 10356] diffusion learning rate: 0.001
2024-11-05 03:09:36,564 - INFO - [diffusion][Epoch 10356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:36,565 - INFO - [diffusion][Epoch 10357] Epoch 10358/12000
2024-11-05 03:09:40,757 - INFO - [diffusion][Epoch 10357] diffusion training Loss: 0.05992795806378126
2024-11-05 03:09:40,759 - INFO - [diffusion][Epoch 10357] diffusion learning rate: 0.001
2024-11-05 03:09:40,798 - INFO - [diffusion][Epoch 10357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:40,800 - INFO - [diffusion][Epoch 10358] Epoch 10359/12000
2024-11-05 03:09:44,859 - INFO - [diffusion][Epoch 10358] diffusion training Loss: 0.06426628492772579
2024-11-05 03:09:44,861 - INFO - [diffusion][Epoch 10358] diffusion learning rate: 0.001
2024-11-05 03:09:44,864 - INFO - [diffusion][Epoch 10358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:44,865 - INFO - [diffusion][Epoch 10359] Epoch 10360/12000
2024-11-05 03:09:49,146 - INFO - [diffusion][Epoch 10359] diffusion training Loss: 0.06415656860917807
2024-11-05 03:09:49,148 - INFO - [diffusion][Epoch 10359] diffusion learning rate: 0.001
2024-11-05 03:09:49,150 - INFO - [diffusion][Epoch 10359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:49,151 - INFO - [diffusion][Epoch 10360] Epoch 10361/12000
2024-11-05 03:09:53,354 - INFO - [diffusion][Epoch 10360] diffusion training Loss: 0.05733597278594971
2024-11-05 03:09:53,356 - INFO - [diffusion][Epoch 10360] diffusion learning rate: 0.001
2024-11-05 03:09:53,358 - INFO - [diffusion][Epoch 10360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:53,360 - INFO - [diffusion][Epoch 10361] Epoch 10362/12000
2024-11-05 03:09:57,644 - INFO - [diffusion][Epoch 10361] diffusion training Loss: 0.061893053352832794
2024-11-05 03:09:57,645 - INFO - [diffusion][Epoch 10361] diffusion learning rate: 0.001
2024-11-05 03:09:57,674 - INFO - [diffusion][Epoch 10361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:57,675 - INFO - [diffusion][Epoch 10362] Epoch 10363/12000
2024-11-05 03:10:01,902 - INFO - [diffusion][Epoch 10362] diffusion training Loss: 0.06278064008802176
2024-11-05 03:10:01,904 - INFO - [diffusion][Epoch 10362] diffusion learning rate: 0.001
2024-11-05 03:10:01,906 - INFO - [diffusion][Epoch 10362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:01,907 - INFO - [diffusion][Epoch 10363] Epoch 10364/12000
2024-11-05 03:10:06,160 - INFO - [diffusion][Epoch 10363] diffusion training Loss: 0.060798888094723225
2024-11-05 03:10:06,162 - INFO - [diffusion][Epoch 10363] diffusion learning rate: 0.001
2024-11-05 03:10:06,164 - INFO - [diffusion][Epoch 10363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:06,166 - INFO - [diffusion][Epoch 10364] Epoch 10365/12000
2024-11-05 03:10:10,971 - INFO - [diffusion][Epoch 10364] diffusion training Loss: 0.06447304598987103
2024-11-05 03:10:10,973 - INFO - [diffusion][Epoch 10364] diffusion learning rate: 0.001
2024-11-05 03:10:10,975 - INFO - [diffusion][Epoch 10364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:10,976 - INFO - [diffusion][Epoch 10365] Epoch 10366/12000
2024-11-05 03:10:15,265 - INFO - [diffusion][Epoch 10365] diffusion training Loss: 0.059344109147787094
2024-11-05 03:10:15,268 - INFO - [diffusion][Epoch 10365] diffusion learning rate: 0.001
2024-11-05 03:10:15,270 - INFO - [diffusion][Epoch 10365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:15,271 - INFO - [diffusion][Epoch 10366] Epoch 10367/12000
2024-11-05 03:10:19,570 - INFO - [diffusion][Epoch 10366] diffusion training Loss: 0.061140560545027256
2024-11-05 03:10:19,572 - INFO - [diffusion][Epoch 10366] diffusion learning rate: 0.001
2024-11-05 03:10:19,573 - INFO - [diffusion][Epoch 10366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:19,575 - INFO - [diffusion][Epoch 10367] Epoch 10368/12000
2024-11-05 03:10:23,802 - INFO - [diffusion][Epoch 10367] diffusion training Loss: 0.05712880194187164
2024-11-05 03:10:23,804 - INFO - [diffusion][Epoch 10367] diffusion learning rate: 0.001
2024-11-05 03:10:23,806 - INFO - [diffusion][Epoch 10367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:23,807 - INFO - [diffusion][Epoch 10368] Epoch 10369/12000
2024-11-05 03:10:27,976 - INFO - [diffusion][Epoch 10368] diffusion training Loss: 0.06252314615994692
2024-11-05 03:10:27,978 - INFO - [diffusion][Epoch 10368] diffusion learning rate: 0.001
2024-11-05 03:10:27,979 - INFO - [diffusion][Epoch 10368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:27,980 - INFO - [diffusion][Epoch 10369] Epoch 10370/12000
2024-11-05 03:10:32,173 - INFO - [diffusion][Epoch 10369] diffusion training Loss: 0.06465352512896061
2024-11-05 03:10:32,175 - INFO - [diffusion][Epoch 10369] diffusion learning rate: 0.001
2024-11-05 03:10:32,176 - INFO - [diffusion][Epoch 10369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:32,178 - INFO - [diffusion][Epoch 10370] Epoch 10371/12000
2024-11-05 03:10:36,397 - INFO - [diffusion][Epoch 10370] diffusion training Loss: 0.059884145855903625
2024-11-05 03:10:36,399 - INFO - [diffusion][Epoch 10370] diffusion learning rate: 0.001
2024-11-05 03:10:36,400 - INFO - [diffusion][Epoch 10370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:36,402 - INFO - [diffusion][Epoch 10371] Epoch 10372/12000
2024-11-05 03:10:40,692 - INFO - [diffusion][Epoch 10371] diffusion training Loss: 0.06316820532083511
2024-11-05 03:10:40,694 - INFO - [diffusion][Epoch 10371] diffusion learning rate: 0.001
2024-11-05 03:10:40,696 - INFO - [diffusion][Epoch 10371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:40,697 - INFO - [diffusion][Epoch 10372] Epoch 10373/12000
2024-11-05 03:10:44,907 - INFO - [diffusion][Epoch 10372] diffusion training Loss: 0.05722411163151264
2024-11-05 03:10:44,909 - INFO - [diffusion][Epoch 10372] diffusion learning rate: 0.001
2024-11-05 03:10:44,910 - INFO - [diffusion][Epoch 10372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:44,912 - INFO - [diffusion][Epoch 10373] Epoch 10374/12000
2024-11-05 03:10:49,068 - INFO - [diffusion][Epoch 10373] diffusion training Loss: 0.05785938259214163
2024-11-05 03:10:49,070 - INFO - [diffusion][Epoch 10373] diffusion learning rate: 0.001
2024-11-05 03:10:49,071 - INFO - [diffusion][Epoch 10373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:49,072 - INFO - [diffusion][Epoch 10374] Epoch 10375/12000
2024-11-05 03:10:53,274 - INFO - [diffusion][Epoch 10374] diffusion training Loss: 0.05598546098917723
2024-11-05 03:10:53,277 - INFO - [diffusion][Epoch 10374] diffusion learning rate: 0.001
2024-11-05 03:10:53,279 - INFO - [diffusion][Epoch 10374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:53,280 - INFO - [diffusion][Epoch 10375] Epoch 10376/12000
2024-11-05 03:10:57,359 - INFO - [diffusion][Epoch 10375] diffusion training Loss: 0.06109273247420788
2024-11-05 03:10:57,361 - INFO - [diffusion][Epoch 10375] diffusion learning rate: 0.001
2024-11-05 03:10:57,363 - INFO - [diffusion][Epoch 10375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:57,364 - INFO - [diffusion][Epoch 10376] Epoch 10377/12000
2024-11-05 03:11:01,622 - INFO - [diffusion][Epoch 10376] diffusion training Loss: 0.06472696270793676
2024-11-05 03:11:01,883 - INFO - [diffusion][Epoch 10376] diffusion learning rate: 0.001
2024-11-05 03:11:01,885 - INFO - [diffusion][Epoch 10376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:01,886 - INFO - [diffusion][Epoch 10377] Epoch 10378/12000
2024-11-05 03:11:06,127 - INFO - [diffusion][Epoch 10377] diffusion training Loss: 0.06087701302021742
2024-11-05 03:11:06,129 - INFO - [diffusion][Epoch 10377] diffusion learning rate: 0.001
2024-11-05 03:11:06,131 - INFO - [diffusion][Epoch 10377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:06,132 - INFO - [diffusion][Epoch 10378] Epoch 10379/12000
2024-11-05 03:11:10,309 - INFO - [diffusion][Epoch 10378] diffusion training Loss: 0.06233322061598301
2024-11-05 03:11:10,311 - INFO - [diffusion][Epoch 10378] diffusion learning rate: 0.001
2024-11-05 03:11:10,313 - INFO - [diffusion][Epoch 10378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:10,314 - INFO - [diffusion][Epoch 10379] Epoch 10380/12000
2024-11-05 03:11:14,589 - INFO - [diffusion][Epoch 10379] diffusion training Loss: 0.055231986567378044
2024-11-05 03:11:14,591 - INFO - [diffusion][Epoch 10379] diffusion learning rate: 0.001
2024-11-05 03:11:14,593 - INFO - [diffusion][Epoch 10379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:14,594 - INFO - [diffusion][Epoch 10380] Epoch 10381/12000
2024-11-05 03:11:18,833 - INFO - [diffusion][Epoch 10380] diffusion training Loss: 0.059712545946240425
2024-11-05 03:11:18,835 - INFO - [diffusion][Epoch 10380] diffusion learning rate: 0.001
2024-11-05 03:11:18,837 - INFO - [diffusion][Epoch 10380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:18,839 - INFO - [diffusion][Epoch 10381] Epoch 10382/12000
2024-11-05 03:11:22,973 - INFO - [diffusion][Epoch 10381] diffusion training Loss: 0.06484984420239925
2024-11-05 03:11:22,975 - INFO - [diffusion][Epoch 10381] diffusion learning rate: 0.001
2024-11-05 03:11:22,978 - INFO - [diffusion][Epoch 10381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:22,979 - INFO - [diffusion][Epoch 10382] Epoch 10383/12000
2024-11-05 03:11:27,226 - INFO - [diffusion][Epoch 10382] diffusion training Loss: 0.05655477195978165
2024-11-05 03:11:27,228 - INFO - [diffusion][Epoch 10382] diffusion learning rate: 0.001
2024-11-05 03:11:27,230 - INFO - [diffusion][Epoch 10382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:27,231 - INFO - [diffusion][Epoch 10383] Epoch 10384/12000
2024-11-05 03:11:31,456 - INFO - [diffusion][Epoch 10383] diffusion training Loss: 0.059933336451649666
2024-11-05 03:11:31,458 - INFO - [diffusion][Epoch 10383] diffusion learning rate: 0.001
2024-11-05 03:11:31,460 - INFO - [diffusion][Epoch 10383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:31,461 - INFO - [diffusion][Epoch 10384] Epoch 10385/12000
2024-11-05 03:11:35,649 - INFO - [diffusion][Epoch 10384] diffusion training Loss: 0.056200199760496616
2024-11-05 03:11:35,651 - INFO - [diffusion][Epoch 10384] diffusion learning rate: 0.001
2024-11-05 03:11:35,652 - INFO - [diffusion][Epoch 10384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:35,654 - INFO - [diffusion][Epoch 10385] Epoch 10386/12000
2024-11-05 03:11:40,105 - INFO - [diffusion][Epoch 10385] diffusion training Loss: 0.059552387334406376
2024-11-05 03:11:40,108 - INFO - [diffusion][Epoch 10385] diffusion learning rate: 0.001
2024-11-05 03:11:40,110 - INFO - [diffusion][Epoch 10385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:40,111 - INFO - [diffusion][Epoch 10386] Epoch 10387/12000
2024-11-05 03:11:44,259 - INFO - [diffusion][Epoch 10386] diffusion training Loss: 0.061604440212249756
2024-11-05 03:11:44,261 - INFO - [diffusion][Epoch 10386] diffusion learning rate: 0.001
2024-11-05 03:11:44,263 - INFO - [diffusion][Epoch 10386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:44,264 - INFO - [diffusion][Epoch 10387] Epoch 10388/12000
2024-11-05 03:11:48,393 - INFO - [diffusion][Epoch 10387] diffusion training Loss: 0.06034379173070192
2024-11-05 03:11:48,395 - INFO - [diffusion][Epoch 10387] diffusion learning rate: 0.001
2024-11-05 03:11:48,396 - INFO - [diffusion][Epoch 10387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:48,398 - INFO - [diffusion][Epoch 10388] Epoch 10389/12000
2024-11-05 03:11:52,567 - INFO - [diffusion][Epoch 10388] diffusion training Loss: 0.060380334965884686
2024-11-05 03:11:52,570 - INFO - [diffusion][Epoch 10388] diffusion learning rate: 0.001
2024-11-05 03:11:52,572 - INFO - [diffusion][Epoch 10388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:52,573 - INFO - [diffusion][Epoch 10389] Epoch 10390/12000
2024-11-05 03:11:56,787 - INFO - [diffusion][Epoch 10389] diffusion training Loss: 0.062495799735188484
2024-11-05 03:11:56,789 - INFO - [diffusion][Epoch 10389] diffusion learning rate: 0.001
2024-11-05 03:11:56,791 - INFO - [diffusion][Epoch 10389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:56,792 - INFO - [diffusion][Epoch 10390] Epoch 10391/12000
2024-11-05 03:12:01,134 - INFO - [diffusion][Epoch 10390] diffusion training Loss: 0.05940634850412607
2024-11-05 03:12:01,136 - INFO - [diffusion][Epoch 10390] diffusion learning rate: 0.001
2024-11-05 03:12:01,138 - INFO - [diffusion][Epoch 10390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:01,140 - INFO - [diffusion][Epoch 10391] Epoch 10392/12000
2024-11-05 03:12:05,415 - INFO - [diffusion][Epoch 10391] diffusion training Loss: 0.06303512305021286
2024-11-05 03:12:05,417 - INFO - [diffusion][Epoch 10391] diffusion learning rate: 0.001
2024-11-05 03:12:05,441 - INFO - [diffusion][Epoch 10391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:05,442 - INFO - [diffusion][Epoch 10392] Epoch 10393/12000
2024-11-05 03:12:09,662 - INFO - [diffusion][Epoch 10392] diffusion training Loss: 0.05877833627164364
2024-11-05 03:12:09,663 - INFO - [diffusion][Epoch 10392] diffusion learning rate: 0.001
2024-11-05 03:12:09,665 - INFO - [diffusion][Epoch 10392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:09,666 - INFO - [diffusion][Epoch 10393] Epoch 10394/12000
2024-11-05 03:12:13,887 - INFO - [diffusion][Epoch 10393] diffusion training Loss: 0.06085341330617666
2024-11-05 03:12:13,889 - INFO - [diffusion][Epoch 10393] diffusion learning rate: 0.001
2024-11-05 03:12:13,891 - INFO - [diffusion][Epoch 10393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:13,892 - INFO - [diffusion][Epoch 10394] Epoch 10395/12000
2024-11-05 03:12:18,107 - INFO - [diffusion][Epoch 10394] diffusion training Loss: 0.06309001706540585
2024-11-05 03:12:18,109 - INFO - [diffusion][Epoch 10394] diffusion learning rate: 0.001
2024-11-05 03:12:18,111 - INFO - [diffusion][Epoch 10394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:18,112 - INFO - [diffusion][Epoch 10395] Epoch 10396/12000
2024-11-05 03:12:22,196 - INFO - [diffusion][Epoch 10395] diffusion training Loss: 0.06098258122801781
2024-11-05 03:12:22,199 - INFO - [diffusion][Epoch 10395] diffusion learning rate: 0.001
2024-11-05 03:12:22,202 - INFO - [diffusion][Epoch 10395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:22,203 - INFO - [diffusion][Epoch 10396] Epoch 10397/12000
2024-11-05 03:12:26,307 - INFO - [diffusion][Epoch 10396] diffusion training Loss: 0.06236602831631899
2024-11-05 03:12:26,309 - INFO - [diffusion][Epoch 10396] diffusion learning rate: 0.001
2024-11-05 03:12:26,311 - INFO - [diffusion][Epoch 10396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:26,312 - INFO - [diffusion][Epoch 10397] Epoch 10398/12000
2024-11-05 03:12:30,414 - INFO - [diffusion][Epoch 10397] diffusion training Loss: 0.06093203742057085
2024-11-05 03:12:30,416 - INFO - [diffusion][Epoch 10397] diffusion learning rate: 0.001
2024-11-05 03:12:30,417 - INFO - [diffusion][Epoch 10397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:30,419 - INFO - [diffusion][Epoch 10398] Epoch 10399/12000
2024-11-05 03:12:34,655 - INFO - [diffusion][Epoch 10398] diffusion training Loss: 0.05846587009727955
2024-11-05 03:12:34,657 - INFO - [diffusion][Epoch 10398] diffusion learning rate: 0.001
2024-11-05 03:12:34,684 - INFO - [diffusion][Epoch 10398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:34,685 - INFO - [diffusion][Epoch 10399] Epoch 10400/12000
2024-11-05 03:12:38,615 - INFO - [diffusion][Epoch 10399] diffusion training Loss: 0.05885479133576155
2024-11-05 03:12:38,616 - INFO - [diffusion][Epoch 10399] diffusion learning rate: 0.001
2024-11-05 03:12:38,618 - INFO - [diffusion][Epoch 10399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:38,619 - INFO - [diffusion][Epoch 10400] Epoch 10401/12000
2024-11-05 03:12:42,868 - INFO - [diffusion][Epoch 10400] diffusion training Loss: 0.05639562848955393
2024-11-05 03:12:42,870 - INFO - [diffusion][Epoch 10400] diffusion learning rate: 0.001
2024-11-05 03:12:42,871 - INFO - [diffusion][Epoch 10400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:42,873 - INFO - [diffusion][Epoch 10401] Epoch 10402/12000
2024-11-05 03:12:47,144 - INFO - [diffusion][Epoch 10401] diffusion training Loss: 0.06283323373645544
2024-11-05 03:12:47,146 - INFO - [diffusion][Epoch 10401] diffusion learning rate: 0.001
2024-11-05 03:12:47,148 - INFO - [diffusion][Epoch 10401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:47,149 - INFO - [diffusion][Epoch 10402] Epoch 10403/12000
2024-11-05 03:12:51,440 - INFO - [diffusion][Epoch 10402] diffusion training Loss: 0.06423875503242016
2024-11-05 03:12:51,442 - INFO - [diffusion][Epoch 10402] diffusion learning rate: 0.001
2024-11-05 03:12:51,445 - INFO - [diffusion][Epoch 10402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:51,446 - INFO - [diffusion][Epoch 10403] Epoch 10404/12000
2024-11-05 03:12:55,641 - INFO - [diffusion][Epoch 10403] diffusion training Loss: 0.06446252204477787
2024-11-05 03:12:55,643 - INFO - [diffusion][Epoch 10403] diffusion learning rate: 0.001
2024-11-05 03:12:55,645 - INFO - [diffusion][Epoch 10403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:55,646 - INFO - [diffusion][Epoch 10404] Epoch 10405/12000
2024-11-05 03:12:59,672 - INFO - [diffusion][Epoch 10404] diffusion training Loss: 0.06389711797237396
2024-11-05 03:12:59,676 - INFO - [diffusion][Epoch 10404] diffusion learning rate: 0.001
2024-11-05 03:12:59,677 - INFO - [diffusion][Epoch 10404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:59,679 - INFO - [diffusion][Epoch 10405] Epoch 10406/12000
2024-11-05 03:13:03,976 - INFO - [diffusion][Epoch 10405] diffusion training Loss: 0.060816070064902306
2024-11-05 03:13:03,978 - INFO - [diffusion][Epoch 10405] diffusion learning rate: 0.001
2024-11-05 03:13:03,980 - INFO - [diffusion][Epoch 10405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:03,981 - INFO - [diffusion][Epoch 10406] Epoch 10407/12000
2024-11-05 03:13:08,675 - INFO - [diffusion][Epoch 10406] diffusion training Loss: 0.05430024303495884
2024-11-05 03:13:08,677 - INFO - [diffusion][Epoch 10406] diffusion learning rate: 0.001
2024-11-05 03:13:08,679 - INFO - [diffusion][Epoch 10406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:08,681 - INFO - [diffusion][Epoch 10407] Epoch 10408/12000
2024-11-05 03:13:12,688 - INFO - [diffusion][Epoch 10407] diffusion training Loss: 0.05944116227328777
2024-11-05 03:13:12,690 - INFO - [diffusion][Epoch 10407] diffusion learning rate: 0.001
2024-11-05 03:13:12,692 - INFO - [diffusion][Epoch 10407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:12,693 - INFO - [diffusion][Epoch 10408] Epoch 10409/12000
2024-11-05 03:13:16,852 - INFO - [diffusion][Epoch 10408] diffusion training Loss: 0.05796957388520241
2024-11-05 03:13:16,854 - INFO - [diffusion][Epoch 10408] diffusion learning rate: 0.001
2024-11-05 03:13:16,855 - INFO - [diffusion][Epoch 10408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:16,857 - INFO - [diffusion][Epoch 10409] Epoch 10410/12000
2024-11-05 03:13:20,940 - INFO - [diffusion][Epoch 10409] diffusion training Loss: 0.061795356683433056
2024-11-05 03:13:20,942 - INFO - [diffusion][Epoch 10409] diffusion learning rate: 0.001
2024-11-05 03:13:20,943 - INFO - [diffusion][Epoch 10409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:20,945 - INFO - [diffusion][Epoch 10410] Epoch 10411/12000
2024-11-05 03:13:25,068 - INFO - [diffusion][Epoch 10410] diffusion training Loss: 0.06142316944897175
2024-11-05 03:13:25,070 - INFO - [diffusion][Epoch 10410] diffusion learning rate: 0.001
2024-11-05 03:13:25,072 - INFO - [diffusion][Epoch 10410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:25,073 - INFO - [diffusion][Epoch 10411] Epoch 10412/12000
2024-11-05 03:13:29,208 - INFO - [diffusion][Epoch 10411] diffusion training Loss: 0.060093631967902184
2024-11-05 03:13:29,210 - INFO - [diffusion][Epoch 10411] diffusion learning rate: 0.001
2024-11-05 03:13:29,212 - INFO - [diffusion][Epoch 10411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:29,213 - INFO - [diffusion][Epoch 10412] Epoch 10413/12000
2024-11-05 03:13:33,476 - INFO - [diffusion][Epoch 10412] diffusion training Loss: 0.05616339575499296
2024-11-05 03:13:33,478 - INFO - [diffusion][Epoch 10412] diffusion learning rate: 0.001
2024-11-05 03:13:33,480 - INFO - [diffusion][Epoch 10412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:33,481 - INFO - [diffusion][Epoch 10413] Epoch 10414/12000
2024-11-05 03:13:37,710 - INFO - [diffusion][Epoch 10413] diffusion training Loss: 0.06360640656203032
2024-11-05 03:13:37,712 - INFO - [diffusion][Epoch 10413] diffusion learning rate: 0.001
2024-11-05 03:13:37,713 - INFO - [diffusion][Epoch 10413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:37,714 - INFO - [diffusion][Epoch 10414] Epoch 10415/12000
2024-11-05 03:13:41,752 - INFO - [diffusion][Epoch 10414] diffusion training Loss: 0.0593908354640007
2024-11-05 03:13:41,754 - INFO - [diffusion][Epoch 10414] diffusion learning rate: 0.001
2024-11-05 03:13:41,756 - INFO - [diffusion][Epoch 10414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:41,757 - INFO - [diffusion][Epoch 10415] Epoch 10416/12000
2024-11-05 03:13:46,006 - INFO - [diffusion][Epoch 10415] diffusion training Loss: 0.055024489760398865
2024-11-05 03:13:46,008 - INFO - [diffusion][Epoch 10415] diffusion learning rate: 0.001
2024-11-05 03:13:46,010 - INFO - [diffusion][Epoch 10415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:46,012 - INFO - [diffusion][Epoch 10416] Epoch 10417/12000
2024-11-05 03:13:50,308 - INFO - [diffusion][Epoch 10416] diffusion training Loss: 0.06315699126571417
2024-11-05 03:13:50,311 - INFO - [diffusion][Epoch 10416] diffusion learning rate: 0.001
2024-11-05 03:13:50,313 - INFO - [diffusion][Epoch 10416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:50,315 - INFO - [diffusion][Epoch 10417] Epoch 10418/12000
2024-11-05 03:13:54,297 - INFO - [diffusion][Epoch 10417] diffusion training Loss: 0.06301587074995041
2024-11-05 03:13:54,299 - INFO - [diffusion][Epoch 10417] diffusion learning rate: 0.001
2024-11-05 03:13:54,301 - INFO - [diffusion][Epoch 10417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:54,303 - INFO - [diffusion][Epoch 10418] Epoch 10419/12000
2024-11-05 03:13:58,443 - INFO - [diffusion][Epoch 10418] diffusion training Loss: 0.06535860616713762
2024-11-05 03:13:58,445 - INFO - [diffusion][Epoch 10418] diffusion learning rate: 0.001
2024-11-05 03:13:58,447 - INFO - [diffusion][Epoch 10418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:58,448 - INFO - [diffusion][Epoch 10419] Epoch 10420/12000
2024-11-05 03:14:02,703 - INFO - [diffusion][Epoch 10419] diffusion training Loss: 0.054952443577349186
2024-11-05 03:14:02,705 - INFO - [diffusion][Epoch 10419] diffusion learning rate: 0.001
2024-11-05 03:14:02,707 - INFO - [diffusion][Epoch 10419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:02,708 - INFO - [diffusion][Epoch 10420] Epoch 10421/12000
2024-11-05 03:14:06,951 - INFO - [diffusion][Epoch 10420] diffusion training Loss: 0.05751795135438442
2024-11-05 03:14:06,953 - INFO - [diffusion][Epoch 10420] diffusion learning rate: 0.001
2024-11-05 03:14:07,003 - INFO - [diffusion][Epoch 10420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:07,004 - INFO - [diffusion][Epoch 10421] Epoch 10422/12000
2024-11-05 03:14:11,289 - INFO - [diffusion][Epoch 10421] diffusion training Loss: 0.05852621700614691
2024-11-05 03:14:11,291 - INFO - [diffusion][Epoch 10421] diffusion learning rate: 0.001
2024-11-05 03:14:11,293 - INFO - [diffusion][Epoch 10421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:11,294 - INFO - [diffusion][Epoch 10422] Epoch 10423/12000
2024-11-05 03:14:15,568 - INFO - [diffusion][Epoch 10422] diffusion training Loss: 0.057999324053525925
2024-11-05 03:14:15,570 - INFO - [diffusion][Epoch 10422] diffusion learning rate: 0.001
2024-11-05 03:14:15,572 - INFO - [diffusion][Epoch 10422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:15,574 - INFO - [diffusion][Epoch 10423] Epoch 10424/12000
2024-11-05 03:14:19,717 - INFO - [diffusion][Epoch 10423] diffusion training Loss: 0.05643798504024744
2024-11-05 03:14:19,719 - INFO - [diffusion][Epoch 10423] diffusion learning rate: 0.001
2024-11-05 03:14:19,721 - INFO - [diffusion][Epoch 10423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:19,722 - INFO - [diffusion][Epoch 10424] Epoch 10425/12000
2024-11-05 03:14:23,914 - INFO - [diffusion][Epoch 10424] diffusion training Loss: 0.065220738761127
2024-11-05 03:14:23,916 - INFO - [diffusion][Epoch 10424] diffusion learning rate: 0.001
2024-11-05 03:14:23,917 - INFO - [diffusion][Epoch 10424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:23,919 - INFO - [diffusion][Epoch 10425] Epoch 10426/12000
2024-11-05 03:14:28,142 - INFO - [diffusion][Epoch 10425] diffusion training Loss: 0.060807257890701294
2024-11-05 03:14:28,144 - INFO - [diffusion][Epoch 10425] diffusion learning rate: 0.001
2024-11-05 03:14:28,146 - INFO - [diffusion][Epoch 10425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:28,147 - INFO - [diffusion][Epoch 10426] Epoch 10427/12000
2024-11-05 03:14:32,361 - INFO - [diffusion][Epoch 10426] diffusion training Loss: 0.058705076575279236
2024-11-05 03:14:32,363 - INFO - [diffusion][Epoch 10426] diffusion learning rate: 0.001
2024-11-05 03:14:32,365 - INFO - [diffusion][Epoch 10426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:32,367 - INFO - [diffusion][Epoch 10427] Epoch 10428/12000
2024-11-05 03:14:37,059 - INFO - [diffusion][Epoch 10427] diffusion training Loss: 0.058567105792462826
2024-11-05 03:14:37,061 - INFO - [diffusion][Epoch 10427] diffusion learning rate: 0.001
2024-11-05 03:14:37,063 - INFO - [diffusion][Epoch 10427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:37,064 - INFO - [diffusion][Epoch 10428] Epoch 10429/12000
2024-11-05 03:14:41,283 - INFO - [diffusion][Epoch 10428] diffusion training Loss: 0.058280223980546
2024-11-05 03:14:41,285 - INFO - [diffusion][Epoch 10428] diffusion learning rate: 0.001
2024-11-05 03:14:41,287 - INFO - [diffusion][Epoch 10428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:41,288 - INFO - [diffusion][Epoch 10429] Epoch 10430/12000
2024-11-05 03:14:45,534 - INFO - [diffusion][Epoch 10429] diffusion training Loss: 0.05926314555108547
2024-11-05 03:14:45,536 - INFO - [diffusion][Epoch 10429] diffusion learning rate: 0.001
2024-11-05 03:14:45,538 - INFO - [diffusion][Epoch 10429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:45,539 - INFO - [diffusion][Epoch 10430] Epoch 10431/12000
2024-11-05 03:14:49,820 - INFO - [diffusion][Epoch 10430] diffusion training Loss: 0.05849539581686258
2024-11-05 03:14:49,822 - INFO - [diffusion][Epoch 10430] diffusion learning rate: 0.001
2024-11-05 03:14:49,824 - INFO - [diffusion][Epoch 10430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:49,825 - INFO - [diffusion][Epoch 10431] Epoch 10432/12000
2024-11-05 03:14:53,990 - INFO - [diffusion][Epoch 10431] diffusion training Loss: 0.06547785457223654
2024-11-05 03:14:53,992 - INFO - [diffusion][Epoch 10431] diffusion learning rate: 0.001
2024-11-05 03:14:53,993 - INFO - [diffusion][Epoch 10431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:53,995 - INFO - [diffusion][Epoch 10432] Epoch 10433/12000
2024-11-05 03:14:58,286 - INFO - [diffusion][Epoch 10432] diffusion training Loss: 0.060142843052744865
2024-11-05 03:14:58,288 - INFO - [diffusion][Epoch 10432] diffusion learning rate: 0.001
2024-11-05 03:14:58,290 - INFO - [diffusion][Epoch 10432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:58,291 - INFO - [diffusion][Epoch 10433] Epoch 10434/12000
2024-11-05 03:15:02,520 - INFO - [diffusion][Epoch 10433] diffusion training Loss: 0.05894600972533226
2024-11-05 03:15:02,522 - INFO - [diffusion][Epoch 10433] diffusion learning rate: 0.001
2024-11-05 03:15:02,524 - INFO - [diffusion][Epoch 10433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:02,526 - INFO - [diffusion][Epoch 10434] Epoch 10435/12000
2024-11-05 03:15:06,731 - INFO - [diffusion][Epoch 10434] diffusion training Loss: 0.06184003408998251
2024-11-05 03:15:06,734 - INFO - [diffusion][Epoch 10434] diffusion learning rate: 0.001
2024-11-05 03:15:06,736 - INFO - [diffusion][Epoch 10434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:06,737 - INFO - [diffusion][Epoch 10435] Epoch 10436/12000
2024-11-05 03:15:10,993 - INFO - [diffusion][Epoch 10435] diffusion training Loss: 0.06048308312892914
2024-11-05 03:15:10,996 - INFO - [diffusion][Epoch 10435] diffusion learning rate: 0.001
2024-11-05 03:15:10,998 - INFO - [diffusion][Epoch 10435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:11,000 - INFO - [diffusion][Epoch 10436] Epoch 10437/12000
2024-11-05 03:15:15,354 - INFO - [diffusion][Epoch 10436] diffusion training Loss: 0.06708810105919838
2024-11-05 03:15:15,356 - INFO - [diffusion][Epoch 10436] diffusion learning rate: 0.001
2024-11-05 03:15:15,358 - INFO - [diffusion][Epoch 10436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:15,360 - INFO - [diffusion][Epoch 10437] Epoch 10438/12000
2024-11-05 03:15:19,386 - INFO - [diffusion][Epoch 10437] diffusion training Loss: 0.05804277118295431
2024-11-05 03:15:19,387 - INFO - [diffusion][Epoch 10437] diffusion learning rate: 0.001
2024-11-05 03:15:19,389 - INFO - [diffusion][Epoch 10437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:19,390 - INFO - [diffusion][Epoch 10438] Epoch 10439/12000
2024-11-05 03:15:23,493 - INFO - [diffusion][Epoch 10438] diffusion training Loss: 0.054047681391239166
2024-11-05 03:15:23,496 - INFO - [diffusion][Epoch 10438] diffusion learning rate: 0.001
2024-11-05 03:15:23,497 - INFO - [diffusion][Epoch 10438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:23,499 - INFO - [diffusion][Epoch 10439] Epoch 10440/12000
2024-11-05 03:15:27,530 - INFO - [diffusion][Epoch 10439] diffusion training Loss: 0.059874702244997025
2024-11-05 03:15:27,532 - INFO - [diffusion][Epoch 10439] diffusion learning rate: 0.001
2024-11-05 03:15:27,534 - INFO - [diffusion][Epoch 10439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:27,536 - INFO - [diffusion][Epoch 10440] Epoch 10441/12000
2024-11-05 03:15:31,649 - INFO - [diffusion][Epoch 10440] diffusion training Loss: 0.0586233651265502
2024-11-05 03:15:31,651 - INFO - [diffusion][Epoch 10440] diffusion learning rate: 0.001
2024-11-05 03:15:31,653 - INFO - [diffusion][Epoch 10440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:31,655 - INFO - [diffusion][Epoch 10441] Epoch 10442/12000
2024-11-05 03:15:35,811 - INFO - [diffusion][Epoch 10441] diffusion training Loss: 0.05799884907901287
2024-11-05 03:15:35,814 - INFO - [diffusion][Epoch 10441] diffusion learning rate: 0.001
2024-11-05 03:15:35,816 - INFO - [diffusion][Epoch 10441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:35,817 - INFO - [diffusion][Epoch 10442] Epoch 10443/12000
2024-11-05 03:15:39,924 - INFO - [diffusion][Epoch 10442] diffusion training Loss: 0.0624171132221818
2024-11-05 03:15:39,927 - INFO - [diffusion][Epoch 10442] diffusion learning rate: 0.001
2024-11-05 03:15:39,929 - INFO - [diffusion][Epoch 10442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:39,930 - INFO - [diffusion][Epoch 10443] Epoch 10444/12000
2024-11-05 03:15:44,170 - INFO - [diffusion][Epoch 10443] diffusion training Loss: 0.06547177769243717
2024-11-05 03:15:44,172 - INFO - [diffusion][Epoch 10443] diffusion learning rate: 0.001
2024-11-05 03:15:44,173 - INFO - [diffusion][Epoch 10443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:44,175 - INFO - [diffusion][Epoch 10444] Epoch 10445/12000
2024-11-05 03:15:48,328 - INFO - [diffusion][Epoch 10444] diffusion training Loss: 0.05632567126303911
2024-11-05 03:15:48,331 - INFO - [diffusion][Epoch 10444] diffusion learning rate: 0.001
2024-11-05 03:15:48,333 - INFO - [diffusion][Epoch 10444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:48,334 - INFO - [diffusion][Epoch 10445] Epoch 10446/12000
2024-11-05 03:15:52,499 - INFO - [diffusion][Epoch 10445] diffusion training Loss: 0.05786747485399246
2024-11-05 03:15:52,501 - INFO - [diffusion][Epoch 10445] diffusion learning rate: 0.001
2024-11-05 03:15:52,502 - INFO - [diffusion][Epoch 10445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:52,504 - INFO - [diffusion][Epoch 10446] Epoch 10447/12000
2024-11-05 03:15:56,779 - INFO - [diffusion][Epoch 10446] diffusion training Loss: 0.058249710127711296
2024-11-05 03:15:56,781 - INFO - [diffusion][Epoch 10446] diffusion learning rate: 0.001
2024-11-05 03:15:56,783 - INFO - [diffusion][Epoch 10446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:56,785 - INFO - [diffusion][Epoch 10447] Epoch 10448/12000
2024-11-05 03:16:01,037 - INFO - [diffusion][Epoch 10447] diffusion training Loss: 0.06028017122298479
2024-11-05 03:16:01,039 - INFO - [diffusion][Epoch 10447] diffusion learning rate: 0.001
2024-11-05 03:16:01,041 - INFO - [diffusion][Epoch 10447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:01,043 - INFO - [diffusion][Epoch 10448] Epoch 10449/12000
2024-11-05 03:16:05,304 - INFO - [diffusion][Epoch 10448] diffusion training Loss: 0.05859183892607689
2024-11-05 03:16:05,306 - INFO - [diffusion][Epoch 10448] diffusion learning rate: 0.001
2024-11-05 03:16:05,308 - INFO - [diffusion][Epoch 10448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:05,310 - INFO - [diffusion][Epoch 10449] Epoch 10450/12000
2024-11-05 03:16:09,531 - INFO - [diffusion][Epoch 10449] diffusion training Loss: 0.06526590138673782
2024-11-05 03:16:09,533 - INFO - [diffusion][Epoch 10449] diffusion learning rate: 0.001
2024-11-05 03:16:09,535 - INFO - [diffusion][Epoch 10449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:09,536 - INFO - [diffusion][Epoch 10450] Epoch 10451/12000
2024-11-05 03:16:13,840 - INFO - [diffusion][Epoch 10450] diffusion training Loss: 0.05412996467202902
2024-11-05 03:16:13,842 - INFO - [diffusion][Epoch 10450] diffusion learning rate: 0.001
2024-11-05 03:16:13,843 - INFO - [diffusion][Epoch 10450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:13,844 - INFO - [diffusion][Epoch 10451] Epoch 10452/12000
2024-11-05 03:16:18,168 - INFO - [diffusion][Epoch 10451] diffusion training Loss: 0.05462055467069149
2024-11-05 03:16:18,171 - INFO - [diffusion][Epoch 10451] diffusion learning rate: 0.001
2024-11-05 03:16:18,172 - INFO - [diffusion][Epoch 10451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:18,174 - INFO - [diffusion][Epoch 10452] Epoch 10453/12000
2024-11-05 03:16:22,367 - INFO - [diffusion][Epoch 10452] diffusion training Loss: 0.060651655308902264
2024-11-05 03:16:22,369 - INFO - [diffusion][Epoch 10452] diffusion learning rate: 0.001
2024-11-05 03:16:22,371 - INFO - [diffusion][Epoch 10452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:22,373 - INFO - [diffusion][Epoch 10453] Epoch 10454/12000
2024-11-05 03:16:26,608 - INFO - [diffusion][Epoch 10453] diffusion training Loss: 0.06583272200077772
2024-11-05 03:16:26,610 - INFO - [diffusion][Epoch 10453] diffusion learning rate: 0.001
2024-11-05 03:16:26,612 - INFO - [diffusion][Epoch 10453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:26,613 - INFO - [diffusion][Epoch 10454] Epoch 10455/12000
2024-11-05 03:16:30,874 - INFO - [diffusion][Epoch 10454] diffusion training Loss: 0.06176400650292635
2024-11-05 03:16:30,876 - INFO - [diffusion][Epoch 10454] diffusion learning rate: 0.001
2024-11-05 03:16:30,878 - INFO - [diffusion][Epoch 10454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:30,880 - INFO - [diffusion][Epoch 10455] Epoch 10456/12000
2024-11-05 03:16:35,102 - INFO - [diffusion][Epoch 10455] diffusion training Loss: 0.05651077348738909
2024-11-05 03:16:35,104 - INFO - [diffusion][Epoch 10455] diffusion learning rate: 0.001
2024-11-05 03:16:35,106 - INFO - [diffusion][Epoch 10455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:35,108 - INFO - [diffusion][Epoch 10456] Epoch 10457/12000
2024-11-05 03:16:39,253 - INFO - [diffusion][Epoch 10456] diffusion training Loss: 0.06626256089657545
2024-11-05 03:16:39,255 - INFO - [diffusion][Epoch 10456] diffusion learning rate: 0.001
2024-11-05 03:16:39,257 - INFO - [diffusion][Epoch 10456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:39,258 - INFO - [diffusion][Epoch 10457] Epoch 10458/12000
2024-11-05 03:16:43,468 - INFO - [diffusion][Epoch 10457] diffusion training Loss: 0.06136111542582512
2024-11-05 03:16:43,470 - INFO - [diffusion][Epoch 10457] diffusion learning rate: 0.001
2024-11-05 03:16:43,472 - INFO - [diffusion][Epoch 10457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:43,474 - INFO - [diffusion][Epoch 10458] Epoch 10459/12000
2024-11-05 03:16:47,672 - INFO - [diffusion][Epoch 10458] diffusion training Loss: 0.05629042815417051
2024-11-05 03:16:47,674 - INFO - [diffusion][Epoch 10458] diffusion learning rate: 0.001
2024-11-05 03:16:47,676 - INFO - [diffusion][Epoch 10458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:47,677 - INFO - [diffusion][Epoch 10459] Epoch 10460/12000
2024-11-05 03:16:51,795 - INFO - [diffusion][Epoch 10459] diffusion training Loss: 0.05974823236465454
2024-11-05 03:16:51,797 - INFO - [diffusion][Epoch 10459] diffusion learning rate: 0.001
2024-11-05 03:16:51,799 - INFO - [diffusion][Epoch 10459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:51,800 - INFO - [diffusion][Epoch 10460] Epoch 10461/12000
2024-11-05 03:16:56,039 - INFO - [diffusion][Epoch 10460] diffusion training Loss: 0.059354680590331554
2024-11-05 03:16:56,041 - INFO - [diffusion][Epoch 10460] diffusion learning rate: 0.001
2024-11-05 03:16:56,043 - INFO - [diffusion][Epoch 10460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:56,044 - INFO - [diffusion][Epoch 10461] Epoch 10462/12000
2024-11-05 03:17:00,377 - INFO - [diffusion][Epoch 10461] diffusion training Loss: 0.058635491877794266
2024-11-05 03:17:00,379 - INFO - [diffusion][Epoch 10461] diffusion learning rate: 0.001
2024-11-05 03:17:00,381 - INFO - [diffusion][Epoch 10461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:00,382 - INFO - [diffusion][Epoch 10462] Epoch 10463/12000
2024-11-05 03:17:04,636 - INFO - [diffusion][Epoch 10462] diffusion training Loss: 0.07215073704719543
2024-11-05 03:17:04,638 - INFO - [diffusion][Epoch 10462] diffusion learning rate: 0.001
2024-11-05 03:17:04,640 - INFO - [diffusion][Epoch 10462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:04,641 - INFO - [diffusion][Epoch 10463] Epoch 10464/12000
2024-11-05 03:17:08,857 - INFO - [diffusion][Epoch 10463] diffusion training Loss: 0.06286022160202265
2024-11-05 03:17:08,859 - INFO - [diffusion][Epoch 10463] diffusion learning rate: 0.001
2024-11-05 03:17:08,861 - INFO - [diffusion][Epoch 10463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:08,862 - INFO - [diffusion][Epoch 10464] Epoch 10465/12000
2024-11-05 03:17:13,061 - INFO - [diffusion][Epoch 10464] diffusion training Loss: 0.06052993144840002
2024-11-05 03:17:13,063 - INFO - [diffusion][Epoch 10464] diffusion learning rate: 0.001
2024-11-05 03:17:13,065 - INFO - [diffusion][Epoch 10464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:13,066 - INFO - [diffusion][Epoch 10465] Epoch 10466/12000
2024-11-05 03:17:17,327 - INFO - [diffusion][Epoch 10465] diffusion training Loss: 0.05616709403693676
2024-11-05 03:17:17,329 - INFO - [diffusion][Epoch 10465] diffusion learning rate: 0.001
2024-11-05 03:17:17,331 - INFO - [diffusion][Epoch 10465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:17,332 - INFO - [diffusion][Epoch 10466] Epoch 10467/12000
2024-11-05 03:17:21,623 - INFO - [diffusion][Epoch 10466] diffusion training Loss: 0.061950162053108215
2024-11-05 03:17:21,624 - INFO - [diffusion][Epoch 10466] diffusion learning rate: 0.001
2024-11-05 03:17:21,626 - INFO - [diffusion][Epoch 10466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:21,628 - INFO - [diffusion][Epoch 10467] Epoch 10468/12000
2024-11-05 03:17:26,483 - INFO - [diffusion][Epoch 10467] diffusion training Loss: 0.06292213313281536
2024-11-05 03:17:26,485 - INFO - [diffusion][Epoch 10467] diffusion learning rate: 0.001
2024-11-05 03:17:26,487 - INFO - [diffusion][Epoch 10467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:26,488 - INFO - [diffusion][Epoch 10468] Epoch 10469/12000
2024-11-05 03:17:30,830 - INFO - [diffusion][Epoch 10468] diffusion training Loss: 0.05932440422475338
2024-11-05 03:17:30,832 - INFO - [diffusion][Epoch 10468] diffusion learning rate: 0.001
2024-11-05 03:17:30,833 - INFO - [diffusion][Epoch 10468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:30,834 - INFO - [diffusion][Epoch 10469] Epoch 10470/12000
2024-11-05 03:17:35,069 - INFO - [diffusion][Epoch 10469] diffusion training Loss: 0.05964195355772972
2024-11-05 03:17:35,071 - INFO - [diffusion][Epoch 10469] diffusion learning rate: 0.001
2024-11-05 03:17:35,073 - INFO - [diffusion][Epoch 10469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:35,074 - INFO - [diffusion][Epoch 10470] Epoch 10471/12000
2024-11-05 03:17:39,266 - INFO - [diffusion][Epoch 10470] diffusion training Loss: 0.0572564909234643
2024-11-05 03:17:39,268 - INFO - [diffusion][Epoch 10470] diffusion learning rate: 0.001
2024-11-05 03:17:39,269 - INFO - [diffusion][Epoch 10470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:39,271 - INFO - [diffusion][Epoch 10471] Epoch 10472/12000
2024-11-05 03:17:43,435 - INFO - [diffusion][Epoch 10471] diffusion training Loss: 0.060478647239506245
2024-11-05 03:17:43,437 - INFO - [diffusion][Epoch 10471] diffusion learning rate: 0.001
2024-11-05 03:17:43,438 - INFO - [diffusion][Epoch 10471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:43,440 - INFO - [diffusion][Epoch 10472] Epoch 10473/12000
2024-11-05 03:17:47,690 - INFO - [diffusion][Epoch 10472] diffusion training Loss: 0.06107676960527897
2024-11-05 03:17:47,692 - INFO - [diffusion][Epoch 10472] diffusion learning rate: 0.001
2024-11-05 03:17:47,694 - INFO - [diffusion][Epoch 10472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:47,695 - INFO - [diffusion][Epoch 10473] Epoch 10474/12000
2024-11-05 03:17:52,014 - INFO - [diffusion][Epoch 10473] diffusion training Loss: 0.057954185642302036
2024-11-05 03:17:52,016 - INFO - [diffusion][Epoch 10473] diffusion learning rate: 0.001
2024-11-05 03:17:52,018 - INFO - [diffusion][Epoch 10473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:52,019 - INFO - [diffusion][Epoch 10474] Epoch 10475/12000
2024-11-05 03:17:56,237 - INFO - [diffusion][Epoch 10474] diffusion training Loss: 0.06428423337638378
2024-11-05 03:17:56,239 - INFO - [diffusion][Epoch 10474] diffusion learning rate: 0.001
2024-11-05 03:17:56,240 - INFO - [diffusion][Epoch 10474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:56,242 - INFO - [diffusion][Epoch 10475] Epoch 10476/12000
2024-11-05 03:18:00,453 - INFO - [diffusion][Epoch 10475] diffusion training Loss: 0.05954502336680889
2024-11-05 03:18:00,455 - INFO - [diffusion][Epoch 10475] diffusion learning rate: 0.001
2024-11-05 03:18:00,457 - INFO - [diffusion][Epoch 10475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:00,459 - INFO - [diffusion][Epoch 10476] Epoch 10477/12000
2024-11-05 03:18:04,621 - INFO - [diffusion][Epoch 10476] diffusion training Loss: 0.06333274394273758
2024-11-05 03:18:04,623 - INFO - [diffusion][Epoch 10476] diffusion learning rate: 0.001
2024-11-05 03:18:04,624 - INFO - [diffusion][Epoch 10476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:04,626 - INFO - [diffusion][Epoch 10477] Epoch 10478/12000
2024-11-05 03:18:08,768 - INFO - [diffusion][Epoch 10477] diffusion training Loss: 0.05332465469837189
2024-11-05 03:18:08,770 - INFO - [diffusion][Epoch 10477] diffusion learning rate: 0.001
2024-11-05 03:18:08,771 - INFO - [diffusion][Epoch 10477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:08,773 - INFO - [diffusion][Epoch 10478] Epoch 10479/12000
2024-11-05 03:18:12,434 - INFO - [diffusion][Epoch 10478] diffusion training Loss: 0.06602023728191853
2024-11-05 03:18:12,499 - INFO - [diffusion][Epoch 10478] diffusion learning rate: 0.001
2024-11-05 03:18:12,501 - INFO - [diffusion][Epoch 10478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:12,503 - INFO - [diffusion][Epoch 10479] Epoch 10480/12000
2024-11-05 03:18:16,692 - INFO - [diffusion][Epoch 10479] diffusion training Loss: 0.05840189382433891
2024-11-05 03:18:16,694 - INFO - [diffusion][Epoch 10479] diffusion learning rate: 0.001
2024-11-05 03:18:16,696 - INFO - [diffusion][Epoch 10479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:16,697 - INFO - [diffusion][Epoch 10480] Epoch 10481/12000
2024-11-05 03:18:20,952 - INFO - [diffusion][Epoch 10480] diffusion training Loss: 0.06322860438376665
2024-11-05 03:18:20,954 - INFO - [diffusion][Epoch 10480] diffusion learning rate: 0.001
2024-11-05 03:18:20,982 - INFO - [diffusion][Epoch 10480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:20,983 - INFO - [diffusion][Epoch 10481] Epoch 10482/12000
2024-11-05 03:18:25,166 - INFO - [diffusion][Epoch 10481] diffusion training Loss: 0.05888054519891739
2024-11-05 03:18:25,168 - INFO - [diffusion][Epoch 10481] diffusion learning rate: 0.001
2024-11-05 03:18:25,170 - INFO - [diffusion][Epoch 10481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:25,171 - INFO - [diffusion][Epoch 10482] Epoch 10483/12000
2024-11-05 03:18:29,402 - INFO - [diffusion][Epoch 10482] diffusion training Loss: 0.06365097314119339
2024-11-05 03:18:29,404 - INFO - [diffusion][Epoch 10482] diffusion learning rate: 0.001
2024-11-05 03:18:29,405 - INFO - [diffusion][Epoch 10482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:29,407 - INFO - [diffusion][Epoch 10483] Epoch 10484/12000
2024-11-05 03:18:33,666 - INFO - [diffusion][Epoch 10483] diffusion training Loss: 0.059710619039833546
2024-11-05 03:18:33,668 - INFO - [diffusion][Epoch 10483] diffusion learning rate: 0.001
2024-11-05 03:18:33,670 - INFO - [diffusion][Epoch 10483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:33,671 - INFO - [diffusion][Epoch 10484] Epoch 10485/12000
2024-11-05 03:18:37,710 - INFO - [diffusion][Epoch 10484] diffusion training Loss: 0.060589784756302834
2024-11-05 03:18:37,712 - INFO - [diffusion][Epoch 10484] diffusion learning rate: 0.001
2024-11-05 03:18:37,714 - INFO - [diffusion][Epoch 10484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:37,715 - INFO - [diffusion][Epoch 10485] Epoch 10486/12000
2024-11-05 03:18:41,874 - INFO - [diffusion][Epoch 10485] diffusion training Loss: 0.060108560137450695
2024-11-05 03:18:41,876 - INFO - [diffusion][Epoch 10485] diffusion learning rate: 0.001
2024-11-05 03:18:41,878 - INFO - [diffusion][Epoch 10485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:41,879 - INFO - [diffusion][Epoch 10486] Epoch 10487/12000
2024-11-05 03:18:46,094 - INFO - [diffusion][Epoch 10486] diffusion training Loss: 0.06192326545715332
2024-11-05 03:18:46,095 - INFO - [diffusion][Epoch 10486] diffusion learning rate: 0.001
2024-11-05 03:18:46,097 - INFO - [diffusion][Epoch 10486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:46,098 - INFO - [diffusion][Epoch 10487] Epoch 10488/12000
2024-11-05 03:18:50,395 - INFO - [diffusion][Epoch 10487] diffusion training Loss: 0.06003763061016798
2024-11-05 03:18:50,397 - INFO - [diffusion][Epoch 10487] diffusion learning rate: 0.001
2024-11-05 03:18:50,399 - INFO - [diffusion][Epoch 10487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:50,401 - INFO - [diffusion][Epoch 10488] Epoch 10489/12000
2024-11-05 03:18:54,898 - INFO - [diffusion][Epoch 10488] diffusion training Loss: 0.056683942675590515
2024-11-05 03:18:54,900 - INFO - [diffusion][Epoch 10488] diffusion learning rate: 0.001
2024-11-05 03:18:54,902 - INFO - [diffusion][Epoch 10488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:54,903 - INFO - [diffusion][Epoch 10489] Epoch 10490/12000
2024-11-05 03:18:58,967 - INFO - [diffusion][Epoch 10489] diffusion training Loss: 0.06165419053286314
2024-11-05 03:18:58,988 - INFO - [diffusion][Epoch 10489] diffusion learning rate: 0.001
2024-11-05 03:18:58,990 - INFO - [diffusion][Epoch 10489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:58,991 - INFO - [diffusion][Epoch 10490] Epoch 10491/12000
2024-11-05 03:19:03,059 - INFO - [diffusion][Epoch 10490] diffusion training Loss: 0.058945233933627605
2024-11-05 03:19:03,060 - INFO - [diffusion][Epoch 10490] diffusion learning rate: 0.001
2024-11-05 03:19:03,062 - INFO - [diffusion][Epoch 10490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:03,064 - INFO - [diffusion][Epoch 10491] Epoch 10492/12000
2024-11-05 03:19:07,165 - INFO - [diffusion][Epoch 10491] diffusion training Loss: 0.05909341108053923
2024-11-05 03:19:07,167 - INFO - [diffusion][Epoch 10491] diffusion learning rate: 0.001
2024-11-05 03:19:07,169 - INFO - [diffusion][Epoch 10491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:07,170 - INFO - [diffusion][Epoch 10492] Epoch 10493/12000
2024-11-05 03:19:11,341 - INFO - [diffusion][Epoch 10492] diffusion training Loss: 0.05806833133101463
2024-11-05 03:19:11,343 - INFO - [diffusion][Epoch 10492] diffusion learning rate: 0.001
2024-11-05 03:19:11,345 - INFO - [diffusion][Epoch 10492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:11,346 - INFO - [diffusion][Epoch 10493] Epoch 10494/12000
2024-11-05 03:19:15,630 - INFO - [diffusion][Epoch 10493] diffusion training Loss: 0.05844016186892986
2024-11-05 03:19:15,632 - INFO - [diffusion][Epoch 10493] diffusion learning rate: 0.001
2024-11-05 03:19:15,634 - INFO - [diffusion][Epoch 10493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:15,635 - INFO - [diffusion][Epoch 10494] Epoch 10495/12000
2024-11-05 03:19:19,890 - INFO - [diffusion][Epoch 10494] diffusion training Loss: 0.055886080488562584
2024-11-05 03:19:19,892 - INFO - [diffusion][Epoch 10494] diffusion learning rate: 0.001
2024-11-05 03:19:19,894 - INFO - [diffusion][Epoch 10494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:19,895 - INFO - [diffusion][Epoch 10495] Epoch 10496/12000
2024-11-05 03:19:23,791 - INFO - [diffusion][Epoch 10495] diffusion training Loss: 0.0597792062908411
2024-11-05 03:19:23,794 - INFO - [diffusion][Epoch 10495] diffusion learning rate: 0.001
2024-11-05 03:19:23,796 - INFO - [diffusion][Epoch 10495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:23,798 - INFO - [diffusion][Epoch 10496] Epoch 10497/12000
2024-11-05 03:19:27,834 - INFO - [diffusion][Epoch 10496] diffusion training Loss: 0.057371133007109165
2024-11-05 03:19:27,836 - INFO - [diffusion][Epoch 10496] diffusion learning rate: 0.001
2024-11-05 03:19:27,838 - INFO - [diffusion][Epoch 10496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:27,839 - INFO - [diffusion][Epoch 10497] Epoch 10498/12000
2024-11-05 03:19:31,879 - INFO - [diffusion][Epoch 10497] diffusion training Loss: 0.061365265399217606
2024-11-05 03:19:31,881 - INFO - [diffusion][Epoch 10497] diffusion learning rate: 0.001
2024-11-05 03:19:31,883 - INFO - [diffusion][Epoch 10497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:31,884 - INFO - [diffusion][Epoch 10498] Epoch 10499/12000
2024-11-05 03:19:35,958 - INFO - [diffusion][Epoch 10498] diffusion training Loss: 0.06231090612709522
2024-11-05 03:19:35,973 - INFO - [diffusion][Epoch 10498] diffusion learning rate: 0.001
2024-11-05 03:19:35,974 - INFO - [diffusion][Epoch 10498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:35,975 - INFO - [diffusion][Epoch 10499] Epoch 10500/12000
2024-11-05 03:19:40,139 - INFO - [diffusion][Epoch 10499] diffusion training Loss: 0.05415344703942537
2024-11-05 03:19:40,141 - INFO - [diffusion][Epoch 10499] diffusion learning rate: 0.001
2024-11-05 03:19:40,143 - INFO - [diffusion][Epoch 10499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:40,144 - INFO - [diffusion][Epoch 10500] Epoch 10501/12000
2024-11-05 03:19:44,230 - INFO - [diffusion][Epoch 10500] diffusion training Loss: 0.06292138062417507
2024-11-05 03:19:44,232 - INFO - [diffusion][Epoch 10500] diffusion learning rate: 0.001
2024-11-05 03:19:44,234 - INFO - [diffusion][Epoch 10500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:44,235 - INFO - [diffusion][Epoch 10501] Epoch 10502/12000
2024-11-05 03:19:48,369 - INFO - [diffusion][Epoch 10501] diffusion training Loss: 0.05794388521462679
2024-11-05 03:19:48,371 - INFO - [diffusion][Epoch 10501] diffusion learning rate: 0.001
2024-11-05 03:19:48,373 - INFO - [diffusion][Epoch 10501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:48,374 - INFO - [diffusion][Epoch 10502] Epoch 10503/12000
2024-11-05 03:19:52,592 - INFO - [diffusion][Epoch 10502] diffusion training Loss: 0.05966707970947027
2024-11-05 03:19:52,594 - INFO - [diffusion][Epoch 10502] diffusion learning rate: 0.001
2024-11-05 03:19:52,596 - INFO - [diffusion][Epoch 10502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:52,597 - INFO - [diffusion][Epoch 10503] Epoch 10504/12000
2024-11-05 03:19:56,871 - INFO - [diffusion][Epoch 10503] diffusion training Loss: 0.060878172516822815
2024-11-05 03:19:56,872 - INFO - [diffusion][Epoch 10503] diffusion learning rate: 0.001
2024-11-05 03:19:56,874 - INFO - [diffusion][Epoch 10503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:56,875 - INFO - [diffusion][Epoch 10504] Epoch 10505/12000
2024-11-05 03:20:01,054 - INFO - [diffusion][Epoch 10504] diffusion training Loss: 0.06127756088972092
2024-11-05 03:20:01,056 - INFO - [diffusion][Epoch 10504] diffusion learning rate: 0.001
2024-11-05 03:20:01,058 - INFO - [diffusion][Epoch 10504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:01,059 - INFO - [diffusion][Epoch 10505] Epoch 10506/12000
2024-11-05 03:20:05,318 - INFO - [diffusion][Epoch 10505] diffusion training Loss: 0.06511877849698067
2024-11-05 03:20:05,321 - INFO - [diffusion][Epoch 10505] diffusion learning rate: 0.001
2024-11-05 03:20:05,322 - INFO - [diffusion][Epoch 10505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:05,324 - INFO - [diffusion][Epoch 10506] Epoch 10507/12000
2024-11-05 03:20:09,530 - INFO - [diffusion][Epoch 10506] diffusion training Loss: 0.06186496000736952
2024-11-05 03:20:09,533 - INFO - [diffusion][Epoch 10506] diffusion learning rate: 0.001
2024-11-05 03:20:09,535 - INFO - [diffusion][Epoch 10506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:09,536 - INFO - [diffusion][Epoch 10507] Epoch 10508/12000
2024-11-05 03:20:13,816 - INFO - [diffusion][Epoch 10507] diffusion training Loss: 0.057801405899226665
2024-11-05 03:20:13,818 - INFO - [diffusion][Epoch 10507] diffusion learning rate: 0.001
2024-11-05 03:20:13,820 - INFO - [diffusion][Epoch 10507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:13,821 - INFO - [diffusion][Epoch 10508] Epoch 10509/12000
2024-11-05 03:20:18,096 - INFO - [diffusion][Epoch 10508] diffusion training Loss: 0.06025503668934107
2024-11-05 03:20:18,098 - INFO - [diffusion][Epoch 10508] diffusion learning rate: 0.001
2024-11-05 03:20:18,100 - INFO - [diffusion][Epoch 10508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:18,101 - INFO - [diffusion][Epoch 10509] Epoch 10510/12000
2024-11-05 03:20:22,280 - INFO - [diffusion][Epoch 10509] diffusion training Loss: 0.0590269984677434
2024-11-05 03:20:22,282 - INFO - [diffusion][Epoch 10509] diffusion learning rate: 0.001
2024-11-05 03:20:22,284 - INFO - [diffusion][Epoch 10509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:22,285 - INFO - [diffusion][Epoch 10510] Epoch 10511/12000
2024-11-05 03:20:26,479 - INFO - [diffusion][Epoch 10510] diffusion training Loss: 0.05806105490773916
2024-11-05 03:20:26,481 - INFO - [diffusion][Epoch 10510] diffusion learning rate: 0.001
2024-11-05 03:20:26,484 - INFO - [diffusion][Epoch 10510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:26,485 - INFO - [diffusion][Epoch 10511] Epoch 10512/12000
2024-11-05 03:20:30,664 - INFO - [diffusion][Epoch 10511] diffusion training Loss: 0.05428460892289877
2024-11-05 03:20:30,666 - INFO - [diffusion][Epoch 10511] diffusion learning rate: 0.001
2024-11-05 03:20:30,668 - INFO - [diffusion][Epoch 10511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:30,669 - INFO - [diffusion][Epoch 10512] Epoch 10513/12000
2024-11-05 03:20:34,916 - INFO - [diffusion][Epoch 10512] diffusion training Loss: 0.057124825194478035
2024-11-05 03:20:34,917 - INFO - [diffusion][Epoch 10512] diffusion learning rate: 0.001
2024-11-05 03:20:34,919 - INFO - [diffusion][Epoch 10512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:34,920 - INFO - [diffusion][Epoch 10513] Epoch 10514/12000
2024-11-05 03:20:39,008 - INFO - [diffusion][Epoch 10513] diffusion training Loss: 0.05932474695146084
2024-11-05 03:20:39,010 - INFO - [diffusion][Epoch 10513] diffusion learning rate: 0.001
2024-11-05 03:20:39,012 - INFO - [diffusion][Epoch 10513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:39,013 - INFO - [diffusion][Epoch 10514] Epoch 10515/12000
2024-11-05 03:20:43,194 - INFO - [diffusion][Epoch 10514] diffusion training Loss: 0.059167332015931606
2024-11-05 03:20:43,196 - INFO - [diffusion][Epoch 10514] diffusion learning rate: 0.001
2024-11-05 03:20:43,198 - INFO - [diffusion][Epoch 10514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:43,200 - INFO - [diffusion][Epoch 10515] Epoch 10516/12000
2024-11-05 03:20:47,334 - INFO - [diffusion][Epoch 10515] diffusion training Loss: 0.06258691847324371
2024-11-05 03:20:47,336 - INFO - [diffusion][Epoch 10515] diffusion learning rate: 0.001
2024-11-05 03:20:47,338 - INFO - [diffusion][Epoch 10515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:47,339 - INFO - [diffusion][Epoch 10516] Epoch 10517/12000
2024-11-05 03:20:51,505 - INFO - [diffusion][Epoch 10516] diffusion training Loss: 0.057389828376471996
2024-11-05 03:20:51,507 - INFO - [diffusion][Epoch 10516] diffusion learning rate: 0.001
2024-11-05 03:20:51,509 - INFO - [diffusion][Epoch 10516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:51,510 - INFO - [diffusion][Epoch 10517] Epoch 10518/12000
2024-11-05 03:20:55,687 - INFO - [diffusion][Epoch 10517] diffusion training Loss: 0.06474619824439287
2024-11-05 03:20:55,689 - INFO - [diffusion][Epoch 10517] diffusion learning rate: 0.001
2024-11-05 03:20:55,691 - INFO - [diffusion][Epoch 10517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:55,693 - INFO - [diffusion][Epoch 10518] Epoch 10519/12000
2024-11-05 03:20:59,987 - INFO - [diffusion][Epoch 10518] diffusion training Loss: 0.06107295025140047
2024-11-05 03:20:59,988 - INFO - [diffusion][Epoch 10518] diffusion learning rate: 0.001
2024-11-05 03:20:59,990 - INFO - [diffusion][Epoch 10518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:59,991 - INFO - [diffusion][Epoch 10519] Epoch 10520/12000
2024-11-05 03:21:04,219 - INFO - [diffusion][Epoch 10519] diffusion training Loss: 0.0622053574770689
2024-11-05 03:21:04,222 - INFO - [diffusion][Epoch 10519] diffusion learning rate: 0.001
2024-11-05 03:21:04,224 - INFO - [diffusion][Epoch 10519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:04,225 - INFO - [diffusion][Epoch 10520] Epoch 10521/12000
2024-11-05 03:21:08,446 - INFO - [diffusion][Epoch 10520] diffusion training Loss: 0.07252513617277145
2024-11-05 03:21:08,448 - INFO - [diffusion][Epoch 10520] diffusion learning rate: 0.001
2024-11-05 03:21:08,450 - INFO - [diffusion][Epoch 10520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:08,451 - INFO - [diffusion][Epoch 10521] Epoch 10522/12000
2024-11-05 03:21:12,662 - INFO - [diffusion][Epoch 10521] diffusion training Loss: 0.058914014138281345
2024-11-05 03:21:12,664 - INFO - [diffusion][Epoch 10521] diffusion learning rate: 0.001
2024-11-05 03:21:12,692 - INFO - [diffusion][Epoch 10521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:12,693 - INFO - [diffusion][Epoch 10522] Epoch 10523/12000
2024-11-05 03:21:16,901 - INFO - [diffusion][Epoch 10522] diffusion training Loss: 0.06478068232536316
2024-11-05 03:21:16,903 - INFO - [diffusion][Epoch 10522] diffusion learning rate: 0.001
2024-11-05 03:21:16,905 - INFO - [diffusion][Epoch 10522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:16,906 - INFO - [diffusion][Epoch 10523] Epoch 10524/12000
2024-11-05 03:21:21,147 - INFO - [diffusion][Epoch 10523] diffusion training Loss: 0.06279466766864061
2024-11-05 03:21:21,149 - INFO - [diffusion][Epoch 10523] diffusion learning rate: 0.001
2024-11-05 03:21:21,152 - INFO - [diffusion][Epoch 10523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:21,153 - INFO - [diffusion][Epoch 10524] Epoch 10525/12000
2024-11-05 03:21:25,429 - INFO - [diffusion][Epoch 10524] diffusion training Loss: 0.06296468712389469
2024-11-05 03:21:25,432 - INFO - [diffusion][Epoch 10524] diffusion learning rate: 0.001
2024-11-05 03:21:25,434 - INFO - [diffusion][Epoch 10524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:25,435 - INFO - [diffusion][Epoch 10525] Epoch 10526/12000
2024-11-05 03:21:29,673 - INFO - [diffusion][Epoch 10525] diffusion training Loss: 0.06466703303158283
2024-11-05 03:21:29,675 - INFO - [diffusion][Epoch 10525] diffusion learning rate: 0.001
2024-11-05 03:21:29,677 - INFO - [diffusion][Epoch 10525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:29,678 - INFO - [diffusion][Epoch 10526] Epoch 10527/12000
2024-11-05 03:21:33,911 - INFO - [diffusion][Epoch 10526] diffusion training Loss: 0.058745002374053
2024-11-05 03:21:33,913 - INFO - [diffusion][Epoch 10526] diffusion learning rate: 0.001
2024-11-05 03:21:33,915 - INFO - [diffusion][Epoch 10526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:33,916 - INFO - [diffusion][Epoch 10527] Epoch 10528/12000
2024-11-05 03:21:38,123 - INFO - [diffusion][Epoch 10527] diffusion training Loss: 0.06054362375289202
2024-11-05 03:21:38,125 - INFO - [diffusion][Epoch 10527] diffusion learning rate: 0.001
2024-11-05 03:21:38,127 - INFO - [diffusion][Epoch 10527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:38,129 - INFO - [diffusion][Epoch 10528] Epoch 10529/12000
2024-11-05 03:21:42,253 - INFO - [diffusion][Epoch 10528] diffusion training Loss: 0.0646996758878231
2024-11-05 03:21:42,255 - INFO - [diffusion][Epoch 10528] diffusion learning rate: 0.001
2024-11-05 03:21:42,257 - INFO - [diffusion][Epoch 10528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:42,258 - INFO - [diffusion][Epoch 10529] Epoch 10530/12000
2024-11-05 03:21:46,564 - INFO - [diffusion][Epoch 10529] diffusion training Loss: 0.06298847775906324
2024-11-05 03:21:46,566 - INFO - [diffusion][Epoch 10529] diffusion learning rate: 0.001
2024-11-05 03:21:46,568 - INFO - [diffusion][Epoch 10529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:46,569 - INFO - [diffusion][Epoch 10530] Epoch 10531/12000
2024-11-05 03:21:50,984 - INFO - [diffusion][Epoch 10530] diffusion training Loss: 0.06517650373280048
2024-11-05 03:21:50,987 - INFO - [diffusion][Epoch 10530] diffusion learning rate: 0.001
2024-11-05 03:21:50,989 - INFO - [diffusion][Epoch 10530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:50,990 - INFO - [diffusion][Epoch 10531] Epoch 10532/12000
2024-11-05 03:21:55,230 - INFO - [diffusion][Epoch 10531] diffusion training Loss: 0.0625397963449359
2024-11-05 03:21:55,232 - INFO - [diffusion][Epoch 10531] diffusion learning rate: 0.001
2024-11-05 03:21:55,234 - INFO - [diffusion][Epoch 10531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:55,235 - INFO - [diffusion][Epoch 10532] Epoch 10533/12000
2024-11-05 03:21:59,353 - INFO - [diffusion][Epoch 10532] diffusion training Loss: 0.06186981778591871
2024-11-05 03:21:59,356 - INFO - [diffusion][Epoch 10532] diffusion learning rate: 0.001
2024-11-05 03:21:59,357 - INFO - [diffusion][Epoch 10532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:59,359 - INFO - [diffusion][Epoch 10533] Epoch 10534/12000
2024-11-05 03:22:03,550 - INFO - [diffusion][Epoch 10533] diffusion training Loss: 0.06248829234391451
2024-11-05 03:22:03,552 - INFO - [diffusion][Epoch 10533] diffusion learning rate: 0.001
2024-11-05 03:22:03,554 - INFO - [diffusion][Epoch 10533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:03,555 - INFO - [diffusion][Epoch 10534] Epoch 10535/12000
2024-11-05 03:22:07,812 - INFO - [diffusion][Epoch 10534] diffusion training Loss: 0.06595374271273613
2024-11-05 03:22:07,814 - INFO - [diffusion][Epoch 10534] diffusion learning rate: 0.001
2024-11-05 03:22:07,816 - INFO - [diffusion][Epoch 10534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:07,817 - INFO - [diffusion][Epoch 10535] Epoch 10536/12000
2024-11-05 03:22:11,962 - INFO - [diffusion][Epoch 10535] diffusion training Loss: 0.06326571851968765
2024-11-05 03:22:11,964 - INFO - [diffusion][Epoch 10535] diffusion learning rate: 0.001
2024-11-05 03:22:11,966 - INFO - [diffusion][Epoch 10535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:11,967 - INFO - [diffusion][Epoch 10536] Epoch 10537/12000
2024-11-05 03:22:16,217 - INFO - [diffusion][Epoch 10536] diffusion training Loss: 0.0660531735047698
2024-11-05 03:22:16,219 - INFO - [diffusion][Epoch 10536] diffusion learning rate: 0.001
2024-11-05 03:22:16,221 - INFO - [diffusion][Epoch 10536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:16,222 - INFO - [diffusion][Epoch 10537] Epoch 10538/12000
2024-11-05 03:22:20,510 - INFO - [diffusion][Epoch 10537] diffusion training Loss: 0.05759438592940569
2024-11-05 03:22:20,512 - INFO - [diffusion][Epoch 10537] diffusion learning rate: 0.001
2024-11-05 03:22:20,514 - INFO - [diffusion][Epoch 10537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:20,515 - INFO - [diffusion][Epoch 10538] Epoch 10539/12000
2024-11-05 03:22:24,676 - INFO - [diffusion][Epoch 10538] diffusion training Loss: 0.063572539947927
2024-11-05 03:22:24,678 - INFO - [diffusion][Epoch 10538] diffusion learning rate: 0.001
2024-11-05 03:22:24,680 - INFO - [diffusion][Epoch 10538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:24,682 - INFO - [diffusion][Epoch 10539] Epoch 10540/12000
2024-11-05 03:22:28,672 - INFO - [diffusion][Epoch 10539] diffusion training Loss: 0.06897022388875484
2024-11-05 03:22:28,674 - INFO - [diffusion][Epoch 10539] diffusion learning rate: 0.001
2024-11-05 03:22:28,676 - INFO - [diffusion][Epoch 10539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:28,677 - INFO - [diffusion][Epoch 10540] Epoch 10541/12000
2024-11-05 03:22:32,903 - INFO - [diffusion][Epoch 10540] diffusion training Loss: 0.06090536620467901
2024-11-05 03:22:32,906 - INFO - [diffusion][Epoch 10540] diffusion learning rate: 0.001
2024-11-05 03:22:32,908 - INFO - [diffusion][Epoch 10540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:32,910 - INFO - [diffusion][Epoch 10541] Epoch 10542/12000
2024-11-05 03:22:36,897 - INFO - [diffusion][Epoch 10541] diffusion training Loss: 0.06067000608891249
2024-11-05 03:22:36,900 - INFO - [diffusion][Epoch 10541] diffusion learning rate: 0.001
2024-11-05 03:22:36,902 - INFO - [diffusion][Epoch 10541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:36,903 - INFO - [diffusion][Epoch 10542] Epoch 10543/12000
2024-11-05 03:22:40,968 - INFO - [diffusion][Epoch 10542] diffusion training Loss: 0.05927104037255049
2024-11-05 03:22:40,970 - INFO - [diffusion][Epoch 10542] diffusion learning rate: 0.001
2024-11-05 03:22:40,972 - INFO - [diffusion][Epoch 10542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:40,973 - INFO - [diffusion][Epoch 10543] Epoch 10544/12000
2024-11-05 03:22:45,241 - INFO - [diffusion][Epoch 10543] diffusion training Loss: 0.0581737644970417
2024-11-05 03:22:45,243 - INFO - [diffusion][Epoch 10543] diffusion learning rate: 0.001
2024-11-05 03:22:45,245 - INFO - [diffusion][Epoch 10543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:45,246 - INFO - [diffusion][Epoch 10544] Epoch 10545/12000
2024-11-05 03:22:49,406 - INFO - [diffusion][Epoch 10544] diffusion training Loss: 0.05600824113935232
2024-11-05 03:22:49,408 - INFO - [diffusion][Epoch 10544] diffusion learning rate: 0.001
2024-11-05 03:22:49,410 - INFO - [diffusion][Epoch 10544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:49,411 - INFO - [diffusion][Epoch 10545] Epoch 10546/12000
2024-11-05 03:22:53,533 - INFO - [diffusion][Epoch 10545] diffusion training Loss: 0.06197248958051205
2024-11-05 03:22:53,534 - INFO - [diffusion][Epoch 10545] diffusion learning rate: 0.001
2024-11-05 03:22:53,536 - INFO - [diffusion][Epoch 10545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:53,537 - INFO - [diffusion][Epoch 10546] Epoch 10547/12000
2024-11-05 03:22:57,542 - INFO - [diffusion][Epoch 10546] diffusion training Loss: 0.060969313606619835
2024-11-05 03:22:57,544 - INFO - [diffusion][Epoch 10546] diffusion learning rate: 0.001
2024-11-05 03:22:57,546 - INFO - [diffusion][Epoch 10546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:57,547 - INFO - [diffusion][Epoch 10547] Epoch 10548/12000
2024-11-05 03:23:01,687 - INFO - [diffusion][Epoch 10547] diffusion training Loss: 0.06686681229621172
2024-11-05 03:23:01,689 - INFO - [diffusion][Epoch 10547] diffusion learning rate: 0.001
2024-11-05 03:23:01,691 - INFO - [diffusion][Epoch 10547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:01,692 - INFO - [diffusion][Epoch 10548] Epoch 10549/12000
2024-11-05 03:23:05,914 - INFO - [diffusion][Epoch 10548] diffusion training Loss: 0.06612987257540226
2024-11-05 03:23:05,916 - INFO - [diffusion][Epoch 10548] diffusion learning rate: 0.001
2024-11-05 03:23:05,917 - INFO - [diffusion][Epoch 10548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:05,919 - INFO - [diffusion][Epoch 10549] Epoch 10550/12000
2024-11-05 03:23:10,172 - INFO - [diffusion][Epoch 10549] diffusion training Loss: 0.06011868640780449
2024-11-05 03:23:10,174 - INFO - [diffusion][Epoch 10549] diffusion learning rate: 0.001
2024-11-05 03:23:10,176 - INFO - [diffusion][Epoch 10549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:10,177 - INFO - [diffusion][Epoch 10550] Epoch 10551/12000
2024-11-05 03:23:14,213 - INFO - [diffusion][Epoch 10550] diffusion training Loss: 0.06042355392128229
2024-11-05 03:23:14,215 - INFO - [diffusion][Epoch 10550] diffusion learning rate: 0.001
2024-11-05 03:23:14,217 - INFO - [diffusion][Epoch 10550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:14,218 - INFO - [diffusion][Epoch 10551] Epoch 10552/12000
2024-11-05 03:23:18,450 - INFO - [diffusion][Epoch 10551] diffusion training Loss: 0.06361173465847969
2024-11-05 03:23:18,452 - INFO - [diffusion][Epoch 10551] diffusion learning rate: 0.001
2024-11-05 03:23:18,454 - INFO - [diffusion][Epoch 10551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:18,455 - INFO - [diffusion][Epoch 10552] Epoch 10553/12000
2024-11-05 03:23:22,593 - INFO - [diffusion][Epoch 10552] diffusion training Loss: 0.06190014723688364
2024-11-05 03:23:22,595 - INFO - [diffusion][Epoch 10552] diffusion learning rate: 0.001
2024-11-05 03:23:22,597 - INFO - [diffusion][Epoch 10552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:22,598 - INFO - [diffusion][Epoch 10553] Epoch 10554/12000
2024-11-05 03:23:26,724 - INFO - [diffusion][Epoch 10553] diffusion training Loss: 0.06474591791629791
2024-11-05 03:23:26,726 - INFO - [diffusion][Epoch 10553] diffusion learning rate: 0.001
2024-11-05 03:23:26,728 - INFO - [diffusion][Epoch 10553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:26,729 - INFO - [diffusion][Epoch 10554] Epoch 10555/12000
2024-11-05 03:23:30,939 - INFO - [diffusion][Epoch 10554] diffusion training Loss: 0.058759624138474464
2024-11-05 03:23:30,941 - INFO - [diffusion][Epoch 10554] diffusion learning rate: 0.001
2024-11-05 03:23:30,943 - INFO - [diffusion][Epoch 10554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:30,944 - INFO - [diffusion][Epoch 10555] Epoch 10556/12000
2024-11-05 03:23:35,149 - INFO - [diffusion][Epoch 10555] diffusion training Loss: 0.058738344348967075
2024-11-05 03:23:35,152 - INFO - [diffusion][Epoch 10555] diffusion learning rate: 0.001
2024-11-05 03:23:35,154 - INFO - [diffusion][Epoch 10555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:35,156 - INFO - [diffusion][Epoch 10556] Epoch 10557/12000
2024-11-05 03:23:39,279 - INFO - [diffusion][Epoch 10556] diffusion training Loss: 0.055897194892168045
2024-11-05 03:23:39,281 - INFO - [diffusion][Epoch 10556] diffusion learning rate: 0.001
2024-11-05 03:23:39,283 - INFO - [diffusion][Epoch 10556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:39,284 - INFO - [diffusion][Epoch 10557] Epoch 10558/12000
2024-11-05 03:23:43,566 - INFO - [diffusion][Epoch 10557] diffusion training Loss: 0.053788390941917896
2024-11-05 03:23:43,568 - INFO - [diffusion][Epoch 10557] diffusion learning rate: 0.001
2024-11-05 03:23:43,569 - INFO - [diffusion][Epoch 10557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:43,571 - INFO - [diffusion][Epoch 10558] Epoch 10559/12000
2024-11-05 03:23:47,608 - INFO - [diffusion][Epoch 10558] diffusion training Loss: 0.06103553809225559
2024-11-05 03:23:47,610 - INFO - [diffusion][Epoch 10558] diffusion learning rate: 0.001
2024-11-05 03:23:47,612 - INFO - [diffusion][Epoch 10558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:47,613 - INFO - [diffusion][Epoch 10559] Epoch 10560/12000
2024-11-05 03:23:51,815 - INFO - [diffusion][Epoch 10559] diffusion training Loss: 0.05628820043057203
2024-11-05 03:23:51,817 - INFO - [diffusion][Epoch 10559] diffusion learning rate: 0.001
2024-11-05 03:23:51,851 - INFO - [diffusion][Epoch 10559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:51,852 - INFO - [diffusion][Epoch 10560] Epoch 10561/12000
2024-11-05 03:23:56,031 - INFO - [diffusion][Epoch 10560] diffusion training Loss: 0.06303438078612089
2024-11-05 03:23:56,033 - INFO - [diffusion][Epoch 10560] diffusion learning rate: 0.001
2024-11-05 03:23:56,035 - INFO - [diffusion][Epoch 10560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:56,036 - INFO - [diffusion][Epoch 10561] Epoch 10562/12000
2024-11-05 03:24:00,256 - INFO - [diffusion][Epoch 10561] diffusion training Loss: 0.0583959948271513
2024-11-05 03:24:00,258 - INFO - [diffusion][Epoch 10561] diffusion learning rate: 0.001
2024-11-05 03:24:00,260 - INFO - [diffusion][Epoch 10561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:00,261 - INFO - [diffusion][Epoch 10562] Epoch 10563/12000
2024-11-05 03:24:04,545 - INFO - [diffusion][Epoch 10562] diffusion training Loss: 0.06264217384159565
2024-11-05 03:24:04,547 - INFO - [diffusion][Epoch 10562] diffusion learning rate: 0.001
2024-11-05 03:24:04,549 - INFO - [diffusion][Epoch 10562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:04,550 - INFO - [diffusion][Epoch 10563] Epoch 10564/12000
2024-11-05 03:24:08,789 - INFO - [diffusion][Epoch 10563] diffusion training Loss: 0.05775295663625002
2024-11-05 03:24:08,807 - INFO - [diffusion][Epoch 10563] diffusion learning rate: 0.001
2024-11-05 03:24:08,809 - INFO - [diffusion][Epoch 10563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:08,810 - INFO - [diffusion][Epoch 10564] Epoch 10565/12000
2024-11-05 03:24:12,958 - INFO - [diffusion][Epoch 10564] diffusion training Loss: 0.06198792438954115
2024-11-05 03:24:13,159 - INFO - [diffusion][Epoch 10564] diffusion learning rate: 0.001
2024-11-05 03:24:13,161 - INFO - [diffusion][Epoch 10564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:13,163 - INFO - [diffusion][Epoch 10565] Epoch 10566/12000
2024-11-05 03:24:17,279 - INFO - [diffusion][Epoch 10565] diffusion training Loss: 0.06398571096360683
2024-11-05 03:24:17,281 - INFO - [diffusion][Epoch 10565] diffusion learning rate: 0.001
2024-11-05 03:24:17,283 - INFO - [diffusion][Epoch 10565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:17,285 - INFO - [diffusion][Epoch 10566] Epoch 10567/12000
2024-11-05 03:24:21,323 - INFO - [diffusion][Epoch 10566] diffusion training Loss: 0.06024441868066788
2024-11-05 03:24:21,325 - INFO - [diffusion][Epoch 10566] diffusion learning rate: 0.001
2024-11-05 03:24:21,326 - INFO - [diffusion][Epoch 10566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:21,328 - INFO - [diffusion][Epoch 10567] Epoch 10568/12000
2024-11-05 03:24:25,636 - INFO - [diffusion][Epoch 10567] diffusion training Loss: 0.06239460967481136
2024-11-05 03:24:25,639 - INFO - [diffusion][Epoch 10567] diffusion learning rate: 0.001
2024-11-05 03:24:25,641 - INFO - [diffusion][Epoch 10567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:25,642 - INFO - [diffusion][Epoch 10568] Epoch 10569/12000
2024-11-05 03:24:29,908 - INFO - [diffusion][Epoch 10568] diffusion training Loss: 0.05862267781049013
2024-11-05 03:24:29,910 - INFO - [diffusion][Epoch 10568] diffusion learning rate: 0.001
2024-11-05 03:24:29,911 - INFO - [diffusion][Epoch 10568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:29,913 - INFO - [diffusion][Epoch 10569] Epoch 10570/12000
2024-11-05 03:24:34,216 - INFO - [diffusion][Epoch 10569] diffusion training Loss: 0.06242633517831564
2024-11-05 03:24:34,218 - INFO - [diffusion][Epoch 10569] diffusion learning rate: 0.001
2024-11-05 03:24:34,220 - INFO - [diffusion][Epoch 10569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:34,221 - INFO - [diffusion][Epoch 10570] Epoch 10571/12000
2024-11-05 03:24:38,440 - INFO - [diffusion][Epoch 10570] diffusion training Loss: 0.05920127872377634
2024-11-05 03:24:38,442 - INFO - [diffusion][Epoch 10570] diffusion learning rate: 0.001
2024-11-05 03:24:38,444 - INFO - [diffusion][Epoch 10570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:38,445 - INFO - [diffusion][Epoch 10571] Epoch 10572/12000
2024-11-05 03:24:43,177 - INFO - [diffusion][Epoch 10571] diffusion training Loss: 0.06038955319672823
2024-11-05 03:24:43,179 - INFO - [diffusion][Epoch 10571] diffusion learning rate: 0.001
2024-11-05 03:24:43,181 - INFO - [diffusion][Epoch 10571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:43,182 - INFO - [diffusion][Epoch 10572] Epoch 10573/12000
2024-11-05 03:24:47,546 - INFO - [diffusion][Epoch 10572] diffusion training Loss: 0.05997140146791935
2024-11-05 03:24:47,548 - INFO - [diffusion][Epoch 10572] diffusion learning rate: 0.001
2024-11-05 03:24:47,550 - INFO - [diffusion][Epoch 10572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:47,551 - INFO - [diffusion][Epoch 10573] Epoch 10574/12000
2024-11-05 03:24:51,752 - INFO - [diffusion][Epoch 10573] diffusion training Loss: 0.057878805324435234
2024-11-05 03:24:51,754 - INFO - [diffusion][Epoch 10573] diffusion learning rate: 0.001
2024-11-05 03:24:51,781 - INFO - [diffusion][Epoch 10573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:51,783 - INFO - [diffusion][Epoch 10574] Epoch 10575/12000
2024-11-05 03:24:55,990 - INFO - [diffusion][Epoch 10574] diffusion training Loss: 0.053578298538923264
2024-11-05 03:24:55,992 - INFO - [diffusion][Epoch 10574] diffusion learning rate: 0.001
2024-11-05 03:24:55,994 - INFO - [diffusion][Epoch 10574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:55,995 - INFO - [diffusion][Epoch 10575] Epoch 10576/12000
2024-11-05 03:25:00,226 - INFO - [diffusion][Epoch 10575] diffusion training Loss: 0.057565344497561455
2024-11-05 03:25:00,228 - INFO - [diffusion][Epoch 10575] diffusion learning rate: 0.001
2024-11-05 03:25:00,230 - INFO - [diffusion][Epoch 10575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:00,231 - INFO - [diffusion][Epoch 10576] Epoch 10577/12000
2024-11-05 03:25:04,532 - INFO - [diffusion][Epoch 10576] diffusion training Loss: 0.05948927532881498
2024-11-05 03:25:04,534 - INFO - [diffusion][Epoch 10576] diffusion learning rate: 0.001
2024-11-05 03:25:04,536 - INFO - [diffusion][Epoch 10576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:04,537 - INFO - [diffusion][Epoch 10577] Epoch 10578/12000
2024-11-05 03:25:08,751 - INFO - [diffusion][Epoch 10577] diffusion training Loss: 0.06463119387626648
2024-11-05 03:25:08,754 - INFO - [diffusion][Epoch 10577] diffusion learning rate: 0.001
2024-11-05 03:25:08,782 - INFO - [diffusion][Epoch 10577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:08,784 - INFO - [diffusion][Epoch 10578] Epoch 10579/12000
2024-11-05 03:25:12,943 - INFO - [diffusion][Epoch 10578] diffusion training Loss: 0.058062804862856865
2024-11-05 03:25:12,945 - INFO - [diffusion][Epoch 10578] diffusion learning rate: 0.001
2024-11-05 03:25:12,946 - INFO - [diffusion][Epoch 10578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:12,948 - INFO - [diffusion][Epoch 10579] Epoch 10580/12000
2024-11-05 03:25:17,135 - INFO - [diffusion][Epoch 10579] diffusion training Loss: 0.05937175825238228
2024-11-05 03:25:17,137 - INFO - [diffusion][Epoch 10579] diffusion learning rate: 0.001
2024-11-05 03:25:17,139 - INFO - [diffusion][Epoch 10579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:17,140 - INFO - [diffusion][Epoch 10580] Epoch 10581/12000
2024-11-05 03:25:21,472 - INFO - [diffusion][Epoch 10580] diffusion training Loss: 0.05617538280785084
2024-11-05 03:25:21,475 - INFO - [diffusion][Epoch 10580] diffusion learning rate: 0.001
2024-11-05 03:25:21,476 - INFO - [diffusion][Epoch 10580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:21,478 - INFO - [diffusion][Epoch 10581] Epoch 10582/12000
2024-11-05 03:25:25,564 - INFO - [diffusion][Epoch 10581] diffusion training Loss: 0.062288117595016956
2024-11-05 03:25:25,567 - INFO - [diffusion][Epoch 10581] diffusion learning rate: 0.001
2024-11-05 03:25:25,568 - INFO - [diffusion][Epoch 10581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:25,569 - INFO - [diffusion][Epoch 10582] Epoch 10583/12000
2024-11-05 03:25:29,916 - INFO - [diffusion][Epoch 10582] diffusion training Loss: 0.061615864746272564
2024-11-05 03:25:29,918 - INFO - [diffusion][Epoch 10582] diffusion learning rate: 0.001
2024-11-05 03:25:29,937 - INFO - [diffusion][Epoch 10582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:29,939 - INFO - [diffusion][Epoch 10583] Epoch 10584/12000
2024-11-05 03:25:34,186 - INFO - [diffusion][Epoch 10583] diffusion training Loss: 0.05864116083830595
2024-11-05 03:25:34,187 - INFO - [diffusion][Epoch 10583] diffusion learning rate: 0.001
2024-11-05 03:25:34,189 - INFO - [diffusion][Epoch 10583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:34,191 - INFO - [diffusion][Epoch 10584] Epoch 10585/12000
2024-11-05 03:25:38,457 - INFO - [diffusion][Epoch 10584] diffusion training Loss: 0.056820170022547245
2024-11-05 03:25:38,460 - INFO - [diffusion][Epoch 10584] diffusion learning rate: 0.001
2024-11-05 03:25:38,462 - INFO - [diffusion][Epoch 10584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:38,463 - INFO - [diffusion][Epoch 10585] Epoch 10586/12000
2024-11-05 03:25:42,542 - INFO - [diffusion][Epoch 10585] diffusion training Loss: 0.0627381531521678
2024-11-05 03:25:42,545 - INFO - [diffusion][Epoch 10585] diffusion learning rate: 0.001
2024-11-05 03:25:42,547 - INFO - [diffusion][Epoch 10585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:42,548 - INFO - [diffusion][Epoch 10586] Epoch 10587/12000
2024-11-05 03:25:46,702 - INFO - [diffusion][Epoch 10586] diffusion training Loss: 0.05971705634146929
2024-11-05 03:25:46,704 - INFO - [diffusion][Epoch 10586] diffusion learning rate: 0.001
2024-11-05 03:25:46,706 - INFO - [diffusion][Epoch 10586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:46,707 - INFO - [diffusion][Epoch 10587] Epoch 10588/12000
2024-11-05 03:25:51,047 - INFO - [diffusion][Epoch 10587] diffusion training Loss: 0.05976306181401014
2024-11-05 03:25:51,050 - INFO - [diffusion][Epoch 10587] diffusion learning rate: 0.001
2024-11-05 03:25:51,052 - INFO - [diffusion][Epoch 10587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:51,053 - INFO - [diffusion][Epoch 10588] Epoch 10589/12000
2024-11-05 03:25:55,330 - INFO - [diffusion][Epoch 10588] diffusion training Loss: 0.06322102062404156
2024-11-05 03:25:55,332 - INFO - [diffusion][Epoch 10588] diffusion learning rate: 0.001
2024-11-05 03:25:55,335 - INFO - [diffusion][Epoch 10588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:55,336 - INFO - [diffusion][Epoch 10589] Epoch 10590/12000
2024-11-05 03:25:59,364 - INFO - [diffusion][Epoch 10589] diffusion training Loss: 0.05554745811969042
2024-11-05 03:25:59,366 - INFO - [diffusion][Epoch 10589] diffusion learning rate: 0.001
2024-11-05 03:25:59,368 - INFO - [diffusion][Epoch 10589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:59,370 - INFO - [diffusion][Epoch 10590] Epoch 10591/12000
2024-11-05 03:26:03,040 - INFO - [diffusion][Epoch 10590] diffusion training Loss: 0.060599736869335175
2024-11-05 03:26:03,043 - INFO - [diffusion][Epoch 10590] diffusion learning rate: 0.001
2024-11-05 03:26:03,044 - INFO - [diffusion][Epoch 10590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:03,046 - INFO - [diffusion][Epoch 10591] Epoch 10592/12000
2024-11-05 03:26:07,178 - INFO - [diffusion][Epoch 10591] diffusion training Loss: 0.05697145499289036
2024-11-05 03:26:07,180 - INFO - [diffusion][Epoch 10591] diffusion learning rate: 0.001
2024-11-05 03:26:07,199 - INFO - [diffusion][Epoch 10591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:07,200 - INFO - [diffusion][Epoch 10592] Epoch 10593/12000
2024-11-05 03:26:11,481 - INFO - [diffusion][Epoch 10592] diffusion training Loss: 0.0616790484637022
2024-11-05 03:26:11,484 - INFO - [diffusion][Epoch 10592] diffusion learning rate: 0.001
2024-11-05 03:26:11,486 - INFO - [diffusion][Epoch 10592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:11,487 - INFO - [diffusion][Epoch 10593] Epoch 10594/12000
2024-11-05 03:26:15,462 - INFO - [diffusion][Epoch 10593] diffusion training Loss: 0.0632713632658124
2024-11-05 03:26:15,465 - INFO - [diffusion][Epoch 10593] diffusion learning rate: 0.001
2024-11-05 03:26:15,467 - INFO - [diffusion][Epoch 10593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:15,469 - INFO - [diffusion][Epoch 10594] Epoch 10595/12000
2024-11-05 03:26:19,586 - INFO - [diffusion][Epoch 10594] diffusion training Loss: 0.05977696552872658
2024-11-05 03:26:19,587 - INFO - [diffusion][Epoch 10594] diffusion learning rate: 0.001
2024-11-05 03:26:19,589 - INFO - [diffusion][Epoch 10594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:19,591 - INFO - [diffusion][Epoch 10595] Epoch 10596/12000
2024-11-05 03:26:23,801 - INFO - [diffusion][Epoch 10595] diffusion training Loss: 0.06297141406685114
2024-11-05 03:26:23,803 - INFO - [diffusion][Epoch 10595] diffusion learning rate: 0.001
2024-11-05 03:26:23,805 - INFO - [diffusion][Epoch 10595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:23,806 - INFO - [diffusion][Epoch 10596] Epoch 10597/12000
2024-11-05 03:26:28,032 - INFO - [diffusion][Epoch 10596] diffusion training Loss: 0.06818428728729486
2024-11-05 03:26:28,034 - INFO - [diffusion][Epoch 10596] diffusion learning rate: 0.001
2024-11-05 03:26:28,036 - INFO - [diffusion][Epoch 10596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:28,037 - INFO - [diffusion][Epoch 10597] Epoch 10598/12000
2024-11-05 03:26:32,288 - INFO - [diffusion][Epoch 10597] diffusion training Loss: 0.058527120389044285
2024-11-05 03:26:32,291 - INFO - [diffusion][Epoch 10597] diffusion learning rate: 0.001
2024-11-05 03:26:32,293 - INFO - [diffusion][Epoch 10597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:32,294 - INFO - [diffusion][Epoch 10598] Epoch 10599/12000
2024-11-05 03:26:36,488 - INFO - [diffusion][Epoch 10598] diffusion training Loss: 0.06060377508401871
2024-11-05 03:26:36,491 - INFO - [diffusion][Epoch 10598] diffusion learning rate: 0.001
2024-11-05 03:26:36,493 - INFO - [diffusion][Epoch 10598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:36,494 - INFO - [diffusion][Epoch 10599] Epoch 10600/12000
2024-11-05 03:26:40,826 - INFO - [diffusion][Epoch 10599] diffusion training Loss: 0.057581852190196514
2024-11-05 03:26:40,828 - INFO - [diffusion][Epoch 10599] diffusion learning rate: 0.001
2024-11-05 03:26:40,830 - INFO - [diffusion][Epoch 10599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:40,831 - INFO - [diffusion][Epoch 10600] Epoch 10601/12000
2024-11-05 03:26:44,985 - INFO - [diffusion][Epoch 10600] diffusion training Loss: 0.06293035857379436
2024-11-05 03:26:44,988 - INFO - [diffusion][Epoch 10600] diffusion learning rate: 0.001
2024-11-05 03:26:44,990 - INFO - [diffusion][Epoch 10600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:44,991 - INFO - [diffusion][Epoch 10601] Epoch 10602/12000
2024-11-05 03:26:49,126 - INFO - [diffusion][Epoch 10601] diffusion training Loss: 0.06212944537401199
2024-11-05 03:26:49,129 - INFO - [diffusion][Epoch 10601] diffusion learning rate: 0.001
2024-11-05 03:26:49,132 - INFO - [diffusion][Epoch 10601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:49,133 - INFO - [diffusion][Epoch 10602] Epoch 10603/12000
2024-11-05 03:26:53,300 - INFO - [diffusion][Epoch 10602] diffusion training Loss: 0.06397666968405247
2024-11-05 03:26:53,302 - INFO - [diffusion][Epoch 10602] diffusion learning rate: 0.001
2024-11-05 03:26:53,305 - INFO - [diffusion][Epoch 10602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:53,308 - INFO - [diffusion][Epoch 10603] Epoch 10604/12000
2024-11-05 03:26:57,513 - INFO - [diffusion][Epoch 10603] diffusion training Loss: 0.05805040057748556
2024-11-05 03:26:57,515 - INFO - [diffusion][Epoch 10603] diffusion learning rate: 0.001
2024-11-05 03:26:57,517 - INFO - [diffusion][Epoch 10603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:57,518 - INFO - [diffusion][Epoch 10604] Epoch 10605/12000
2024-11-05 03:27:01,526 - INFO - [diffusion][Epoch 10604] diffusion training Loss: 0.06411709263920784
2024-11-05 03:27:01,528 - INFO - [diffusion][Epoch 10604] diffusion learning rate: 0.001
2024-11-05 03:27:01,530 - INFO - [diffusion][Epoch 10604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:01,531 - INFO - [diffusion][Epoch 10605] Epoch 10606/12000
2024-11-05 03:27:05,578 - INFO - [diffusion][Epoch 10605] diffusion training Loss: 0.06000945996493101
2024-11-05 03:27:05,580 - INFO - [diffusion][Epoch 10605] diffusion learning rate: 0.001
2024-11-05 03:27:05,582 - INFO - [diffusion][Epoch 10605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:05,583 - INFO - [diffusion][Epoch 10606] Epoch 10607/12000
2024-11-05 03:27:09,284 - INFO - [diffusion][Epoch 10606] diffusion training Loss: 0.05607120972126722
2024-11-05 03:27:09,286 - INFO - [diffusion][Epoch 10606] diffusion learning rate: 0.001
2024-11-05 03:27:09,288 - INFO - [diffusion][Epoch 10606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:09,289 - INFO - [diffusion][Epoch 10607] Epoch 10608/12000
2024-11-05 03:27:13,382 - INFO - [diffusion][Epoch 10607] diffusion training Loss: 0.059298004023730755
2024-11-05 03:27:13,385 - INFO - [diffusion][Epoch 10607] diffusion learning rate: 0.001
2024-11-05 03:27:13,387 - INFO - [diffusion][Epoch 10607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:13,389 - INFO - [diffusion][Epoch 10608] Epoch 10609/12000
2024-11-05 03:27:17,477 - INFO - [diffusion][Epoch 10608] diffusion training Loss: 0.05830254498869181
2024-11-05 03:27:17,478 - INFO - [diffusion][Epoch 10608] diffusion learning rate: 0.001
2024-11-05 03:27:17,480 - INFO - [diffusion][Epoch 10608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:17,481 - INFO - [diffusion][Epoch 10609] Epoch 10610/12000
2024-11-05 03:27:21,663 - INFO - [diffusion][Epoch 10609] diffusion training Loss: 0.06344297342002392
2024-11-05 03:27:21,666 - INFO - [diffusion][Epoch 10609] diffusion learning rate: 0.001
2024-11-05 03:27:21,667 - INFO - [diffusion][Epoch 10609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:21,669 - INFO - [diffusion][Epoch 10610] Epoch 10611/12000
2024-11-05 03:27:25,769 - INFO - [diffusion][Epoch 10610] diffusion training Loss: 0.06144959107041359
2024-11-05 03:27:25,772 - INFO - [diffusion][Epoch 10610] diffusion learning rate: 0.001
2024-11-05 03:27:25,774 - INFO - [diffusion][Epoch 10610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:25,775 - INFO - [diffusion][Epoch 10611] Epoch 10612/12000
2024-11-05 03:27:29,795 - INFO - [diffusion][Epoch 10611] diffusion training Loss: 0.06079519633203745
2024-11-05 03:27:29,797 - INFO - [diffusion][Epoch 10611] diffusion learning rate: 0.001
2024-11-05 03:27:29,799 - INFO - [diffusion][Epoch 10611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:29,800 - INFO - [diffusion][Epoch 10612] Epoch 10613/12000
2024-11-05 03:27:34,043 - INFO - [diffusion][Epoch 10612] diffusion training Loss: 0.056511866860091686
2024-11-05 03:27:34,045 - INFO - [diffusion][Epoch 10612] diffusion learning rate: 0.001
2024-11-05 03:27:34,047 - INFO - [diffusion][Epoch 10612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:34,048 - INFO - [diffusion][Epoch 10613] Epoch 10614/12000
2024-11-05 03:27:38,420 - INFO - [diffusion][Epoch 10613] diffusion training Loss: 0.060215843841433525
2024-11-05 03:27:38,422 - INFO - [diffusion][Epoch 10613] diffusion learning rate: 0.001
2024-11-05 03:27:38,424 - INFO - [diffusion][Epoch 10613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:38,425 - INFO - [diffusion][Epoch 10614] Epoch 10615/12000
2024-11-05 03:27:42,527 - INFO - [diffusion][Epoch 10614] diffusion training Loss: 0.05797238647937775
2024-11-05 03:27:42,530 - INFO - [diffusion][Epoch 10614] diffusion learning rate: 0.001
2024-11-05 03:27:42,532 - INFO - [diffusion][Epoch 10614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:42,533 - INFO - [diffusion][Epoch 10615] Epoch 10616/12000
2024-11-05 03:27:46,805 - INFO - [diffusion][Epoch 10615] diffusion training Loss: 0.06303861178457737
2024-11-05 03:27:46,807 - INFO - [diffusion][Epoch 10615] diffusion learning rate: 0.001
2024-11-05 03:27:46,808 - INFO - [diffusion][Epoch 10615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:46,810 - INFO - [diffusion][Epoch 10616] Epoch 10617/12000
2024-11-05 03:27:51,085 - INFO - [diffusion][Epoch 10616] diffusion training Loss: 0.05930794216692448
2024-11-05 03:27:51,087 - INFO - [diffusion][Epoch 10616] diffusion learning rate: 0.001
2024-11-05 03:27:51,089 - INFO - [diffusion][Epoch 10616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:51,090 - INFO - [diffusion][Epoch 10617] Epoch 10618/12000
2024-11-05 03:27:55,321 - INFO - [diffusion][Epoch 10617] diffusion training Loss: 0.0548506174236536
2024-11-05 03:27:55,323 - INFO - [diffusion][Epoch 10617] diffusion learning rate: 0.001
2024-11-05 03:27:55,325 - INFO - [diffusion][Epoch 10617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:55,326 - INFO - [diffusion][Epoch 10618] Epoch 10619/12000
2024-11-05 03:27:59,603 - INFO - [diffusion][Epoch 10618] diffusion training Loss: 0.06092459987848997
2024-11-05 03:27:59,605 - INFO - [diffusion][Epoch 10618] diffusion learning rate: 0.001
2024-11-05 03:27:59,613 - INFO - [diffusion][Epoch 10618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:59,614 - INFO - [diffusion][Epoch 10619] Epoch 10620/12000
2024-11-05 03:28:03,786 - INFO - [diffusion][Epoch 10619] diffusion training Loss: 0.05688994284719229
2024-11-05 03:28:03,788 - INFO - [diffusion][Epoch 10619] diffusion learning rate: 0.001
2024-11-05 03:28:03,791 - INFO - [diffusion][Epoch 10619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:03,793 - INFO - [diffusion][Epoch 10620] Epoch 10621/12000
2024-11-05 03:28:08,034 - INFO - [diffusion][Epoch 10620] diffusion training Loss: 0.056378817185759544
2024-11-05 03:28:08,036 - INFO - [diffusion][Epoch 10620] diffusion learning rate: 0.001
2024-11-05 03:28:08,038 - INFO - [diffusion][Epoch 10620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:08,040 - INFO - [diffusion][Epoch 10621] Epoch 10622/12000
2024-11-05 03:28:12,215 - INFO - [diffusion][Epoch 10621] diffusion training Loss: 0.063236721791327
2024-11-05 03:28:12,217 - INFO - [diffusion][Epoch 10621] diffusion learning rate: 0.001
2024-11-05 03:28:12,249 - INFO - [diffusion][Epoch 10621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:12,250 - INFO - [diffusion][Epoch 10622] Epoch 10623/12000
2024-11-05 03:28:16,166 - INFO - [diffusion][Epoch 10622] diffusion training Loss: 0.057070380076766014
2024-11-05 03:28:16,169 - INFO - [diffusion][Epoch 10622] diffusion learning rate: 0.001
2024-11-05 03:28:16,171 - INFO - [diffusion][Epoch 10622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:16,172 - INFO - [diffusion][Epoch 10623] Epoch 10624/12000
2024-11-05 03:28:20,238 - INFO - [diffusion][Epoch 10623] diffusion training Loss: 0.05924461130052805
2024-11-05 03:28:20,240 - INFO - [diffusion][Epoch 10623] diffusion learning rate: 0.001
2024-11-05 03:28:20,242 - INFO - [diffusion][Epoch 10623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:20,243 - INFO - [diffusion][Epoch 10624] Epoch 10625/12000
2024-11-05 03:28:24,348 - INFO - [diffusion][Epoch 10624] diffusion training Loss: 0.06108126789331436
2024-11-05 03:28:24,350 - INFO - [diffusion][Epoch 10624] diffusion learning rate: 0.001
2024-11-05 03:28:24,351 - INFO - [diffusion][Epoch 10624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:24,353 - INFO - [diffusion][Epoch 10625] Epoch 10626/12000
2024-11-05 03:28:28,445 - INFO - [diffusion][Epoch 10625] diffusion training Loss: 0.05772578250616789
2024-11-05 03:28:28,448 - INFO - [diffusion][Epoch 10625] diffusion learning rate: 0.001
2024-11-05 03:28:28,450 - INFO - [diffusion][Epoch 10625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:28,452 - INFO - [diffusion][Epoch 10626] Epoch 10627/12000
2024-11-05 03:28:32,525 - INFO - [diffusion][Epoch 10626] diffusion training Loss: 0.05440185032784939
2024-11-05 03:28:32,528 - INFO - [diffusion][Epoch 10626] diffusion learning rate: 0.001
2024-11-05 03:28:32,561 - INFO - [diffusion][Epoch 10626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:32,563 - INFO - [diffusion][Epoch 10627] Epoch 10628/12000
2024-11-05 03:28:36,838 - INFO - [diffusion][Epoch 10627] diffusion training Loss: 0.06023037061095238
2024-11-05 03:28:36,840 - INFO - [diffusion][Epoch 10627] diffusion learning rate: 0.001
2024-11-05 03:28:36,842 - INFO - [diffusion][Epoch 10627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:36,843 - INFO - [diffusion][Epoch 10628] Epoch 10629/12000
2024-11-05 03:28:40,993 - INFO - [diffusion][Epoch 10628] diffusion training Loss: 0.06207416299730539
2024-11-05 03:28:40,995 - INFO - [diffusion][Epoch 10628] diffusion learning rate: 0.001
2024-11-05 03:28:40,997 - INFO - [diffusion][Epoch 10628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:40,999 - INFO - [diffusion][Epoch 10629] Epoch 10630/12000
2024-11-05 03:28:45,056 - INFO - [diffusion][Epoch 10629] diffusion training Loss: 0.05785673391073942
2024-11-05 03:28:45,059 - INFO - [diffusion][Epoch 10629] diffusion learning rate: 0.001
2024-11-05 03:28:45,061 - INFO - [diffusion][Epoch 10629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:45,062 - INFO - [diffusion][Epoch 10630] Epoch 10631/12000
2024-11-05 03:28:49,162 - INFO - [diffusion][Epoch 10630] diffusion training Loss: 0.061871969141066074
2024-11-05 03:28:49,164 - INFO - [diffusion][Epoch 10630] diffusion learning rate: 0.001
2024-11-05 03:28:49,167 - INFO - [diffusion][Epoch 10630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:49,168 - INFO - [diffusion][Epoch 10631] Epoch 10632/12000
2024-11-05 03:28:53,284 - INFO - [diffusion][Epoch 10631] diffusion training Loss: 0.05560687929391861
2024-11-05 03:28:53,286 - INFO - [diffusion][Epoch 10631] diffusion learning rate: 0.001
2024-11-05 03:28:53,328 - INFO - [diffusion][Epoch 10631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:53,329 - INFO - [diffusion][Epoch 10632] Epoch 10633/12000
2024-11-05 03:28:57,422 - INFO - [diffusion][Epoch 10632] diffusion training Loss: 0.057026589289307594
2024-11-05 03:28:57,425 - INFO - [diffusion][Epoch 10632] diffusion learning rate: 0.001
2024-11-05 03:28:57,427 - INFO - [diffusion][Epoch 10632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:57,428 - INFO - [diffusion][Epoch 10633] Epoch 10634/12000
2024-11-05 03:29:01,527 - INFO - [diffusion][Epoch 10633] diffusion training Loss: 0.06345728784799576
2024-11-05 03:29:01,530 - INFO - [diffusion][Epoch 10633] diffusion learning rate: 0.001
2024-11-05 03:29:01,531 - INFO - [diffusion][Epoch 10633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:01,533 - INFO - [diffusion][Epoch 10634] Epoch 10635/12000
2024-11-05 03:29:05,658 - INFO - [diffusion][Epoch 10634] diffusion training Loss: 0.05801751744002104
2024-11-05 03:29:05,660 - INFO - [diffusion][Epoch 10634] diffusion learning rate: 0.001
2024-11-05 03:29:05,662 - INFO - [diffusion][Epoch 10634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:05,663 - INFO - [diffusion][Epoch 10635] Epoch 10636/12000
2024-11-05 03:29:09,785 - INFO - [diffusion][Epoch 10635] diffusion training Loss: 0.06304028909653425
2024-11-05 03:29:09,787 - INFO - [diffusion][Epoch 10635] diffusion learning rate: 0.001
2024-11-05 03:29:09,789 - INFO - [diffusion][Epoch 10635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:09,791 - INFO - [diffusion][Epoch 10636] Epoch 10637/12000
2024-11-05 03:29:13,829 - INFO - [diffusion][Epoch 10636] diffusion training Loss: 0.06197061575949192
2024-11-05 03:29:13,831 - INFO - [diffusion][Epoch 10636] diffusion learning rate: 0.001
2024-11-05 03:29:13,833 - INFO - [diffusion][Epoch 10636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:13,834 - INFO - [diffusion][Epoch 10637] Epoch 10638/12000
2024-11-05 03:29:17,998 - INFO - [diffusion][Epoch 10637] diffusion training Loss: 0.06418434344232082
2024-11-05 03:29:18,001 - INFO - [diffusion][Epoch 10637] diffusion learning rate: 0.001
2024-11-05 03:29:18,003 - INFO - [diffusion][Epoch 10637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:18,004 - INFO - [diffusion][Epoch 10638] Epoch 10639/12000
2024-11-05 03:29:22,218 - INFO - [diffusion][Epoch 10638] diffusion training Loss: 0.06056452728807926
2024-11-05 03:29:22,220 - INFO - [diffusion][Epoch 10638] diffusion learning rate: 0.001
2024-11-05 03:29:22,222 - INFO - [diffusion][Epoch 10638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:22,224 - INFO - [diffusion][Epoch 10639] Epoch 10640/12000
2024-11-05 03:29:26,403 - INFO - [diffusion][Epoch 10639] diffusion training Loss: 0.06348377093672752
2024-11-05 03:29:26,406 - INFO - [diffusion][Epoch 10639] diffusion learning rate: 0.001
2024-11-05 03:29:26,408 - INFO - [diffusion][Epoch 10639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:26,409 - INFO - [diffusion][Epoch 10640] Epoch 10641/12000
2024-11-05 03:29:30,619 - INFO - [diffusion][Epoch 10640] diffusion training Loss: 0.062749651260674
2024-11-05 03:29:30,621 - INFO - [diffusion][Epoch 10640] diffusion learning rate: 0.001
2024-11-05 03:29:30,623 - INFO - [diffusion][Epoch 10640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:30,624 - INFO - [diffusion][Epoch 10641] Epoch 10642/12000
2024-11-05 03:29:34,838 - INFO - [diffusion][Epoch 10641] diffusion training Loss: 0.06179044395685196
2024-11-05 03:29:34,840 - INFO - [diffusion][Epoch 10641] diffusion learning rate: 0.001
2024-11-05 03:29:34,842 - INFO - [diffusion][Epoch 10641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:34,843 - INFO - [diffusion][Epoch 10642] Epoch 10643/12000
2024-11-05 03:29:39,049 - INFO - [diffusion][Epoch 10642] diffusion training Loss: 0.06358392536640167
2024-11-05 03:29:39,051 - INFO - [diffusion][Epoch 10642] diffusion learning rate: 0.001
2024-11-05 03:29:39,053 - INFO - [diffusion][Epoch 10642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:39,054 - INFO - [diffusion][Epoch 10643] Epoch 10644/12000
2024-11-05 03:29:43,313 - INFO - [diffusion][Epoch 10643] diffusion training Loss: 0.06220799870789051
2024-11-05 03:29:43,315 - INFO - [diffusion][Epoch 10643] diffusion learning rate: 0.001
2024-11-05 03:29:43,317 - INFO - [diffusion][Epoch 10643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:43,318 - INFO - [diffusion][Epoch 10644] Epoch 10645/12000
2024-11-05 03:29:47,460 - INFO - [diffusion][Epoch 10644] diffusion training Loss: 0.06167535576969385
2024-11-05 03:29:47,463 - INFO - [diffusion][Epoch 10644] diffusion learning rate: 0.001
2024-11-05 03:29:47,465 - INFO - [diffusion][Epoch 10644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:47,466 - INFO - [diffusion][Epoch 10645] Epoch 10646/12000
2024-11-05 03:29:51,696 - INFO - [diffusion][Epoch 10645] diffusion training Loss: 0.06227654870599508
2024-11-05 03:29:51,698 - INFO - [diffusion][Epoch 10645] diffusion learning rate: 0.001
2024-11-05 03:29:51,700 - INFO - [diffusion][Epoch 10645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:51,701 - INFO - [diffusion][Epoch 10646] Epoch 10647/12000
2024-11-05 03:29:55,895 - INFO - [diffusion][Epoch 10646] diffusion training Loss: 0.057740005664527416
2024-11-05 03:29:55,898 - INFO - [diffusion][Epoch 10646] diffusion learning rate: 0.001
2024-11-05 03:29:55,901 - INFO - [diffusion][Epoch 10646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:55,903 - INFO - [diffusion][Epoch 10647] Epoch 10648/12000
2024-11-05 03:30:00,025 - INFO - [diffusion][Epoch 10647] diffusion training Loss: 0.0617172010242939
2024-11-05 03:30:00,027 - INFO - [diffusion][Epoch 10647] diffusion learning rate: 0.001
2024-11-05 03:30:00,029 - INFO - [diffusion][Epoch 10647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:00,030 - INFO - [diffusion][Epoch 10648] Epoch 10649/12000
2024-11-05 03:30:04,079 - INFO - [diffusion][Epoch 10648] diffusion training Loss: 0.06382852420210838
2024-11-05 03:30:04,081 - INFO - [diffusion][Epoch 10648] diffusion learning rate: 0.001
2024-11-05 03:30:04,083 - INFO - [diffusion][Epoch 10648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:04,084 - INFO - [diffusion][Epoch 10649] Epoch 10650/12000
2024-11-05 03:30:08,291 - INFO - [diffusion][Epoch 10649] diffusion training Loss: 0.06251508370041847
2024-11-05 03:30:08,293 - INFO - [diffusion][Epoch 10649] diffusion learning rate: 0.001
2024-11-05 03:30:08,295 - INFO - [diffusion][Epoch 10649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:08,296 - INFO - [diffusion][Epoch 10650] Epoch 10651/12000
2024-11-05 03:30:12,525 - INFO - [diffusion][Epoch 10650] diffusion training Loss: 0.056924356147646904
2024-11-05 03:30:12,527 - INFO - [diffusion][Epoch 10650] diffusion learning rate: 0.001
2024-11-05 03:30:12,529 - INFO - [diffusion][Epoch 10650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:12,530 - INFO - [diffusion][Epoch 10651] Epoch 10652/12000
2024-11-05 03:30:16,712 - INFO - [diffusion][Epoch 10651] diffusion training Loss: 0.06395526230335236
2024-11-05 03:30:16,715 - INFO - [diffusion][Epoch 10651] diffusion learning rate: 0.001
2024-11-05 03:30:16,716 - INFO - [diffusion][Epoch 10651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:16,718 - INFO - [diffusion][Epoch 10652] Epoch 10653/12000
2024-11-05 03:30:20,887 - INFO - [diffusion][Epoch 10652] diffusion training Loss: 0.0591742442920804
2024-11-05 03:30:20,890 - INFO - [diffusion][Epoch 10652] diffusion learning rate: 0.001
2024-11-05 03:30:20,892 - INFO - [diffusion][Epoch 10652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:20,894 - INFO - [diffusion][Epoch 10653] Epoch 10654/12000
2024-11-05 03:30:25,257 - INFO - [diffusion][Epoch 10653] diffusion training Loss: 0.0643383776769042
2024-11-05 03:30:25,259 - INFO - [diffusion][Epoch 10653] diffusion learning rate: 0.001
2024-11-05 03:30:25,261 - INFO - [diffusion][Epoch 10653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:25,262 - INFO - [diffusion][Epoch 10654] Epoch 10655/12000
2024-11-05 03:30:29,149 - INFO - [diffusion][Epoch 10654] diffusion training Loss: 0.05936157424002886
2024-11-05 03:30:29,151 - INFO - [diffusion][Epoch 10654] diffusion learning rate: 0.001
2024-11-05 03:30:29,153 - INFO - [diffusion][Epoch 10654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:29,154 - INFO - [diffusion][Epoch 10655] Epoch 10656/12000
2024-11-05 03:30:33,478 - INFO - [diffusion][Epoch 10655] diffusion training Loss: 0.06045675463974476
2024-11-05 03:30:33,481 - INFO - [diffusion][Epoch 10655] diffusion learning rate: 0.001
2024-11-05 03:30:33,483 - INFO - [diffusion][Epoch 10655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:33,484 - INFO - [diffusion][Epoch 10656] Epoch 10657/12000
2024-11-05 03:30:37,569 - INFO - [diffusion][Epoch 10656] diffusion training Loss: 0.05694880522787571
2024-11-05 03:30:37,571 - INFO - [diffusion][Epoch 10656] diffusion learning rate: 0.001
2024-11-05 03:30:37,572 - INFO - [diffusion][Epoch 10656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:37,574 - INFO - [diffusion][Epoch 10657] Epoch 10658/12000
2024-11-05 03:30:41,748 - INFO - [diffusion][Epoch 10657] diffusion training Loss: 0.06258126441389322
2024-11-05 03:30:41,751 - INFO - [diffusion][Epoch 10657] diffusion learning rate: 0.001
2024-11-05 03:30:41,752 - INFO - [diffusion][Epoch 10657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:41,754 - INFO - [diffusion][Epoch 10658] Epoch 10659/12000
2024-11-05 03:30:45,777 - INFO - [diffusion][Epoch 10658] diffusion training Loss: 0.05965638905763626
2024-11-05 03:30:45,779 - INFO - [diffusion][Epoch 10658] diffusion learning rate: 0.001
2024-11-05 03:30:45,781 - INFO - [diffusion][Epoch 10658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:45,783 - INFO - [diffusion][Epoch 10659] Epoch 10660/12000
2024-11-05 03:30:49,998 - INFO - [diffusion][Epoch 10659] diffusion training Loss: 0.0655762329697609
2024-11-05 03:30:50,000 - INFO - [diffusion][Epoch 10659] diffusion learning rate: 0.001
2024-11-05 03:30:50,002 - INFO - [diffusion][Epoch 10659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:50,003 - INFO - [diffusion][Epoch 10660] Epoch 10661/12000
2024-11-05 03:30:54,055 - INFO - [diffusion][Epoch 10660] diffusion training Loss: 0.05900253634899855
2024-11-05 03:30:54,057 - INFO - [diffusion][Epoch 10660] diffusion learning rate: 0.001
2024-11-05 03:30:54,059 - INFO - [diffusion][Epoch 10660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:54,060 - INFO - [diffusion][Epoch 10661] Epoch 10662/12000
2024-11-05 03:30:58,082 - INFO - [diffusion][Epoch 10661] diffusion training Loss: 0.05571840796619654
2024-11-05 03:30:58,109 - INFO - [diffusion][Epoch 10661] diffusion learning rate: 0.001
2024-11-05 03:30:58,112 - INFO - [diffusion][Epoch 10661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:58,113 - INFO - [diffusion][Epoch 10662] Epoch 10663/12000
2024-11-05 03:31:02,358 - INFO - [diffusion][Epoch 10662] diffusion training Loss: 0.05740961804986
2024-11-05 03:31:02,361 - INFO - [diffusion][Epoch 10662] diffusion learning rate: 0.001
2024-11-05 03:31:02,363 - INFO - [diffusion][Epoch 10662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:02,364 - INFO - [diffusion][Epoch 10663] Epoch 10664/12000
2024-11-05 03:31:06,653 - INFO - [diffusion][Epoch 10663] diffusion training Loss: 0.05916446540504694
2024-11-05 03:31:06,656 - INFO - [diffusion][Epoch 10663] diffusion learning rate: 0.001
2024-11-05 03:31:06,658 - INFO - [diffusion][Epoch 10663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:06,659 - INFO - [diffusion][Epoch 10664] Epoch 10665/12000
2024-11-05 03:31:10,981 - INFO - [diffusion][Epoch 10664] diffusion training Loss: 0.0588421979919076
2024-11-05 03:31:10,983 - INFO - [diffusion][Epoch 10664] diffusion learning rate: 0.001
2024-11-05 03:31:10,985 - INFO - [diffusion][Epoch 10664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:10,987 - INFO - [diffusion][Epoch 10665] Epoch 10666/12000
2024-11-05 03:31:15,121 - INFO - [diffusion][Epoch 10665] diffusion training Loss: 0.05492470785975456
2024-11-05 03:31:15,123 - INFO - [diffusion][Epoch 10665] diffusion learning rate: 0.001
2024-11-05 03:31:15,125 - INFO - [diffusion][Epoch 10665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:15,126 - INFO - [diffusion][Epoch 10666] Epoch 10667/12000
2024-11-05 03:31:19,310 - INFO - [diffusion][Epoch 10666] diffusion training Loss: 0.059433202259242535
2024-11-05 03:31:19,312 - INFO - [diffusion][Epoch 10666] diffusion learning rate: 0.001
2024-11-05 03:31:19,314 - INFO - [diffusion][Epoch 10666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:19,315 - INFO - [diffusion][Epoch 10667] Epoch 10668/12000
2024-11-05 03:31:23,512 - INFO - [diffusion][Epoch 10667] diffusion training Loss: 0.05721093248575926
2024-11-05 03:31:23,514 - INFO - [diffusion][Epoch 10667] diffusion learning rate: 0.001
2024-11-05 03:31:23,517 - INFO - [diffusion][Epoch 10667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:23,518 - INFO - [diffusion][Epoch 10668] Epoch 10669/12000
2024-11-05 03:31:27,796 - INFO - [diffusion][Epoch 10668] diffusion training Loss: 0.05673021823167801
2024-11-05 03:31:27,799 - INFO - [diffusion][Epoch 10668] diffusion learning rate: 0.001
2024-11-05 03:31:27,801 - INFO - [diffusion][Epoch 10668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:27,802 - INFO - [diffusion][Epoch 10669] Epoch 10670/12000
2024-11-05 03:31:32,072 - INFO - [diffusion][Epoch 10669] diffusion training Loss: 0.06472043227404356
2024-11-05 03:31:32,074 - INFO - [diffusion][Epoch 10669] diffusion learning rate: 0.001
2024-11-05 03:31:32,076 - INFO - [diffusion][Epoch 10669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:32,077 - INFO - [diffusion][Epoch 10670] Epoch 10671/12000
2024-11-05 03:31:36,366 - INFO - [diffusion][Epoch 10670] diffusion training Loss: 0.058850642293691635
2024-11-05 03:31:36,369 - INFO - [diffusion][Epoch 10670] diffusion learning rate: 0.001
2024-11-05 03:31:36,371 - INFO - [diffusion][Epoch 10670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:36,372 - INFO - [diffusion][Epoch 10671] Epoch 10672/12000
2024-11-05 03:31:40,636 - INFO - [diffusion][Epoch 10671] diffusion training Loss: 0.06420861650258303
2024-11-05 03:31:40,638 - INFO - [diffusion][Epoch 10671] diffusion learning rate: 0.001
2024-11-05 03:31:40,640 - INFO - [diffusion][Epoch 10671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:40,641 - INFO - [diffusion][Epoch 10672] Epoch 10673/12000
2024-11-05 03:31:44,808 - INFO - [diffusion][Epoch 10672] diffusion training Loss: 0.06242355518043041
2024-11-05 03:31:44,810 - INFO - [diffusion][Epoch 10672] diffusion learning rate: 0.001
2024-11-05 03:31:44,812 - INFO - [diffusion][Epoch 10672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:44,813 - INFO - [diffusion][Epoch 10673] Epoch 10674/12000
2024-11-05 03:31:48,967 - INFO - [diffusion][Epoch 10673] diffusion training Loss: 0.06108477711677551
2024-11-05 03:31:48,969 - INFO - [diffusion][Epoch 10673] diffusion learning rate: 0.001
2024-11-05 03:31:48,971 - INFO - [diffusion][Epoch 10673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:48,972 - INFO - [diffusion][Epoch 10674] Epoch 10675/12000
2024-11-05 03:31:53,094 - INFO - [diffusion][Epoch 10674] diffusion training Loss: 0.060414670035243034
2024-11-05 03:31:53,096 - INFO - [diffusion][Epoch 10674] diffusion learning rate: 0.001
2024-11-05 03:31:53,098 - INFO - [diffusion][Epoch 10674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:53,099 - INFO - [diffusion][Epoch 10675] Epoch 10676/12000
2024-11-05 03:31:57,593 - INFO - [diffusion][Epoch 10675] diffusion training Loss: 0.05615631304681301
2024-11-05 03:31:57,595 - INFO - [diffusion][Epoch 10675] diffusion learning rate: 0.001
2024-11-05 03:31:57,596 - INFO - [diffusion][Epoch 10675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:57,598 - INFO - [diffusion][Epoch 10676] Epoch 10677/12000
2024-11-05 03:32:01,826 - INFO - [diffusion][Epoch 10676] diffusion training Loss: 0.06263864040374756
2024-11-05 03:32:01,828 - INFO - [diffusion][Epoch 10676] diffusion learning rate: 0.001
2024-11-05 03:32:01,830 - INFO - [diffusion][Epoch 10676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:01,831 - INFO - [diffusion][Epoch 10677] Epoch 10678/12000
2024-11-05 03:32:06,115 - INFO - [diffusion][Epoch 10677] diffusion training Loss: 0.06449661124497652
2024-11-05 03:32:06,117 - INFO - [diffusion][Epoch 10677] diffusion learning rate: 0.001
2024-11-05 03:32:06,119 - INFO - [diffusion][Epoch 10677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:06,120 - INFO - [diffusion][Epoch 10678] Epoch 10679/12000
2024-11-05 03:32:10,362 - INFO - [diffusion][Epoch 10678] diffusion training Loss: 0.06128376442939043
2024-11-05 03:32:10,365 - INFO - [diffusion][Epoch 10678] diffusion learning rate: 0.001
2024-11-05 03:32:10,367 - INFO - [diffusion][Epoch 10678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:10,368 - INFO - [diffusion][Epoch 10679] Epoch 10680/12000
2024-11-05 03:32:14,555 - INFO - [diffusion][Epoch 10679] diffusion training Loss: 0.06257245596498251
2024-11-05 03:32:14,557 - INFO - [diffusion][Epoch 10679] diffusion learning rate: 0.001
2024-11-05 03:32:14,559 - INFO - [diffusion][Epoch 10679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:14,560 - INFO - [diffusion][Epoch 10680] Epoch 10681/12000
2024-11-05 03:32:18,635 - INFO - [diffusion][Epoch 10680] diffusion training Loss: 0.0681737121194601
2024-11-05 03:32:18,637 - INFO - [diffusion][Epoch 10680] diffusion learning rate: 0.001
2024-11-05 03:32:18,639 - INFO - [diffusion][Epoch 10680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:18,640 - INFO - [diffusion][Epoch 10681] Epoch 10682/12000
2024-11-05 03:32:22,737 - INFO - [diffusion][Epoch 10681] diffusion training Loss: 0.058904954232275486
2024-11-05 03:32:22,739 - INFO - [diffusion][Epoch 10681] diffusion learning rate: 0.001
2024-11-05 03:32:22,741 - INFO - [diffusion][Epoch 10681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:22,742 - INFO - [diffusion][Epoch 10682] Epoch 10683/12000
2024-11-05 03:32:26,930 - INFO - [diffusion][Epoch 10682] diffusion training Loss: 0.06170716416090727
2024-11-05 03:32:26,933 - INFO - [diffusion][Epoch 10682] diffusion learning rate: 0.001
2024-11-05 03:32:26,934 - INFO - [diffusion][Epoch 10682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:26,936 - INFO - [diffusion][Epoch 10683] Epoch 10684/12000
2024-11-05 03:32:31,060 - INFO - [diffusion][Epoch 10683] diffusion training Loss: 0.05775050912052393
2024-11-05 03:32:31,063 - INFO - [diffusion][Epoch 10683] diffusion learning rate: 0.001
2024-11-05 03:32:31,065 - INFO - [diffusion][Epoch 10683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:31,067 - INFO - [diffusion][Epoch 10684] Epoch 10685/12000
2024-11-05 03:32:34,952 - INFO - [diffusion][Epoch 10684] diffusion training Loss: 0.0595685038715601
2024-11-05 03:32:34,954 - INFO - [diffusion][Epoch 10684] diffusion learning rate: 0.001
2024-11-05 03:32:34,956 - INFO - [diffusion][Epoch 10684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:34,957 - INFO - [diffusion][Epoch 10685] Epoch 10686/12000
2024-11-05 03:32:39,269 - INFO - [diffusion][Epoch 10685] diffusion training Loss: 0.06230129674077034
2024-11-05 03:32:39,272 - INFO - [diffusion][Epoch 10685] diffusion learning rate: 0.001
2024-11-05 03:32:39,273 - INFO - [diffusion][Epoch 10685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:39,275 - INFO - [diffusion][Epoch 10686] Epoch 10687/12000
2024-11-05 03:32:43,230 - INFO - [diffusion][Epoch 10686] diffusion training Loss: 0.06312507297843695
2024-11-05 03:32:43,233 - INFO - [diffusion][Epoch 10686] diffusion learning rate: 0.001
2024-11-05 03:32:43,235 - INFO - [diffusion][Epoch 10686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:43,237 - INFO - [diffusion][Epoch 10687] Epoch 10688/12000
2024-11-05 03:32:47,329 - INFO - [diffusion][Epoch 10687] diffusion training Loss: 0.05826837383210659
2024-11-05 03:32:47,331 - INFO - [diffusion][Epoch 10687] diffusion learning rate: 0.001
2024-11-05 03:32:47,333 - INFO - [diffusion][Epoch 10687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:47,334 - INFO - [diffusion][Epoch 10688] Epoch 10689/12000
2024-11-05 03:32:51,442 - INFO - [diffusion][Epoch 10688] diffusion training Loss: 0.05854490492492914
2024-11-05 03:32:51,444 - INFO - [diffusion][Epoch 10688] diffusion learning rate: 0.001
2024-11-05 03:32:51,446 - INFO - [diffusion][Epoch 10688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:51,448 - INFO - [diffusion][Epoch 10689] Epoch 10690/12000
2024-11-05 03:32:55,661 - INFO - [diffusion][Epoch 10689] diffusion training Loss: 0.05970146972686052
2024-11-05 03:32:55,664 - INFO - [diffusion][Epoch 10689] diffusion learning rate: 0.001
2024-11-05 03:32:55,666 - INFO - [diffusion][Epoch 10689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:55,667 - INFO - [diffusion][Epoch 10690] Epoch 10691/12000
2024-11-05 03:32:59,847 - INFO - [diffusion][Epoch 10690] diffusion training Loss: 0.06114039849489927
2024-11-05 03:32:59,849 - INFO - [diffusion][Epoch 10690] diffusion learning rate: 0.001
2024-11-05 03:32:59,851 - INFO - [diffusion][Epoch 10690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:59,852 - INFO - [diffusion][Epoch 10691] Epoch 10692/12000
2024-11-05 03:33:04,102 - INFO - [diffusion][Epoch 10691] diffusion training Loss: 0.058878897689282894
2024-11-05 03:33:04,104 - INFO - [diffusion][Epoch 10691] diffusion learning rate: 0.001
2024-11-05 03:33:04,107 - INFO - [diffusion][Epoch 10691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:04,108 - INFO - [diffusion][Epoch 10692] Epoch 10693/12000
2024-11-05 03:33:08,158 - INFO - [diffusion][Epoch 10692] diffusion training Loss: 0.06192183494567871
2024-11-05 03:33:08,160 - INFO - [diffusion][Epoch 10692] diffusion learning rate: 0.001
2024-11-05 03:33:08,162 - INFO - [diffusion][Epoch 10692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:08,163 - INFO - [diffusion][Epoch 10693] Epoch 10694/12000
2024-11-05 03:33:12,386 - INFO - [diffusion][Epoch 10693] diffusion training Loss: 0.05442393943667412
2024-11-05 03:33:12,505 - INFO - [diffusion][Epoch 10693] diffusion learning rate: 0.001
2024-11-05 03:33:12,507 - INFO - [diffusion][Epoch 10693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:12,508 - INFO - [diffusion][Epoch 10694] Epoch 10695/12000
2024-11-05 03:33:16,622 - INFO - [diffusion][Epoch 10694] diffusion training Loss: 0.05627413373440504
2024-11-05 03:33:16,624 - INFO - [diffusion][Epoch 10694] diffusion learning rate: 0.001
2024-11-05 03:33:16,627 - INFO - [diffusion][Epoch 10694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:16,629 - INFO - [diffusion][Epoch 10695] Epoch 10696/12000
2024-11-05 03:33:20,672 - INFO - [diffusion][Epoch 10695] diffusion training Loss: 0.05574511084705591
2024-11-05 03:33:20,674 - INFO - [diffusion][Epoch 10695] diffusion learning rate: 0.001
2024-11-05 03:33:20,676 - INFO - [diffusion][Epoch 10695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:20,677 - INFO - [diffusion][Epoch 10696] Epoch 10697/12000
2024-11-05 03:33:25,412 - INFO - [diffusion][Epoch 10696] diffusion training Loss: 0.05840005446225405
2024-11-05 03:33:25,413 - INFO - [diffusion][Epoch 10696] diffusion learning rate: 0.001
2024-11-05 03:33:25,415 - INFO - [diffusion][Epoch 10696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:25,417 - INFO - [diffusion][Epoch 10697] Epoch 10698/12000
2024-11-05 03:33:29,653 - INFO - [diffusion][Epoch 10697] diffusion training Loss: 0.05848056171089411
2024-11-05 03:33:29,655 - INFO - [diffusion][Epoch 10697] diffusion learning rate: 0.001
2024-11-05 03:33:29,694 - INFO - [diffusion][Epoch 10697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:29,695 - INFO - [diffusion][Epoch 10698] Epoch 10699/12000
2024-11-05 03:33:33,788 - INFO - [diffusion][Epoch 10698] diffusion training Loss: 0.06383351515978575
2024-11-05 03:33:33,791 - INFO - [diffusion][Epoch 10698] diffusion learning rate: 0.001
2024-11-05 03:33:33,793 - INFO - [diffusion][Epoch 10698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:33,795 - INFO - [diffusion][Epoch 10699] Epoch 10700/12000
2024-11-05 03:33:37,845 - INFO - [diffusion][Epoch 10699] diffusion training Loss: 0.06556882243603468
2024-11-05 03:33:37,847 - INFO - [diffusion][Epoch 10699] diffusion learning rate: 0.001
2024-11-05 03:33:37,849 - INFO - [diffusion][Epoch 10699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:37,850 - INFO - [diffusion][Epoch 10700] Epoch 10701/12000
2024-11-05 03:33:41,910 - INFO - [diffusion][Epoch 10700] diffusion training Loss: 0.05766226723790169
2024-11-05 03:33:41,912 - INFO - [diffusion][Epoch 10700] diffusion learning rate: 0.001
2024-11-05 03:33:41,914 - INFO - [diffusion][Epoch 10700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:41,915 - INFO - [diffusion][Epoch 10701] Epoch 10702/12000
2024-11-05 03:33:46,198 - INFO - [diffusion][Epoch 10701] diffusion training Loss: 0.06144162639975548
2024-11-05 03:33:46,200 - INFO - [diffusion][Epoch 10701] diffusion learning rate: 0.001
2024-11-05 03:33:46,202 - INFO - [diffusion][Epoch 10701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:46,203 - INFO - [diffusion][Epoch 10702] Epoch 10703/12000
2024-11-05 03:33:50,382 - INFO - [diffusion][Epoch 10702] diffusion training Loss: 0.05897375661879778
2024-11-05 03:33:50,384 - INFO - [diffusion][Epoch 10702] diffusion learning rate: 0.001
2024-11-05 03:33:50,386 - INFO - [diffusion][Epoch 10702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:50,387 - INFO - [diffusion][Epoch 10703] Epoch 10704/12000
2024-11-05 03:33:54,572 - INFO - [diffusion][Epoch 10703] diffusion training Loss: 0.06100530829280615
2024-11-05 03:33:54,574 - INFO - [diffusion][Epoch 10703] diffusion learning rate: 0.001
2024-11-05 03:33:54,576 - INFO - [diffusion][Epoch 10703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:54,577 - INFO - [diffusion][Epoch 10704] Epoch 10705/12000
2024-11-05 03:33:58,609 - INFO - [diffusion][Epoch 10704] diffusion training Loss: 0.0613455343991518
2024-11-05 03:33:58,611 - INFO - [diffusion][Epoch 10704] diffusion learning rate: 0.001
2024-11-05 03:33:58,613 - INFO - [diffusion][Epoch 10704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:58,615 - INFO - [diffusion][Epoch 10705] Epoch 10706/12000
2024-11-05 03:34:02,895 - INFO - [diffusion][Epoch 10705] diffusion training Loss: 0.061981525272130966
2024-11-05 03:34:02,897 - INFO - [diffusion][Epoch 10705] diffusion learning rate: 0.001
2024-11-05 03:34:02,899 - INFO - [diffusion][Epoch 10705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:02,900 - INFO - [diffusion][Epoch 10706] Epoch 10707/12000
2024-11-05 03:34:07,014 - INFO - [diffusion][Epoch 10706] diffusion training Loss: 0.061900436878204346
2024-11-05 03:34:07,016 - INFO - [diffusion][Epoch 10706] diffusion learning rate: 0.001
2024-11-05 03:34:07,018 - INFO - [diffusion][Epoch 10706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:07,020 - INFO - [diffusion][Epoch 10707] Epoch 10708/12000
2024-11-05 03:34:11,205 - INFO - [diffusion][Epoch 10707] diffusion training Loss: 0.0615299241617322
2024-11-05 03:34:11,208 - INFO - [diffusion][Epoch 10707] diffusion learning rate: 0.001
2024-11-05 03:34:11,210 - INFO - [diffusion][Epoch 10707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:11,211 - INFO - [diffusion][Epoch 10708] Epoch 10709/12000
2024-11-05 03:34:15,457 - INFO - [diffusion][Epoch 10708] diffusion training Loss: 0.05668780952692032
2024-11-05 03:34:15,459 - INFO - [diffusion][Epoch 10708] diffusion learning rate: 0.001
2024-11-05 03:34:15,461 - INFO - [diffusion][Epoch 10708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:15,462 - INFO - [diffusion][Epoch 10709] Epoch 10710/12000
2024-11-05 03:34:19,713 - INFO - [diffusion][Epoch 10709] diffusion training Loss: 0.060366714373230934
2024-11-05 03:34:19,715 - INFO - [diffusion][Epoch 10709] diffusion learning rate: 0.001
2024-11-05 03:34:19,717 - INFO - [diffusion][Epoch 10709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:19,719 - INFO - [diffusion][Epoch 10710] Epoch 10711/12000
2024-11-05 03:34:23,803 - INFO - [diffusion][Epoch 10710] diffusion training Loss: 0.06241979729384184
2024-11-05 03:34:23,805 - INFO - [diffusion][Epoch 10710] diffusion learning rate: 0.001
2024-11-05 03:34:23,807 - INFO - [diffusion][Epoch 10710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:23,809 - INFO - [diffusion][Epoch 10711] Epoch 10712/12000
2024-11-05 03:34:28,024 - INFO - [diffusion][Epoch 10711] diffusion training Loss: 0.06078419927507639
2024-11-05 03:34:28,027 - INFO - [diffusion][Epoch 10711] diffusion learning rate: 0.001
2024-11-05 03:34:28,029 - INFO - [diffusion][Epoch 10711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:28,030 - INFO - [diffusion][Epoch 10712] Epoch 10713/12000
2024-11-05 03:34:32,180 - INFO - [diffusion][Epoch 10712] diffusion training Loss: 0.055345501750707626
2024-11-05 03:34:32,182 - INFO - [diffusion][Epoch 10712] diffusion learning rate: 0.001
2024-11-05 03:34:32,184 - INFO - [diffusion][Epoch 10712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:32,185 - INFO - [diffusion][Epoch 10713] Epoch 10714/12000
2024-11-05 03:34:36,390 - INFO - [diffusion][Epoch 10713] diffusion training Loss: 0.05461722891777754
2024-11-05 03:34:36,392 - INFO - [diffusion][Epoch 10713] diffusion learning rate: 0.001
2024-11-05 03:34:36,393 - INFO - [diffusion][Epoch 10713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:36,395 - INFO - [diffusion][Epoch 10714] Epoch 10715/12000
2024-11-05 03:34:40,597 - INFO - [diffusion][Epoch 10714] diffusion training Loss: 0.05408620275557041
2024-11-05 03:34:40,600 - INFO - [diffusion][Epoch 10714] diffusion learning rate: 0.001
2024-11-05 03:34:40,602 - INFO - [diffusion][Epoch 10714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:40,603 - INFO - [diffusion][Epoch 10715] Epoch 10716/12000
2024-11-05 03:34:44,755 - INFO - [diffusion][Epoch 10715] diffusion training Loss: 0.05788842123001814
2024-11-05 03:34:44,757 - INFO - [diffusion][Epoch 10715] diffusion learning rate: 0.001
2024-11-05 03:34:44,759 - INFO - [diffusion][Epoch 10715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:44,761 - INFO - [diffusion][Epoch 10716] Epoch 10717/12000
2024-11-05 03:34:48,943 - INFO - [diffusion][Epoch 10716] diffusion training Loss: 0.0580521784722805
2024-11-05 03:34:48,945 - INFO - [diffusion][Epoch 10716] diffusion learning rate: 0.001
2024-11-05 03:34:48,947 - INFO - [diffusion][Epoch 10716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:48,949 - INFO - [diffusion][Epoch 10717] Epoch 10718/12000
2024-11-05 03:34:53,067 - INFO - [diffusion][Epoch 10717] diffusion training Loss: 0.05836600996553898
2024-11-05 03:34:53,070 - INFO - [diffusion][Epoch 10717] diffusion learning rate: 0.001
2024-11-05 03:34:53,072 - INFO - [diffusion][Epoch 10717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:53,073 - INFO - [diffusion][Epoch 10718] Epoch 10719/12000
2024-11-05 03:34:57,299 - INFO - [diffusion][Epoch 10718] diffusion training Loss: 0.059162053279578686
2024-11-05 03:34:57,301 - INFO - [diffusion][Epoch 10718] diffusion learning rate: 0.001
2024-11-05 03:34:57,303 - INFO - [diffusion][Epoch 10718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:57,304 - INFO - [diffusion][Epoch 10719] Epoch 10720/12000
2024-11-05 03:35:01,417 - INFO - [diffusion][Epoch 10719] diffusion training Loss: 0.06000605132430792
2024-11-05 03:35:01,419 - INFO - [diffusion][Epoch 10719] diffusion learning rate: 0.001
2024-11-05 03:35:01,421 - INFO - [diffusion][Epoch 10719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:01,422 - INFO - [diffusion][Epoch 10720] Epoch 10721/12000
2024-11-05 03:35:05,700 - INFO - [diffusion][Epoch 10720] diffusion training Loss: 0.05551072861999273
2024-11-05 03:35:05,703 - INFO - [diffusion][Epoch 10720] diffusion learning rate: 0.001
2024-11-05 03:35:05,705 - INFO - [diffusion][Epoch 10720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:05,707 - INFO - [diffusion][Epoch 10721] Epoch 10722/12000
2024-11-05 03:35:09,951 - INFO - [diffusion][Epoch 10721] diffusion training Loss: 0.05927352886646986
2024-11-05 03:35:09,953 - INFO - [diffusion][Epoch 10721] diffusion learning rate: 0.001
2024-11-05 03:35:09,954 - INFO - [diffusion][Epoch 10721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:09,956 - INFO - [diffusion][Epoch 10722] Epoch 10723/12000
2024-11-05 03:35:14,261 - INFO - [diffusion][Epoch 10722] diffusion training Loss: 0.05794522911310196
2024-11-05 03:35:14,263 - INFO - [diffusion][Epoch 10722] diffusion learning rate: 0.001
2024-11-05 03:35:14,265 - INFO - [diffusion][Epoch 10722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:14,266 - INFO - [diffusion][Epoch 10723] Epoch 10724/12000
2024-11-05 03:35:18,517 - INFO - [diffusion][Epoch 10723] diffusion training Loss: 0.06411747448146343
2024-11-05 03:35:18,519 - INFO - [diffusion][Epoch 10723] diffusion learning rate: 0.001
2024-11-05 03:35:18,521 - INFO - [diffusion][Epoch 10723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:18,523 - INFO - [diffusion][Epoch 10724] Epoch 10725/12000
2024-11-05 03:35:22,709 - INFO - [diffusion][Epoch 10724] diffusion training Loss: 0.060926320031285286
2024-11-05 03:35:22,711 - INFO - [diffusion][Epoch 10724] diffusion learning rate: 0.001
2024-11-05 03:35:22,713 - INFO - [diffusion][Epoch 10724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:22,714 - INFO - [diffusion][Epoch 10725] Epoch 10726/12000
2024-11-05 03:35:26,792 - INFO - [diffusion][Epoch 10725] diffusion training Loss: 0.057819425128400326
2024-11-05 03:35:26,794 - INFO - [diffusion][Epoch 10725] diffusion learning rate: 0.001
2024-11-05 03:35:26,796 - INFO - [diffusion][Epoch 10725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:26,797 - INFO - [diffusion][Epoch 10726] Epoch 10727/12000
2024-11-05 03:35:30,822 - INFO - [diffusion][Epoch 10726] diffusion training Loss: 0.05930748116225004
2024-11-05 03:35:30,824 - INFO - [diffusion][Epoch 10726] diffusion learning rate: 0.001
2024-11-05 03:35:30,852 - INFO - [diffusion][Epoch 10726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:30,853 - INFO - [diffusion][Epoch 10727] Epoch 10728/12000
2024-11-05 03:35:35,154 - INFO - [diffusion][Epoch 10727] diffusion training Loss: 0.05854621343314648
2024-11-05 03:35:35,157 - INFO - [diffusion][Epoch 10727] diffusion learning rate: 0.001
2024-11-05 03:35:35,159 - INFO - [diffusion][Epoch 10727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:35,160 - INFO - [diffusion][Epoch 10728] Epoch 10729/12000
2024-11-05 03:35:39,409 - INFO - [diffusion][Epoch 10728] diffusion training Loss: 0.05971947405487299
2024-11-05 03:35:39,411 - INFO - [diffusion][Epoch 10728] diffusion learning rate: 0.001
2024-11-05 03:35:39,413 - INFO - [diffusion][Epoch 10728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:39,414 - INFO - [diffusion][Epoch 10729] Epoch 10730/12000
2024-11-05 03:35:43,613 - INFO - [diffusion][Epoch 10729] diffusion training Loss: 0.05997262895107269
2024-11-05 03:35:43,615 - INFO - [diffusion][Epoch 10729] diffusion learning rate: 0.001
2024-11-05 03:35:43,617 - INFO - [diffusion][Epoch 10729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:43,618 - INFO - [diffusion][Epoch 10730] Epoch 10731/12000
2024-11-05 03:35:47,871 - INFO - [diffusion][Epoch 10730] diffusion training Loss: 0.06107613164931536
2024-11-05 03:35:47,873 - INFO - [diffusion][Epoch 10730] diffusion learning rate: 0.001
2024-11-05 03:35:47,876 - INFO - [diffusion][Epoch 10730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:47,878 - INFO - [diffusion][Epoch 10731] Epoch 10732/12000
2024-11-05 03:35:52,010 - INFO - [diffusion][Epoch 10731] diffusion training Loss: 0.05673068109899759
2024-11-05 03:35:52,012 - INFO - [diffusion][Epoch 10731] diffusion learning rate: 0.001
2024-11-05 03:35:52,014 - INFO - [diffusion][Epoch 10731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:52,015 - INFO - [diffusion][Epoch 10732] Epoch 10733/12000
2024-11-05 03:35:56,299 - INFO - [diffusion][Epoch 10732] diffusion training Loss: 0.06549447868019342
2024-11-05 03:35:56,301 - INFO - [diffusion][Epoch 10732] diffusion learning rate: 0.001
2024-11-05 03:35:56,302 - INFO - [diffusion][Epoch 10732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:56,304 - INFO - [diffusion][Epoch 10733] Epoch 10734/12000
2024-11-05 03:36:00,342 - INFO - [diffusion][Epoch 10733] diffusion training Loss: 0.06394652184098959
2024-11-05 03:36:00,344 - INFO - [diffusion][Epoch 10733] diffusion learning rate: 0.001
2024-11-05 03:36:00,346 - INFO - [diffusion][Epoch 10733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:00,348 - INFO - [diffusion][Epoch 10734] Epoch 10735/12000
2024-11-05 03:36:04,603 - INFO - [diffusion][Epoch 10734] diffusion training Loss: 0.05819581635296345
2024-11-05 03:36:04,606 - INFO - [diffusion][Epoch 10734] diffusion learning rate: 0.001
2024-11-05 03:36:04,607 - INFO - [diffusion][Epoch 10734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:04,609 - INFO - [diffusion][Epoch 10735] Epoch 10736/12000
2024-11-05 03:36:08,757 - INFO - [diffusion][Epoch 10735] diffusion training Loss: 0.0581190399825573
2024-11-05 03:36:08,759 - INFO - [diffusion][Epoch 10735] diffusion learning rate: 0.001
2024-11-05 03:36:08,761 - INFO - [diffusion][Epoch 10735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:08,762 - INFO - [diffusion][Epoch 10736] Epoch 10737/12000
2024-11-05 03:36:13,049 - INFO - [diffusion][Epoch 10736] diffusion training Loss: 0.059150814078748226
2024-11-05 03:36:13,052 - INFO - [diffusion][Epoch 10736] diffusion learning rate: 0.001
2024-11-05 03:36:13,054 - INFO - [diffusion][Epoch 10736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:13,055 - INFO - [diffusion][Epoch 10737] Epoch 10738/12000
2024-11-05 03:36:17,693 - INFO - [diffusion][Epoch 10737] diffusion training Loss: 0.05910344794392586
2024-11-05 03:36:17,695 - INFO - [diffusion][Epoch 10737] diffusion learning rate: 0.001
2024-11-05 03:36:17,722 - INFO - [diffusion][Epoch 10737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:17,723 - INFO - [diffusion][Epoch 10738] Epoch 10739/12000
2024-11-05 03:36:21,809 - INFO - [diffusion][Epoch 10738] diffusion training Loss: 0.056865861639380455
2024-11-05 03:36:21,811 - INFO - [diffusion][Epoch 10738] diffusion learning rate: 0.001
2024-11-05 03:36:21,812 - INFO - [diffusion][Epoch 10738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:21,814 - INFO - [diffusion][Epoch 10739] Epoch 10740/12000
2024-11-05 03:36:25,913 - INFO - [diffusion][Epoch 10739] diffusion training Loss: 0.06221826933324337
2024-11-05 03:36:25,915 - INFO - [diffusion][Epoch 10739] diffusion learning rate: 0.001
2024-11-05 03:36:25,917 - INFO - [diffusion][Epoch 10739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:25,918 - INFO - [diffusion][Epoch 10740] Epoch 10741/12000
2024-11-05 03:36:30,041 - INFO - [diffusion][Epoch 10740] diffusion training Loss: 0.06346068251878023
2024-11-05 03:36:30,044 - INFO - [diffusion][Epoch 10740] diffusion learning rate: 0.001
2024-11-05 03:36:30,070 - INFO - [diffusion][Epoch 10740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:30,072 - INFO - [diffusion][Epoch 10741] Epoch 10742/12000
2024-11-05 03:36:34,290 - INFO - [diffusion][Epoch 10741] diffusion training Loss: 0.0575542189180851
2024-11-05 03:36:34,292 - INFO - [diffusion][Epoch 10741] diffusion learning rate: 0.001
2024-11-05 03:36:34,294 - INFO - [diffusion][Epoch 10741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:34,295 - INFO - [diffusion][Epoch 10742] Epoch 10743/12000
2024-11-05 03:36:38,535 - INFO - [diffusion][Epoch 10742] diffusion training Loss: 0.05478745140135288
2024-11-05 03:36:38,537 - INFO - [diffusion][Epoch 10742] diffusion learning rate: 0.001
2024-11-05 03:36:38,539 - INFO - [diffusion][Epoch 10742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:38,540 - INFO - [diffusion][Epoch 10743] Epoch 10744/12000
2024-11-05 03:36:42,859 - INFO - [diffusion][Epoch 10743] diffusion training Loss: 0.05996764451265335
2024-11-05 03:36:42,861 - INFO - [diffusion][Epoch 10743] diffusion learning rate: 0.001
2024-11-05 03:36:42,862 - INFO - [diffusion][Epoch 10743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:42,864 - INFO - [diffusion][Epoch 10744] Epoch 10745/12000
2024-11-05 03:36:46,980 - INFO - [diffusion][Epoch 10744] diffusion training Loss: 0.06129396054893732
2024-11-05 03:36:46,982 - INFO - [diffusion][Epoch 10744] diffusion learning rate: 0.001
2024-11-05 03:36:46,984 - INFO - [diffusion][Epoch 10744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:46,985 - INFO - [diffusion][Epoch 10745] Epoch 10746/12000
2024-11-05 03:36:51,125 - INFO - [diffusion][Epoch 10745] diffusion training Loss: 0.06104698870331049
2024-11-05 03:36:51,127 - INFO - [diffusion][Epoch 10745] diffusion learning rate: 0.001
2024-11-05 03:36:51,129 - INFO - [diffusion][Epoch 10745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:51,130 - INFO - [diffusion][Epoch 10746] Epoch 10747/12000
2024-11-05 03:36:55,341 - INFO - [diffusion][Epoch 10746] diffusion training Loss: 0.05983054544776678
2024-11-05 03:36:55,343 - INFO - [diffusion][Epoch 10746] diffusion learning rate: 0.001
2024-11-05 03:36:55,345 - INFO - [diffusion][Epoch 10746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:55,347 - INFO - [diffusion][Epoch 10747] Epoch 10748/12000
2024-11-05 03:36:59,489 - INFO - [diffusion][Epoch 10747] diffusion training Loss: 0.0597275011241436
2024-11-05 03:36:59,491 - INFO - [diffusion][Epoch 10747] diffusion learning rate: 0.001
2024-11-05 03:36:59,493 - INFO - [diffusion][Epoch 10747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:59,494 - INFO - [diffusion][Epoch 10748] Epoch 10749/12000
2024-11-05 03:37:03,680 - INFO - [diffusion][Epoch 10748] diffusion training Loss: 0.0604036757722497
2024-11-05 03:37:03,682 - INFO - [diffusion][Epoch 10748] diffusion learning rate: 0.001
2024-11-05 03:37:03,684 - INFO - [diffusion][Epoch 10748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:03,686 - INFO - [diffusion][Epoch 10749] Epoch 10750/12000
2024-11-05 03:37:07,764 - INFO - [diffusion][Epoch 10749] diffusion training Loss: 0.06109941937029362
2024-11-05 03:37:07,766 - INFO - [diffusion][Epoch 10749] diffusion learning rate: 0.001
2024-11-05 03:37:07,768 - INFO - [diffusion][Epoch 10749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:07,770 - INFO - [diffusion][Epoch 10750] Epoch 10751/12000
2024-11-05 03:37:11,790 - INFO - [diffusion][Epoch 10750] diffusion training Loss: 0.06010410375893116
2024-11-05 03:37:11,793 - INFO - [diffusion][Epoch 10750] diffusion learning rate: 0.001
2024-11-05 03:37:11,794 - INFO - [diffusion][Epoch 10750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:11,796 - INFO - [diffusion][Epoch 10751] Epoch 10752/12000
2024-11-05 03:37:16,079 - INFO - [diffusion][Epoch 10751] diffusion training Loss: 0.06230981834232807
2024-11-05 03:37:16,081 - INFO - [diffusion][Epoch 10751] diffusion learning rate: 0.001
2024-11-05 03:37:16,083 - INFO - [diffusion][Epoch 10751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:16,084 - INFO - [diffusion][Epoch 10752] Epoch 10753/12000
2024-11-05 03:37:20,377 - INFO - [diffusion][Epoch 10752] diffusion training Loss: 0.053982555866241455
2024-11-05 03:37:20,380 - INFO - [diffusion][Epoch 10752] diffusion learning rate: 0.001
2024-11-05 03:37:20,381 - INFO - [diffusion][Epoch 10752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:20,382 - INFO - [diffusion][Epoch 10753] Epoch 10754/12000
2024-11-05 03:37:24,634 - INFO - [diffusion][Epoch 10753] diffusion training Loss: 0.05643680039793253
2024-11-05 03:37:24,637 - INFO - [diffusion][Epoch 10753] diffusion learning rate: 0.001
2024-11-05 03:37:24,639 - INFO - [diffusion][Epoch 10753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:24,640 - INFO - [diffusion][Epoch 10754] Epoch 10755/12000
2024-11-05 03:37:28,938 - INFO - [diffusion][Epoch 10754] diffusion training Loss: 0.05809589009732008
2024-11-05 03:37:28,941 - INFO - [diffusion][Epoch 10754] diffusion learning rate: 0.001
2024-11-05 03:37:28,944 - INFO - [diffusion][Epoch 10754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:28,945 - INFO - [diffusion][Epoch 10755] Epoch 10756/12000
2024-11-05 03:37:33,246 - INFO - [diffusion][Epoch 10755] diffusion training Loss: 0.060042052529752254
2024-11-05 03:37:33,248 - INFO - [diffusion][Epoch 10755] diffusion learning rate: 0.001
2024-11-05 03:37:33,250 - INFO - [diffusion][Epoch 10755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:33,251 - INFO - [diffusion][Epoch 10756] Epoch 10757/12000
2024-11-05 03:37:37,341 - INFO - [diffusion][Epoch 10756] diffusion training Loss: 0.05919788219034672
2024-11-05 03:37:37,343 - INFO - [diffusion][Epoch 10756] diffusion learning rate: 0.001
2024-11-05 03:37:37,344 - INFO - [diffusion][Epoch 10756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:37,346 - INFO - [diffusion][Epoch 10757] Epoch 10758/12000
2024-11-05 03:37:41,495 - INFO - [diffusion][Epoch 10757] diffusion training Loss: 0.05808669701218605
2024-11-05 03:37:41,497 - INFO - [diffusion][Epoch 10757] diffusion learning rate: 0.001
2024-11-05 03:37:41,500 - INFO - [diffusion][Epoch 10757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:41,501 - INFO - [diffusion][Epoch 10758] Epoch 10759/12000
2024-11-05 03:37:46,258 - INFO - [diffusion][Epoch 10758] diffusion training Loss: 0.056411207653582096
2024-11-05 03:37:46,260 - INFO - [diffusion][Epoch 10758] diffusion learning rate: 0.001
2024-11-05 03:37:46,262 - INFO - [diffusion][Epoch 10758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:46,264 - INFO - [diffusion][Epoch 10759] Epoch 10760/12000
2024-11-05 03:37:50,489 - INFO - [diffusion][Epoch 10759] diffusion training Loss: 0.06267239898443222
2024-11-05 03:37:50,491 - INFO - [diffusion][Epoch 10759] diffusion learning rate: 0.001
2024-11-05 03:37:50,493 - INFO - [diffusion][Epoch 10759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:50,495 - INFO - [diffusion][Epoch 10760] Epoch 10761/12000
2024-11-05 03:37:54,658 - INFO - [diffusion][Epoch 10760] diffusion training Loss: 0.057773529551923275
2024-11-05 03:37:54,660 - INFO - [diffusion][Epoch 10760] diffusion learning rate: 0.001
2024-11-05 03:37:54,662 - INFO - [diffusion][Epoch 10760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:54,664 - INFO - [diffusion][Epoch 10761] Epoch 10762/12000
2024-11-05 03:37:58,785 - INFO - [diffusion][Epoch 10761] diffusion training Loss: 0.06288110371679068
2024-11-05 03:37:58,787 - INFO - [diffusion][Epoch 10761] diffusion learning rate: 0.001
2024-11-05 03:37:58,789 - INFO - [diffusion][Epoch 10761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:58,790 - INFO - [diffusion][Epoch 10762] Epoch 10763/12000
2024-11-05 03:38:02,863 - INFO - [diffusion][Epoch 10762] diffusion training Loss: 0.060026184655725956
2024-11-05 03:38:02,866 - INFO - [diffusion][Epoch 10762] diffusion learning rate: 0.001
2024-11-05 03:38:02,868 - INFO - [diffusion][Epoch 10762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:02,869 - INFO - [diffusion][Epoch 10763] Epoch 10764/12000
2024-11-05 03:38:06,759 - INFO - [diffusion][Epoch 10763] diffusion training Loss: 0.05888298433274031
2024-11-05 03:38:06,761 - INFO - [diffusion][Epoch 10763] diffusion learning rate: 0.001
2024-11-05 03:38:06,763 - INFO - [diffusion][Epoch 10763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:06,765 - INFO - [diffusion][Epoch 10764] Epoch 10765/12000
2024-11-05 03:38:10,949 - INFO - [diffusion][Epoch 10764] diffusion training Loss: 0.058065079152584076
2024-11-05 03:38:10,951 - INFO - [diffusion][Epoch 10764] diffusion learning rate: 0.001
2024-11-05 03:38:10,953 - INFO - [diffusion][Epoch 10764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:10,954 - INFO - [diffusion][Epoch 10765] Epoch 10766/12000
2024-11-05 03:38:15,111 - INFO - [diffusion][Epoch 10765] diffusion training Loss: 0.05989363603293896
2024-11-05 03:38:15,114 - INFO - [diffusion][Epoch 10765] diffusion learning rate: 0.001
2024-11-05 03:38:15,116 - INFO - [diffusion][Epoch 10765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:15,117 - INFO - [diffusion][Epoch 10766] Epoch 10767/12000
2024-11-05 03:38:19,114 - INFO - [diffusion][Epoch 10766] diffusion training Loss: 0.0571771040558815
2024-11-05 03:38:19,116 - INFO - [diffusion][Epoch 10766] diffusion learning rate: 0.001
2024-11-05 03:38:19,118 - INFO - [diffusion][Epoch 10766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:19,119 - INFO - [diffusion][Epoch 10767] Epoch 10768/12000
2024-11-05 03:38:23,200 - INFO - [diffusion][Epoch 10767] diffusion training Loss: 0.05885152891278267
2024-11-05 03:38:23,203 - INFO - [diffusion][Epoch 10767] diffusion learning rate: 0.001
2024-11-05 03:38:23,206 - INFO - [diffusion][Epoch 10767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:23,207 - INFO - [diffusion][Epoch 10768] Epoch 10769/12000
2024-11-05 03:38:27,283 - INFO - [diffusion][Epoch 10768] diffusion training Loss: 0.06172326859086752
2024-11-05 03:38:27,285 - INFO - [diffusion][Epoch 10768] diffusion learning rate: 0.001
2024-11-05 03:38:27,288 - INFO - [diffusion][Epoch 10768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:27,289 - INFO - [diffusion][Epoch 10769] Epoch 10770/12000
2024-11-05 03:38:31,386 - INFO - [diffusion][Epoch 10769] diffusion training Loss: 0.057406472973525524
2024-11-05 03:38:31,388 - INFO - [diffusion][Epoch 10769] diffusion learning rate: 0.001
2024-11-05 03:38:31,417 - INFO - [diffusion][Epoch 10769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:31,418 - INFO - [diffusion][Epoch 10770] Epoch 10771/12000
2024-11-05 03:38:35,591 - INFO - [diffusion][Epoch 10770] diffusion training Loss: 0.05726311821490526
2024-11-05 03:38:35,594 - INFO - [diffusion][Epoch 10770] diffusion learning rate: 0.001
2024-11-05 03:38:35,596 - INFO - [diffusion][Epoch 10770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:35,597 - INFO - [diffusion][Epoch 10771] Epoch 10772/12000
2024-11-05 03:38:39,684 - INFO - [diffusion][Epoch 10771] diffusion training Loss: 0.05690671131014824
2024-11-05 03:38:39,687 - INFO - [diffusion][Epoch 10771] diffusion learning rate: 0.001
2024-11-05 03:38:39,689 - INFO - [diffusion][Epoch 10771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:39,690 - INFO - [diffusion][Epoch 10772] Epoch 10773/12000
2024-11-05 03:38:43,855 - INFO - [diffusion][Epoch 10772] diffusion training Loss: 0.05895267054438591
2024-11-05 03:38:43,858 - INFO - [diffusion][Epoch 10772] diffusion learning rate: 0.001
2024-11-05 03:38:43,860 - INFO - [diffusion][Epoch 10772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:43,861 - INFO - [diffusion][Epoch 10773] Epoch 10774/12000
2024-11-05 03:38:48,021 - INFO - [diffusion][Epoch 10773] diffusion training Loss: 0.0583505742251873
2024-11-05 03:38:48,023 - INFO - [diffusion][Epoch 10773] diffusion learning rate: 0.001
2024-11-05 03:38:48,026 - INFO - [diffusion][Epoch 10773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:48,027 - INFO - [diffusion][Epoch 10774] Epoch 10775/12000
2024-11-05 03:38:52,128 - INFO - [diffusion][Epoch 10774] diffusion training Loss: 0.05660530366003513
2024-11-05 03:38:52,130 - INFO - [diffusion][Epoch 10774] diffusion learning rate: 0.001
2024-11-05 03:38:52,132 - INFO - [diffusion][Epoch 10774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:52,133 - INFO - [diffusion][Epoch 10775] Epoch 10776/12000
2024-11-05 03:38:56,243 - INFO - [diffusion][Epoch 10775] diffusion training Loss: 0.05793108232319355
2024-11-05 03:38:56,245 - INFO - [diffusion][Epoch 10775] diffusion learning rate: 0.001
2024-11-05 03:38:56,247 - INFO - [diffusion][Epoch 10775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:56,248 - INFO - [diffusion][Epoch 10776] Epoch 10777/12000
2024-11-05 03:39:00,410 - INFO - [diffusion][Epoch 10776] diffusion training Loss: 0.05971709918230772
2024-11-05 03:39:00,413 - INFO - [diffusion][Epoch 10776] diffusion learning rate: 0.001
2024-11-05 03:39:00,414 - INFO - [diffusion][Epoch 10776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:00,416 - INFO - [diffusion][Epoch 10777] Epoch 10778/12000
2024-11-05 03:39:04,604 - INFO - [diffusion][Epoch 10777] diffusion training Loss: 0.060336483642458916
2024-11-05 03:39:04,607 - INFO - [diffusion][Epoch 10777] diffusion learning rate: 0.001
2024-11-05 03:39:04,609 - INFO - [diffusion][Epoch 10777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:04,611 - INFO - [diffusion][Epoch 10778] Epoch 10779/12000
2024-11-05 03:39:09,120 - INFO - [diffusion][Epoch 10778] diffusion training Loss: 0.05702846031636
2024-11-05 03:39:09,122 - INFO - [diffusion][Epoch 10778] diffusion learning rate: 0.001
2024-11-05 03:39:09,155 - INFO - [diffusion][Epoch 10778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:09,156 - INFO - [diffusion][Epoch 10779] Epoch 10780/12000
2024-11-05 03:39:13,151 - INFO - [diffusion][Epoch 10779] diffusion training Loss: 0.05574194435030222
2024-11-05 03:39:13,153 - INFO - [diffusion][Epoch 10779] diffusion learning rate: 0.001
2024-11-05 03:39:13,155 - INFO - [diffusion][Epoch 10779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:13,157 - INFO - [diffusion][Epoch 10780] Epoch 10781/12000
2024-11-05 03:39:17,284 - INFO - [diffusion][Epoch 10780] diffusion training Loss: 0.05853777937591076
2024-11-05 03:39:17,286 - INFO - [diffusion][Epoch 10780] diffusion learning rate: 0.001
2024-11-05 03:39:17,288 - INFO - [diffusion][Epoch 10780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:17,289 - INFO - [diffusion][Epoch 10781] Epoch 10782/12000
2024-11-05 03:39:21,482 - INFO - [diffusion][Epoch 10781] diffusion training Loss: 0.058555844239890575
2024-11-05 03:39:21,484 - INFO - [diffusion][Epoch 10781] diffusion learning rate: 0.001
2024-11-05 03:39:21,486 - INFO - [diffusion][Epoch 10781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:21,487 - INFO - [diffusion][Epoch 10782] Epoch 10783/12000
2024-11-05 03:39:25,799 - INFO - [diffusion][Epoch 10782] diffusion training Loss: 0.06226677820086479
2024-11-05 03:39:25,801 - INFO - [diffusion][Epoch 10782] diffusion learning rate: 0.001
2024-11-05 03:39:25,803 - INFO - [diffusion][Epoch 10782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:25,804 - INFO - [diffusion][Epoch 10783] Epoch 10784/12000
2024-11-05 03:39:30,005 - INFO - [diffusion][Epoch 10783] diffusion training Loss: 0.061195557937026024
2024-11-05 03:39:30,007 - INFO - [diffusion][Epoch 10783] diffusion learning rate: 0.001
2024-11-05 03:39:30,009 - INFO - [diffusion][Epoch 10783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:30,010 - INFO - [diffusion][Epoch 10784] Epoch 10785/12000
2024-11-05 03:39:34,154 - INFO - [diffusion][Epoch 10784] diffusion training Loss: 0.061382757499814034
2024-11-05 03:39:34,156 - INFO - [diffusion][Epoch 10784] diffusion learning rate: 0.001
2024-11-05 03:39:34,158 - INFO - [diffusion][Epoch 10784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:34,159 - INFO - [diffusion][Epoch 10785] Epoch 10786/12000
2024-11-05 03:39:38,256 - INFO - [diffusion][Epoch 10785] diffusion training Loss: 0.059870014898478985
2024-11-05 03:39:38,258 - INFO - [diffusion][Epoch 10785] diffusion learning rate: 0.001
2024-11-05 03:39:38,260 - INFO - [diffusion][Epoch 10785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:38,261 - INFO - [diffusion][Epoch 10786] Epoch 10787/12000
2024-11-05 03:39:42,509 - INFO - [diffusion][Epoch 10786] diffusion training Loss: 0.06032091472297907
2024-11-05 03:39:42,511 - INFO - [diffusion][Epoch 10786] diffusion learning rate: 0.001
2024-11-05 03:39:42,513 - INFO - [diffusion][Epoch 10786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:42,515 - INFO - [diffusion][Epoch 10787] Epoch 10788/12000
2024-11-05 03:39:46,701 - INFO - [diffusion][Epoch 10787] diffusion training Loss: 0.06486013997346163
2024-11-05 03:39:46,704 - INFO - [diffusion][Epoch 10787] diffusion learning rate: 0.001
2024-11-05 03:39:46,706 - INFO - [diffusion][Epoch 10787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:46,707 - INFO - [diffusion][Epoch 10788] Epoch 10789/12000
2024-11-05 03:39:50,982 - INFO - [diffusion][Epoch 10788] diffusion training Loss: 0.05969025753438473
2024-11-05 03:39:50,984 - INFO - [diffusion][Epoch 10788] diffusion learning rate: 0.001
2024-11-05 03:39:50,986 - INFO - [diffusion][Epoch 10788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:50,987 - INFO - [diffusion][Epoch 10789] Epoch 10790/12000
2024-11-05 03:39:55,240 - INFO - [diffusion][Epoch 10789] diffusion training Loss: 0.053600543178617954
2024-11-05 03:39:55,243 - INFO - [diffusion][Epoch 10789] diffusion learning rate: 0.001
2024-11-05 03:39:55,244 - INFO - [diffusion][Epoch 10789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:55,246 - INFO - [diffusion][Epoch 10790] Epoch 10791/12000
2024-11-05 03:39:59,599 - INFO - [diffusion][Epoch 10790] diffusion training Loss: 0.06266802921891212
2024-11-05 03:39:59,600 - INFO - [diffusion][Epoch 10790] diffusion learning rate: 0.001
2024-11-05 03:39:59,602 - INFO - [diffusion][Epoch 10790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:59,604 - INFO - [diffusion][Epoch 10791] Epoch 10792/12000
2024-11-05 03:40:03,839 - INFO - [diffusion][Epoch 10791] diffusion training Loss: 0.055816843174397945
2024-11-05 03:40:03,841 - INFO - [diffusion][Epoch 10791] diffusion learning rate: 0.001
2024-11-05 03:40:03,843 - INFO - [diffusion][Epoch 10791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:03,844 - INFO - [diffusion][Epoch 10792] Epoch 10793/12000
2024-11-05 03:40:08,064 - INFO - [diffusion][Epoch 10792] diffusion training Loss: 0.055614614859223366
2024-11-05 03:40:08,066 - INFO - [diffusion][Epoch 10792] diffusion learning rate: 0.001
2024-11-05 03:40:08,068 - INFO - [diffusion][Epoch 10792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:08,069 - INFO - [diffusion][Epoch 10793] Epoch 10794/12000
2024-11-05 03:40:12,229 - INFO - [diffusion][Epoch 10793] diffusion training Loss: 0.05969604104757309
2024-11-05 03:40:12,231 - INFO - [diffusion][Epoch 10793] diffusion learning rate: 0.001
2024-11-05 03:40:12,233 - INFO - [diffusion][Epoch 10793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:12,234 - INFO - [diffusion][Epoch 10794] Epoch 10795/12000
2024-11-05 03:40:16,403 - INFO - [diffusion][Epoch 10794] diffusion training Loss: 0.055081442929804325
2024-11-05 03:40:16,406 - INFO - [diffusion][Epoch 10794] diffusion learning rate: 0.001
2024-11-05 03:40:16,432 - INFO - [diffusion][Epoch 10794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:16,434 - INFO - [diffusion][Epoch 10795] Epoch 10796/12000
2024-11-05 03:40:20,266 - INFO - [diffusion][Epoch 10795] diffusion training Loss: 0.05943072959780693
2024-11-05 03:40:20,268 - INFO - [diffusion][Epoch 10795] diffusion learning rate: 0.001
2024-11-05 03:40:20,270 - INFO - [diffusion][Epoch 10795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:20,271 - INFO - [diffusion][Epoch 10796] Epoch 10797/12000
2024-11-05 03:40:24,412 - INFO - [diffusion][Epoch 10796] diffusion training Loss: 0.06346988584846258
2024-11-05 03:40:24,414 - INFO - [diffusion][Epoch 10796] diffusion learning rate: 0.001
2024-11-05 03:40:24,416 - INFO - [diffusion][Epoch 10796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:24,418 - INFO - [diffusion][Epoch 10797] Epoch 10798/12000
2024-11-05 03:40:28,546 - INFO - [diffusion][Epoch 10797] diffusion training Loss: 0.06381389498710632
2024-11-05 03:40:28,548 - INFO - [diffusion][Epoch 10797] diffusion learning rate: 0.001
2024-11-05 03:40:28,550 - INFO - [diffusion][Epoch 10797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:28,551 - INFO - [diffusion][Epoch 10798] Epoch 10799/12000
2024-11-05 03:40:33,466 - INFO - [diffusion][Epoch 10798] diffusion training Loss: 0.056711251847445965
2024-11-05 03:40:33,468 - INFO - [diffusion][Epoch 10798] diffusion learning rate: 0.001
2024-11-05 03:40:33,470 - INFO - [diffusion][Epoch 10798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:33,471 - INFO - [diffusion][Epoch 10799] Epoch 10800/12000
2024-11-05 03:40:37,686 - INFO - [diffusion][Epoch 10799] diffusion training Loss: 0.06265829689800739
2024-11-05 03:40:37,688 - INFO - [diffusion][Epoch 10799] diffusion learning rate: 0.001
2024-11-05 03:40:37,691 - INFO - [diffusion][Epoch 10799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:37,692 - INFO - [diffusion][Epoch 10800] Epoch 10801/12000
2024-11-05 03:40:41,745 - INFO - [diffusion][Epoch 10800] diffusion training Loss: 0.05574960354715586
2024-11-05 03:40:41,748 - INFO - [diffusion][Epoch 10800] diffusion learning rate: 0.001
2024-11-05 03:40:41,750 - INFO - [diffusion][Epoch 10800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:41,751 - INFO - [diffusion][Epoch 10801] Epoch 10802/12000
2024-11-05 03:40:45,938 - INFO - [diffusion][Epoch 10801] diffusion training Loss: 0.059084272012114525
2024-11-05 03:40:45,941 - INFO - [diffusion][Epoch 10801] diffusion learning rate: 0.001
2024-11-05 03:40:45,943 - INFO - [diffusion][Epoch 10801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:45,945 - INFO - [diffusion][Epoch 10802] Epoch 10803/12000
2024-11-05 03:40:50,144 - INFO - [diffusion][Epoch 10802] diffusion training Loss: 0.06054532062262297
2024-11-05 03:40:50,403 - INFO - [diffusion][Epoch 10802] diffusion learning rate: 0.001
2024-11-05 03:40:50,405 - INFO - [diffusion][Epoch 10802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:50,406 - INFO - [diffusion][Epoch 10803] Epoch 10804/12000
2024-11-05 03:40:54,714 - INFO - [diffusion][Epoch 10803] diffusion training Loss: 0.060859037563204765
2024-11-05 03:40:54,716 - INFO - [diffusion][Epoch 10803] diffusion learning rate: 0.001
2024-11-05 03:40:54,718 - INFO - [diffusion][Epoch 10803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:54,719 - INFO - [diffusion][Epoch 10804] Epoch 10805/12000
2024-11-05 03:40:58,922 - INFO - [diffusion][Epoch 10804] diffusion training Loss: 0.06025847978889942
2024-11-05 03:40:58,924 - INFO - [diffusion][Epoch 10804] diffusion learning rate: 0.001
2024-11-05 03:40:58,926 - INFO - [diffusion][Epoch 10804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:58,927 - INFO - [diffusion][Epoch 10805] Epoch 10806/12000
2024-11-05 03:41:03,082 - INFO - [diffusion][Epoch 10805] diffusion training Loss: 0.06461486127227545
2024-11-05 03:41:03,084 - INFO - [diffusion][Epoch 10805] diffusion learning rate: 0.001
2024-11-05 03:41:03,086 - INFO - [diffusion][Epoch 10805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:03,087 - INFO - [diffusion][Epoch 10806] Epoch 10807/12000
2024-11-05 03:41:07,228 - INFO - [diffusion][Epoch 10806] diffusion training Loss: 0.060279592871665955
2024-11-05 03:41:07,231 - INFO - [diffusion][Epoch 10806] diffusion learning rate: 0.001
2024-11-05 03:41:07,233 - INFO - [diffusion][Epoch 10806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:07,234 - INFO - [diffusion][Epoch 10807] Epoch 10808/12000
2024-11-05 03:41:11,568 - INFO - [diffusion][Epoch 10807] diffusion training Loss: 0.06474286410957575
2024-11-05 03:41:11,570 - INFO - [diffusion][Epoch 10807] diffusion learning rate: 0.001
2024-11-05 03:41:11,572 - INFO - [diffusion][Epoch 10807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:11,574 - INFO - [diffusion][Epoch 10808] Epoch 10809/12000
2024-11-05 03:41:15,543 - INFO - [diffusion][Epoch 10808] diffusion training Loss: 0.06274553388357162
2024-11-05 03:41:15,546 - INFO - [diffusion][Epoch 10808] diffusion learning rate: 0.001
2024-11-05 03:41:15,548 - INFO - [diffusion][Epoch 10808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:15,549 - INFO - [diffusion][Epoch 10809] Epoch 10810/12000
2024-11-05 03:41:19,743 - INFO - [diffusion][Epoch 10809] diffusion training Loss: 0.059414093382656574
2024-11-05 03:41:19,745 - INFO - [diffusion][Epoch 10809] diffusion learning rate: 0.001
2024-11-05 03:41:19,747 - INFO - [diffusion][Epoch 10809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:19,748 - INFO - [diffusion][Epoch 10810] Epoch 10811/12000
2024-11-05 03:41:23,910 - INFO - [diffusion][Epoch 10810] diffusion training Loss: 0.0575704975053668
2024-11-05 03:41:23,913 - INFO - [diffusion][Epoch 10810] diffusion learning rate: 0.001
2024-11-05 03:41:23,914 - INFO - [diffusion][Epoch 10810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:23,916 - INFO - [diffusion][Epoch 10811] Epoch 10812/12000
2024-11-05 03:41:28,197 - INFO - [diffusion][Epoch 10811] diffusion training Loss: 0.05777508579194546
2024-11-05 03:41:28,199 - INFO - [diffusion][Epoch 10811] diffusion learning rate: 0.001
2024-11-05 03:41:28,201 - INFO - [diffusion][Epoch 10811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:28,203 - INFO - [diffusion][Epoch 10812] Epoch 10813/12000
2024-11-05 03:41:32,239 - INFO - [diffusion][Epoch 10812] diffusion training Loss: 0.060176349245011806
2024-11-05 03:41:32,241 - INFO - [diffusion][Epoch 10812] diffusion learning rate: 0.001
2024-11-05 03:41:32,243 - INFO - [diffusion][Epoch 10812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:32,244 - INFO - [diffusion][Epoch 10813] Epoch 10814/12000
2024-11-05 03:41:36,322 - INFO - [diffusion][Epoch 10813] diffusion training Loss: 0.06160203740000725
2024-11-05 03:41:36,324 - INFO - [diffusion][Epoch 10813] diffusion learning rate: 0.001
2024-11-05 03:41:36,325 - INFO - [diffusion][Epoch 10813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:36,327 - INFO - [diffusion][Epoch 10814] Epoch 10815/12000
2024-11-05 03:41:40,535 - INFO - [diffusion][Epoch 10814] diffusion training Loss: 0.06328168511390686
2024-11-05 03:41:40,537 - INFO - [diffusion][Epoch 10814] diffusion learning rate: 0.001
2024-11-05 03:41:40,538 - INFO - [diffusion][Epoch 10814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:40,540 - INFO - [diffusion][Epoch 10815] Epoch 10816/12000
2024-11-05 03:41:44,768 - INFO - [diffusion][Epoch 10815] diffusion training Loss: 0.06063562538474798
2024-11-05 03:41:44,770 - INFO - [diffusion][Epoch 10815] diffusion learning rate: 0.001
2024-11-05 03:41:44,772 - INFO - [diffusion][Epoch 10815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:44,773 - INFO - [diffusion][Epoch 10816] Epoch 10817/12000
2024-11-05 03:41:48,995 - INFO - [diffusion][Epoch 10816] diffusion training Loss: 0.05558037757873535
2024-11-05 03:41:48,996 - INFO - [diffusion][Epoch 10816] diffusion learning rate: 0.001
2024-11-05 03:41:48,998 - INFO - [diffusion][Epoch 10816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:49,000 - INFO - [diffusion][Epoch 10817] Epoch 10818/12000
2024-11-05 03:41:53,198 - INFO - [diffusion][Epoch 10817] diffusion training Loss: 0.0564431669190526
2024-11-05 03:41:53,200 - INFO - [diffusion][Epoch 10817] diffusion learning rate: 0.001
2024-11-05 03:41:53,202 - INFO - [diffusion][Epoch 10817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:53,203 - INFO - [diffusion][Epoch 10818] Epoch 10819/12000
2024-11-05 03:41:57,775 - INFO - [diffusion][Epoch 10818] diffusion training Loss: 0.05967479944229126
2024-11-05 03:41:57,778 - INFO - [diffusion][Epoch 10818] diffusion learning rate: 0.001
2024-11-05 03:41:57,779 - INFO - [diffusion][Epoch 10818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:57,781 - INFO - [diffusion][Epoch 10819] Epoch 10820/12000
2024-11-05 03:42:01,844 - INFO - [diffusion][Epoch 10819] diffusion training Loss: 0.059087635949254036
2024-11-05 03:42:01,846 - INFO - [diffusion][Epoch 10819] diffusion learning rate: 0.001
2024-11-05 03:42:01,848 - INFO - [diffusion][Epoch 10819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:01,849 - INFO - [diffusion][Epoch 10820] Epoch 10821/12000
2024-11-05 03:42:05,986 - INFO - [diffusion][Epoch 10820] diffusion training Loss: 0.06168304476886988
2024-11-05 03:42:05,988 - INFO - [diffusion][Epoch 10820] diffusion learning rate: 0.001
2024-11-05 03:42:05,990 - INFO - [diffusion][Epoch 10820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:05,992 - INFO - [diffusion][Epoch 10821] Epoch 10822/12000
2024-11-05 03:42:10,061 - INFO - [diffusion][Epoch 10821] diffusion training Loss: 0.06353306863456964
2024-11-05 03:42:10,063 - INFO - [diffusion][Epoch 10821] diffusion learning rate: 0.001
2024-11-05 03:42:10,065 - INFO - [diffusion][Epoch 10821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:10,066 - INFO - [diffusion][Epoch 10822] Epoch 10823/12000
2024-11-05 03:42:14,211 - INFO - [diffusion][Epoch 10822] diffusion training Loss: 0.0580728929489851
2024-11-05 03:42:14,213 - INFO - [diffusion][Epoch 10822] diffusion learning rate: 0.001
2024-11-05 03:42:14,215 - INFO - [diffusion][Epoch 10822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:14,216 - INFO - [diffusion][Epoch 10823] Epoch 10824/12000
2024-11-05 03:42:18,315 - INFO - [diffusion][Epoch 10823] diffusion training Loss: 0.05656903516501188
2024-11-05 03:42:18,318 - INFO - [diffusion][Epoch 10823] diffusion learning rate: 0.001
2024-11-05 03:42:18,319 - INFO - [diffusion][Epoch 10823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:18,320 - INFO - [diffusion][Epoch 10824] Epoch 10825/12000
2024-11-05 03:42:22,364 - INFO - [diffusion][Epoch 10824] diffusion training Loss: 0.05352789722383022
2024-11-05 03:42:22,366 - INFO - [diffusion][Epoch 10824] diffusion learning rate: 0.001
2024-11-05 03:42:22,368 - INFO - [diffusion][Epoch 10824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:22,369 - INFO - [diffusion][Epoch 10825] Epoch 10826/12000
2024-11-05 03:42:26,411 - INFO - [diffusion][Epoch 10825] diffusion training Loss: 0.06285354495048523
2024-11-05 03:42:26,413 - INFO - [diffusion][Epoch 10825] diffusion learning rate: 0.001
2024-11-05 03:42:26,415 - INFO - [diffusion][Epoch 10825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:26,416 - INFO - [diffusion][Epoch 10826] Epoch 10827/12000
2024-11-05 03:42:30,488 - INFO - [diffusion][Epoch 10826] diffusion training Loss: 0.0652210284024477
2024-11-05 03:42:30,491 - INFO - [diffusion][Epoch 10826] diffusion learning rate: 0.001
2024-11-05 03:42:30,494 - INFO - [diffusion][Epoch 10826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:30,495 - INFO - [diffusion][Epoch 10827] Epoch 10828/12000
2024-11-05 03:42:34,724 - INFO - [diffusion][Epoch 10827] diffusion training Loss: 0.05773618258535862
2024-11-05 03:42:34,726 - INFO - [diffusion][Epoch 10827] diffusion learning rate: 0.001
2024-11-05 03:42:34,728 - INFO - [diffusion][Epoch 10827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:34,729 - INFO - [diffusion][Epoch 10828] Epoch 10829/12000
2024-11-05 03:42:38,760 - INFO - [diffusion][Epoch 10828] diffusion training Loss: 0.05673843715339899
2024-11-05 03:42:38,762 - INFO - [diffusion][Epoch 10828] diffusion learning rate: 0.001
2024-11-05 03:42:38,764 - INFO - [diffusion][Epoch 10828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:38,765 - INFO - [diffusion][Epoch 10829] Epoch 10830/12000
2024-11-05 03:42:42,787 - INFO - [diffusion][Epoch 10829] diffusion training Loss: 0.056214723736047745
2024-11-05 03:42:42,789 - INFO - [diffusion][Epoch 10829] diffusion learning rate: 0.001
2024-11-05 03:42:42,792 - INFO - [diffusion][Epoch 10829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:42,793 - INFO - [diffusion][Epoch 10830] Epoch 10831/12000
2024-11-05 03:42:46,849 - INFO - [diffusion][Epoch 10830] diffusion training Loss: 0.05348581448197365
2024-11-05 03:42:46,851 - INFO - [diffusion][Epoch 10830] diffusion learning rate: 0.001
2024-11-05 03:42:46,853 - INFO - [diffusion][Epoch 10830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:46,854 - INFO - [diffusion][Epoch 10831] Epoch 10832/12000
2024-11-05 03:42:51,088 - INFO - [diffusion][Epoch 10831] diffusion training Loss: 0.05967085622251034
2024-11-05 03:42:51,090 - INFO - [diffusion][Epoch 10831] diffusion learning rate: 0.001
2024-11-05 03:42:51,092 - INFO - [diffusion][Epoch 10831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:51,093 - INFO - [diffusion][Epoch 10832] Epoch 10833/12000
2024-11-05 03:42:55,206 - INFO - [diffusion][Epoch 10832] diffusion training Loss: 0.0639286283403635
2024-11-05 03:42:55,209 - INFO - [diffusion][Epoch 10832] diffusion learning rate: 0.001
2024-11-05 03:42:55,210 - INFO - [diffusion][Epoch 10832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:55,212 - INFO - [diffusion][Epoch 10833] Epoch 10834/12000
2024-11-05 03:42:59,229 - INFO - [diffusion][Epoch 10833] diffusion training Loss: 0.0554653350263834
2024-11-05 03:42:59,232 - INFO - [diffusion][Epoch 10833] diffusion learning rate: 0.001
2024-11-05 03:42:59,234 - INFO - [diffusion][Epoch 10833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:59,236 - INFO - [diffusion][Epoch 10834] Epoch 10835/12000
2024-11-05 03:43:03,350 - INFO - [diffusion][Epoch 10834] diffusion training Loss: 0.0615856796503067
2024-11-05 03:43:03,352 - INFO - [diffusion][Epoch 10834] diffusion learning rate: 0.001
2024-11-05 03:43:03,354 - INFO - [diffusion][Epoch 10834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:03,355 - INFO - [diffusion][Epoch 10835] Epoch 10836/12000
2024-11-05 03:43:07,536 - INFO - [diffusion][Epoch 10835] diffusion training Loss: 0.05874189734458923
2024-11-05 03:43:07,538 - INFO - [diffusion][Epoch 10835] diffusion learning rate: 0.001
2024-11-05 03:43:07,540 - INFO - [diffusion][Epoch 10835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:07,541 - INFO - [diffusion][Epoch 10836] Epoch 10837/12000
2024-11-05 03:43:11,596 - INFO - [diffusion][Epoch 10836] diffusion training Loss: 0.06121974717825651
2024-11-05 03:43:11,598 - INFO - [diffusion][Epoch 10836] diffusion learning rate: 0.001
2024-11-05 03:43:11,601 - INFO - [diffusion][Epoch 10836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:11,602 - INFO - [diffusion][Epoch 10837] Epoch 10838/12000
2024-11-05 03:43:15,699 - INFO - [diffusion][Epoch 10837] diffusion training Loss: 0.057768591679632664
2024-11-05 03:43:15,701 - INFO - [diffusion][Epoch 10837] diffusion learning rate: 0.001
2024-11-05 03:43:15,703 - INFO - [diffusion][Epoch 10837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:15,705 - INFO - [diffusion][Epoch 10838] Epoch 10839/12000
2024-11-05 03:43:20,205 - INFO - [diffusion][Epoch 10838] diffusion training Loss: 0.06077202595770359
2024-11-05 03:43:20,207 - INFO - [diffusion][Epoch 10838] diffusion learning rate: 0.001
2024-11-05 03:43:20,208 - INFO - [diffusion][Epoch 10838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:20,210 - INFO - [diffusion][Epoch 10839] Epoch 10840/12000
2024-11-05 03:43:24,522 - INFO - [diffusion][Epoch 10839] diffusion training Loss: 0.06103574391454458
2024-11-05 03:43:24,525 - INFO - [diffusion][Epoch 10839] diffusion learning rate: 0.001
2024-11-05 03:43:24,526 - INFO - [diffusion][Epoch 10839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:24,528 - INFO - [diffusion][Epoch 10840] Epoch 10841/12000
2024-11-05 03:43:28,638 - INFO - [diffusion][Epoch 10840] diffusion training Loss: 0.06106858793646097
2024-11-05 03:43:28,641 - INFO - [diffusion][Epoch 10840] diffusion learning rate: 0.001
2024-11-05 03:43:28,642 - INFO - [diffusion][Epoch 10840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:28,644 - INFO - [diffusion][Epoch 10841] Epoch 10842/12000
2024-11-05 03:43:32,955 - INFO - [diffusion][Epoch 10841] diffusion training Loss: 0.056046824902296066
2024-11-05 03:43:32,958 - INFO - [diffusion][Epoch 10841] diffusion learning rate: 0.001
2024-11-05 03:43:32,959 - INFO - [diffusion][Epoch 10841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:32,961 - INFO - [diffusion][Epoch 10842] Epoch 10843/12000
2024-11-05 03:43:37,143 - INFO - [diffusion][Epoch 10842] diffusion training Loss: 0.06057028379291296
2024-11-05 03:43:37,146 - INFO - [diffusion][Epoch 10842] diffusion learning rate: 0.001
2024-11-05 03:43:37,148 - INFO - [diffusion][Epoch 10842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:37,150 - INFO - [diffusion][Epoch 10843] Epoch 10844/12000
2024-11-05 03:43:41,260 - INFO - [diffusion][Epoch 10843] diffusion training Loss: 0.058337483555078506
2024-11-05 03:43:41,262 - INFO - [diffusion][Epoch 10843] diffusion learning rate: 0.001
2024-11-05 03:43:41,264 - INFO - [diffusion][Epoch 10843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:41,265 - INFO - [diffusion][Epoch 10844] Epoch 10845/12000
2024-11-05 03:43:45,387 - INFO - [diffusion][Epoch 10844] diffusion training Loss: 0.06331976782530546
2024-11-05 03:43:45,389 - INFO - [diffusion][Epoch 10844] diffusion learning rate: 0.001
2024-11-05 03:43:45,391 - INFO - [diffusion][Epoch 10844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:45,392 - INFO - [diffusion][Epoch 10845] Epoch 10846/12000
2024-11-05 03:43:49,331 - INFO - [diffusion][Epoch 10845] diffusion training Loss: 0.05234950874000788
2024-11-05 03:43:49,333 - INFO - [diffusion][Epoch 10845] diffusion learning rate: 0.001
2024-11-05 03:43:49,335 - INFO - [diffusion][Epoch 10845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:49,336 - INFO - [diffusion][Epoch 10846] Epoch 10847/12000
2024-11-05 03:43:53,551 - INFO - [diffusion][Epoch 10846] diffusion training Loss: 0.06084597855806351
2024-11-05 03:43:53,553 - INFO - [diffusion][Epoch 10846] diffusion learning rate: 0.001
2024-11-05 03:43:53,555 - INFO - [diffusion][Epoch 10846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:53,556 - INFO - [diffusion][Epoch 10847] Epoch 10848/12000
2024-11-05 03:43:57,841 - INFO - [diffusion][Epoch 10847] diffusion training Loss: 0.061539165675640106
2024-11-05 03:43:57,844 - INFO - [diffusion][Epoch 10847] diffusion learning rate: 0.001
2024-11-05 03:43:57,848 - INFO - [diffusion][Epoch 10847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:57,850 - INFO - [diffusion][Epoch 10848] Epoch 10849/12000
2024-11-05 03:44:02,019 - INFO - [diffusion][Epoch 10848] diffusion training Loss: 0.056349169462919235
2024-11-05 03:44:02,041 - INFO - [diffusion][Epoch 10848] diffusion learning rate: 0.001
2024-11-05 03:44:02,043 - INFO - [diffusion][Epoch 10848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:02,045 - INFO - [diffusion][Epoch 10849] Epoch 10850/12000
2024-11-05 03:44:06,117 - INFO - [diffusion][Epoch 10849] diffusion training Loss: 0.06118071544915438
2024-11-05 03:44:06,119 - INFO - [diffusion][Epoch 10849] diffusion learning rate: 0.001
2024-11-05 03:44:06,121 - INFO - [diffusion][Epoch 10849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:06,123 - INFO - [diffusion][Epoch 10850] Epoch 10851/12000
2024-11-05 03:44:10,124 - INFO - [diffusion][Epoch 10850] diffusion training Loss: 0.05936255119740963
2024-11-05 03:44:10,126 - INFO - [diffusion][Epoch 10850] diffusion learning rate: 0.001
2024-11-05 03:44:10,128 - INFO - [diffusion][Epoch 10850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:10,129 - INFO - [diffusion][Epoch 10851] Epoch 10852/12000
2024-11-05 03:44:14,386 - INFO - [diffusion][Epoch 10851] diffusion training Loss: 0.05833468306809664
2024-11-05 03:44:14,388 - INFO - [diffusion][Epoch 10851] diffusion learning rate: 0.001
2024-11-05 03:44:14,390 - INFO - [diffusion][Epoch 10851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:14,391 - INFO - [diffusion][Epoch 10852] Epoch 10853/12000
2024-11-05 03:44:18,507 - INFO - [diffusion][Epoch 10852] diffusion training Loss: 0.057745032012462616
2024-11-05 03:44:18,509 - INFO - [diffusion][Epoch 10852] diffusion learning rate: 0.001
2024-11-05 03:44:18,559 - INFO - [diffusion][Epoch 10852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:18,560 - INFO - [diffusion][Epoch 10853] Epoch 10854/12000
2024-11-05 03:44:22,566 - INFO - [diffusion][Epoch 10853] diffusion training Loss: 0.05896058492362499
2024-11-05 03:44:22,568 - INFO - [diffusion][Epoch 10853] diffusion learning rate: 0.001
2024-11-05 03:44:22,570 - INFO - [diffusion][Epoch 10853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:22,571 - INFO - [diffusion][Epoch 10854] Epoch 10855/12000
2024-11-05 03:44:26,626 - INFO - [diffusion][Epoch 10854] diffusion training Loss: 0.0612719152122736
2024-11-05 03:44:26,628 - INFO - [diffusion][Epoch 10854] diffusion learning rate: 0.001
2024-11-05 03:44:26,629 - INFO - [diffusion][Epoch 10854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:26,631 - INFO - [diffusion][Epoch 10855] Epoch 10856/12000
2024-11-05 03:44:30,759 - INFO - [diffusion][Epoch 10855] diffusion training Loss: 0.05655238218605518
2024-11-05 03:44:30,761 - INFO - [diffusion][Epoch 10855] diffusion learning rate: 0.001
2024-11-05 03:44:30,763 - INFO - [diffusion][Epoch 10855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:30,764 - INFO - [diffusion][Epoch 10856] Epoch 10857/12000
2024-11-05 03:44:34,987 - INFO - [diffusion][Epoch 10856] diffusion training Loss: 0.058647590689361095
2024-11-05 03:44:34,989 - INFO - [diffusion][Epoch 10856] diffusion learning rate: 0.001
2024-11-05 03:44:35,030 - INFO - [diffusion][Epoch 10856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:35,031 - INFO - [diffusion][Epoch 10857] Epoch 10858/12000
2024-11-05 03:44:39,088 - INFO - [diffusion][Epoch 10857] diffusion training Loss: 0.0601076427847147
2024-11-05 03:44:39,091 - INFO - [diffusion][Epoch 10857] diffusion learning rate: 0.001
2024-11-05 03:44:39,093 - INFO - [diffusion][Epoch 10857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:39,094 - INFO - [diffusion][Epoch 10858] Epoch 10859/12000
2024-11-05 03:44:43,797 - INFO - [diffusion][Epoch 10858] diffusion training Loss: 0.06089073605835438
2024-11-05 03:44:43,799 - INFO - [diffusion][Epoch 10858] diffusion learning rate: 0.001
2024-11-05 03:44:43,800 - INFO - [diffusion][Epoch 10858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:43,802 - INFO - [diffusion][Epoch 10859] Epoch 10860/12000
2024-11-05 03:44:47,875 - INFO - [diffusion][Epoch 10859] diffusion training Loss: 0.06018070783466101
2024-11-05 03:44:47,877 - INFO - [diffusion][Epoch 10859] diffusion learning rate: 0.001
2024-11-05 03:44:47,879 - INFO - [diffusion][Epoch 10859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:47,880 - INFO - [diffusion][Epoch 10860] Epoch 10861/12000
2024-11-05 03:44:52,066 - INFO - [diffusion][Epoch 10860] diffusion training Loss: 0.05836099945008755
2024-11-05 03:44:52,068 - INFO - [diffusion][Epoch 10860] diffusion learning rate: 0.001
2024-11-05 03:44:52,070 - INFO - [diffusion][Epoch 10860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:52,072 - INFO - [diffusion][Epoch 10861] Epoch 10862/12000
2024-11-05 03:44:56,028 - INFO - [diffusion][Epoch 10861] diffusion training Loss: 0.056182993575930595
2024-11-05 03:44:56,030 - INFO - [diffusion][Epoch 10861] diffusion learning rate: 0.001
2024-11-05 03:44:56,032 - INFO - [diffusion][Epoch 10861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:56,033 - INFO - [diffusion][Epoch 10862] Epoch 10863/12000
2024-11-05 03:45:00,295 - INFO - [diffusion][Epoch 10862] diffusion training Loss: 0.055153447203338146
2024-11-05 03:45:00,298 - INFO - [diffusion][Epoch 10862] diffusion learning rate: 0.001
2024-11-05 03:45:00,300 - INFO - [diffusion][Epoch 10862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:00,301 - INFO - [diffusion][Epoch 10863] Epoch 10864/12000
2024-11-05 03:45:04,576 - INFO - [diffusion][Epoch 10863] diffusion training Loss: 0.06275273580104113
2024-11-05 03:45:04,578 - INFO - [diffusion][Epoch 10863] diffusion learning rate: 0.001
2024-11-05 03:45:04,580 - INFO - [diffusion][Epoch 10863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:04,581 - INFO - [diffusion][Epoch 10864] Epoch 10865/12000
2024-11-05 03:45:08,808 - INFO - [diffusion][Epoch 10864] diffusion training Loss: 0.059701425954699516
2024-11-05 03:45:08,810 - INFO - [diffusion][Epoch 10864] diffusion learning rate: 0.001
2024-11-05 03:45:08,812 - INFO - [diffusion][Epoch 10864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:08,813 - INFO - [diffusion][Epoch 10865] Epoch 10866/12000
2024-11-05 03:45:12,899 - INFO - [diffusion][Epoch 10865] diffusion training Loss: 0.058931415900588036
2024-11-05 03:45:12,901 - INFO - [diffusion][Epoch 10865] diffusion learning rate: 0.001
2024-11-05 03:45:12,904 - INFO - [diffusion][Epoch 10865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:12,905 - INFO - [diffusion][Epoch 10866] Epoch 10867/12000
2024-11-05 03:45:17,016 - INFO - [diffusion][Epoch 10866] diffusion training Loss: 0.06137951277196407
2024-11-05 03:45:17,018 - INFO - [diffusion][Epoch 10866] diffusion learning rate: 0.001
2024-11-05 03:45:17,020 - INFO - [diffusion][Epoch 10866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:17,021 - INFO - [diffusion][Epoch 10867] Epoch 10868/12000
2024-11-05 03:45:21,318 - INFO - [diffusion][Epoch 10867] diffusion training Loss: 0.056278216652572155
2024-11-05 03:45:21,320 - INFO - [diffusion][Epoch 10867] diffusion learning rate: 0.001
2024-11-05 03:45:21,322 - INFO - [diffusion][Epoch 10867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:21,323 - INFO - [diffusion][Epoch 10868] Epoch 10869/12000
2024-11-05 03:45:25,561 - INFO - [diffusion][Epoch 10868] diffusion training Loss: 0.06344729848206043
2024-11-05 03:45:25,563 - INFO - [diffusion][Epoch 10868] diffusion learning rate: 0.001
2024-11-05 03:45:25,565 - INFO - [diffusion][Epoch 10868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:25,567 - INFO - [diffusion][Epoch 10869] Epoch 10870/12000
2024-11-05 03:45:29,890 - INFO - [diffusion][Epoch 10869] diffusion training Loss: 0.06421940680593252
2024-11-05 03:45:29,893 - INFO - [diffusion][Epoch 10869] diffusion learning rate: 0.001
2024-11-05 03:45:29,921 - INFO - [diffusion][Epoch 10869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:29,922 - INFO - [diffusion][Epoch 10870] Epoch 10871/12000
2024-11-05 03:45:34,168 - INFO - [diffusion][Epoch 10870] diffusion training Loss: 0.055513360537588596
2024-11-05 03:45:34,171 - INFO - [diffusion][Epoch 10870] diffusion learning rate: 0.001
2024-11-05 03:45:34,173 - INFO - [diffusion][Epoch 10870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:34,174 - INFO - [diffusion][Epoch 10871] Epoch 10872/12000
2024-11-05 03:45:38,308 - INFO - [diffusion][Epoch 10871] diffusion training Loss: 0.05921321175992489
2024-11-05 03:45:38,310 - INFO - [diffusion][Epoch 10871] diffusion learning rate: 0.001
2024-11-05 03:45:38,312 - INFO - [diffusion][Epoch 10871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:38,314 - INFO - [diffusion][Epoch 10872] Epoch 10873/12000
2024-11-05 03:45:42,466 - INFO - [diffusion][Epoch 10872] diffusion training Loss: 0.05692620947957039
2024-11-05 03:45:42,468 - INFO - [diffusion][Epoch 10872] diffusion learning rate: 0.001
2024-11-05 03:45:42,470 - INFO - [diffusion][Epoch 10872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:42,471 - INFO - [diffusion][Epoch 10873] Epoch 10874/12000
2024-11-05 03:45:46,673 - INFO - [diffusion][Epoch 10873] diffusion training Loss: 0.058422764763236046
2024-11-05 03:45:46,676 - INFO - [diffusion][Epoch 10873] diffusion learning rate: 0.001
2024-11-05 03:45:46,704 - INFO - [diffusion][Epoch 10873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:46,705 - INFO - [diffusion][Epoch 10874] Epoch 10875/12000
2024-11-05 03:45:50,891 - INFO - [diffusion][Epoch 10874] diffusion training Loss: 0.057357458397746086
2024-11-05 03:45:50,893 - INFO - [diffusion][Epoch 10874] diffusion learning rate: 0.001
2024-11-05 03:45:50,895 - INFO - [diffusion][Epoch 10874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:50,896 - INFO - [diffusion][Epoch 10875] Epoch 10876/12000
2024-11-05 03:45:55,021 - INFO - [diffusion][Epoch 10875] diffusion training Loss: 0.05990795139223337
2024-11-05 03:45:55,023 - INFO - [diffusion][Epoch 10875] diffusion learning rate: 0.001
2024-11-05 03:45:55,025 - INFO - [diffusion][Epoch 10875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:55,026 - INFO - [diffusion][Epoch 10876] Epoch 10877/12000
2024-11-05 03:45:59,074 - INFO - [diffusion][Epoch 10876] diffusion training Loss: 0.05629706662148237
2024-11-05 03:45:59,076 - INFO - [diffusion][Epoch 10876] diffusion learning rate: 0.001
2024-11-05 03:45:59,078 - INFO - [diffusion][Epoch 10876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:59,079 - INFO - [diffusion][Epoch 10877] Epoch 10878/12000
2024-11-05 03:46:02,978 - INFO - [diffusion][Epoch 10877] diffusion training Loss: 0.0590179692953825
2024-11-05 03:46:02,980 - INFO - [diffusion][Epoch 10877] diffusion learning rate: 0.001
2024-11-05 03:46:02,982 - INFO - [diffusion][Epoch 10877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:02,983 - INFO - [diffusion][Epoch 10878] Epoch 10879/12000
2024-11-05 03:46:07,094 - INFO - [diffusion][Epoch 10878] diffusion training Loss: 0.057040778920054436
2024-11-05 03:46:07,096 - INFO - [diffusion][Epoch 10878] diffusion learning rate: 0.001
2024-11-05 03:46:07,098 - INFO - [diffusion][Epoch 10878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:07,099 - INFO - [diffusion][Epoch 10879] Epoch 10880/12000
2024-11-05 03:46:11,194 - INFO - [diffusion][Epoch 10879] diffusion training Loss: 0.06293807830661535
2024-11-05 03:46:11,196 - INFO - [diffusion][Epoch 10879] diffusion learning rate: 0.001
2024-11-05 03:46:11,198 - INFO - [diffusion][Epoch 10879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:11,199 - INFO - [diffusion][Epoch 10880] Epoch 10881/12000
2024-11-05 03:46:15,282 - INFO - [diffusion][Epoch 10880] diffusion training Loss: 0.0649226950481534
2024-11-05 03:46:15,284 - INFO - [diffusion][Epoch 10880] diffusion learning rate: 0.001
2024-11-05 03:46:15,286 - INFO - [diffusion][Epoch 10880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:15,287 - INFO - [diffusion][Epoch 10881] Epoch 10882/12000
2024-11-05 03:46:19,459 - INFO - [diffusion][Epoch 10881] diffusion training Loss: 0.056795465759932995
2024-11-05 03:46:19,462 - INFO - [diffusion][Epoch 10881] diffusion learning rate: 0.001
2024-11-05 03:46:19,464 - INFO - [diffusion][Epoch 10881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:19,466 - INFO - [diffusion][Epoch 10882] Epoch 10883/12000
2024-11-05 03:46:23,515 - INFO - [diffusion][Epoch 10882] diffusion training Loss: 0.05612538754940033
2024-11-05 03:46:23,517 - INFO - [diffusion][Epoch 10882] diffusion learning rate: 0.001
2024-11-05 03:46:23,519 - INFO - [diffusion][Epoch 10882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:23,520 - INFO - [diffusion][Epoch 10883] Epoch 10884/12000
2024-11-05 03:46:27,706 - INFO - [diffusion][Epoch 10883] diffusion training Loss: 0.06044017057865858
2024-11-05 03:46:27,709 - INFO - [diffusion][Epoch 10883] diffusion learning rate: 0.001
2024-11-05 03:46:27,711 - INFO - [diffusion][Epoch 10883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:27,712 - INFO - [diffusion][Epoch 10884] Epoch 10885/12000
2024-11-05 03:46:31,758 - INFO - [diffusion][Epoch 10884] diffusion training Loss: 0.059125238098204136
2024-11-05 03:46:31,761 - INFO - [diffusion][Epoch 10884] diffusion learning rate: 0.001
2024-11-05 03:46:31,763 - INFO - [diffusion][Epoch 10884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:31,765 - INFO - [diffusion][Epoch 10885] Epoch 10886/12000
2024-11-05 03:46:35,824 - INFO - [diffusion][Epoch 10885] diffusion training Loss: 0.05411578994244337
2024-11-05 03:46:35,827 - INFO - [diffusion][Epoch 10885] diffusion learning rate: 0.001
2024-11-05 03:46:35,855 - INFO - [diffusion][Epoch 10885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:35,856 - INFO - [diffusion][Epoch 10886] Epoch 10887/12000
2024-11-05 03:46:39,957 - INFO - [diffusion][Epoch 10886] diffusion training Loss: 0.06070447247475386
2024-11-05 03:46:39,959 - INFO - [diffusion][Epoch 10886] diffusion learning rate: 0.001
2024-11-05 03:46:39,961 - INFO - [diffusion][Epoch 10886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:39,962 - INFO - [diffusion][Epoch 10887] Epoch 10888/12000
2024-11-05 03:46:44,121 - INFO - [diffusion][Epoch 10887] diffusion training Loss: 0.06003884691745043
2024-11-05 03:46:44,123 - INFO - [diffusion][Epoch 10887] diffusion learning rate: 0.001
2024-11-05 03:46:44,125 - INFO - [diffusion][Epoch 10887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:44,126 - INFO - [diffusion][Epoch 10888] Epoch 10889/12000
2024-11-05 03:46:48,365 - INFO - [diffusion][Epoch 10888] diffusion training Loss: 0.06016864441335201
2024-11-05 03:46:48,367 - INFO - [diffusion][Epoch 10888] diffusion learning rate: 0.001
2024-11-05 03:46:48,369 - INFO - [diffusion][Epoch 10888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:48,370 - INFO - [diffusion][Epoch 10889] Epoch 10890/12000
2024-11-05 03:46:52,468 - INFO - [diffusion][Epoch 10889] diffusion training Loss: 0.05812312103807926
2024-11-05 03:46:52,470 - INFO - [diffusion][Epoch 10889] diffusion learning rate: 0.001
2024-11-05 03:46:52,473 - INFO - [diffusion][Epoch 10889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:52,474 - INFO - [diffusion][Epoch 10890] Epoch 10891/12000
2024-11-05 03:46:56,778 - INFO - [diffusion][Epoch 10890] diffusion training Loss: 0.06348581425845623
2024-11-05 03:46:56,781 - INFO - [diffusion][Epoch 10890] diffusion learning rate: 0.001
2024-11-05 03:46:56,783 - INFO - [diffusion][Epoch 10890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:56,784 - INFO - [diffusion][Epoch 10891] Epoch 10892/12000
2024-11-05 03:47:00,983 - INFO - [diffusion][Epoch 10891] diffusion training Loss: 0.06000941712409258
2024-11-05 03:47:00,985 - INFO - [diffusion][Epoch 10891] diffusion learning rate: 0.001
2024-11-05 03:47:00,987 - INFO - [diffusion][Epoch 10891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:00,989 - INFO - [diffusion][Epoch 10892] Epoch 10893/12000
2024-11-05 03:47:04,971 - INFO - [diffusion][Epoch 10892] diffusion training Loss: 0.056791024282574654
2024-11-05 03:47:04,973 - INFO - [diffusion][Epoch 10892] diffusion learning rate: 0.001
2024-11-05 03:47:04,975 - INFO - [diffusion][Epoch 10892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:04,976 - INFO - [diffusion][Epoch 10893] Epoch 10894/12000
2024-11-05 03:47:09,100 - INFO - [diffusion][Epoch 10893] diffusion training Loss: 0.060953548178076744
2024-11-05 03:47:09,103 - INFO - [diffusion][Epoch 10893] diffusion learning rate: 0.001
2024-11-05 03:47:09,105 - INFO - [diffusion][Epoch 10893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:09,107 - INFO - [diffusion][Epoch 10894] Epoch 10895/12000
2024-11-05 03:47:13,289 - INFO - [diffusion][Epoch 10894] diffusion training Loss: 0.06155953370034695
2024-11-05 03:47:13,291 - INFO - [diffusion][Epoch 10894] diffusion learning rate: 0.001
2024-11-05 03:47:13,293 - INFO - [diffusion][Epoch 10894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:13,294 - INFO - [diffusion][Epoch 10895] Epoch 10896/12000
2024-11-05 03:47:17,314 - INFO - [diffusion][Epoch 10895] diffusion training Loss: 0.0591988367959857
2024-11-05 03:47:17,317 - INFO - [diffusion][Epoch 10895] diffusion learning rate: 0.001
2024-11-05 03:47:17,319 - INFO - [diffusion][Epoch 10895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:17,320 - INFO - [diffusion][Epoch 10896] Epoch 10897/12000
2024-11-05 03:47:21,450 - INFO - [diffusion][Epoch 10896] diffusion training Loss: 0.06036585941910744
2024-11-05 03:47:21,452 - INFO - [diffusion][Epoch 10896] diffusion learning rate: 0.001
2024-11-05 03:47:21,454 - INFO - [diffusion][Epoch 10896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:21,455 - INFO - [diffusion][Epoch 10897] Epoch 10898/12000
2024-11-05 03:47:25,524 - INFO - [diffusion][Epoch 10897] diffusion training Loss: 0.05728201661258936
2024-11-05 03:47:25,527 - INFO - [diffusion][Epoch 10897] diffusion learning rate: 0.001
2024-11-05 03:47:25,528 - INFO - [diffusion][Epoch 10897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:25,530 - INFO - [diffusion][Epoch 10898] Epoch 10899/12000
2024-11-05 03:47:29,651 - INFO - [diffusion][Epoch 10898] diffusion training Loss: 0.0613802382722497
2024-11-05 03:47:29,653 - INFO - [diffusion][Epoch 10898] diffusion learning rate: 0.001
2024-11-05 03:47:29,655 - INFO - [diffusion][Epoch 10898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:29,656 - INFO - [diffusion][Epoch 10899] Epoch 10900/12000
2024-11-05 03:47:33,842 - INFO - [diffusion][Epoch 10899] diffusion training Loss: 0.059148979373276234
2024-11-05 03:47:33,846 - INFO - [diffusion][Epoch 10899] diffusion learning rate: 0.001
2024-11-05 03:47:33,848 - INFO - [diffusion][Epoch 10899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:33,850 - INFO - [diffusion][Epoch 10900] Epoch 10901/12000
2024-11-05 03:47:38,015 - INFO - [diffusion][Epoch 10900] diffusion training Loss: 0.06393638532608747
2024-11-05 03:47:38,017 - INFO - [diffusion][Epoch 10900] diffusion learning rate: 0.001
2024-11-05 03:47:38,019 - INFO - [diffusion][Epoch 10900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:38,021 - INFO - [diffusion][Epoch 10901] Epoch 10902/12000
2024-11-05 03:47:42,075 - INFO - [diffusion][Epoch 10901] diffusion training Loss: 0.05847222916781902
2024-11-05 03:47:42,077 - INFO - [diffusion][Epoch 10901] diffusion learning rate: 0.001
2024-11-05 03:47:42,080 - INFO - [diffusion][Epoch 10901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:42,081 - INFO - [diffusion][Epoch 10902] Epoch 10903/12000
2024-11-05 03:47:46,331 - INFO - [diffusion][Epoch 10902] diffusion training Loss: 0.057264577597379684
2024-11-05 03:47:46,333 - INFO - [diffusion][Epoch 10902] diffusion learning rate: 0.001
2024-11-05 03:47:46,335 - INFO - [diffusion][Epoch 10902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:46,336 - INFO - [diffusion][Epoch 10903] Epoch 10904/12000
2024-11-05 03:47:50,545 - INFO - [diffusion][Epoch 10903] diffusion training Loss: 0.05840473249554634
2024-11-05 03:47:50,547 - INFO - [diffusion][Epoch 10903] diffusion learning rate: 0.001
2024-11-05 03:47:50,549 - INFO - [diffusion][Epoch 10903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:50,550 - INFO - [diffusion][Epoch 10904] Epoch 10905/12000
2024-11-05 03:47:54,587 - INFO - [diffusion][Epoch 10904] diffusion training Loss: 0.06285218894481659
2024-11-05 03:47:54,589 - INFO - [diffusion][Epoch 10904] diffusion learning rate: 0.001
2024-11-05 03:47:54,591 - INFO - [diffusion][Epoch 10904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:54,592 - INFO - [diffusion][Epoch 10905] Epoch 10906/12000
2024-11-05 03:47:58,805 - INFO - [diffusion][Epoch 10905] diffusion training Loss: 0.05744052026420832
2024-11-05 03:47:58,807 - INFO - [diffusion][Epoch 10905] diffusion learning rate: 0.001
2024-11-05 03:47:58,809 - INFO - [diffusion][Epoch 10905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:58,810 - INFO - [diffusion][Epoch 10906] Epoch 10907/12000
2024-11-05 03:48:03,032 - INFO - [diffusion][Epoch 10906] diffusion training Loss: 0.05962282605469227
2024-11-05 03:48:03,034 - INFO - [diffusion][Epoch 10906] diffusion learning rate: 0.001
2024-11-05 03:48:03,036 - INFO - [diffusion][Epoch 10906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:03,038 - INFO - [diffusion][Epoch 10907] Epoch 10908/12000
2024-11-05 03:48:07,146 - INFO - [diffusion][Epoch 10907] diffusion training Loss: 0.062341902405023575
2024-11-05 03:48:07,149 - INFO - [diffusion][Epoch 10907] diffusion learning rate: 0.001
2024-11-05 03:48:07,151 - INFO - [diffusion][Epoch 10907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:07,153 - INFO - [diffusion][Epoch 10908] Epoch 10909/12000
2024-11-05 03:48:11,329 - INFO - [diffusion][Epoch 10908] diffusion training Loss: 0.05628959368914366
2024-11-05 03:48:11,332 - INFO - [diffusion][Epoch 10908] diffusion learning rate: 0.001
2024-11-05 03:48:11,333 - INFO - [diffusion][Epoch 10908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:11,335 - INFO - [diffusion][Epoch 10909] Epoch 10910/12000
2024-11-05 03:48:15,491 - INFO - [diffusion][Epoch 10909] diffusion training Loss: 0.053924242965877056
2024-11-05 03:48:15,493 - INFO - [diffusion][Epoch 10909] diffusion learning rate: 0.001
2024-11-05 03:48:15,495 - INFO - [diffusion][Epoch 10909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:15,496 - INFO - [diffusion][Epoch 10910] Epoch 10911/12000
2024-11-05 03:48:19,593 - INFO - [diffusion][Epoch 10910] diffusion training Loss: 0.058451720513403416
2024-11-05 03:48:19,595 - INFO - [diffusion][Epoch 10910] diffusion learning rate: 0.001
2024-11-05 03:48:19,597 - INFO - [diffusion][Epoch 10910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:19,599 - INFO - [diffusion][Epoch 10911] Epoch 10912/12000
2024-11-05 03:48:23,890 - INFO - [diffusion][Epoch 10911] diffusion training Loss: 0.05903494078665972
2024-11-05 03:48:23,892 - INFO - [diffusion][Epoch 10911] diffusion learning rate: 0.001
2024-11-05 03:48:23,894 - INFO - [diffusion][Epoch 10911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:23,896 - INFO - [diffusion][Epoch 10912] Epoch 10913/12000
2024-11-05 03:48:27,893 - INFO - [diffusion][Epoch 10912] diffusion training Loss: 0.06074232514947653
2024-11-05 03:48:27,896 - INFO - [diffusion][Epoch 10912] diffusion learning rate: 0.001
2024-11-05 03:48:27,898 - INFO - [diffusion][Epoch 10912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:27,899 - INFO - [diffusion][Epoch 10913] Epoch 10914/12000
2024-11-05 03:48:31,996 - INFO - [diffusion][Epoch 10913] diffusion training Loss: 0.053641894832253456
2024-11-05 03:48:31,998 - INFO - [diffusion][Epoch 10913] diffusion learning rate: 0.001
2024-11-05 03:48:32,039 - INFO - [diffusion][Epoch 10913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:32,040 - INFO - [diffusion][Epoch 10914] Epoch 10915/12000
2024-11-05 03:48:36,216 - INFO - [diffusion][Epoch 10914] diffusion training Loss: 0.05563974194228649
2024-11-05 03:48:36,218 - INFO - [diffusion][Epoch 10914] diffusion learning rate: 0.001
2024-11-05 03:48:36,220 - INFO - [diffusion][Epoch 10914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:36,221 - INFO - [diffusion][Epoch 10915] Epoch 10916/12000
2024-11-05 03:48:40,429 - INFO - [diffusion][Epoch 10915] diffusion training Loss: 0.0629957728087902
2024-11-05 03:48:40,431 - INFO - [diffusion][Epoch 10915] diffusion learning rate: 0.001
2024-11-05 03:48:40,432 - INFO - [diffusion][Epoch 10915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:40,434 - INFO - [diffusion][Epoch 10916] Epoch 10917/12000
2024-11-05 03:48:44,629 - INFO - [diffusion][Epoch 10916] diffusion training Loss: 0.05951232463121414
2024-11-05 03:48:44,631 - INFO - [diffusion][Epoch 10916] diffusion learning rate: 0.001
2024-11-05 03:48:44,633 - INFO - [diffusion][Epoch 10916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:44,635 - INFO - [diffusion][Epoch 10917] Epoch 10918/12000
2024-11-05 03:48:48,771 - INFO - [diffusion][Epoch 10917] diffusion training Loss: 0.060648215003311634
2024-11-05 03:48:48,773 - INFO - [diffusion][Epoch 10917] diffusion learning rate: 0.001
2024-11-05 03:48:48,775 - INFO - [diffusion][Epoch 10917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:48,777 - INFO - [diffusion][Epoch 10918] Epoch 10919/12000
2024-11-05 03:48:52,749 - INFO - [diffusion][Epoch 10918] diffusion training Loss: 0.06249731872230768
2024-11-05 03:48:52,751 - INFO - [diffusion][Epoch 10918] diffusion learning rate: 0.001
2024-11-05 03:48:52,752 - INFO - [diffusion][Epoch 10918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:52,754 - INFO - [diffusion][Epoch 10919] Epoch 10920/12000
2024-11-05 03:48:56,908 - INFO - [diffusion][Epoch 10919] diffusion training Loss: 0.05926722940057516
2024-11-05 03:48:56,910 - INFO - [diffusion][Epoch 10919] diffusion learning rate: 0.001
2024-11-05 03:48:56,912 - INFO - [diffusion][Epoch 10919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:56,913 - INFO - [diffusion][Epoch 10920] Epoch 10921/12000
2024-11-05 03:49:01,745 - INFO - [diffusion][Epoch 10920] diffusion training Loss: 0.06555014848709106
2024-11-05 03:49:01,747 - INFO - [diffusion][Epoch 10920] diffusion learning rate: 0.001
2024-11-05 03:49:01,749 - INFO - [diffusion][Epoch 10920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:01,750 - INFO - [diffusion][Epoch 10921] Epoch 10922/12000
2024-11-05 03:49:05,853 - INFO - [diffusion][Epoch 10921] diffusion training Loss: 0.06223405059427023
2024-11-05 03:49:05,856 - INFO - [diffusion][Epoch 10921] diffusion learning rate: 0.001
2024-11-05 03:49:05,858 - INFO - [diffusion][Epoch 10921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:05,859 - INFO - [diffusion][Epoch 10922] Epoch 10923/12000
2024-11-05 03:49:10,002 - INFO - [diffusion][Epoch 10922] diffusion training Loss: 0.058312831446528435
2024-11-05 03:49:10,004 - INFO - [diffusion][Epoch 10922] diffusion learning rate: 0.001
2024-11-05 03:49:10,006 - INFO - [diffusion][Epoch 10922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:10,007 - INFO - [diffusion][Epoch 10923] Epoch 10924/12000
2024-11-05 03:49:14,283 - INFO - [diffusion][Epoch 10923] diffusion training Loss: 0.05455914791673422
2024-11-05 03:49:14,285 - INFO - [diffusion][Epoch 10923] diffusion learning rate: 0.001
2024-11-05 03:49:14,288 - INFO - [diffusion][Epoch 10923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:14,289 - INFO - [diffusion][Epoch 10924] Epoch 10925/12000
2024-11-05 03:49:18,527 - INFO - [diffusion][Epoch 10924] diffusion training Loss: 0.06029117293655872
2024-11-05 03:49:18,529 - INFO - [diffusion][Epoch 10924] diffusion learning rate: 0.001
2024-11-05 03:49:18,530 - INFO - [diffusion][Epoch 10924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:18,532 - INFO - [diffusion][Epoch 10925] Epoch 10926/12000
2024-11-05 03:49:22,503 - INFO - [diffusion][Epoch 10925] diffusion training Loss: 0.06007305346429348
2024-11-05 03:49:22,505 - INFO - [diffusion][Epoch 10925] diffusion learning rate: 0.001
2024-11-05 03:49:22,507 - INFO - [diffusion][Epoch 10925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:22,508 - INFO - [diffusion][Epoch 10926] Epoch 10927/12000
2024-11-05 03:49:26,826 - INFO - [diffusion][Epoch 10926] diffusion training Loss: 0.05967259593307972
2024-11-05 03:49:26,828 - INFO - [diffusion][Epoch 10926] diffusion learning rate: 0.001
2024-11-05 03:49:26,830 - INFO - [diffusion][Epoch 10926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:26,831 - INFO - [diffusion][Epoch 10927] Epoch 10928/12000
2024-11-05 03:49:31,061 - INFO - [diffusion][Epoch 10927] diffusion training Loss: 0.05962540954351425
2024-11-05 03:49:31,063 - INFO - [diffusion][Epoch 10927] diffusion learning rate: 0.001
2024-11-05 03:49:31,065 - INFO - [diffusion][Epoch 10927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:31,066 - INFO - [diffusion][Epoch 10928] Epoch 10929/12000
2024-11-05 03:49:35,078 - INFO - [diffusion][Epoch 10928] diffusion training Loss: 0.06455290131270885
2024-11-05 03:49:35,080 - INFO - [diffusion][Epoch 10928] diffusion learning rate: 0.001
2024-11-05 03:49:35,081 - INFO - [diffusion][Epoch 10928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:35,083 - INFO - [diffusion][Epoch 10929] Epoch 10930/12000
2024-11-05 03:49:39,335 - INFO - [diffusion][Epoch 10929] diffusion training Loss: 0.062152872793376446
2024-11-05 03:49:39,337 - INFO - [diffusion][Epoch 10929] diffusion learning rate: 0.001
2024-11-05 03:49:39,339 - INFO - [diffusion][Epoch 10929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:39,340 - INFO - [diffusion][Epoch 10930] Epoch 10931/12000
2024-11-05 03:49:43,512 - INFO - [diffusion][Epoch 10930] diffusion training Loss: 0.05803488660603762
2024-11-05 03:49:43,514 - INFO - [diffusion][Epoch 10930] diffusion learning rate: 0.001
2024-11-05 03:49:43,516 - INFO - [diffusion][Epoch 10930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:43,517 - INFO - [diffusion][Epoch 10931] Epoch 10932/12000
2024-11-05 03:49:47,580 - INFO - [diffusion][Epoch 10931] diffusion training Loss: 0.06066817697137594
2024-11-05 03:49:47,582 - INFO - [diffusion][Epoch 10931] diffusion learning rate: 0.001
2024-11-05 03:49:47,584 - INFO - [diffusion][Epoch 10931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:47,585 - INFO - [diffusion][Epoch 10932] Epoch 10933/12000
2024-11-05 03:49:51,722 - INFO - [diffusion][Epoch 10932] diffusion training Loss: 0.05600122269243002
2024-11-05 03:49:51,724 - INFO - [diffusion][Epoch 10932] diffusion learning rate: 0.001
2024-11-05 03:49:51,726 - INFO - [diffusion][Epoch 10932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:51,727 - INFO - [diffusion][Epoch 10933] Epoch 10934/12000
2024-11-05 03:49:55,878 - INFO - [diffusion][Epoch 10933] diffusion training Loss: 0.06146650388836861
2024-11-05 03:49:55,880 - INFO - [diffusion][Epoch 10933] diffusion learning rate: 0.001
2024-11-05 03:49:55,882 - INFO - [diffusion][Epoch 10933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:55,883 - INFO - [diffusion][Epoch 10934] Epoch 10935/12000
2024-11-05 03:50:00,048 - INFO - [diffusion][Epoch 10934] diffusion training Loss: 0.05507730133831501
2024-11-05 03:50:00,050 - INFO - [diffusion][Epoch 10934] diffusion learning rate: 0.001
2024-11-05 03:50:00,052 - INFO - [diffusion][Epoch 10934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:00,054 - INFO - [diffusion][Epoch 10935] Epoch 10936/12000
2024-11-05 03:50:04,139 - INFO - [diffusion][Epoch 10935] diffusion training Loss: 0.06294671352952719
2024-11-05 03:50:04,141 - INFO - [diffusion][Epoch 10935] diffusion learning rate: 0.001
2024-11-05 03:50:04,143 - INFO - [diffusion][Epoch 10935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:04,144 - INFO - [diffusion][Epoch 10936] Epoch 10937/12000
2024-11-05 03:50:08,242 - INFO - [diffusion][Epoch 10936] diffusion training Loss: 0.060863714665174484
2024-11-05 03:50:08,244 - INFO - [diffusion][Epoch 10936] diffusion learning rate: 0.001
2024-11-05 03:50:08,246 - INFO - [diffusion][Epoch 10936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:08,247 - INFO - [diffusion][Epoch 10937] Epoch 10938/12000
2024-11-05 03:50:12,342 - INFO - [diffusion][Epoch 10937] diffusion training Loss: 0.06074234191328287
2024-11-05 03:50:12,344 - INFO - [diffusion][Epoch 10937] diffusion learning rate: 0.001
2024-11-05 03:50:12,364 - INFO - [diffusion][Epoch 10937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:12,366 - INFO - [diffusion][Epoch 10938] Epoch 10939/12000
2024-11-05 03:50:16,419 - INFO - [diffusion][Epoch 10938] diffusion training Loss: 0.055192419327795506
2024-11-05 03:50:16,421 - INFO - [diffusion][Epoch 10938] diffusion learning rate: 0.001
2024-11-05 03:50:16,423 - INFO - [diffusion][Epoch 10938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:16,424 - INFO - [diffusion][Epoch 10939] Epoch 10940/12000
2024-11-05 03:50:20,610 - INFO - [diffusion][Epoch 10939] diffusion training Loss: 0.05812991689890623
2024-11-05 03:50:20,612 - INFO - [diffusion][Epoch 10939] diffusion learning rate: 0.001
2024-11-05 03:50:20,614 - INFO - [diffusion][Epoch 10939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:20,615 - INFO - [diffusion][Epoch 10940] Epoch 10941/12000
2024-11-05 03:50:24,872 - INFO - [diffusion][Epoch 10940] diffusion training Loss: 0.06039626616984606
2024-11-05 03:50:24,874 - INFO - [diffusion][Epoch 10940] diffusion learning rate: 0.001
2024-11-05 03:50:24,876 - INFO - [diffusion][Epoch 10940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:24,877 - INFO - [diffusion][Epoch 10941] Epoch 10942/12000
2024-11-05 03:50:29,059 - INFO - [diffusion][Epoch 10941] diffusion training Loss: 0.060523321852087975
2024-11-05 03:50:29,061 - INFO - [diffusion][Epoch 10941] diffusion learning rate: 0.001
2024-11-05 03:50:29,063 - INFO - [diffusion][Epoch 10941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:29,064 - INFO - [diffusion][Epoch 10942] Epoch 10943/12000
2024-11-05 03:50:33,287 - INFO - [diffusion][Epoch 10942] diffusion training Loss: 0.061864446848630905
2024-11-05 03:50:33,290 - INFO - [diffusion][Epoch 10942] diffusion learning rate: 0.001
2024-11-05 03:50:33,292 - INFO - [diffusion][Epoch 10942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:33,293 - INFO - [diffusion][Epoch 10943] Epoch 10944/12000
2024-11-05 03:50:37,557 - INFO - [diffusion][Epoch 10943] diffusion training Loss: 0.05987037252634764
2024-11-05 03:50:37,560 - INFO - [diffusion][Epoch 10943] diffusion learning rate: 0.001
2024-11-05 03:50:37,562 - INFO - [diffusion][Epoch 10943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:37,563 - INFO - [diffusion][Epoch 10944] Epoch 10945/12000
2024-11-05 03:50:41,833 - INFO - [diffusion][Epoch 10944] diffusion training Loss: 0.056676653213799
2024-11-05 03:50:41,835 - INFO - [diffusion][Epoch 10944] diffusion learning rate: 0.001
2024-11-05 03:50:41,837 - INFO - [diffusion][Epoch 10944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:41,838 - INFO - [diffusion][Epoch 10945] Epoch 10946/12000
2024-11-05 03:50:46,038 - INFO - [diffusion][Epoch 10945] diffusion training Loss: 0.061362432315945625
2024-11-05 03:50:46,040 - INFO - [diffusion][Epoch 10945] diffusion learning rate: 0.001
2024-11-05 03:50:46,042 - INFO - [diffusion][Epoch 10945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:46,043 - INFO - [diffusion][Epoch 10946] Epoch 10947/12000
2024-11-05 03:50:50,236 - INFO - [diffusion][Epoch 10946] diffusion training Loss: 0.05885034054517746
2024-11-05 03:50:50,238 - INFO - [diffusion][Epoch 10946] diffusion learning rate: 0.001
2024-11-05 03:50:50,240 - INFO - [diffusion][Epoch 10946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:50,241 - INFO - [diffusion][Epoch 10947] Epoch 10948/12000
2024-11-05 03:50:54,399 - INFO - [diffusion][Epoch 10947] diffusion training Loss: 0.05997719708830118
2024-11-05 03:50:54,401 - INFO - [diffusion][Epoch 10947] diffusion learning rate: 0.001
2024-11-05 03:50:54,404 - INFO - [diffusion][Epoch 10947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:54,405 - INFO - [diffusion][Epoch 10948] Epoch 10949/12000
2024-11-05 03:50:58,408 - INFO - [diffusion][Epoch 10948] diffusion training Loss: 0.053353555500507355
2024-11-05 03:50:58,410 - INFO - [diffusion][Epoch 10948] diffusion learning rate: 0.001
2024-11-05 03:50:58,412 - INFO - [diffusion][Epoch 10948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:58,413 - INFO - [diffusion][Epoch 10949] Epoch 10950/12000
2024-11-05 03:51:02,355 - INFO - [diffusion][Epoch 10949] diffusion training Loss: 0.0655592642724514
2024-11-05 03:51:02,357 - INFO - [diffusion][Epoch 10949] diffusion learning rate: 0.001
2024-11-05 03:51:02,358 - INFO - [diffusion][Epoch 10949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:02,360 - INFO - [diffusion][Epoch 10950] Epoch 10951/12000
2024-11-05 03:51:06,457 - INFO - [diffusion][Epoch 10950] diffusion training Loss: 0.05847790464758873
2024-11-05 03:51:06,459 - INFO - [diffusion][Epoch 10950] diffusion learning rate: 0.001
2024-11-05 03:51:06,461 - INFO - [diffusion][Epoch 10950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:06,462 - INFO - [diffusion][Epoch 10951] Epoch 10952/12000
2024-11-05 03:51:10,619 - INFO - [diffusion][Epoch 10951] diffusion training Loss: 0.05805228743702173
2024-11-05 03:51:10,621 - INFO - [diffusion][Epoch 10951] diffusion learning rate: 0.001
2024-11-05 03:51:10,623 - INFO - [diffusion][Epoch 10951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:10,624 - INFO - [diffusion][Epoch 10952] Epoch 10953/12000
2024-11-05 03:51:14,697 - INFO - [diffusion][Epoch 10952] diffusion training Loss: 0.05893170274794102
2024-11-05 03:51:14,699 - INFO - [diffusion][Epoch 10952] diffusion learning rate: 0.001
2024-11-05 03:51:14,701 - INFO - [diffusion][Epoch 10952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:14,703 - INFO - [diffusion][Epoch 10953] Epoch 10954/12000
2024-11-05 03:51:18,896 - INFO - [diffusion][Epoch 10953] diffusion training Loss: 0.05657458119094372
2024-11-05 03:51:18,898 - INFO - [diffusion][Epoch 10953] diffusion learning rate: 0.001
2024-11-05 03:51:18,900 - INFO - [diffusion][Epoch 10953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:18,902 - INFO - [diffusion][Epoch 10954] Epoch 10955/12000
2024-11-05 03:51:23,111 - INFO - [diffusion][Epoch 10954] diffusion training Loss: 0.06382206082344055
2024-11-05 03:51:23,113 - INFO - [diffusion][Epoch 10954] diffusion learning rate: 0.001
2024-11-05 03:51:23,115 - INFO - [diffusion][Epoch 10954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:23,116 - INFO - [diffusion][Epoch 10955] Epoch 10956/12000
2024-11-05 03:51:27,251 - INFO - [diffusion][Epoch 10955] diffusion training Loss: 0.06374386232346296
2024-11-05 03:51:27,253 - INFO - [diffusion][Epoch 10955] diffusion learning rate: 0.001
2024-11-05 03:51:27,255 - INFO - [diffusion][Epoch 10955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:27,257 - INFO - [diffusion][Epoch 10956] Epoch 10957/12000
2024-11-05 03:51:31,385 - INFO - [diffusion][Epoch 10956] diffusion training Loss: 0.06079690158367157
2024-11-05 03:51:31,387 - INFO - [diffusion][Epoch 10956] diffusion learning rate: 0.001
2024-11-05 03:51:31,389 - INFO - [diffusion][Epoch 10956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:31,390 - INFO - [diffusion][Epoch 10957] Epoch 10958/12000
2024-11-05 03:51:35,655 - INFO - [diffusion][Epoch 10957] diffusion training Loss: 0.06069306377321482
2024-11-05 03:51:35,657 - INFO - [diffusion][Epoch 10957] diffusion learning rate: 0.001
2024-11-05 03:51:35,659 - INFO - [diffusion][Epoch 10957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:35,661 - INFO - [diffusion][Epoch 10958] Epoch 10959/12000
2024-11-05 03:51:39,927 - INFO - [diffusion][Epoch 10958] diffusion training Loss: 0.05845533311367035
2024-11-05 03:51:39,929 - INFO - [diffusion][Epoch 10958] diffusion learning rate: 0.001
2024-11-05 03:51:39,931 - INFO - [diffusion][Epoch 10958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:39,932 - INFO - [diffusion][Epoch 10959] Epoch 10960/12000
2024-11-05 03:51:43,823 - INFO - [diffusion][Epoch 10959] diffusion training Loss: 0.06825094856321812
2024-11-05 03:51:43,825 - INFO - [diffusion][Epoch 10959] diffusion learning rate: 0.001
2024-11-05 03:51:43,827 - INFO - [diffusion][Epoch 10959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:43,828 - INFO - [diffusion][Epoch 10960] Epoch 10961/12000
2024-11-05 03:51:48,019 - INFO - [diffusion][Epoch 10960] diffusion training Loss: 0.062377145513892174
2024-11-05 03:51:48,022 - INFO - [diffusion][Epoch 10960] diffusion learning rate: 0.001
2024-11-05 03:51:48,024 - INFO - [diffusion][Epoch 10960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:48,025 - INFO - [diffusion][Epoch 10961] Epoch 10962/12000
2024-11-05 03:51:52,147 - INFO - [diffusion][Epoch 10961] diffusion training Loss: 0.05720006953924894
2024-11-05 03:51:52,149 - INFO - [diffusion][Epoch 10961] diffusion learning rate: 0.001
2024-11-05 03:51:52,151 - INFO - [diffusion][Epoch 10961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:52,152 - INFO - [diffusion][Epoch 10962] Epoch 10963/12000
2024-11-05 03:51:56,307 - INFO - [diffusion][Epoch 10962] diffusion training Loss: 0.06134713348001242
2024-11-05 03:51:56,309 - INFO - [diffusion][Epoch 10962] diffusion learning rate: 0.001
2024-11-05 03:51:56,311 - INFO - [diffusion][Epoch 10962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:56,312 - INFO - [diffusion][Epoch 10963] Epoch 10964/12000
2024-11-05 03:52:00,493 - INFO - [diffusion][Epoch 10963] diffusion training Loss: 0.06121376249939203
2024-11-05 03:52:00,495 - INFO - [diffusion][Epoch 10963] diffusion learning rate: 0.001
2024-11-05 03:52:00,497 - INFO - [diffusion][Epoch 10963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:00,499 - INFO - [diffusion][Epoch 10964] Epoch 10965/12000
2024-11-05 03:52:05,162 - INFO - [diffusion][Epoch 10964] diffusion training Loss: 0.0636416282504797
2024-11-05 03:52:05,165 - INFO - [diffusion][Epoch 10964] diffusion learning rate: 0.001
2024-11-05 03:52:05,167 - INFO - [diffusion][Epoch 10964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:05,168 - INFO - [diffusion][Epoch 10965] Epoch 10966/12000
2024-11-05 03:52:09,376 - INFO - [diffusion][Epoch 10965] diffusion training Loss: 0.057091266848146915
2024-11-05 03:52:09,378 - INFO - [diffusion][Epoch 10965] diffusion learning rate: 0.001
2024-11-05 03:52:09,380 - INFO - [diffusion][Epoch 10965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:09,382 - INFO - [diffusion][Epoch 10966] Epoch 10967/12000
2024-11-05 03:52:13,657 - INFO - [diffusion][Epoch 10966] diffusion training Loss: 0.057877558283507824
2024-11-05 03:52:13,659 - INFO - [diffusion][Epoch 10966] diffusion learning rate: 0.001
2024-11-05 03:52:13,661 - INFO - [diffusion][Epoch 10966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:13,662 - INFO - [diffusion][Epoch 10967] Epoch 10968/12000
2024-11-05 03:52:17,940 - INFO - [diffusion][Epoch 10967] diffusion training Loss: 0.06089738290756941
2024-11-05 03:52:17,942 - INFO - [diffusion][Epoch 10967] diffusion learning rate: 0.001
2024-11-05 03:52:17,944 - INFO - [diffusion][Epoch 10967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:17,945 - INFO - [diffusion][Epoch 10968] Epoch 10969/12000
2024-11-05 03:52:22,210 - INFO - [diffusion][Epoch 10968] diffusion training Loss: 0.06201416999101639
2024-11-05 03:52:22,212 - INFO - [diffusion][Epoch 10968] diffusion learning rate: 0.001
2024-11-05 03:52:22,214 - INFO - [diffusion][Epoch 10968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:22,215 - INFO - [diffusion][Epoch 10969] Epoch 10970/12000
2024-11-05 03:52:26,468 - INFO - [diffusion][Epoch 10969] diffusion training Loss: 0.05948794446885586
2024-11-05 03:52:26,470 - INFO - [diffusion][Epoch 10969] diffusion learning rate: 0.001
2024-11-05 03:52:26,471 - INFO - [diffusion][Epoch 10969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:26,472 - INFO - [diffusion][Epoch 10970] Epoch 10971/12000
2024-11-05 03:52:30,550 - INFO - [diffusion][Epoch 10970] diffusion training Loss: 0.05830163788050413
2024-11-05 03:52:30,552 - INFO - [diffusion][Epoch 10970] diffusion learning rate: 0.001
2024-11-05 03:52:30,554 - INFO - [diffusion][Epoch 10970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:30,555 - INFO - [diffusion][Epoch 10971] Epoch 10972/12000
2024-11-05 03:52:34,802 - INFO - [diffusion][Epoch 10971] diffusion training Loss: 0.06246728356927633
2024-11-05 03:52:34,805 - INFO - [diffusion][Epoch 10971] diffusion learning rate: 0.001
2024-11-05 03:52:34,807 - INFO - [diffusion][Epoch 10971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:34,808 - INFO - [diffusion][Epoch 10972] Epoch 10973/12000
2024-11-05 03:52:38,974 - INFO - [diffusion][Epoch 10972] diffusion training Loss: 0.05839371308684349
2024-11-05 03:52:38,976 - INFO - [diffusion][Epoch 10972] diffusion learning rate: 0.001
2024-11-05 03:52:38,978 - INFO - [diffusion][Epoch 10972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:38,979 - INFO - [diffusion][Epoch 10973] Epoch 10974/12000
2024-11-05 03:52:43,143 - INFO - [diffusion][Epoch 10973] diffusion training Loss: 0.059462408535182476
2024-11-05 03:52:43,145 - INFO - [diffusion][Epoch 10973] diffusion learning rate: 0.001
2024-11-05 03:52:43,147 - INFO - [diffusion][Epoch 10973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:43,148 - INFO - [diffusion][Epoch 10974] Epoch 10975/12000
2024-11-05 03:52:47,282 - INFO - [diffusion][Epoch 10974] diffusion training Loss: 0.0621352382004261
2024-11-05 03:52:47,284 - INFO - [diffusion][Epoch 10974] diffusion learning rate: 0.001
2024-11-05 03:52:47,287 - INFO - [diffusion][Epoch 10974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:47,288 - INFO - [diffusion][Epoch 10975] Epoch 10976/12000
2024-11-05 03:52:51,513 - INFO - [diffusion][Epoch 10975] diffusion training Loss: 0.059597515501081944
2024-11-05 03:52:51,515 - INFO - [diffusion][Epoch 10975] diffusion learning rate: 0.001
2024-11-05 03:52:51,516 - INFO - [diffusion][Epoch 10975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:51,518 - INFO - [diffusion][Epoch 10976] Epoch 10977/12000
2024-11-05 03:52:55,657 - INFO - [diffusion][Epoch 10976] diffusion training Loss: 0.06191593036055565
2024-11-05 03:52:55,659 - INFO - [diffusion][Epoch 10976] diffusion learning rate: 0.001
2024-11-05 03:52:55,661 - INFO - [diffusion][Epoch 10976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:55,662 - INFO - [diffusion][Epoch 10977] Epoch 10978/12000
2024-11-05 03:52:59,958 - INFO - [diffusion][Epoch 10977] diffusion training Loss: 0.062458332628011703
2024-11-05 03:52:59,960 - INFO - [diffusion][Epoch 10977] diffusion learning rate: 0.001
2024-11-05 03:52:59,961 - INFO - [diffusion][Epoch 10977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:59,963 - INFO - [diffusion][Epoch 10978] Epoch 10979/12000
2024-11-05 03:53:04,164 - INFO - [diffusion][Epoch 10978] diffusion training Loss: 0.059058439917862415
2024-11-05 03:53:04,167 - INFO - [diffusion][Epoch 10978] diffusion learning rate: 0.001
2024-11-05 03:53:04,169 - INFO - [diffusion][Epoch 10978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:04,170 - INFO - [diffusion][Epoch 10979] Epoch 10980/12000
2024-11-05 03:53:08,503 - INFO - [diffusion][Epoch 10979] diffusion training Loss: 0.06388036534190178
2024-11-05 03:53:08,505 - INFO - [diffusion][Epoch 10979] diffusion learning rate: 0.001
2024-11-05 03:53:08,507 - INFO - [diffusion][Epoch 10979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:08,508 - INFO - [diffusion][Epoch 10980] Epoch 10981/12000
2024-11-05 03:53:12,729 - INFO - [diffusion][Epoch 10980] diffusion training Loss: 0.06342669017612934
2024-11-05 03:53:13,080 - INFO - [diffusion][Epoch 10980] diffusion learning rate: 0.001
2024-11-05 03:53:13,083 - INFO - [diffusion][Epoch 10980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:13,084 - INFO - [diffusion][Epoch 10981] Epoch 10982/12000
2024-11-05 03:53:17,329 - INFO - [diffusion][Epoch 10981] diffusion training Loss: 0.054720109328627586
2024-11-05 03:53:17,331 - INFO - [diffusion][Epoch 10981] diffusion learning rate: 0.001
2024-11-05 03:53:17,333 - INFO - [diffusion][Epoch 10981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:17,334 - INFO - [diffusion][Epoch 10982] Epoch 10983/12000
2024-11-05 03:53:21,598 - INFO - [diffusion][Epoch 10982] diffusion training Loss: 0.05875482875853777
2024-11-05 03:53:21,601 - INFO - [diffusion][Epoch 10982] diffusion learning rate: 0.001
2024-11-05 03:53:21,603 - INFO - [diffusion][Epoch 10982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:21,604 - INFO - [diffusion][Epoch 10983] Epoch 10984/12000
2024-11-05 03:53:25,784 - INFO - [diffusion][Epoch 10983] diffusion training Loss: 0.06065691448748112
2024-11-05 03:53:25,786 - INFO - [diffusion][Epoch 10983] diffusion learning rate: 0.001
2024-11-05 03:53:25,788 - INFO - [diffusion][Epoch 10983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:25,789 - INFO - [diffusion][Epoch 10984] Epoch 10985/12000
2024-11-05 03:53:30,309 - INFO - [diffusion][Epoch 10984] diffusion training Loss: 0.0610714927315712
2024-11-05 03:53:30,311 - INFO - [diffusion][Epoch 10984] diffusion learning rate: 0.001
2024-11-05 03:53:30,314 - INFO - [diffusion][Epoch 10984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:30,315 - INFO - [diffusion][Epoch 10985] Epoch 10986/12000
2024-11-05 03:53:35,081 - INFO - [diffusion][Epoch 10985] diffusion training Loss: 0.061403555795550346
2024-11-05 03:53:35,084 - INFO - [diffusion][Epoch 10985] diffusion learning rate: 0.001
2024-11-05 03:53:35,086 - INFO - [diffusion][Epoch 10985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:35,087 - INFO - [diffusion][Epoch 10986] Epoch 10987/12000
2024-11-05 03:53:39,286 - INFO - [diffusion][Epoch 10986] diffusion training Loss: 0.053592621348798275
2024-11-05 03:53:39,288 - INFO - [diffusion][Epoch 10986] diffusion learning rate: 0.001
2024-11-05 03:53:39,290 - INFO - [diffusion][Epoch 10986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:39,291 - INFO - [diffusion][Epoch 10987] Epoch 10988/12000
2024-11-05 03:53:43,523 - INFO - [diffusion][Epoch 10987] diffusion training Loss: 0.05592608544975519
2024-11-05 03:53:43,525 - INFO - [diffusion][Epoch 10987] diffusion learning rate: 0.001
2024-11-05 03:53:43,527 - INFO - [diffusion][Epoch 10987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:43,528 - INFO - [diffusion][Epoch 10988] Epoch 10989/12000
2024-11-05 03:53:47,811 - INFO - [diffusion][Epoch 10988] diffusion training Loss: 0.0614111740142107
2024-11-05 03:53:47,813 - INFO - [diffusion][Epoch 10988] diffusion learning rate: 0.001
2024-11-05 03:53:47,814 - INFO - [diffusion][Epoch 10988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:47,816 - INFO - [diffusion][Epoch 10989] Epoch 10990/12000
2024-11-05 03:53:52,118 - INFO - [diffusion][Epoch 10989] diffusion training Loss: 0.06037976313382387
2024-11-05 03:53:52,120 - INFO - [diffusion][Epoch 10989] diffusion learning rate: 0.001
2024-11-05 03:53:52,122 - INFO - [diffusion][Epoch 10989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:52,124 - INFO - [diffusion][Epoch 10990] Epoch 10991/12000
2024-11-05 03:53:56,323 - INFO - [diffusion][Epoch 10990] diffusion training Loss: 0.0615085382014513
2024-11-05 03:53:56,325 - INFO - [diffusion][Epoch 10990] diffusion learning rate: 0.001
2024-11-05 03:53:56,326 - INFO - [diffusion][Epoch 10990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:56,328 - INFO - [diffusion][Epoch 10991] Epoch 10992/12000
2024-11-05 03:54:00,355 - INFO - [diffusion][Epoch 10991] diffusion training Loss: 0.06288340222090483
2024-11-05 03:54:00,357 - INFO - [diffusion][Epoch 10991] diffusion learning rate: 0.001
2024-11-05 03:54:00,358 - INFO - [diffusion][Epoch 10991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:00,360 - INFO - [diffusion][Epoch 10992] Epoch 10993/12000
2024-11-05 03:54:04,485 - INFO - [diffusion][Epoch 10992] diffusion training Loss: 0.057367173954844475
2024-11-05 03:54:04,488 - INFO - [diffusion][Epoch 10992] diffusion learning rate: 0.001
2024-11-05 03:54:04,490 - INFO - [diffusion][Epoch 10992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:04,491 - INFO - [diffusion][Epoch 10993] Epoch 10994/12000
2024-11-05 03:54:08,511 - INFO - [diffusion][Epoch 10993] diffusion training Loss: 0.06183421891182661
2024-11-05 03:54:08,513 - INFO - [diffusion][Epoch 10993] diffusion learning rate: 0.001
2024-11-05 03:54:08,515 - INFO - [diffusion][Epoch 10993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:08,517 - INFO - [diffusion][Epoch 10994] Epoch 10995/12000
2024-11-05 03:54:12,565 - INFO - [diffusion][Epoch 10994] diffusion training Loss: 0.05871696211397648
2024-11-05 03:54:12,568 - INFO - [diffusion][Epoch 10994] diffusion learning rate: 0.001
2024-11-05 03:54:12,569 - INFO - [diffusion][Epoch 10994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:12,570 - INFO - [diffusion][Epoch 10995] Epoch 10996/12000
2024-11-05 03:54:16,582 - INFO - [diffusion][Epoch 10995] diffusion training Loss: 0.05831962265074253
2024-11-05 03:54:16,584 - INFO - [diffusion][Epoch 10995] diffusion learning rate: 0.001
2024-11-05 03:54:16,585 - INFO - [diffusion][Epoch 10995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:16,587 - INFO - [diffusion][Epoch 10996] Epoch 10997/12000
2024-11-05 03:54:20,702 - INFO - [diffusion][Epoch 10996] diffusion training Loss: 0.05601228680461645
2024-11-05 03:54:20,704 - INFO - [diffusion][Epoch 10996] diffusion learning rate: 0.001
2024-11-05 03:54:20,706 - INFO - [diffusion][Epoch 10996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:20,707 - INFO - [diffusion][Epoch 10997] Epoch 10998/12000
2024-11-05 03:54:24,898 - INFO - [diffusion][Epoch 10997] diffusion training Loss: 0.054648841731250286
2024-11-05 03:54:24,900 - INFO - [diffusion][Epoch 10997] diffusion learning rate: 0.001
2024-11-05 03:54:24,902 - INFO - [diffusion][Epoch 10997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:24,903 - INFO - [diffusion][Epoch 10998] Epoch 10999/12000
2024-11-05 03:54:29,120 - INFO - [diffusion][Epoch 10998] diffusion training Loss: 0.05920213367789984
2024-11-05 03:54:29,121 - INFO - [diffusion][Epoch 10998] diffusion learning rate: 0.001
2024-11-05 03:54:29,123 - INFO - [diffusion][Epoch 10998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:29,125 - INFO - [diffusion][Epoch 10999] Epoch 11000/12000
2024-11-05 03:54:33,318 - INFO - [diffusion][Epoch 10999] diffusion training Loss: 0.05722355004400015
2024-11-05 03:54:33,320 - INFO - [diffusion][Epoch 10999] diffusion learning rate: 0.001
2024-11-05 03:54:33,429 - INFO - [diffusion][Epoch 10999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:33,430 - INFO - [diffusion][Epoch 11000] Epoch 11001/12000
2024-11-05 03:54:37,582 - INFO - [diffusion][Epoch 11000] diffusion training Loss: 0.06114054471254349
2024-11-05 03:54:37,584 - INFO - [diffusion][Epoch 11000] diffusion learning rate: 0.001
2024-11-05 03:54:37,586 - INFO - [diffusion][Epoch 11000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:37,587 - INFO - [diffusion][Epoch 11001] Epoch 11002/12000
2024-11-05 03:54:41,981 - INFO - [diffusion][Epoch 11001] diffusion training Loss: 0.05961444694548845
2024-11-05 03:54:41,983 - INFO - [diffusion][Epoch 11001] diffusion learning rate: 0.001
2024-11-05 03:54:41,985 - INFO - [diffusion][Epoch 11001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:41,986 - INFO - [diffusion][Epoch 11002] Epoch 11003/12000
2024-11-05 03:54:46,172 - INFO - [diffusion][Epoch 11002] diffusion training Loss: 0.06465194933116436
2024-11-05 03:54:46,174 - INFO - [diffusion][Epoch 11002] diffusion learning rate: 0.001
2024-11-05 03:54:46,176 - INFO - [diffusion][Epoch 11002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:46,177 - INFO - [diffusion][Epoch 11003] Epoch 11004/12000
2024-11-05 03:54:50,476 - INFO - [diffusion][Epoch 11003] diffusion training Loss: 0.058964711613953114
2024-11-05 03:54:50,478 - INFO - [diffusion][Epoch 11003] diffusion learning rate: 0.001
2024-11-05 03:54:50,480 - INFO - [diffusion][Epoch 11003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:50,481 - INFO - [diffusion][Epoch 11004] Epoch 11005/12000
2024-11-05 03:54:54,690 - INFO - [diffusion][Epoch 11004] diffusion training Loss: 0.055489818565547466
2024-11-05 03:54:54,692 - INFO - [diffusion][Epoch 11004] diffusion learning rate: 0.001
2024-11-05 03:54:54,694 - INFO - [diffusion][Epoch 11004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:54,695 - INFO - [diffusion][Epoch 11005] Epoch 11006/12000
2024-11-05 03:54:59,345 - INFO - [diffusion][Epoch 11005] diffusion training Loss: 0.058394089341163635
2024-11-05 03:54:59,348 - INFO - [diffusion][Epoch 11005] diffusion learning rate: 0.001
2024-11-05 03:54:59,349 - INFO - [diffusion][Epoch 11005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:59,351 - INFO - [diffusion][Epoch 11006] Epoch 11007/12000
2024-11-05 03:55:03,683 - INFO - [diffusion][Epoch 11006] diffusion training Loss: 0.0596496369689703
2024-11-05 03:55:03,685 - INFO - [diffusion][Epoch 11006] diffusion learning rate: 0.001
2024-11-05 03:55:03,687 - INFO - [diffusion][Epoch 11006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:03,688 - INFO - [diffusion][Epoch 11007] Epoch 11008/12000
2024-11-05 03:55:07,586 - INFO - [diffusion][Epoch 11007] diffusion training Loss: 0.05489043053239584
2024-11-05 03:55:07,588 - INFO - [diffusion][Epoch 11007] diffusion learning rate: 0.001
2024-11-05 03:55:07,590 - INFO - [diffusion][Epoch 11007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:07,592 - INFO - [diffusion][Epoch 11008] Epoch 11009/12000
2024-11-05 03:55:11,767 - INFO - [diffusion][Epoch 11008] diffusion training Loss: 0.05772583093494177
2024-11-05 03:55:11,769 - INFO - [diffusion][Epoch 11008] diffusion learning rate: 0.001
2024-11-05 03:55:11,771 - INFO - [diffusion][Epoch 11008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:11,772 - INFO - [diffusion][Epoch 11009] Epoch 11010/12000
2024-11-05 03:55:15,933 - INFO - [diffusion][Epoch 11009] diffusion training Loss: 0.06162519194185734
2024-11-05 03:55:15,935 - INFO - [diffusion][Epoch 11009] diffusion learning rate: 0.001
2024-11-05 03:55:15,937 - INFO - [diffusion][Epoch 11009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:15,938 - INFO - [diffusion][Epoch 11010] Epoch 11011/12000
2024-11-05 03:55:20,093 - INFO - [diffusion][Epoch 11010] diffusion training Loss: 0.06030798517167568
2024-11-05 03:55:20,095 - INFO - [diffusion][Epoch 11010] diffusion learning rate: 0.001
2024-11-05 03:55:20,097 - INFO - [diffusion][Epoch 11010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:20,099 - INFO - [diffusion][Epoch 11011] Epoch 11012/12000
2024-11-05 03:55:24,282 - INFO - [diffusion][Epoch 11011] diffusion training Loss: 0.060022130608558655
2024-11-05 03:55:24,284 - INFO - [diffusion][Epoch 11011] diffusion learning rate: 0.001
2024-11-05 03:55:24,285 - INFO - [diffusion][Epoch 11011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:24,287 - INFO - [diffusion][Epoch 11012] Epoch 11013/12000
2024-11-05 03:55:28,536 - INFO - [diffusion][Epoch 11012] diffusion training Loss: 0.059764524921774864
2024-11-05 03:55:28,538 - INFO - [diffusion][Epoch 11012] diffusion learning rate: 0.001
2024-11-05 03:55:28,540 - INFO - [diffusion][Epoch 11012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:28,541 - INFO - [diffusion][Epoch 11013] Epoch 11014/12000
2024-11-05 03:55:32,662 - INFO - [diffusion][Epoch 11013] diffusion training Loss: 0.05881376564502716
2024-11-05 03:55:32,665 - INFO - [diffusion][Epoch 11013] diffusion learning rate: 0.001
2024-11-05 03:55:32,668 - INFO - [diffusion][Epoch 11013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:32,670 - INFO - [diffusion][Epoch 11014] Epoch 11015/12000
2024-11-05 03:55:36,816 - INFO - [diffusion][Epoch 11014] diffusion training Loss: 0.05326041206717491
2024-11-05 03:55:36,818 - INFO - [diffusion][Epoch 11014] diffusion learning rate: 0.001
2024-11-05 03:55:36,820 - INFO - [diffusion][Epoch 11014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:36,821 - INFO - [diffusion][Epoch 11015] Epoch 11016/12000
2024-11-05 03:55:40,845 - INFO - [diffusion][Epoch 11015] diffusion training Loss: 0.06029233429580927
2024-11-05 03:55:40,846 - INFO - [diffusion][Epoch 11015] diffusion learning rate: 0.001
2024-11-05 03:55:40,848 - INFO - [diffusion][Epoch 11015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:40,850 - INFO - [diffusion][Epoch 11016] Epoch 11017/12000
2024-11-05 03:55:44,845 - INFO - [diffusion][Epoch 11016] diffusion training Loss: 0.06179838441312313
2024-11-05 03:55:44,847 - INFO - [diffusion][Epoch 11016] diffusion learning rate: 0.001
2024-11-05 03:55:44,849 - INFO - [diffusion][Epoch 11016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:44,850 - INFO - [diffusion][Epoch 11017] Epoch 11018/12000
2024-11-05 03:55:49,005 - INFO - [diffusion][Epoch 11017] diffusion training Loss: 0.05796065088361502
2024-11-05 03:55:49,008 - INFO - [diffusion][Epoch 11017] diffusion learning rate: 0.001
2024-11-05 03:55:49,009 - INFO - [diffusion][Epoch 11017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:49,011 - INFO - [diffusion][Epoch 11018] Epoch 11019/12000
2024-11-05 03:55:53,225 - INFO - [diffusion][Epoch 11018] diffusion training Loss: 0.05846176389604807
2024-11-05 03:55:53,227 - INFO - [diffusion][Epoch 11018] diffusion learning rate: 0.001
2024-11-05 03:55:53,228 - INFO - [diffusion][Epoch 11018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:53,230 - INFO - [diffusion][Epoch 11019] Epoch 11020/12000
2024-11-05 03:55:57,317 - INFO - [diffusion][Epoch 11019] diffusion training Loss: 0.054641968570649624
2024-11-05 03:55:57,319 - INFO - [diffusion][Epoch 11019] diffusion learning rate: 0.001
2024-11-05 03:55:57,321 - INFO - [diffusion][Epoch 11019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:57,322 - INFO - [diffusion][Epoch 11020] Epoch 11021/12000
2024-11-05 03:56:01,389 - INFO - [diffusion][Epoch 11020] diffusion training Loss: 0.06034885346889496
2024-11-05 03:56:01,392 - INFO - [diffusion][Epoch 11020] diffusion learning rate: 0.001
2024-11-05 03:56:01,396 - INFO - [diffusion][Epoch 11020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:01,398 - INFO - [diffusion][Epoch 11021] Epoch 11022/12000
2024-11-05 03:56:05,586 - INFO - [diffusion][Epoch 11021] diffusion training Loss: 0.0669914847239852
2024-11-05 03:56:05,588 - INFO - [diffusion][Epoch 11021] diffusion learning rate: 0.001
2024-11-05 03:56:05,590 - INFO - [diffusion][Epoch 11021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:05,591 - INFO - [diffusion][Epoch 11022] Epoch 11023/12000
2024-11-05 03:56:09,736 - INFO - [diffusion][Epoch 11022] diffusion training Loss: 0.06081007234752178
2024-11-05 03:56:09,738 - INFO - [diffusion][Epoch 11022] diffusion learning rate: 0.001
2024-11-05 03:56:09,740 - INFO - [diffusion][Epoch 11022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:09,741 - INFO - [diffusion][Epoch 11023] Epoch 11024/12000
2024-11-05 03:56:13,953 - INFO - [diffusion][Epoch 11023] diffusion training Loss: 0.06303604785352945
2024-11-05 03:56:13,956 - INFO - [diffusion][Epoch 11023] diffusion learning rate: 0.001
2024-11-05 03:56:13,958 - INFO - [diffusion][Epoch 11023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:13,959 - INFO - [diffusion][Epoch 11024] Epoch 11025/12000
2024-11-05 03:56:17,951 - INFO - [diffusion][Epoch 11024] diffusion training Loss: 0.06165640614926815
2024-11-05 03:56:17,953 - INFO - [diffusion][Epoch 11024] diffusion learning rate: 0.001
2024-11-05 03:56:17,954 - INFO - [diffusion][Epoch 11024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:17,956 - INFO - [diffusion][Epoch 11025] Epoch 11026/12000
2024-11-05 03:56:22,101 - INFO - [diffusion][Epoch 11025] diffusion training Loss: 0.05886607989668846
2024-11-05 03:56:22,103 - INFO - [diffusion][Epoch 11025] diffusion learning rate: 0.001
2024-11-05 03:56:22,105 - INFO - [diffusion][Epoch 11025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:22,106 - INFO - [diffusion][Epoch 11026] Epoch 11027/12000
2024-11-05 03:56:26,634 - INFO - [diffusion][Epoch 11026] diffusion training Loss: 0.05804818216711283
2024-11-05 03:56:26,637 - INFO - [diffusion][Epoch 11026] diffusion learning rate: 0.001
2024-11-05 03:56:26,639 - INFO - [diffusion][Epoch 11026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:26,640 - INFO - [diffusion][Epoch 11027] Epoch 11028/12000
2024-11-05 03:56:30,780 - INFO - [diffusion][Epoch 11027] diffusion training Loss: 0.0575434360653162
2024-11-05 03:56:30,782 - INFO - [diffusion][Epoch 11027] diffusion learning rate: 0.001
2024-11-05 03:56:30,802 - INFO - [diffusion][Epoch 11027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:30,803 - INFO - [diffusion][Epoch 11028] Epoch 11029/12000
2024-11-05 03:56:34,919 - INFO - [diffusion][Epoch 11028] diffusion training Loss: 0.05799820367246866
2024-11-05 03:56:34,921 - INFO - [diffusion][Epoch 11028] diffusion learning rate: 0.001
2024-11-05 03:56:34,923 - INFO - [diffusion][Epoch 11028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:34,924 - INFO - [diffusion][Epoch 11029] Epoch 11030/12000
2024-11-05 03:56:39,050 - INFO - [diffusion][Epoch 11029] diffusion training Loss: 0.06126779317855835
2024-11-05 03:56:39,052 - INFO - [diffusion][Epoch 11029] diffusion learning rate: 0.001
2024-11-05 03:56:39,054 - INFO - [diffusion][Epoch 11029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:39,055 - INFO - [diffusion][Epoch 11030] Epoch 11031/12000
2024-11-05 03:56:43,154 - INFO - [diffusion][Epoch 11030] diffusion training Loss: 0.05785857327282429
2024-11-05 03:56:43,156 - INFO - [diffusion][Epoch 11030] diffusion learning rate: 0.001
2024-11-05 03:56:43,158 - INFO - [diffusion][Epoch 11030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:43,159 - INFO - [diffusion][Epoch 11031] Epoch 11032/12000
2024-11-05 03:56:47,269 - INFO - [diffusion][Epoch 11031] diffusion training Loss: 0.06357918307185173
2024-11-05 03:56:47,271 - INFO - [diffusion][Epoch 11031] diffusion learning rate: 0.001
2024-11-05 03:56:47,273 - INFO - [diffusion][Epoch 11031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:47,274 - INFO - [diffusion][Epoch 11032] Epoch 11033/12000
2024-11-05 03:56:51,536 - INFO - [diffusion][Epoch 11032] diffusion training Loss: 0.05651503335684538
2024-11-05 03:56:51,538 - INFO - [diffusion][Epoch 11032] diffusion learning rate: 0.001
2024-11-05 03:56:51,540 - INFO - [diffusion][Epoch 11032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:51,541 - INFO - [diffusion][Epoch 11033] Epoch 11034/12000
2024-11-05 03:56:55,762 - INFO - [diffusion][Epoch 11033] diffusion training Loss: 0.05833797622472048
2024-11-05 03:56:55,764 - INFO - [diffusion][Epoch 11033] diffusion learning rate: 0.001
2024-11-05 03:56:55,766 - INFO - [diffusion][Epoch 11033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:55,767 - INFO - [diffusion][Epoch 11034] Epoch 11035/12000
2024-11-05 03:57:00,041 - INFO - [diffusion][Epoch 11034] diffusion training Loss: 0.06132679991424084
2024-11-05 03:57:00,043 - INFO - [diffusion][Epoch 11034] diffusion learning rate: 0.001
2024-11-05 03:57:00,045 - INFO - [diffusion][Epoch 11034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:00,046 - INFO - [diffusion][Epoch 11035] Epoch 11036/12000
2024-11-05 03:57:04,284 - INFO - [diffusion][Epoch 11035] diffusion training Loss: 0.0608497504144907
2024-11-05 03:57:04,286 - INFO - [diffusion][Epoch 11035] diffusion learning rate: 0.001
2024-11-05 03:57:04,288 - INFO - [diffusion][Epoch 11035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:04,289 - INFO - [diffusion][Epoch 11036] Epoch 11037/12000
2024-11-05 03:57:08,486 - INFO - [diffusion][Epoch 11036] diffusion training Loss: 0.05885406956076622
2024-11-05 03:57:08,488 - INFO - [diffusion][Epoch 11036] diffusion learning rate: 0.001
2024-11-05 03:57:08,490 - INFO - [diffusion][Epoch 11036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:08,491 - INFO - [diffusion][Epoch 11037] Epoch 11038/12000
2024-11-05 03:57:12,698 - INFO - [diffusion][Epoch 11037] diffusion training Loss: 0.05634354893118143
2024-11-05 03:57:12,699 - INFO - [diffusion][Epoch 11037] diffusion learning rate: 0.001
2024-11-05 03:57:12,701 - INFO - [diffusion][Epoch 11037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:12,702 - INFO - [diffusion][Epoch 11038] Epoch 11039/12000
2024-11-05 03:57:16,879 - INFO - [diffusion][Epoch 11038] diffusion training Loss: 0.05797032918781042
2024-11-05 03:57:16,881 - INFO - [diffusion][Epoch 11038] diffusion learning rate: 0.001
2024-11-05 03:57:16,883 - INFO - [diffusion][Epoch 11038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:16,884 - INFO - [diffusion][Epoch 11039] Epoch 11040/12000
2024-11-05 03:57:21,163 - INFO - [diffusion][Epoch 11039] diffusion training Loss: 0.06182464864104986
2024-11-05 03:57:21,166 - INFO - [diffusion][Epoch 11039] diffusion learning rate: 0.001
2024-11-05 03:57:21,168 - INFO - [diffusion][Epoch 11039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:21,170 - INFO - [diffusion][Epoch 11040] Epoch 11041/12000
2024-11-05 03:57:25,379 - INFO - [diffusion][Epoch 11040] diffusion training Loss: 0.060045016929507256
2024-11-05 03:57:25,382 - INFO - [diffusion][Epoch 11040] diffusion learning rate: 0.001
2024-11-05 03:57:25,384 - INFO - [diffusion][Epoch 11040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:25,385 - INFO - [diffusion][Epoch 11041] Epoch 11042/12000
2024-11-05 03:57:29,377 - INFO - [diffusion][Epoch 11041] diffusion training Loss: 0.05877603590488434
2024-11-05 03:57:29,379 - INFO - [diffusion][Epoch 11041] diffusion learning rate: 0.001
2024-11-05 03:57:29,380 - INFO - [diffusion][Epoch 11041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:29,382 - INFO - [diffusion][Epoch 11042] Epoch 11043/12000
2024-11-05 03:57:33,531 - INFO - [diffusion][Epoch 11042] diffusion training Loss: 0.059757040813565254
2024-11-05 03:57:33,533 - INFO - [diffusion][Epoch 11042] diffusion learning rate: 0.001
2024-11-05 03:57:33,535 - INFO - [diffusion][Epoch 11042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:33,536 - INFO - [diffusion][Epoch 11043] Epoch 11044/12000
2024-11-05 03:57:37,797 - INFO - [diffusion][Epoch 11043] diffusion training Loss: 0.05955484416335821
2024-11-05 03:57:37,799 - INFO - [diffusion][Epoch 11043] diffusion learning rate: 0.001
2024-11-05 03:57:37,800 - INFO - [diffusion][Epoch 11043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:37,802 - INFO - [diffusion][Epoch 11044] Epoch 11045/12000
2024-11-05 03:57:42,064 - INFO - [diffusion][Epoch 11044] diffusion training Loss: 0.06054975092411041
2024-11-05 03:57:42,066 - INFO - [diffusion][Epoch 11044] diffusion learning rate: 0.001
2024-11-05 03:57:42,068 - INFO - [diffusion][Epoch 11044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:42,069 - INFO - [diffusion][Epoch 11045] Epoch 11046/12000
2024-11-05 03:57:46,218 - INFO - [diffusion][Epoch 11045] diffusion training Loss: 0.0567829255014658
2024-11-05 03:57:46,220 - INFO - [diffusion][Epoch 11045] diffusion learning rate: 0.001
2024-11-05 03:57:46,221 - INFO - [diffusion][Epoch 11045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:46,223 - INFO - [diffusion][Epoch 11046] Epoch 11047/12000
2024-11-05 03:57:50,373 - INFO - [diffusion][Epoch 11046] diffusion training Loss: 0.05658195447176695
2024-11-05 03:57:50,375 - INFO - [diffusion][Epoch 11046] diffusion learning rate: 0.001
2024-11-05 03:57:50,377 - INFO - [diffusion][Epoch 11046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:50,378 - INFO - [diffusion][Epoch 11047] Epoch 11048/12000
2024-11-05 03:57:54,523 - INFO - [diffusion][Epoch 11047] diffusion training Loss: 0.06030536815524101
2024-11-05 03:57:54,525 - INFO - [diffusion][Epoch 11047] diffusion learning rate: 0.001
2024-11-05 03:57:54,545 - INFO - [diffusion][Epoch 11047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:54,546 - INFO - [diffusion][Epoch 11048] Epoch 11049/12000
2024-11-05 03:57:58,704 - INFO - [diffusion][Epoch 11048] diffusion training Loss: 0.05800046864897013
2024-11-05 03:57:58,706 - INFO - [diffusion][Epoch 11048] diffusion learning rate: 0.001
2024-11-05 03:57:58,708 - INFO - [diffusion][Epoch 11048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:58,709 - INFO - [diffusion][Epoch 11049] Epoch 11050/12000
2024-11-05 03:58:02,850 - INFO - [diffusion][Epoch 11049] diffusion training Loss: 0.05664659105241299
2024-11-05 03:58:02,851 - INFO - [diffusion][Epoch 11049] diffusion learning rate: 0.001
2024-11-05 03:58:02,853 - INFO - [diffusion][Epoch 11049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:02,854 - INFO - [diffusion][Epoch 11050] Epoch 11051/12000
2024-11-05 03:58:07,027 - INFO - [diffusion][Epoch 11050] diffusion training Loss: 0.05084203835576773
2024-11-05 03:58:07,028 - INFO - [diffusion][Epoch 11050] diffusion learning rate: 0.001
2024-11-05 03:58:07,030 - INFO - [diffusion][Epoch 11050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:07,032 - INFO - [diffusion][Epoch 11051] Epoch 11052/12000
2024-11-05 03:58:11,025 - INFO - [diffusion][Epoch 11051] diffusion training Loss: 0.05789592210203409
2024-11-05 03:58:11,027 - INFO - [diffusion][Epoch 11051] diffusion learning rate: 0.001
2024-11-05 03:58:11,077 - INFO - [diffusion][Epoch 11051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:11,078 - INFO - [diffusion][Epoch 11052] Epoch 11053/12000
2024-11-05 03:58:15,102 - INFO - [diffusion][Epoch 11052] diffusion training Loss: 0.06001644302159548
2024-11-05 03:58:15,104 - INFO - [diffusion][Epoch 11052] diffusion learning rate: 0.001
2024-11-05 03:58:15,106 - INFO - [diffusion][Epoch 11052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:15,107 - INFO - [diffusion][Epoch 11053] Epoch 11054/12000
2024-11-05 03:58:19,177 - INFO - [diffusion][Epoch 11053] diffusion training Loss: 0.06297539174556732
2024-11-05 03:58:19,179 - INFO - [diffusion][Epoch 11053] diffusion learning rate: 0.001
2024-11-05 03:58:19,181 - INFO - [diffusion][Epoch 11053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:19,182 - INFO - [diffusion][Epoch 11054] Epoch 11055/12000
2024-11-05 03:58:23,283 - INFO - [diffusion][Epoch 11054] diffusion training Loss: 0.061289344914257526
2024-11-05 03:58:23,285 - INFO - [diffusion][Epoch 11054] diffusion learning rate: 0.001
2024-11-05 03:58:23,286 - INFO - [diffusion][Epoch 11054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:23,288 - INFO - [diffusion][Epoch 11055] Epoch 11056/12000
2024-11-05 03:58:27,424 - INFO - [diffusion][Epoch 11055] diffusion training Loss: 0.06045438535511494
2024-11-05 03:58:27,426 - INFO - [diffusion][Epoch 11055] diffusion learning rate: 0.001
2024-11-05 03:58:27,446 - INFO - [diffusion][Epoch 11055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:27,447 - INFO - [diffusion][Epoch 11056] Epoch 11057/12000
2024-11-05 03:58:31,548 - INFO - [diffusion][Epoch 11056] diffusion training Loss: 0.06574738398194313
2024-11-05 03:58:31,549 - INFO - [diffusion][Epoch 11056] diffusion learning rate: 0.001
2024-11-05 03:58:31,551 - INFO - [diffusion][Epoch 11056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:31,552 - INFO - [diffusion][Epoch 11057] Epoch 11058/12000
2024-11-05 03:58:35,156 - INFO - [diffusion][Epoch 11057] diffusion training Loss: 0.06205897592008114
2024-11-05 03:58:35,158 - INFO - [diffusion][Epoch 11057] diffusion learning rate: 0.001
2024-11-05 03:58:35,160 - INFO - [diffusion][Epoch 11057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:35,161 - INFO - [diffusion][Epoch 11058] Epoch 11059/12000
2024-11-05 03:58:39,284 - INFO - [diffusion][Epoch 11058] diffusion training Loss: 0.06281229853630066
2024-11-05 03:58:39,285 - INFO - [diffusion][Epoch 11058] diffusion learning rate: 0.001
2024-11-05 03:58:39,287 - INFO - [diffusion][Epoch 11058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:39,288 - INFO - [diffusion][Epoch 11059] Epoch 11060/12000
2024-11-05 03:58:43,443 - INFO - [diffusion][Epoch 11059] diffusion training Loss: 0.0593827273696661
2024-11-05 03:58:43,446 - INFO - [diffusion][Epoch 11059] diffusion learning rate: 0.001
2024-11-05 03:58:43,447 - INFO - [diffusion][Epoch 11059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:43,449 - INFO - [diffusion][Epoch 11060] Epoch 11061/12000
2024-11-05 03:58:47,646 - INFO - [diffusion][Epoch 11060] diffusion training Loss: 0.061676064506173134
2024-11-05 03:58:47,648 - INFO - [diffusion][Epoch 11060] diffusion learning rate: 0.001
2024-11-05 03:58:47,650 - INFO - [diffusion][Epoch 11060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:47,651 - INFO - [diffusion][Epoch 11061] Epoch 11062/12000
2024-11-05 03:58:51,784 - INFO - [diffusion][Epoch 11061] diffusion training Loss: 0.05813836958259344
2024-11-05 03:58:51,786 - INFO - [diffusion][Epoch 11061] diffusion learning rate: 0.001
2024-11-05 03:58:51,787 - INFO - [diffusion][Epoch 11061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:51,788 - INFO - [diffusion][Epoch 11062] Epoch 11063/12000
2024-11-05 03:58:56,041 - INFO - [diffusion][Epoch 11062] diffusion training Loss: 0.05955958925187588
2024-11-05 03:58:56,043 - INFO - [diffusion][Epoch 11062] diffusion learning rate: 0.001
2024-11-05 03:58:56,045 - INFO - [diffusion][Epoch 11062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:56,046 - INFO - [diffusion][Epoch 11063] Epoch 11064/12000
2024-11-05 03:59:00,331 - INFO - [diffusion][Epoch 11063] diffusion training Loss: 0.06337226089090109
2024-11-05 03:59:00,333 - INFO - [diffusion][Epoch 11063] diffusion learning rate: 0.001
2024-11-05 03:59:00,335 - INFO - [diffusion][Epoch 11063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:00,337 - INFO - [diffusion][Epoch 11064] Epoch 11065/12000
2024-11-05 03:59:04,598 - INFO - [diffusion][Epoch 11064] diffusion training Loss: 0.05713381804525852
2024-11-05 03:59:04,600 - INFO - [diffusion][Epoch 11064] diffusion learning rate: 0.001
2024-11-05 03:59:04,602 - INFO - [diffusion][Epoch 11064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:04,603 - INFO - [diffusion][Epoch 11065] Epoch 11066/12000
2024-11-05 03:59:08,837 - INFO - [diffusion][Epoch 11065] diffusion training Loss: 0.0587243614718318
2024-11-05 03:59:08,839 - INFO - [diffusion][Epoch 11065] diffusion learning rate: 0.001
2024-11-05 03:59:08,841 - INFO - [diffusion][Epoch 11065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:08,842 - INFO - [diffusion][Epoch 11066] Epoch 11067/12000
2024-11-05 03:59:13,098 - INFO - [diffusion][Epoch 11066] diffusion training Loss: 0.06259137485176325
2024-11-05 03:59:13,100 - INFO - [diffusion][Epoch 11066] diffusion learning rate: 0.001
2024-11-05 03:59:13,102 - INFO - [diffusion][Epoch 11066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:13,103 - INFO - [diffusion][Epoch 11067] Epoch 11068/12000
2024-11-05 03:59:17,823 - INFO - [diffusion][Epoch 11067] diffusion training Loss: 0.05847790651023388
2024-11-05 03:59:17,825 - INFO - [diffusion][Epoch 11067] diffusion learning rate: 0.001
2024-11-05 03:59:17,827 - INFO - [diffusion][Epoch 11067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:17,828 - INFO - [diffusion][Epoch 11068] Epoch 11069/12000
2024-11-05 03:59:21,962 - INFO - [diffusion][Epoch 11068] diffusion training Loss: 0.06192404590547085
2024-11-05 03:59:21,964 - INFO - [diffusion][Epoch 11068] diffusion learning rate: 0.001
2024-11-05 03:59:21,965 - INFO - [diffusion][Epoch 11068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:21,967 - INFO - [diffusion][Epoch 11069] Epoch 11070/12000
2024-11-05 03:59:26,157 - INFO - [diffusion][Epoch 11069] diffusion training Loss: 0.057639836333692074
2024-11-05 03:59:26,159 - INFO - [diffusion][Epoch 11069] diffusion learning rate: 0.001
2024-11-05 03:59:26,161 - INFO - [diffusion][Epoch 11069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:26,162 - INFO - [diffusion][Epoch 11070] Epoch 11071/12000
2024-11-05 03:59:30,289 - INFO - [diffusion][Epoch 11070] diffusion training Loss: 0.055016202852129936
2024-11-05 03:59:30,291 - INFO - [diffusion][Epoch 11070] diffusion learning rate: 0.001
2024-11-05 03:59:30,293 - INFO - [diffusion][Epoch 11070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:30,295 - INFO - [diffusion][Epoch 11071] Epoch 11072/12000
2024-11-05 03:59:34,498 - INFO - [diffusion][Epoch 11071] diffusion training Loss: 0.05696341209113598
2024-11-05 03:59:34,500 - INFO - [diffusion][Epoch 11071] diffusion learning rate: 0.001
2024-11-05 03:59:34,501 - INFO - [diffusion][Epoch 11071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:34,503 - INFO - [diffusion][Epoch 11072] Epoch 11073/12000
2024-11-05 03:59:38,785 - INFO - [diffusion][Epoch 11072] diffusion training Loss: 0.05991274118423462
2024-11-05 03:59:38,787 - INFO - [diffusion][Epoch 11072] diffusion learning rate: 0.001
2024-11-05 03:59:38,788 - INFO - [diffusion][Epoch 11072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:38,790 - INFO - [diffusion][Epoch 11073] Epoch 11074/12000
2024-11-05 03:59:43,081 - INFO - [diffusion][Epoch 11073] diffusion training Loss: 0.056568099185824394
2024-11-05 03:59:43,083 - INFO - [diffusion][Epoch 11073] diffusion learning rate: 0.001
2024-11-05 03:59:43,085 - INFO - [diffusion][Epoch 11073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:43,086 - INFO - [diffusion][Epoch 11074] Epoch 11075/12000
2024-11-05 03:59:47,079 - INFO - [diffusion][Epoch 11074] diffusion training Loss: 0.06014593783766031
2024-11-05 03:59:47,081 - INFO - [diffusion][Epoch 11074] diffusion learning rate: 0.001
2024-11-05 03:59:47,083 - INFO - [diffusion][Epoch 11074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:47,084 - INFO - [diffusion][Epoch 11075] Epoch 11076/12000
2024-11-05 03:59:51,240 - INFO - [diffusion][Epoch 11075] diffusion training Loss: 0.056734771467745304
2024-11-05 03:59:51,242 - INFO - [diffusion][Epoch 11075] diffusion learning rate: 0.001
2024-11-05 03:59:51,269 - INFO - [diffusion][Epoch 11075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:51,270 - INFO - [diffusion][Epoch 11076] Epoch 11077/12000
2024-11-05 03:59:55,491 - INFO - [diffusion][Epoch 11076] diffusion training Loss: 0.061123402789235115
2024-11-05 03:59:55,493 - INFO - [diffusion][Epoch 11076] diffusion learning rate: 0.001
2024-11-05 03:59:55,495 - INFO - [diffusion][Epoch 11076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:55,496 - INFO - [diffusion][Epoch 11077] Epoch 11078/12000
2024-11-05 03:59:59,789 - INFO - [diffusion][Epoch 11077] diffusion training Loss: 0.05659033078700304
2024-11-05 03:59:59,791 - INFO - [diffusion][Epoch 11077] diffusion learning rate: 0.001
2024-11-05 03:59:59,792 - INFO - [diffusion][Epoch 11077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:59,794 - INFO - [diffusion][Epoch 11078] Epoch 11079/12000
2024-11-05 04:00:04,077 - INFO - [diffusion][Epoch 11078] diffusion training Loss: 0.06197279132902622
2024-11-05 04:00:04,079 - INFO - [diffusion][Epoch 11078] diffusion learning rate: 0.001
2024-11-05 04:00:04,081 - INFO - [diffusion][Epoch 11078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:04,082 - INFO - [diffusion][Epoch 11079] Epoch 11080/12000
2024-11-05 04:00:08,271 - INFO - [diffusion][Epoch 11079] diffusion training Loss: 0.0572319021448493
2024-11-05 04:00:08,274 - INFO - [diffusion][Epoch 11079] diffusion learning rate: 0.001
2024-11-05 04:00:08,276 - INFO - [diffusion][Epoch 11079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:08,278 - INFO - [diffusion][Epoch 11080] Epoch 11081/12000
2024-11-05 04:00:12,553 - INFO - [diffusion][Epoch 11080] diffusion training Loss: 0.05665496736764908
2024-11-05 04:00:12,555 - INFO - [diffusion][Epoch 11080] diffusion learning rate: 0.001
2024-11-05 04:00:12,557 - INFO - [diffusion][Epoch 11080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:12,559 - INFO - [diffusion][Epoch 11081] Epoch 11082/12000
2024-11-05 04:00:16,759 - INFO - [diffusion][Epoch 11081] diffusion training Loss: 0.05921163596212864
2024-11-05 04:00:16,761 - INFO - [diffusion][Epoch 11081] diffusion learning rate: 0.001
2024-11-05 04:00:16,762 - INFO - [diffusion][Epoch 11081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:16,763 - INFO - [diffusion][Epoch 11082] Epoch 11083/12000
2024-11-05 04:00:20,831 - INFO - [diffusion][Epoch 11082] diffusion training Loss: 0.05852164886891842
2024-11-05 04:00:20,833 - INFO - [diffusion][Epoch 11082] diffusion learning rate: 0.001
2024-11-05 04:00:20,835 - INFO - [diffusion][Epoch 11082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:20,836 - INFO - [diffusion][Epoch 11083] Epoch 11084/12000
2024-11-05 04:00:24,927 - INFO - [diffusion][Epoch 11083] diffusion training Loss: 0.05913117341697216
2024-11-05 04:00:24,929 - INFO - [diffusion][Epoch 11083] diffusion learning rate: 0.001
2024-11-05 04:00:24,931 - INFO - [diffusion][Epoch 11083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:24,932 - INFO - [diffusion][Epoch 11084] Epoch 11085/12000
2024-11-05 04:00:29,164 - INFO - [diffusion][Epoch 11084] diffusion training Loss: 0.0594615638256073
2024-11-05 04:00:29,166 - INFO - [diffusion][Epoch 11084] diffusion learning rate: 0.001
2024-11-05 04:00:29,168 - INFO - [diffusion][Epoch 11084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:29,170 - INFO - [diffusion][Epoch 11085] Epoch 11086/12000
2024-11-05 04:00:33,536 - INFO - [diffusion][Epoch 11085] diffusion training Loss: 0.06481536664068699
2024-11-05 04:00:33,539 - INFO - [diffusion][Epoch 11085] diffusion learning rate: 0.001
2024-11-05 04:00:33,541 - INFO - [diffusion][Epoch 11085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:33,542 - INFO - [diffusion][Epoch 11086] Epoch 11087/12000
2024-11-05 04:00:37,729 - INFO - [diffusion][Epoch 11086] diffusion training Loss: 0.05946133192628622
2024-11-05 04:00:37,731 - INFO - [diffusion][Epoch 11086] diffusion learning rate: 0.001
2024-11-05 04:00:37,734 - INFO - [diffusion][Epoch 11086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:37,735 - INFO - [diffusion][Epoch 11087] Epoch 11088/12000
2024-11-05 04:00:42,311 - INFO - [diffusion][Epoch 11087] diffusion training Loss: 0.05805587023496628
2024-11-05 04:00:42,313 - INFO - [diffusion][Epoch 11087] diffusion learning rate: 0.001
2024-11-05 04:00:42,316 - INFO - [diffusion][Epoch 11087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:42,317 - INFO - [diffusion][Epoch 11088] Epoch 11089/12000
2024-11-05 04:00:46,533 - INFO - [diffusion][Epoch 11088] diffusion training Loss: 0.061070467345416546
2024-11-05 04:00:46,536 - INFO - [diffusion][Epoch 11088] diffusion learning rate: 0.001
2024-11-05 04:00:46,538 - INFO - [diffusion][Epoch 11088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:46,540 - INFO - [diffusion][Epoch 11089] Epoch 11090/12000
2024-11-05 04:00:50,499 - INFO - [diffusion][Epoch 11089] diffusion training Loss: 0.05901556182652712
2024-11-05 04:00:50,502 - INFO - [diffusion][Epoch 11089] diffusion learning rate: 0.001
2024-11-05 04:00:50,544 - INFO - [diffusion][Epoch 11089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:50,546 - INFO - [diffusion][Epoch 11090] Epoch 11091/12000
2024-11-05 04:00:54,715 - INFO - [diffusion][Epoch 11090] diffusion training Loss: 0.05864667613059282
2024-11-05 04:00:54,718 - INFO - [diffusion][Epoch 11090] diffusion learning rate: 0.001
2024-11-05 04:00:54,720 - INFO - [diffusion][Epoch 11090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:54,722 - INFO - [diffusion][Epoch 11091] Epoch 11092/12000
2024-11-05 04:00:58,865 - INFO - [diffusion][Epoch 11091] diffusion training Loss: 0.05706881172955036
2024-11-05 04:00:58,868 - INFO - [diffusion][Epoch 11091] diffusion learning rate: 0.001
2024-11-05 04:00:58,869 - INFO - [diffusion][Epoch 11091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:58,870 - INFO - [diffusion][Epoch 11092] Epoch 11093/12000
2024-11-05 04:01:03,148 - INFO - [diffusion][Epoch 11092] diffusion training Loss: 0.05619583837687969
2024-11-05 04:01:03,150 - INFO - [diffusion][Epoch 11092] diffusion learning rate: 0.001
2024-11-05 04:01:03,152 - INFO - [diffusion][Epoch 11092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:03,153 - INFO - [diffusion][Epoch 11093] Epoch 11094/12000
2024-11-05 04:01:07,479 - INFO - [diffusion][Epoch 11093] diffusion training Loss: 0.05915851425379515
2024-11-05 04:01:07,482 - INFO - [diffusion][Epoch 11093] diffusion learning rate: 0.001
2024-11-05 04:01:07,484 - INFO - [diffusion][Epoch 11093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:07,485 - INFO - [diffusion][Epoch 11094] Epoch 11095/12000
2024-11-05 04:01:11,646 - INFO - [diffusion][Epoch 11094] diffusion training Loss: 0.056403801776468754
2024-11-05 04:01:11,648 - INFO - [diffusion][Epoch 11094] diffusion learning rate: 0.001
2024-11-05 04:01:11,649 - INFO - [diffusion][Epoch 11094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:11,651 - INFO - [diffusion][Epoch 11095] Epoch 11096/12000
2024-11-05 04:01:15,578 - INFO - [diffusion][Epoch 11095] diffusion training Loss: 0.060095430351793766
2024-11-05 04:01:15,580 - INFO - [diffusion][Epoch 11095] diffusion learning rate: 0.001
2024-11-05 04:01:15,581 - INFO - [diffusion][Epoch 11095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:15,583 - INFO - [diffusion][Epoch 11096] Epoch 11097/12000
2024-11-05 04:01:19,871 - INFO - [diffusion][Epoch 11096] diffusion training Loss: 0.05572896171361208
2024-11-05 04:01:19,874 - INFO - [diffusion][Epoch 11096] diffusion learning rate: 0.001
2024-11-05 04:01:19,876 - INFO - [diffusion][Epoch 11096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:19,877 - INFO - [diffusion][Epoch 11097] Epoch 11098/12000
2024-11-05 04:01:23,917 - INFO - [diffusion][Epoch 11097] diffusion training Loss: 0.055022300221025944
2024-11-05 04:01:23,919 - INFO - [diffusion][Epoch 11097] diffusion learning rate: 0.001
2024-11-05 04:01:23,921 - INFO - [diffusion][Epoch 11097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:23,922 - INFO - [diffusion][Epoch 11098] Epoch 11099/12000
2024-11-05 04:01:28,126 - INFO - [diffusion][Epoch 11098] diffusion training Loss: 0.05749298445880413
2024-11-05 04:01:28,128 - INFO - [diffusion][Epoch 11098] diffusion learning rate: 0.001
2024-11-05 04:01:28,130 - INFO - [diffusion][Epoch 11098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:28,131 - INFO - [diffusion][Epoch 11099] Epoch 11100/12000
2024-11-05 04:01:32,405 - INFO - [diffusion][Epoch 11099] diffusion training Loss: 0.05456732586026192
2024-11-05 04:01:32,406 - INFO - [diffusion][Epoch 11099] diffusion learning rate: 0.001
2024-11-05 04:01:32,408 - INFO - [diffusion][Epoch 11099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:32,409 - INFO - [diffusion][Epoch 11100] Epoch 11101/12000
2024-11-05 04:01:36,694 - INFO - [diffusion][Epoch 11100] diffusion training Loss: 0.05763678811490536
2024-11-05 04:01:36,696 - INFO - [diffusion][Epoch 11100] diffusion learning rate: 0.001
2024-11-05 04:01:36,698 - INFO - [diffusion][Epoch 11100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:36,699 - INFO - [diffusion][Epoch 11101] Epoch 11102/12000
2024-11-05 04:01:40,627 - INFO - [diffusion][Epoch 11101] diffusion training Loss: 0.057405343279242516
2024-11-05 04:01:40,629 - INFO - [diffusion][Epoch 11101] diffusion learning rate: 0.001
2024-11-05 04:01:40,631 - INFO - [diffusion][Epoch 11101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:40,633 - INFO - [diffusion][Epoch 11102] Epoch 11103/12000
2024-11-05 04:01:44,132 - INFO - [diffusion][Epoch 11102] diffusion training Loss: 0.06274796649813652
2024-11-05 04:01:44,134 - INFO - [diffusion][Epoch 11102] diffusion learning rate: 0.001
2024-11-05 04:01:44,136 - INFO - [diffusion][Epoch 11102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:44,137 - INFO - [diffusion][Epoch 11103] Epoch 11104/12000
2024-11-05 04:01:47,612 - INFO - [diffusion][Epoch 11103] diffusion training Loss: 0.058084108866751194
2024-11-05 04:01:47,615 - INFO - [diffusion][Epoch 11103] diffusion learning rate: 0.001
2024-11-05 04:01:47,617 - INFO - [diffusion][Epoch 11103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:47,618 - INFO - [diffusion][Epoch 11104] Epoch 11105/12000
2024-11-05 04:01:51,080 - INFO - [diffusion][Epoch 11104] diffusion training Loss: 0.0603651562705636
2024-11-05 04:01:51,082 - INFO - [diffusion][Epoch 11104] diffusion learning rate: 0.001
2024-11-05 04:01:51,084 - INFO - [diffusion][Epoch 11104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:51,085 - INFO - [diffusion][Epoch 11105] Epoch 11106/12000
2024-11-05 04:01:54,574 - INFO - [diffusion][Epoch 11105] diffusion training Loss: 0.05802639666944742
2024-11-05 04:01:54,577 - INFO - [diffusion][Epoch 11105] diffusion learning rate: 0.001
2024-11-05 04:01:54,579 - INFO - [diffusion][Epoch 11105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:54,580 - INFO - [diffusion][Epoch 11106] Epoch 11107/12000
2024-11-05 04:01:58,181 - INFO - [diffusion][Epoch 11106] diffusion training Loss: 0.0573739567771554
2024-11-05 04:01:58,183 - INFO - [diffusion][Epoch 11106] diffusion learning rate: 0.001
2024-11-05 04:01:58,185 - INFO - [diffusion][Epoch 11106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:58,186 - INFO - [diffusion][Epoch 11107] Epoch 11108/12000
2024-11-05 04:02:02,256 - INFO - [diffusion][Epoch 11107] diffusion training Loss: 0.05969530437141657
2024-11-05 04:02:02,258 - INFO - [diffusion][Epoch 11107] diffusion learning rate: 0.001
2024-11-05 04:02:02,260 - INFO - [diffusion][Epoch 11107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:02,261 - INFO - [diffusion][Epoch 11108] Epoch 11109/12000
2024-11-05 04:02:05,176 - INFO - [diffusion][Epoch 11108] diffusion training Loss: 0.05618277285248041
2024-11-05 04:02:05,178 - INFO - [diffusion][Epoch 11108] diffusion learning rate: 0.001
2024-11-05 04:02:05,180 - INFO - [diffusion][Epoch 11108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:05,181 - INFO - [diffusion][Epoch 11109] Epoch 11110/12000
2024-11-05 04:02:08,057 - INFO - [diffusion][Epoch 11109] diffusion training Loss: 0.057011332362890244
2024-11-05 04:02:08,059 - INFO - [diffusion][Epoch 11109] diffusion learning rate: 0.001
2024-11-05 04:02:08,061 - INFO - [diffusion][Epoch 11109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:08,063 - INFO - [diffusion][Epoch 11110] Epoch 11111/12000
2024-11-05 04:02:10,964 - INFO - [diffusion][Epoch 11110] diffusion training Loss: 0.06296429689973593
2024-11-05 04:02:10,966 - INFO - [diffusion][Epoch 11110] diffusion learning rate: 0.001
2024-11-05 04:02:10,968 - INFO - [diffusion][Epoch 11110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:10,969 - INFO - [diffusion][Epoch 11111] Epoch 11112/12000
2024-11-05 04:02:14,155 - INFO - [diffusion][Epoch 11111] diffusion training Loss: 0.058564722537994385
2024-11-05 04:02:14,158 - INFO - [diffusion][Epoch 11111] diffusion learning rate: 0.001
2024-11-05 04:02:14,160 - INFO - [diffusion][Epoch 11111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:14,161 - INFO - [diffusion][Epoch 11112] Epoch 11113/12000
2024-11-05 04:02:16,849 - INFO - [diffusion][Epoch 11112] diffusion training Loss: 0.052624862641096115
2024-11-05 04:02:16,851 - INFO - [diffusion][Epoch 11112] diffusion learning rate: 0.001
2024-11-05 04:02:16,853 - INFO - [diffusion][Epoch 11112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:16,855 - INFO - [diffusion][Epoch 11113] Epoch 11114/12000
2024-11-05 04:02:19,777 - INFO - [diffusion][Epoch 11113] diffusion training Loss: 0.058885359205305576
2024-11-05 04:02:19,779 - INFO - [diffusion][Epoch 11113] diffusion learning rate: 0.001
2024-11-05 04:02:19,781 - INFO - [diffusion][Epoch 11113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:19,782 - INFO - [diffusion][Epoch 11114] Epoch 11115/12000
2024-11-05 04:02:22,874 - INFO - [diffusion][Epoch 11114] diffusion training Loss: 0.06087483186274767
2024-11-05 04:02:22,876 - INFO - [diffusion][Epoch 11114] diffusion learning rate: 0.001
2024-11-05 04:02:22,878 - INFO - [diffusion][Epoch 11114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:22,879 - INFO - [diffusion][Epoch 11115] Epoch 11116/12000
2024-11-05 04:02:25,716 - INFO - [diffusion][Epoch 11115] diffusion training Loss: 0.05560993403196335
2024-11-05 04:02:25,718 - INFO - [diffusion][Epoch 11115] diffusion learning rate: 0.001
2024-11-05 04:02:25,720 - INFO - [diffusion][Epoch 11115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:25,722 - INFO - [diffusion][Epoch 11116] Epoch 11117/12000
2024-11-05 04:02:28,705 - INFO - [diffusion][Epoch 11116] diffusion training Loss: 0.0575947780162096
2024-11-05 04:02:28,728 - INFO - [diffusion][Epoch 11116] diffusion learning rate: 0.001
2024-11-05 04:02:28,734 - INFO - [diffusion][Epoch 11116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:28,736 - INFO - [diffusion][Epoch 11117] Epoch 11118/12000
2024-11-05 04:02:31,658 - INFO - [diffusion][Epoch 11117] diffusion training Loss: 0.058764005079865456
2024-11-05 04:02:31,660 - INFO - [diffusion][Epoch 11117] diffusion learning rate: 0.001
2024-11-05 04:02:31,662 - INFO - [diffusion][Epoch 11117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:31,663 - INFO - [diffusion][Epoch 11118] Epoch 11119/12000
2024-11-05 04:02:34,617 - INFO - [diffusion][Epoch 11118] diffusion training Loss: 0.062240490689873695
2024-11-05 04:02:34,619 - INFO - [diffusion][Epoch 11118] diffusion learning rate: 0.001
2024-11-05 04:02:34,621 - INFO - [diffusion][Epoch 11118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:34,622 - INFO - [diffusion][Epoch 11119] Epoch 11120/12000
2024-11-05 04:02:37,393 - INFO - [diffusion][Epoch 11119] diffusion training Loss: 0.06428566854447126
2024-11-05 04:02:37,395 - INFO - [diffusion][Epoch 11119] diffusion learning rate: 0.001
2024-11-05 04:02:37,398 - INFO - [diffusion][Epoch 11119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:37,399 - INFO - [diffusion][Epoch 11120] Epoch 11121/12000
2024-11-05 04:02:40,370 - INFO - [diffusion][Epoch 11120] diffusion training Loss: 0.06784811988472939
2024-11-05 04:02:40,372 - INFO - [diffusion][Epoch 11120] diffusion learning rate: 0.001
2024-11-05 04:02:40,374 - INFO - [diffusion][Epoch 11120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:40,375 - INFO - [diffusion][Epoch 11121] Epoch 11122/12000
2024-11-05 04:02:43,260 - INFO - [diffusion][Epoch 11121] diffusion training Loss: 0.059475199319422245
2024-11-05 04:02:43,263 - INFO - [diffusion][Epoch 11121] diffusion learning rate: 0.001
2024-11-05 04:02:43,264 - INFO - [diffusion][Epoch 11121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:43,266 - INFO - [diffusion][Epoch 11122] Epoch 11123/12000
2024-11-05 04:02:46,335 - INFO - [diffusion][Epoch 11122] diffusion training Loss: 0.05862930975854397
2024-11-05 04:02:46,337 - INFO - [diffusion][Epoch 11122] diffusion learning rate: 0.001
2024-11-05 04:02:46,339 - INFO - [diffusion][Epoch 11122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:46,341 - INFO - [diffusion][Epoch 11123] Epoch 11124/12000
2024-11-05 04:02:49,154 - INFO - [diffusion][Epoch 11123] diffusion training Loss: 0.0637204721570015
2024-11-05 04:02:49,156 - INFO - [diffusion][Epoch 11123] diffusion learning rate: 0.001
2024-11-05 04:02:49,195 - INFO - [diffusion][Epoch 11123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:49,197 - INFO - [diffusion][Epoch 11124] Epoch 11125/12000
2024-11-05 04:02:52,565 - INFO - [diffusion][Epoch 11124] diffusion training Loss: 0.06053250469267368
2024-11-05 04:02:52,567 - INFO - [diffusion][Epoch 11124] diffusion learning rate: 0.001
2024-11-05 04:02:52,569 - INFO - [diffusion][Epoch 11124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:52,570 - INFO - [diffusion][Epoch 11125] Epoch 11126/12000
2024-11-05 04:02:55,488 - INFO - [diffusion][Epoch 11125] diffusion training Loss: 0.06009075324982405
2024-11-05 04:02:55,490 - INFO - [diffusion][Epoch 11125] diffusion learning rate: 0.001
2024-11-05 04:02:55,492 - INFO - [diffusion][Epoch 11125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:55,493 - INFO - [diffusion][Epoch 11126] Epoch 11127/12000
2024-11-05 04:02:58,809 - INFO - [diffusion][Epoch 11126] diffusion training Loss: 0.05322251841425896
2024-11-05 04:02:58,811 - INFO - [diffusion][Epoch 11126] diffusion learning rate: 0.001
2024-11-05 04:02:58,813 - INFO - [diffusion][Epoch 11126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:58,814 - INFO - [diffusion][Epoch 11127] Epoch 11128/12000
2024-11-05 04:03:02,056 - INFO - [diffusion][Epoch 11127] diffusion training Loss: 0.06134920101612806
2024-11-05 04:03:02,058 - INFO - [diffusion][Epoch 11127] diffusion learning rate: 0.001
2024-11-05 04:03:02,060 - INFO - [diffusion][Epoch 11127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:02,061 - INFO - [diffusion][Epoch 11128] Epoch 11129/12000
2024-11-05 04:03:06,308 - INFO - [diffusion][Epoch 11128] diffusion training Loss: 0.05324804037809372
2024-11-05 04:03:06,310 - INFO - [diffusion][Epoch 11128] diffusion learning rate: 0.001
2024-11-05 04:03:06,312 - INFO - [diffusion][Epoch 11128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:06,314 - INFO - [diffusion][Epoch 11129] Epoch 11130/12000
2024-11-05 04:03:09,802 - INFO - [diffusion][Epoch 11129] diffusion training Loss: 0.06084380578249693
2024-11-05 04:03:09,805 - INFO - [diffusion][Epoch 11129] diffusion learning rate: 0.001
2024-11-05 04:03:09,844 - INFO - [diffusion][Epoch 11129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:09,846 - INFO - [diffusion][Epoch 11130] Epoch 11131/12000
2024-11-05 04:03:12,668 - INFO - [diffusion][Epoch 11130] diffusion training Loss: 0.0573012875393033
2024-11-05 04:03:12,670 - INFO - [diffusion][Epoch 11130] diffusion learning rate: 0.001
2024-11-05 04:03:12,672 - INFO - [diffusion][Epoch 11130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:12,673 - INFO - [diffusion][Epoch 11131] Epoch 11132/12000
2024-11-05 04:03:16,358 - INFO - [diffusion][Epoch 11131] diffusion training Loss: 0.05427822470664978
2024-11-05 04:03:16,360 - INFO - [diffusion][Epoch 11131] diffusion learning rate: 0.001
2024-11-05 04:03:16,362 - INFO - [diffusion][Epoch 11131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:16,363 - INFO - [diffusion][Epoch 11132] Epoch 11133/12000
2024-11-05 04:03:20,689 - INFO - [diffusion][Epoch 11132] diffusion training Loss: 0.062105920165777206
2024-11-05 04:03:20,692 - INFO - [diffusion][Epoch 11132] diffusion learning rate: 0.001
2024-11-05 04:03:20,699 - INFO - [diffusion][Epoch 11132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:20,700 - INFO - [diffusion][Epoch 11133] Epoch 11134/12000
2024-11-05 04:03:23,652 - INFO - [diffusion][Epoch 11133] diffusion training Loss: 0.05539584718644619
2024-11-05 04:03:23,654 - INFO - [diffusion][Epoch 11133] diffusion learning rate: 0.001
2024-11-05 04:03:23,656 - INFO - [diffusion][Epoch 11133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:23,658 - INFO - [diffusion][Epoch 11134] Epoch 11135/12000
2024-11-05 04:03:27,203 - INFO - [diffusion][Epoch 11134] diffusion training Loss: 0.04989476688206196
2024-11-05 04:03:27,205 - INFO - [diffusion][Epoch 11134] diffusion learning rate: 0.001
2024-11-05 04:03:27,207 - INFO - [diffusion][Epoch 11134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:27,208 - INFO - [diffusion][Epoch 11135] Epoch 11136/12000
2024-11-05 04:03:30,094 - INFO - [diffusion][Epoch 11135] diffusion training Loss: 0.06455498468130827
2024-11-05 04:03:30,095 - INFO - [diffusion][Epoch 11135] diffusion learning rate: 0.001
2024-11-05 04:03:30,097 - INFO - [diffusion][Epoch 11135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:30,098 - INFO - [diffusion][Epoch 11136] Epoch 11137/12000
2024-11-05 04:03:33,129 - INFO - [diffusion][Epoch 11136] diffusion training Loss: 0.05645992048084736
2024-11-05 04:03:33,132 - INFO - [diffusion][Epoch 11136] diffusion learning rate: 0.001
2024-11-05 04:03:33,134 - INFO - [diffusion][Epoch 11136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:33,136 - INFO - [diffusion][Epoch 11137] Epoch 11138/12000
2024-11-05 04:03:37,046 - INFO - [diffusion][Epoch 11137] diffusion training Loss: 0.054740093648433685
2024-11-05 04:03:37,048 - INFO - [diffusion][Epoch 11137] diffusion learning rate: 0.001
2024-11-05 04:03:37,050 - INFO - [diffusion][Epoch 11137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:37,051 - INFO - [diffusion][Epoch 11138] Epoch 11139/12000
2024-11-05 04:03:40,527 - INFO - [diffusion][Epoch 11138] diffusion training Loss: 0.06517967954277992
2024-11-05 04:03:40,529 - INFO - [diffusion][Epoch 11138] diffusion learning rate: 0.001
2024-11-05 04:03:40,568 - INFO - [diffusion][Epoch 11138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:40,569 - INFO - [diffusion][Epoch 11139] Epoch 11140/12000
2024-11-05 04:03:44,044 - INFO - [diffusion][Epoch 11139] diffusion training Loss: 0.05474252067506313
2024-11-05 04:03:44,046 - INFO - [diffusion][Epoch 11139] diffusion learning rate: 0.001
2024-11-05 04:03:44,048 - INFO - [diffusion][Epoch 11139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:44,049 - INFO - [diffusion][Epoch 11140] Epoch 11141/12000
2024-11-05 04:03:47,528 - INFO - [diffusion][Epoch 11140] diffusion training Loss: 0.05382510740309954
2024-11-05 04:03:47,530 - INFO - [diffusion][Epoch 11140] diffusion learning rate: 0.001
2024-11-05 04:03:47,532 - INFO - [diffusion][Epoch 11140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:47,534 - INFO - [diffusion][Epoch 11141] Epoch 11142/12000
2024-11-05 04:03:50,952 - INFO - [diffusion][Epoch 11141] diffusion training Loss: 0.060438040643930435
2024-11-05 04:03:50,954 - INFO - [diffusion][Epoch 11141] diffusion learning rate: 0.001
2024-11-05 04:03:50,955 - INFO - [diffusion][Epoch 11141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:50,957 - INFO - [diffusion][Epoch 11142] Epoch 11143/12000
2024-11-05 04:03:54,488 - INFO - [diffusion][Epoch 11142] diffusion training Loss: 0.05872465297579765
2024-11-05 04:03:54,490 - INFO - [diffusion][Epoch 11142] diffusion learning rate: 0.001
2024-11-05 04:03:54,492 - INFO - [diffusion][Epoch 11142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:54,493 - INFO - [diffusion][Epoch 11143] Epoch 11144/12000
2024-11-05 04:03:58,013 - INFO - [diffusion][Epoch 11143] diffusion training Loss: 0.06253446824848652
2024-11-05 04:03:58,015 - INFO - [diffusion][Epoch 11143] diffusion learning rate: 0.001
2024-11-05 04:03:58,017 - INFO - [diffusion][Epoch 11143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:58,018 - INFO - [diffusion][Epoch 11144] Epoch 11145/12000
2024-11-05 04:04:01,392 - INFO - [diffusion][Epoch 11144] diffusion training Loss: 0.06023393012583256
2024-11-05 04:04:01,394 - INFO - [diffusion][Epoch 11144] diffusion learning rate: 0.001
2024-11-05 04:04:01,396 - INFO - [diffusion][Epoch 11144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:01,397 - INFO - [diffusion][Epoch 11145] Epoch 11146/12000
2024-11-05 04:04:04,093 - INFO - [diffusion][Epoch 11145] diffusion training Loss: 0.057529084384441376
2024-11-05 04:04:04,095 - INFO - [diffusion][Epoch 11145] diffusion learning rate: 0.001
2024-11-05 04:04:04,097 - INFO - [diffusion][Epoch 11145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:04,098 - INFO - [diffusion][Epoch 11146] Epoch 11147/12000
2024-11-05 04:04:07,536 - INFO - [diffusion][Epoch 11146] diffusion training Loss: 0.06191908661276102
2024-11-05 04:04:07,538 - INFO - [diffusion][Epoch 11146] diffusion learning rate: 0.001
2024-11-05 04:04:07,540 - INFO - [diffusion][Epoch 11146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:07,542 - INFO - [diffusion][Epoch 11147] Epoch 11148/12000
2024-11-05 04:04:10,887 - INFO - [diffusion][Epoch 11147] diffusion training Loss: 0.06331463903188705
2024-11-05 04:04:10,890 - INFO - [diffusion][Epoch 11147] diffusion learning rate: 0.001
2024-11-05 04:04:10,892 - INFO - [diffusion][Epoch 11147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:10,893 - INFO - [diffusion][Epoch 11148] Epoch 11149/12000
2024-11-05 04:04:14,178 - INFO - [diffusion][Epoch 11148] diffusion training Loss: 0.059007687494158745
2024-11-05 04:04:14,180 - INFO - [diffusion][Epoch 11148] diffusion learning rate: 0.001
2024-11-05 04:04:14,182 - INFO - [diffusion][Epoch 11148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:14,183 - INFO - [diffusion][Epoch 11149] Epoch 11150/12000
2024-11-05 04:04:17,027 - INFO - [diffusion][Epoch 11149] diffusion training Loss: 0.06376983504742384
2024-11-05 04:04:17,029 - INFO - [diffusion][Epoch 11149] diffusion learning rate: 0.001
2024-11-05 04:04:17,031 - INFO - [diffusion][Epoch 11149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:17,032 - INFO - [diffusion][Epoch 11150] Epoch 11151/12000
2024-11-05 04:04:20,291 - INFO - [diffusion][Epoch 11150] diffusion training Loss: 0.0595222394913435
2024-11-05 04:04:20,294 - INFO - [diffusion][Epoch 11150] diffusion learning rate: 0.001
2024-11-05 04:04:20,297 - INFO - [diffusion][Epoch 11150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:20,299 - INFO - [diffusion][Epoch 11151] Epoch 11152/12000
2024-11-05 04:04:23,218 - INFO - [diffusion][Epoch 11151] diffusion training Loss: 0.061160807497799397
2024-11-05 04:04:23,221 - INFO - [diffusion][Epoch 11151] diffusion learning rate: 0.001
2024-11-05 04:04:23,223 - INFO - [diffusion][Epoch 11151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:23,225 - INFO - [diffusion][Epoch 11152] Epoch 11153/12000
2024-11-05 04:04:26,501 - INFO - [diffusion][Epoch 11152] diffusion training Loss: 0.05792383197695017
2024-11-05 04:04:26,503 - INFO - [diffusion][Epoch 11152] diffusion learning rate: 0.001
2024-11-05 04:04:26,505 - INFO - [diffusion][Epoch 11152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:26,506 - INFO - [diffusion][Epoch 11153] Epoch 11154/12000
2024-11-05 04:04:29,318 - INFO - [diffusion][Epoch 11153] diffusion training Loss: 0.05694359540939331
2024-11-05 04:04:29,320 - INFO - [diffusion][Epoch 11153] diffusion learning rate: 0.001
2024-11-05 04:04:29,322 - INFO - [diffusion][Epoch 11153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:29,323 - INFO - [diffusion][Epoch 11154] Epoch 11155/12000
2024-11-05 04:04:32,556 - INFO - [diffusion][Epoch 11154] diffusion training Loss: 0.06225923169404268
2024-11-05 04:04:32,558 - INFO - [diffusion][Epoch 11154] diffusion learning rate: 0.001
2024-11-05 04:04:32,560 - INFO - [diffusion][Epoch 11154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:32,561 - INFO - [diffusion][Epoch 11155] Epoch 11156/12000
2024-11-05 04:04:35,445 - INFO - [diffusion][Epoch 11155] diffusion training Loss: 0.05301581509411335
2024-11-05 04:04:35,447 - INFO - [diffusion][Epoch 11155] diffusion learning rate: 0.001
2024-11-05 04:04:35,449 - INFO - [diffusion][Epoch 11155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:35,451 - INFO - [diffusion][Epoch 11156] Epoch 11157/12000
2024-11-05 04:04:38,454 - INFO - [diffusion][Epoch 11156] diffusion training Loss: 0.05727169569581747
2024-11-05 04:04:38,456 - INFO - [diffusion][Epoch 11156] diffusion learning rate: 0.001
2024-11-05 04:04:38,458 - INFO - [diffusion][Epoch 11156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:38,459 - INFO - [diffusion][Epoch 11157] Epoch 11158/12000
2024-11-05 04:04:41,428 - INFO - [diffusion][Epoch 11157] diffusion training Loss: 0.05374826770275831
2024-11-05 04:04:41,430 - INFO - [diffusion][Epoch 11157] diffusion learning rate: 0.001
2024-11-05 04:04:41,432 - INFO - [diffusion][Epoch 11157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:41,433 - INFO - [diffusion][Epoch 11158] Epoch 11159/12000
2024-11-05 04:04:44,614 - INFO - [diffusion][Epoch 11158] diffusion training Loss: 0.06494014896452427
2024-11-05 04:04:44,616 - INFO - [diffusion][Epoch 11158] diffusion learning rate: 0.001
2024-11-05 04:04:44,617 - INFO - [diffusion][Epoch 11158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:44,619 - INFO - [diffusion][Epoch 11159] Epoch 11160/12000
2024-11-05 04:04:47,492 - INFO - [diffusion][Epoch 11159] diffusion training Loss: 0.05632295273244381
2024-11-05 04:04:47,494 - INFO - [diffusion][Epoch 11159] diffusion learning rate: 0.001
2024-11-05 04:04:47,496 - INFO - [diffusion][Epoch 11159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:47,498 - INFO - [diffusion][Epoch 11160] Epoch 11161/12000
2024-11-05 04:04:50,796 - INFO - [diffusion][Epoch 11160] diffusion training Loss: 0.05697336886078119
2024-11-05 04:04:50,799 - INFO - [diffusion][Epoch 11160] diffusion learning rate: 0.001
2024-11-05 04:04:50,801 - INFO - [diffusion][Epoch 11160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:50,802 - INFO - [diffusion][Epoch 11161] Epoch 11162/12000
2024-11-05 04:04:53,736 - INFO - [diffusion][Epoch 11161] diffusion training Loss: 0.05673355236649513
2024-11-05 04:04:53,738 - INFO - [diffusion][Epoch 11161] diffusion learning rate: 0.001
2024-11-05 04:04:53,740 - INFO - [diffusion][Epoch 11161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:53,742 - INFO - [diffusion][Epoch 11162] Epoch 11163/12000
2024-11-05 04:04:57,463 - INFO - [diffusion][Epoch 11162] diffusion training Loss: 0.06319504044950008
2024-11-05 04:04:57,466 - INFO - [diffusion][Epoch 11162] diffusion learning rate: 0.001
2024-11-05 04:04:57,468 - INFO - [diffusion][Epoch 11162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:57,469 - INFO - [diffusion][Epoch 11163] Epoch 11164/12000
2024-11-05 04:05:01,005 - INFO - [diffusion][Epoch 11163] diffusion training Loss: 0.05351834651082754
2024-11-05 04:05:01,007 - INFO - [diffusion][Epoch 11163] diffusion learning rate: 0.001
2024-11-05 04:05:01,009 - INFO - [diffusion][Epoch 11163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:01,010 - INFO - [diffusion][Epoch 11164] Epoch 11165/12000
2024-11-05 04:05:03,917 - INFO - [diffusion][Epoch 11164] diffusion training Loss: 0.062403446063399315
2024-11-05 04:05:03,919 - INFO - [diffusion][Epoch 11164] diffusion learning rate: 0.001
2024-11-05 04:05:03,921 - INFO - [diffusion][Epoch 11164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:03,922 - INFO - [diffusion][Epoch 11165] Epoch 11166/12000
2024-11-05 04:05:06,778 - INFO - [diffusion][Epoch 11165] diffusion training Loss: 0.054238018579781055
2024-11-05 04:05:06,780 - INFO - [diffusion][Epoch 11165] diffusion learning rate: 0.001
2024-11-05 04:05:06,782 - INFO - [diffusion][Epoch 11165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:06,784 - INFO - [diffusion][Epoch 11166] Epoch 11167/12000
2024-11-05 04:05:11,407 - INFO - [diffusion][Epoch 11166] diffusion training Loss: 0.0637348871678114
2024-11-05 04:05:11,410 - INFO - [diffusion][Epoch 11166] diffusion learning rate: 0.001
2024-11-05 04:05:11,412 - INFO - [diffusion][Epoch 11166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:11,414 - INFO - [diffusion][Epoch 11167] Epoch 11168/12000
2024-11-05 04:05:14,319 - INFO - [diffusion][Epoch 11167] diffusion training Loss: 0.06583842542022467
2024-11-05 04:05:14,321 - INFO - [diffusion][Epoch 11167] diffusion learning rate: 0.001
2024-11-05 04:05:14,323 - INFO - [diffusion][Epoch 11167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:14,324 - INFO - [diffusion][Epoch 11168] Epoch 11169/12000
2024-11-05 04:05:17,534 - INFO - [diffusion][Epoch 11168] diffusion training Loss: 0.06185732688754797
2024-11-05 04:05:17,536 - INFO - [diffusion][Epoch 11168] diffusion learning rate: 0.001
2024-11-05 04:05:17,538 - INFO - [diffusion][Epoch 11168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:17,539 - INFO - [diffusion][Epoch 11169] Epoch 11170/12000
2024-11-05 04:05:21,170 - INFO - [diffusion][Epoch 11169] diffusion training Loss: 0.06037697196006775
2024-11-05 04:05:21,172 - INFO - [diffusion][Epoch 11169] diffusion learning rate: 0.001
2024-11-05 04:05:21,174 - INFO - [diffusion][Epoch 11169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:21,176 - INFO - [diffusion][Epoch 11170] Epoch 11171/12000
2024-11-05 04:05:24,069 - INFO - [diffusion][Epoch 11170] diffusion training Loss: 0.06217971257865429
2024-11-05 04:05:24,071 - INFO - [diffusion][Epoch 11170] diffusion learning rate: 0.001
2024-11-05 04:05:24,073 - INFO - [diffusion][Epoch 11170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:24,074 - INFO - [diffusion][Epoch 11171] Epoch 11172/12000
2024-11-05 04:05:27,711 - INFO - [diffusion][Epoch 11171] diffusion training Loss: 0.05884366296231747
2024-11-05 04:05:27,713 - INFO - [diffusion][Epoch 11171] diffusion learning rate: 0.001
2024-11-05 04:05:27,716 - INFO - [diffusion][Epoch 11171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:27,717 - INFO - [diffusion][Epoch 11172] Epoch 11173/12000
2024-11-05 04:05:31,811 - INFO - [diffusion][Epoch 11172] diffusion training Loss: 0.059253730811178684
2024-11-05 04:05:31,813 - INFO - [diffusion][Epoch 11172] diffusion learning rate: 0.001
2024-11-05 04:05:31,816 - INFO - [diffusion][Epoch 11172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:31,817 - INFO - [diffusion][Epoch 11173] Epoch 11174/12000
2024-11-05 04:05:35,317 - INFO - [diffusion][Epoch 11173] diffusion training Loss: 0.0660761259496212
2024-11-05 04:05:35,320 - INFO - [diffusion][Epoch 11173] diffusion learning rate: 0.001
2024-11-05 04:05:35,321 - INFO - [diffusion][Epoch 11173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:35,323 - INFO - [diffusion][Epoch 11174] Epoch 11175/12000
2024-11-05 04:05:38,763 - INFO - [diffusion][Epoch 11174] diffusion training Loss: 0.0692011546343565
2024-11-05 04:05:38,765 - INFO - [diffusion][Epoch 11174] diffusion learning rate: 0.001
2024-11-05 04:05:38,786 - INFO - [diffusion][Epoch 11174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:38,787 - INFO - [diffusion][Epoch 11175] Epoch 11176/12000
2024-11-05 04:05:42,247 - INFO - [diffusion][Epoch 11175] diffusion training Loss: 0.06022742763161659
2024-11-05 04:05:42,251 - INFO - [diffusion][Epoch 11175] diffusion learning rate: 0.001
2024-11-05 04:05:42,253 - INFO - [diffusion][Epoch 11175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:42,254 - INFO - [diffusion][Epoch 11176] Epoch 11177/12000
2024-11-05 04:05:45,706 - INFO - [diffusion][Epoch 11176] diffusion training Loss: 0.06001371331512928
2024-11-05 04:05:45,709 - INFO - [diffusion][Epoch 11176] diffusion learning rate: 0.001
2024-11-05 04:05:45,711 - INFO - [diffusion][Epoch 11176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:45,712 - INFO - [diffusion][Epoch 11177] Epoch 11178/12000
2024-11-05 04:05:49,131 - INFO - [diffusion][Epoch 11177] diffusion training Loss: 0.05419886764138937
2024-11-05 04:05:49,135 - INFO - [diffusion][Epoch 11177] diffusion learning rate: 0.001
2024-11-05 04:05:49,138 - INFO - [diffusion][Epoch 11177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:49,139 - INFO - [diffusion][Epoch 11178] Epoch 11179/12000
2024-11-05 04:05:52,717 - INFO - [diffusion][Epoch 11178] diffusion training Loss: 0.056282419711351395
2024-11-05 04:05:52,719 - INFO - [diffusion][Epoch 11178] diffusion learning rate: 0.001
2024-11-05 04:05:52,721 - INFO - [diffusion][Epoch 11178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:52,723 - INFO - [diffusion][Epoch 11179] Epoch 11180/12000
2024-11-05 04:05:55,635 - INFO - [diffusion][Epoch 11179] diffusion training Loss: 0.05828553065657616
2024-11-05 04:05:55,637 - INFO - [diffusion][Epoch 11179] diffusion learning rate: 0.001
2024-11-05 04:05:55,639 - INFO - [diffusion][Epoch 11179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:55,640 - INFO - [diffusion][Epoch 11180] Epoch 11181/12000
2024-11-05 04:05:58,799 - INFO - [diffusion][Epoch 11180] diffusion training Loss: 0.060538630932569504
2024-11-05 04:05:58,801 - INFO - [diffusion][Epoch 11180] diffusion learning rate: 0.001
2024-11-05 04:05:58,802 - INFO - [diffusion][Epoch 11180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:58,803 - INFO - [diffusion][Epoch 11181] Epoch 11182/12000
2024-11-05 04:06:01,519 - INFO - [diffusion][Epoch 11181] diffusion training Loss: 0.05600312910974026
2024-11-05 04:06:01,522 - INFO - [diffusion][Epoch 11181] diffusion learning rate: 0.001
2024-11-05 04:06:01,523 - INFO - [diffusion][Epoch 11181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:01,525 - INFO - [diffusion][Epoch 11182] Epoch 11183/12000
2024-11-05 04:06:04,436 - INFO - [diffusion][Epoch 11182] diffusion training Loss: 0.05891568958759308
2024-11-05 04:06:04,438 - INFO - [diffusion][Epoch 11182] diffusion learning rate: 0.001
2024-11-05 04:06:04,440 - INFO - [diffusion][Epoch 11182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:04,441 - INFO - [diffusion][Epoch 11183] Epoch 11184/12000
2024-11-05 04:06:07,415 - INFO - [diffusion][Epoch 11183] diffusion training Loss: 0.057512751780450344
2024-11-05 04:06:07,417 - INFO - [diffusion][Epoch 11183] diffusion learning rate: 0.001
2024-11-05 04:06:07,418 - INFO - [diffusion][Epoch 11183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:07,420 - INFO - [diffusion][Epoch 11184] Epoch 11185/12000
2024-11-05 04:06:10,319 - INFO - [diffusion][Epoch 11184] diffusion training Loss: 0.06086653470993042
2024-11-05 04:06:10,322 - INFO - [diffusion][Epoch 11184] diffusion learning rate: 0.001
2024-11-05 04:06:10,324 - INFO - [diffusion][Epoch 11184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:10,325 - INFO - [diffusion][Epoch 11185] Epoch 11186/12000
2024-11-05 04:06:13,482 - INFO - [diffusion][Epoch 11185] diffusion training Loss: 0.058281552977859974
2024-11-05 04:06:13,485 - INFO - [diffusion][Epoch 11185] diffusion learning rate: 0.001
2024-11-05 04:06:13,487 - INFO - [diffusion][Epoch 11185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:13,488 - INFO - [diffusion][Epoch 11186] Epoch 11187/12000
2024-11-05 04:06:16,327 - INFO - [diffusion][Epoch 11186] diffusion training Loss: 0.05596392601728439
2024-11-05 04:06:16,330 - INFO - [diffusion][Epoch 11186] diffusion learning rate: 0.001
2024-11-05 04:06:16,331 - INFO - [diffusion][Epoch 11186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:16,333 - INFO - [diffusion][Epoch 11187] Epoch 11188/12000
2024-11-05 04:06:19,530 - INFO - [diffusion][Epoch 11187] diffusion training Loss: 0.054723403416574
2024-11-05 04:06:19,532 - INFO - [diffusion][Epoch 11187] diffusion learning rate: 0.001
2024-11-05 04:06:19,534 - INFO - [diffusion][Epoch 11187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:19,535 - INFO - [diffusion][Epoch 11188] Epoch 11189/12000
2024-11-05 04:06:22,690 - INFO - [diffusion][Epoch 11188] diffusion training Loss: 0.05843866989016533
2024-11-05 04:06:22,692 - INFO - [diffusion][Epoch 11188] diffusion learning rate: 0.001
2024-11-05 04:06:22,694 - INFO - [diffusion][Epoch 11188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:22,695 - INFO - [diffusion][Epoch 11189] Epoch 11190/12000
2024-11-05 04:06:25,719 - INFO - [diffusion][Epoch 11189] diffusion training Loss: 0.06187147833406925
2024-11-05 04:06:25,721 - INFO - [diffusion][Epoch 11189] diffusion learning rate: 0.001
2024-11-05 04:06:25,723 - INFO - [diffusion][Epoch 11189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:25,724 - INFO - [diffusion][Epoch 11190] Epoch 11191/12000
2024-11-05 04:06:28,606 - INFO - [diffusion][Epoch 11190] diffusion training Loss: 0.059284357354044914
2024-11-05 04:06:28,608 - INFO - [diffusion][Epoch 11190] diffusion learning rate: 0.001
2024-11-05 04:06:28,635 - INFO - [diffusion][Epoch 11190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:28,637 - INFO - [diffusion][Epoch 11191] Epoch 11192/12000
2024-11-05 04:06:31,719 - INFO - [diffusion][Epoch 11191] diffusion training Loss: 0.05717750173062086
2024-11-05 04:06:31,721 - INFO - [diffusion][Epoch 11191] diffusion learning rate: 0.001
2024-11-05 04:06:31,723 - INFO - [diffusion][Epoch 11191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:31,724 - INFO - [diffusion][Epoch 11192] Epoch 11193/12000
2024-11-05 04:06:34,615 - INFO - [diffusion][Epoch 11192] diffusion training Loss: 0.05721024703234434
2024-11-05 04:06:34,618 - INFO - [diffusion][Epoch 11192] diffusion learning rate: 0.001
2024-11-05 04:06:34,620 - INFO - [diffusion][Epoch 11192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:34,621 - INFO - [diffusion][Epoch 11193] Epoch 11194/12000
2024-11-05 04:06:38,088 - INFO - [diffusion][Epoch 11193] diffusion training Loss: 0.057610293850302696
2024-11-05 04:06:38,090 - INFO - [diffusion][Epoch 11193] diffusion learning rate: 0.001
2024-11-05 04:06:38,092 - INFO - [diffusion][Epoch 11193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:38,093 - INFO - [diffusion][Epoch 11194] Epoch 11195/12000
2024-11-05 04:06:41,036 - INFO - [diffusion][Epoch 11194] diffusion training Loss: 0.05820255167782307
2024-11-05 04:06:41,038 - INFO - [diffusion][Epoch 11194] diffusion learning rate: 0.001
2024-11-05 04:06:41,040 - INFO - [diffusion][Epoch 11194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:41,042 - INFO - [diffusion][Epoch 11195] Epoch 11196/12000
2024-11-05 04:06:44,491 - INFO - [diffusion][Epoch 11195] diffusion training Loss: 0.05978893209248781
2024-11-05 04:06:44,494 - INFO - [diffusion][Epoch 11195] diffusion learning rate: 0.001
2024-11-05 04:06:44,496 - INFO - [diffusion][Epoch 11195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:44,497 - INFO - [diffusion][Epoch 11196] Epoch 11197/12000
2024-11-05 04:06:47,374 - INFO - [diffusion][Epoch 11196] diffusion training Loss: 0.059659216552972794
2024-11-05 04:06:47,376 - INFO - [diffusion][Epoch 11196] diffusion learning rate: 0.001
2024-11-05 04:06:47,378 - INFO - [diffusion][Epoch 11196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:47,380 - INFO - [diffusion][Epoch 11197] Epoch 11198/12000
2024-11-05 04:06:50,434 - INFO - [diffusion][Epoch 11197] diffusion training Loss: 0.058444637805223465
2024-11-05 04:06:50,437 - INFO - [diffusion][Epoch 11197] diffusion learning rate: 0.001
2024-11-05 04:06:50,438 - INFO - [diffusion][Epoch 11197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:50,440 - INFO - [diffusion][Epoch 11198] Epoch 11199/12000
2024-11-05 04:06:54,432 - INFO - [diffusion][Epoch 11198] diffusion training Loss: 0.05880584940314293
2024-11-05 04:06:54,434 - INFO - [diffusion][Epoch 11198] diffusion learning rate: 0.001
2024-11-05 04:06:54,436 - INFO - [diffusion][Epoch 11198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:54,437 - INFO - [diffusion][Epoch 11199] Epoch 11200/12000
2024-11-05 04:06:57,294 - INFO - [diffusion][Epoch 11199] diffusion training Loss: 0.05567267723381519
2024-11-05 04:06:57,295 - INFO - [diffusion][Epoch 11199] diffusion learning rate: 0.001
2024-11-05 04:06:57,297 - INFO - [diffusion][Epoch 11199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:57,298 - INFO - [diffusion][Epoch 11200] Epoch 11201/12000
2024-11-05 04:07:00,183 - INFO - [diffusion][Epoch 11200] diffusion training Loss: 0.05740027129650116
2024-11-05 04:07:00,185 - INFO - [diffusion][Epoch 11200] diffusion learning rate: 0.001
2024-11-05 04:07:00,187 - INFO - [diffusion][Epoch 11200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:00,188 - INFO - [diffusion][Epoch 11201] Epoch 11202/12000
2024-11-05 04:07:04,182 - INFO - [diffusion][Epoch 11201] diffusion training Loss: 0.06151030585169792
2024-11-05 04:07:04,183 - INFO - [diffusion][Epoch 11201] diffusion learning rate: 0.001
2024-11-05 04:07:04,185 - INFO - [diffusion][Epoch 11201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:04,186 - INFO - [diffusion][Epoch 11202] Epoch 11203/12000
2024-11-05 04:07:07,659 - INFO - [diffusion][Epoch 11202] diffusion training Loss: 0.06019205413758755
2024-11-05 04:07:07,661 - INFO - [diffusion][Epoch 11202] diffusion learning rate: 0.001
2024-11-05 04:07:07,663 - INFO - [diffusion][Epoch 11202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:07,665 - INFO - [diffusion][Epoch 11203] Epoch 11204/12000
2024-11-05 04:07:10,512 - INFO - [diffusion][Epoch 11203] diffusion training Loss: 0.05998463183641434
2024-11-05 04:07:10,514 - INFO - [diffusion][Epoch 11203] diffusion learning rate: 0.001
2024-11-05 04:07:10,516 - INFO - [diffusion][Epoch 11203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:10,517 - INFO - [diffusion][Epoch 11204] Epoch 11205/12000
2024-11-05 04:07:14,006 - INFO - [diffusion][Epoch 11204] diffusion training Loss: 0.06575911678373814
2024-11-05 04:07:14,008 - INFO - [diffusion][Epoch 11204] diffusion learning rate: 0.001
2024-11-05 04:07:14,009 - INFO - [diffusion][Epoch 11204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:14,010 - INFO - [diffusion][Epoch 11205] Epoch 11206/12000
2024-11-05 04:07:16,783 - INFO - [diffusion][Epoch 11205] diffusion training Loss: 0.06263483688235283
2024-11-05 04:07:16,799 - INFO - [diffusion][Epoch 11205] diffusion learning rate: 0.001
2024-11-05 04:07:16,800 - INFO - [diffusion][Epoch 11205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:16,801 - INFO - [diffusion][Epoch 11206] Epoch 11207/12000
2024-11-05 04:07:20,447 - INFO - [diffusion][Epoch 11206] diffusion training Loss: 0.060964012518525124
2024-11-05 04:07:20,449 - INFO - [diffusion][Epoch 11206] diffusion learning rate: 0.001
2024-11-05 04:07:20,450 - INFO - [diffusion][Epoch 11206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:20,451 - INFO - [diffusion][Epoch 11207] Epoch 11208/12000
2024-11-05 04:07:24,716 - INFO - [diffusion][Epoch 11207] diffusion training Loss: 0.058273508213460445
2024-11-05 04:07:24,719 - INFO - [diffusion][Epoch 11207] diffusion learning rate: 0.001
2024-11-05 04:07:24,721 - INFO - [diffusion][Epoch 11207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:24,722 - INFO - [diffusion][Epoch 11208] Epoch 11209/12000
2024-11-05 04:07:28,237 - INFO - [diffusion][Epoch 11208] diffusion training Loss: 0.05778977740556002
2024-11-05 04:07:28,239 - INFO - [diffusion][Epoch 11208] diffusion learning rate: 0.001
2024-11-05 04:07:28,241 - INFO - [diffusion][Epoch 11208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:28,242 - INFO - [diffusion][Epoch 11209] Epoch 11210/12000
2024-11-05 04:07:32,295 - INFO - [diffusion][Epoch 11209] diffusion training Loss: 0.062124425545334816
2024-11-05 04:07:32,297 - INFO - [diffusion][Epoch 11209] diffusion learning rate: 0.001
2024-11-05 04:07:32,299 - INFO - [diffusion][Epoch 11209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:32,300 - INFO - [diffusion][Epoch 11210] Epoch 11211/12000
2024-11-05 04:07:35,838 - INFO - [diffusion][Epoch 11210] diffusion training Loss: 0.05435921438038349
2024-11-05 04:07:35,840 - INFO - [diffusion][Epoch 11210] diffusion learning rate: 0.001
2024-11-05 04:07:35,842 - INFO - [diffusion][Epoch 11210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:35,844 - INFO - [diffusion][Epoch 11211] Epoch 11212/12000
2024-11-05 04:07:39,343 - INFO - [diffusion][Epoch 11211] diffusion training Loss: 0.0614454448223114
2024-11-05 04:07:39,345 - INFO - [diffusion][Epoch 11211] diffusion learning rate: 0.001
2024-11-05 04:07:39,347 - INFO - [diffusion][Epoch 11211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:39,348 - INFO - [diffusion][Epoch 11212] Epoch 11213/12000
2024-11-05 04:07:42,865 - INFO - [diffusion][Epoch 11212] diffusion training Loss: 0.062107810750603676
2024-11-05 04:07:42,867 - INFO - [diffusion][Epoch 11212] diffusion learning rate: 0.001
2024-11-05 04:07:42,869 - INFO - [diffusion][Epoch 11212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:42,870 - INFO - [diffusion][Epoch 11213] Epoch 11214/12000
2024-11-05 04:07:46,247 - INFO - [diffusion][Epoch 11213] diffusion training Loss: 0.05881212651729584
2024-11-05 04:07:46,249 - INFO - [diffusion][Epoch 11213] diffusion learning rate: 0.001
2024-11-05 04:07:46,251 - INFO - [diffusion][Epoch 11213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:46,252 - INFO - [diffusion][Epoch 11214] Epoch 11215/12000
2024-11-05 04:07:49,206 - INFO - [diffusion][Epoch 11214] diffusion training Loss: 0.06538874469697475
2024-11-05 04:07:49,208 - INFO - [diffusion][Epoch 11214] diffusion learning rate: 0.001
2024-11-05 04:07:49,211 - INFO - [diffusion][Epoch 11214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:49,212 - INFO - [diffusion][Epoch 11215] Epoch 11216/12000
2024-11-05 04:07:52,089 - INFO - [diffusion][Epoch 11215] diffusion training Loss: 0.054316979832947254
2024-11-05 04:07:52,092 - INFO - [diffusion][Epoch 11215] diffusion learning rate: 0.001
2024-11-05 04:07:52,094 - INFO - [diffusion][Epoch 11215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:52,095 - INFO - [diffusion][Epoch 11216] Epoch 11217/12000
2024-11-05 04:07:54,926 - INFO - [diffusion][Epoch 11216] diffusion training Loss: 0.056523894891142845
2024-11-05 04:07:54,928 - INFO - [diffusion][Epoch 11216] diffusion learning rate: 0.001
2024-11-05 04:07:54,930 - INFO - [diffusion][Epoch 11216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:54,931 - INFO - [diffusion][Epoch 11217] Epoch 11218/12000
2024-11-05 04:07:57,872 - INFO - [diffusion][Epoch 11217] diffusion training Loss: 0.05896150227636099
2024-11-05 04:07:57,874 - INFO - [diffusion][Epoch 11217] diffusion learning rate: 0.001
2024-11-05 04:07:57,876 - INFO - [diffusion][Epoch 11217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:57,877 - INFO - [diffusion][Epoch 11218] Epoch 11219/12000
2024-11-05 04:08:01,219 - INFO - [diffusion][Epoch 11218] diffusion training Loss: 0.061096036806702614
2024-11-05 04:08:01,221 - INFO - [diffusion][Epoch 11218] diffusion learning rate: 0.001
2024-11-05 04:08:01,223 - INFO - [diffusion][Epoch 11218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:01,224 - INFO - [diffusion][Epoch 11219] Epoch 11220/12000
2024-11-05 04:08:04,070 - INFO - [diffusion][Epoch 11219] diffusion training Loss: 0.05479732993990183
2024-11-05 04:08:04,072 - INFO - [diffusion][Epoch 11219] diffusion learning rate: 0.001
2024-11-05 04:08:04,074 - INFO - [diffusion][Epoch 11219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:04,076 - INFO - [diffusion][Epoch 11220] Epoch 11221/12000
2024-11-05 04:08:07,158 - INFO - [diffusion][Epoch 11220] diffusion training Loss: 0.052719252184033394
2024-11-05 04:08:07,160 - INFO - [diffusion][Epoch 11220] diffusion learning rate: 0.001
2024-11-05 04:08:07,161 - INFO - [diffusion][Epoch 11220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:07,163 - INFO - [diffusion][Epoch 11221] Epoch 11222/12000
2024-11-05 04:08:09,943 - INFO - [diffusion][Epoch 11221] diffusion training Loss: 0.060853530652821064
2024-11-05 04:08:09,945 - INFO - [diffusion][Epoch 11221] diffusion learning rate: 0.001
2024-11-05 04:08:09,947 - INFO - [diffusion][Epoch 11221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:09,948 - INFO - [diffusion][Epoch 11222] Epoch 11223/12000
2024-11-05 04:08:12,850 - INFO - [diffusion][Epoch 11222] diffusion training Loss: 0.055898698046803474
2024-11-05 04:08:12,853 - INFO - [diffusion][Epoch 11222] diffusion learning rate: 0.001
2024-11-05 04:08:12,892 - INFO - [diffusion][Epoch 11222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:12,894 - INFO - [diffusion][Epoch 11223] Epoch 11224/12000
2024-11-05 04:08:15,872 - INFO - [diffusion][Epoch 11223] diffusion training Loss: 0.05383148230612278
2024-11-05 04:08:15,875 - INFO - [diffusion][Epoch 11223] diffusion learning rate: 0.001
2024-11-05 04:08:15,876 - INFO - [diffusion][Epoch 11223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:15,878 - INFO - [diffusion][Epoch 11224] Epoch 11225/12000
2024-11-05 04:08:18,794 - INFO - [diffusion][Epoch 11224] diffusion training Loss: 0.05856835097074509
2024-11-05 04:08:18,796 - INFO - [diffusion][Epoch 11224] diffusion learning rate: 0.001
2024-11-05 04:08:18,798 - INFO - [diffusion][Epoch 11224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:18,799 - INFO - [diffusion][Epoch 11225] Epoch 11226/12000
2024-11-05 04:08:21,873 - INFO - [diffusion][Epoch 11225] diffusion training Loss: 0.0583477858453989
2024-11-05 04:08:21,876 - INFO - [diffusion][Epoch 11225] diffusion learning rate: 0.001
2024-11-05 04:08:21,877 - INFO - [diffusion][Epoch 11225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:21,879 - INFO - [diffusion][Epoch 11226] Epoch 11227/12000
2024-11-05 04:08:24,701 - INFO - [diffusion][Epoch 11226] diffusion training Loss: 0.05689102876931429
2024-11-05 04:08:24,703 - INFO - [diffusion][Epoch 11226] diffusion learning rate: 0.001
2024-11-05 04:08:24,706 - INFO - [diffusion][Epoch 11226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:24,707 - INFO - [diffusion][Epoch 11227] Epoch 11228/12000
2024-11-05 04:08:27,979 - INFO - [diffusion][Epoch 11227] diffusion training Loss: 0.057387763634324074
2024-11-05 04:08:27,981 - INFO - [diffusion][Epoch 11227] diffusion learning rate: 0.001
2024-11-05 04:08:27,983 - INFO - [diffusion][Epoch 11227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:27,984 - INFO - [diffusion][Epoch 11228] Epoch 11229/12000
2024-11-05 04:08:30,892 - INFO - [diffusion][Epoch 11228] diffusion training Loss: 0.05862026195973158
2024-11-05 04:08:30,895 - INFO - [diffusion][Epoch 11228] diffusion learning rate: 0.001
2024-11-05 04:08:30,897 - INFO - [diffusion][Epoch 11228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:30,898 - INFO - [diffusion][Epoch 11229] Epoch 11230/12000
2024-11-05 04:08:34,315 - INFO - [diffusion][Epoch 11229] diffusion training Loss: 0.06164845358580351
2024-11-05 04:08:34,317 - INFO - [diffusion][Epoch 11229] diffusion learning rate: 0.001
2024-11-05 04:08:34,318 - INFO - [diffusion][Epoch 11229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:34,320 - INFO - [diffusion][Epoch 11230] Epoch 11231/12000
2024-11-05 04:08:37,167 - INFO - [diffusion][Epoch 11230] diffusion training Loss: 0.05614051688462496
2024-11-05 04:08:37,169 - INFO - [diffusion][Epoch 11230] diffusion learning rate: 0.001
2024-11-05 04:08:37,171 - INFO - [diffusion][Epoch 11230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:37,172 - INFO - [diffusion][Epoch 11231] Epoch 11232/12000
2024-11-05 04:08:40,478 - INFO - [diffusion][Epoch 11231] diffusion training Loss: 0.06086835823953152
2024-11-05 04:08:40,480 - INFO - [diffusion][Epoch 11231] diffusion learning rate: 0.001
2024-11-05 04:08:40,482 - INFO - [diffusion][Epoch 11231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:40,483 - INFO - [diffusion][Epoch 11232] Epoch 11233/12000
2024-11-05 04:08:43,316 - INFO - [diffusion][Epoch 11232] diffusion training Loss: 0.060122485272586346
2024-11-05 04:08:43,318 - INFO - [diffusion][Epoch 11232] diffusion learning rate: 0.001
2024-11-05 04:08:43,320 - INFO - [diffusion][Epoch 11232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:43,321 - INFO - [diffusion][Epoch 11233] Epoch 11234/12000
2024-11-05 04:08:47,079 - INFO - [diffusion][Epoch 11233] diffusion training Loss: 0.05932367313653231
2024-11-05 04:08:47,081 - INFO - [diffusion][Epoch 11233] diffusion learning rate: 0.001
2024-11-05 04:08:47,083 - INFO - [diffusion][Epoch 11233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:47,084 - INFO - [diffusion][Epoch 11234] Epoch 11235/12000
2024-11-05 04:08:51,342 - INFO - [diffusion][Epoch 11234] diffusion training Loss: 0.058089845813810825
2024-11-05 04:08:51,344 - INFO - [diffusion][Epoch 11234] diffusion learning rate: 0.001
2024-11-05 04:08:51,346 - INFO - [diffusion][Epoch 11234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:51,347 - INFO - [diffusion][Epoch 11235] Epoch 11236/12000
2024-11-05 04:08:54,201 - INFO - [diffusion][Epoch 11235] diffusion training Loss: 0.058247173205018044
2024-11-05 04:08:54,204 - INFO - [diffusion][Epoch 11235] diffusion learning rate: 0.001
2024-11-05 04:08:54,206 - INFO - [diffusion][Epoch 11235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:54,207 - INFO - [diffusion][Epoch 11236] Epoch 11237/12000
2024-11-05 04:08:57,505 - INFO - [diffusion][Epoch 11236] diffusion training Loss: 0.057894096709787846
2024-11-05 04:08:57,507 - INFO - [diffusion][Epoch 11236] diffusion learning rate: 0.001
2024-11-05 04:08:57,529 - INFO - [diffusion][Epoch 11236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:57,531 - INFO - [diffusion][Epoch 11237] Epoch 11238/12000
2024-11-05 04:09:01,240 - INFO - [diffusion][Epoch 11237] diffusion training Loss: 0.057480646297335625
2024-11-05 04:09:01,242 - INFO - [diffusion][Epoch 11237] diffusion learning rate: 0.001
2024-11-05 04:09:01,244 - INFO - [diffusion][Epoch 11237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:01,245 - INFO - [diffusion][Epoch 11238] Epoch 11239/12000
2024-11-05 04:09:04,189 - INFO - [diffusion][Epoch 11238] diffusion training Loss: 0.060352712869644165
2024-11-05 04:09:04,191 - INFO - [diffusion][Epoch 11238] diffusion learning rate: 0.001
2024-11-05 04:09:04,193 - INFO - [diffusion][Epoch 11238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:04,194 - INFO - [diffusion][Epoch 11239] Epoch 11240/12000
2024-11-05 04:09:07,103 - INFO - [diffusion][Epoch 11239] diffusion training Loss: 0.06314027775079012
2024-11-05 04:09:07,105 - INFO - [diffusion][Epoch 11239] diffusion learning rate: 0.001
2024-11-05 04:09:07,107 - INFO - [diffusion][Epoch 11239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:07,108 - INFO - [diffusion][Epoch 11240] Epoch 11241/12000
2024-11-05 04:09:10,462 - INFO - [diffusion][Epoch 11240] diffusion training Loss: 0.061321696266531944
2024-11-05 04:09:10,464 - INFO - [diffusion][Epoch 11240] diffusion learning rate: 0.001
2024-11-05 04:09:10,466 - INFO - [diffusion][Epoch 11240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:10,467 - INFO - [diffusion][Epoch 11241] Epoch 11242/12000
2024-11-05 04:09:13,268 - INFO - [diffusion][Epoch 11241] diffusion training Loss: 0.055886488407850266
2024-11-05 04:09:13,270 - INFO - [diffusion][Epoch 11241] diffusion learning rate: 0.001
2024-11-05 04:09:13,272 - INFO - [diffusion][Epoch 11241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:13,273 - INFO - [diffusion][Epoch 11242] Epoch 11243/12000
2024-11-05 04:09:16,925 - INFO - [diffusion][Epoch 11242] diffusion training Loss: 0.05604454968124628
2024-11-05 04:09:16,927 - INFO - [diffusion][Epoch 11242] diffusion learning rate: 0.001
2024-11-05 04:09:16,929 - INFO - [diffusion][Epoch 11242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:16,931 - INFO - [diffusion][Epoch 11243] Epoch 11244/12000
2024-11-05 04:09:20,819 - INFO - [diffusion][Epoch 11243] diffusion training Loss: 0.053398458287119865
2024-11-05 04:09:20,822 - INFO - [diffusion][Epoch 11243] diffusion learning rate: 0.001
2024-11-05 04:09:20,823 - INFO - [diffusion][Epoch 11243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:20,825 - INFO - [diffusion][Epoch 11244] Epoch 11245/12000
2024-11-05 04:09:24,275 - INFO - [diffusion][Epoch 11244] diffusion training Loss: 0.056801329366862774
2024-11-05 04:09:24,288 - INFO - [diffusion][Epoch 11244] diffusion learning rate: 0.001
2024-11-05 04:09:24,295 - INFO - [diffusion][Epoch 11244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:24,296 - INFO - [diffusion][Epoch 11245] Epoch 11246/12000
2024-11-05 04:09:27,714 - INFO - [diffusion][Epoch 11245] diffusion training Loss: 0.056682768277823925
2024-11-05 04:09:27,716 - INFO - [diffusion][Epoch 11245] diffusion learning rate: 0.001
2024-11-05 04:09:27,718 - INFO - [diffusion][Epoch 11245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:27,719 - INFO - [diffusion][Epoch 11246] Epoch 11247/12000
2024-11-05 04:09:31,194 - INFO - [diffusion][Epoch 11246] diffusion training Loss: 0.061159417033195496
2024-11-05 04:09:31,196 - INFO - [diffusion][Epoch 11246] diffusion learning rate: 0.001
2024-11-05 04:09:31,198 - INFO - [diffusion][Epoch 11246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:31,199 - INFO - [diffusion][Epoch 11247] Epoch 11248/12000
2024-11-05 04:09:34,547 - INFO - [diffusion][Epoch 11247] diffusion training Loss: 0.05030903685837984
2024-11-05 04:09:34,549 - INFO - [diffusion][Epoch 11247] diffusion learning rate: 0.001
2024-11-05 04:09:34,551 - INFO - [diffusion][Epoch 11247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:34,552 - INFO - [diffusion][Epoch 11248] Epoch 11249/12000
2024-11-05 04:09:37,899 - INFO - [diffusion][Epoch 11248] diffusion training Loss: 0.061111753806471825
2024-11-05 04:09:37,902 - INFO - [diffusion][Epoch 11248] diffusion learning rate: 0.001
2024-11-05 04:09:37,904 - INFO - [diffusion][Epoch 11248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:37,905 - INFO - [diffusion][Epoch 11249] Epoch 11250/12000
2024-11-05 04:09:41,371 - INFO - [diffusion][Epoch 11249] diffusion training Loss: 0.06395255029201508
2024-11-05 04:09:41,373 - INFO - [diffusion][Epoch 11249] diffusion learning rate: 0.001
2024-11-05 04:09:41,375 - INFO - [diffusion][Epoch 11249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:41,377 - INFO - [diffusion][Epoch 11250] Epoch 11251/12000
2024-11-05 04:09:44,800 - INFO - [diffusion][Epoch 11250] diffusion training Loss: 0.057387600652873516
2024-11-05 04:09:44,802 - INFO - [diffusion][Epoch 11250] diffusion learning rate: 0.001
2024-11-05 04:09:44,804 - INFO - [diffusion][Epoch 11250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:44,805 - INFO - [diffusion][Epoch 11251] Epoch 11252/12000
2024-11-05 04:09:47,673 - INFO - [diffusion][Epoch 11251] diffusion training Loss: 0.06048392038792372
2024-11-05 04:09:47,675 - INFO - [diffusion][Epoch 11251] diffusion learning rate: 0.001
2024-11-05 04:09:47,677 - INFO - [diffusion][Epoch 11251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:47,679 - INFO - [diffusion][Epoch 11252] Epoch 11253/12000
2024-11-05 04:09:50,799 - INFO - [diffusion][Epoch 11252] diffusion training Loss: 0.053306215442717075
2024-11-05 04:09:50,801 - INFO - [diffusion][Epoch 11252] diffusion learning rate: 0.001
2024-11-05 04:09:50,803 - INFO - [diffusion][Epoch 11252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:50,805 - INFO - [diffusion][Epoch 11253] Epoch 11254/12000
2024-11-05 04:09:53,668 - INFO - [diffusion][Epoch 11253] diffusion training Loss: 0.05642423406243324
2024-11-05 04:09:53,670 - INFO - [diffusion][Epoch 11253] diffusion learning rate: 0.001
2024-11-05 04:09:53,672 - INFO - [diffusion][Epoch 11253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:53,673 - INFO - [diffusion][Epoch 11254] Epoch 11255/12000
2024-11-05 04:09:56,892 - INFO - [diffusion][Epoch 11254] diffusion training Loss: 0.059224363416433334
2024-11-05 04:09:56,894 - INFO - [diffusion][Epoch 11254] diffusion learning rate: 0.001
2024-11-05 04:09:56,896 - INFO - [diffusion][Epoch 11254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:56,897 - INFO - [diffusion][Epoch 11255] Epoch 11256/12000
2024-11-05 04:09:59,793 - INFO - [diffusion][Epoch 11255] diffusion training Loss: 0.06289033964276314
2024-11-05 04:09:59,803 - INFO - [diffusion][Epoch 11255] diffusion learning rate: 0.001
2024-11-05 04:09:59,805 - INFO - [diffusion][Epoch 11255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:59,806 - INFO - [diffusion][Epoch 11256] Epoch 11257/12000
2024-11-05 04:10:02,723 - INFO - [diffusion][Epoch 11256] diffusion training Loss: 0.06025144271552563
2024-11-05 04:10:02,725 - INFO - [diffusion][Epoch 11256] diffusion learning rate: 0.001
2024-11-05 04:10:02,727 - INFO - [diffusion][Epoch 11256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:02,728 - INFO - [diffusion][Epoch 11257] Epoch 11258/12000
2024-11-05 04:10:05,604 - INFO - [diffusion][Epoch 11257] diffusion training Loss: 0.06291612051427364
2024-11-05 04:10:05,606 - INFO - [diffusion][Epoch 11257] diffusion learning rate: 0.001
2024-11-05 04:10:05,608 - INFO - [diffusion][Epoch 11257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:05,609 - INFO - [diffusion][Epoch 11258] Epoch 11259/12000
2024-11-05 04:10:08,385 - INFO - [diffusion][Epoch 11258] diffusion training Loss: 0.06452563591301441
2024-11-05 04:10:08,387 - INFO - [diffusion][Epoch 11258] diffusion learning rate: 0.001
2024-11-05 04:10:08,388 - INFO - [diffusion][Epoch 11258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:08,390 - INFO - [diffusion][Epoch 11259] Epoch 11260/12000
2024-11-05 04:10:11,141 - INFO - [diffusion][Epoch 11259] diffusion training Loss: 0.06191593315452337
2024-11-05 04:10:11,143 - INFO - [diffusion][Epoch 11259] diffusion learning rate: 0.001
2024-11-05 04:10:11,144 - INFO - [diffusion][Epoch 11259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:11,145 - INFO - [diffusion][Epoch 11260] Epoch 11261/12000
2024-11-05 04:10:14,351 - INFO - [diffusion][Epoch 11260] diffusion training Loss: 0.06477940455079079
2024-11-05 04:10:14,353 - INFO - [diffusion][Epoch 11260] diffusion learning rate: 0.001
2024-11-05 04:10:14,355 - INFO - [diffusion][Epoch 11260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:14,356 - INFO - [diffusion][Epoch 11261] Epoch 11262/12000
2024-11-05 04:10:17,198 - INFO - [diffusion][Epoch 11261] diffusion training Loss: 0.06563960015773773
2024-11-05 04:10:17,201 - INFO - [diffusion][Epoch 11261] diffusion learning rate: 0.001
2024-11-05 04:10:17,203 - INFO - [diffusion][Epoch 11261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:17,205 - INFO - [diffusion][Epoch 11262] Epoch 11263/12000
2024-11-05 04:10:20,291 - INFO - [diffusion][Epoch 11262] diffusion training Loss: 0.05679245200008154
2024-11-05 04:10:20,293 - INFO - [diffusion][Epoch 11262] diffusion learning rate: 0.001
2024-11-05 04:10:20,295 - INFO - [diffusion][Epoch 11262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:20,296 - INFO - [diffusion][Epoch 11263] Epoch 11264/12000
2024-11-05 04:10:23,097 - INFO - [diffusion][Epoch 11263] diffusion training Loss: 0.05516849737614393
2024-11-05 04:10:23,099 - INFO - [diffusion][Epoch 11263] diffusion learning rate: 0.001
2024-11-05 04:10:23,100 - INFO - [diffusion][Epoch 11263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:23,101 - INFO - [diffusion][Epoch 11264] Epoch 11265/12000
2024-11-05 04:10:26,456 - INFO - [diffusion][Epoch 11264] diffusion training Loss: 0.06152024492621422
2024-11-05 04:10:26,458 - INFO - [diffusion][Epoch 11264] diffusion learning rate: 0.001
2024-11-05 04:10:26,460 - INFO - [diffusion][Epoch 11264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:26,461 - INFO - [diffusion][Epoch 11265] Epoch 11266/12000
2024-11-05 04:10:29,420 - INFO - [diffusion][Epoch 11265] diffusion training Loss: 0.057669367641210556
2024-11-05 04:10:29,422 - INFO - [diffusion][Epoch 11265] diffusion learning rate: 0.001
2024-11-05 04:10:29,424 - INFO - [diffusion][Epoch 11265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:29,425 - INFO - [diffusion][Epoch 11266] Epoch 11267/12000
2024-11-05 04:10:32,376 - INFO - [diffusion][Epoch 11266] diffusion training Loss: 0.06087168864905834
2024-11-05 04:10:32,378 - INFO - [diffusion][Epoch 11266] diffusion learning rate: 0.001
2024-11-05 04:10:32,380 - INFO - [diffusion][Epoch 11266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:32,382 - INFO - [diffusion][Epoch 11267] Epoch 11268/12000
2024-11-05 04:10:35,541 - INFO - [diffusion][Epoch 11267] diffusion training Loss: 0.05727327335625887
2024-11-05 04:10:35,543 - INFO - [diffusion][Epoch 11267] diffusion learning rate: 0.001
2024-11-05 04:10:35,546 - INFO - [diffusion][Epoch 11267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:35,547 - INFO - [diffusion][Epoch 11268] Epoch 11269/12000
2024-11-05 04:10:38,477 - INFO - [diffusion][Epoch 11268] diffusion training Loss: 0.0611928291618824
2024-11-05 04:10:38,479 - INFO - [diffusion][Epoch 11268] diffusion learning rate: 0.001
2024-11-05 04:10:38,481 - INFO - [diffusion][Epoch 11268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:38,482 - INFO - [diffusion][Epoch 11269] Epoch 11270/12000
2024-11-05 04:10:42,819 - INFO - [diffusion][Epoch 11269] diffusion training Loss: 0.061010707169771194
2024-11-05 04:10:42,821 - INFO - [diffusion][Epoch 11269] diffusion learning rate: 0.001
2024-11-05 04:10:42,823 - INFO - [diffusion][Epoch 11269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:42,824 - INFO - [diffusion][Epoch 11270] Epoch 11271/12000
2024-11-05 04:10:46,371 - INFO - [diffusion][Epoch 11270] diffusion training Loss: 0.05908337142318487
2024-11-05 04:10:46,373 - INFO - [diffusion][Epoch 11270] diffusion learning rate: 0.001
2024-11-05 04:10:46,375 - INFO - [diffusion][Epoch 11270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:46,376 - INFO - [diffusion][Epoch 11271] Epoch 11272/12000
2024-11-05 04:10:49,292 - INFO - [diffusion][Epoch 11271] diffusion training Loss: 0.05730530805885792
2024-11-05 04:10:49,294 - INFO - [diffusion][Epoch 11271] diffusion learning rate: 0.001
2024-11-05 04:10:49,296 - INFO - [diffusion][Epoch 11271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:49,298 - INFO - [diffusion][Epoch 11272] Epoch 11273/12000
2024-11-05 04:10:53,104 - INFO - [diffusion][Epoch 11272] diffusion training Loss: 0.05843895301222801
2024-11-05 04:10:53,107 - INFO - [diffusion][Epoch 11272] diffusion learning rate: 0.001
2024-11-05 04:10:53,108 - INFO - [diffusion][Epoch 11272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:53,110 - INFO - [diffusion][Epoch 11273] Epoch 11274/12000
2024-11-05 04:10:57,180 - INFO - [diffusion][Epoch 11273] diffusion training Loss: 0.059063187800347805
2024-11-05 04:10:57,182 - INFO - [diffusion][Epoch 11273] diffusion learning rate: 0.001
2024-11-05 04:10:57,184 - INFO - [diffusion][Epoch 11273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:57,185 - INFO - [diffusion][Epoch 11274] Epoch 11275/12000
2024-11-05 04:11:00,091 - INFO - [diffusion][Epoch 11274] diffusion training Loss: 0.05137556791305542
2024-11-05 04:11:00,093 - INFO - [diffusion][Epoch 11274] diffusion learning rate: 0.001
2024-11-05 04:11:00,095 - INFO - [diffusion][Epoch 11274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:00,096 - INFO - [diffusion][Epoch 11275] Epoch 11276/12000
2024-11-05 04:11:03,159 - INFO - [diffusion][Epoch 11275] diffusion training Loss: 0.06268818490207195
2024-11-05 04:11:03,161 - INFO - [diffusion][Epoch 11275] diffusion learning rate: 0.001
2024-11-05 04:11:03,163 - INFO - [diffusion][Epoch 11275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:03,164 - INFO - [diffusion][Epoch 11276] Epoch 11277/12000
2024-11-05 04:11:06,002 - INFO - [diffusion][Epoch 11276] diffusion training Loss: 0.059249657206237316
2024-11-05 04:11:06,004 - INFO - [diffusion][Epoch 11276] diffusion learning rate: 0.001
2024-11-05 04:11:06,006 - INFO - [diffusion][Epoch 11276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:06,007 - INFO - [diffusion][Epoch 11277] Epoch 11278/12000
2024-11-05 04:11:08,844 - INFO - [diffusion][Epoch 11277] diffusion training Loss: 0.06367087922990322
2024-11-05 04:11:08,846 - INFO - [diffusion][Epoch 11277] diffusion learning rate: 0.001
2024-11-05 04:11:08,848 - INFO - [diffusion][Epoch 11277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:08,849 - INFO - [diffusion][Epoch 11278] Epoch 11279/12000
2024-11-05 04:11:11,928 - INFO - [diffusion][Epoch 11278] diffusion training Loss: 0.058456351049244404
2024-11-05 04:11:11,930 - INFO - [diffusion][Epoch 11278] diffusion learning rate: 0.001
2024-11-05 04:11:11,933 - INFO - [diffusion][Epoch 11278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:11,934 - INFO - [diffusion][Epoch 11279] Epoch 11280/12000
2024-11-05 04:11:15,372 - INFO - [diffusion][Epoch 11279] diffusion training Loss: 0.06341527216136456
2024-11-05 04:11:15,374 - INFO - [diffusion][Epoch 11279] diffusion learning rate: 0.001
2024-11-05 04:11:15,376 - INFO - [diffusion][Epoch 11279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:15,378 - INFO - [diffusion][Epoch 11280] Epoch 11281/12000
2024-11-05 04:11:18,753 - INFO - [diffusion][Epoch 11280] diffusion training Loss: 0.06301285326480865
2024-11-05 04:11:18,756 - INFO - [diffusion][Epoch 11280] diffusion learning rate: 0.001
2024-11-05 04:11:18,758 - INFO - [diffusion][Epoch 11280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:18,759 - INFO - [diffusion][Epoch 11281] Epoch 11282/12000
2024-11-05 04:11:22,172 - INFO - [diffusion][Epoch 11281] diffusion training Loss: 0.05971236154437065
2024-11-05 04:11:22,174 - INFO - [diffusion][Epoch 11281] diffusion learning rate: 0.001
2024-11-05 04:11:22,176 - INFO - [diffusion][Epoch 11281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:22,178 - INFO - [diffusion][Epoch 11282] Epoch 11283/12000
2024-11-05 04:11:25,708 - INFO - [diffusion][Epoch 11282] diffusion training Loss: 0.0627532321959734
2024-11-05 04:11:25,710 - INFO - [diffusion][Epoch 11282] diffusion learning rate: 0.001
2024-11-05 04:11:25,711 - INFO - [diffusion][Epoch 11282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:25,713 - INFO - [diffusion][Epoch 11283] Epoch 11284/12000
2024-11-05 04:11:29,201 - INFO - [diffusion][Epoch 11283] diffusion training Loss: 0.06159592792391777
2024-11-05 04:11:29,203 - INFO - [diffusion][Epoch 11283] diffusion learning rate: 0.001
2024-11-05 04:11:29,205 - INFO - [diffusion][Epoch 11283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:29,206 - INFO - [diffusion][Epoch 11284] Epoch 11285/12000
2024-11-05 04:11:32,647 - INFO - [diffusion][Epoch 11284] diffusion training Loss: 0.0570227038115263
2024-11-05 04:11:32,650 - INFO - [diffusion][Epoch 11284] diffusion learning rate: 0.001
2024-11-05 04:11:32,652 - INFO - [diffusion][Epoch 11284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:32,653 - INFO - [diffusion][Epoch 11285] Epoch 11286/12000
2024-11-05 04:11:36,075 - INFO - [diffusion][Epoch 11285] diffusion training Loss: 0.06023524887859821
2024-11-05 04:11:36,077 - INFO - [diffusion][Epoch 11285] diffusion learning rate: 0.001
2024-11-05 04:11:36,079 - INFO - [diffusion][Epoch 11285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:36,080 - INFO - [diffusion][Epoch 11286] Epoch 11287/12000
2024-11-05 04:11:39,494 - INFO - [diffusion][Epoch 11286] diffusion training Loss: 0.06349659152328968
2024-11-05 04:11:39,496 - INFO - [diffusion][Epoch 11286] diffusion learning rate: 0.001
2024-11-05 04:11:39,498 - INFO - [diffusion][Epoch 11286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:39,499 - INFO - [diffusion][Epoch 11287] Epoch 11288/12000
2024-11-05 04:11:42,356 - INFO - [diffusion][Epoch 11287] diffusion training Loss: 0.059468540363013744
2024-11-05 04:11:42,358 - INFO - [diffusion][Epoch 11287] diffusion learning rate: 0.001
2024-11-05 04:11:42,360 - INFO - [diffusion][Epoch 11287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:42,361 - INFO - [diffusion][Epoch 11288] Epoch 11289/12000
2024-11-05 04:11:45,366 - INFO - [diffusion][Epoch 11288] diffusion training Loss: 0.058824834413826466
2024-11-05 04:11:45,368 - INFO - [diffusion][Epoch 11288] diffusion learning rate: 0.001
2024-11-05 04:11:45,370 - INFO - [diffusion][Epoch 11288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:45,371 - INFO - [diffusion][Epoch 11289] Epoch 11290/12000
2024-11-05 04:11:48,332 - INFO - [diffusion][Epoch 11289] diffusion training Loss: 0.06007290072739124
2024-11-05 04:11:48,334 - INFO - [diffusion][Epoch 11289] diffusion learning rate: 0.001
2024-11-05 04:11:48,336 - INFO - [diffusion][Epoch 11289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:48,337 - INFO - [diffusion][Epoch 11290] Epoch 11291/12000
2024-11-05 04:11:51,857 - INFO - [diffusion][Epoch 11290] diffusion training Loss: 0.061669270507991314
2024-11-05 04:11:51,859 - INFO - [diffusion][Epoch 11290] diffusion learning rate: 0.001
2024-11-05 04:11:51,861 - INFO - [diffusion][Epoch 11290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:51,862 - INFO - [diffusion][Epoch 11291] Epoch 11292/12000
2024-11-05 04:11:54,735 - INFO - [diffusion][Epoch 11291] diffusion training Loss: 0.05935190711170435
2024-11-05 04:11:54,737 - INFO - [diffusion][Epoch 11291] diffusion learning rate: 0.001
2024-11-05 04:11:54,739 - INFO - [diffusion][Epoch 11291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:54,740 - INFO - [diffusion][Epoch 11292] Epoch 11293/12000
2024-11-05 04:11:57,659 - INFO - [diffusion][Epoch 11292] diffusion training Loss: 0.059274994768202305
2024-11-05 04:11:57,662 - INFO - [diffusion][Epoch 11292] diffusion learning rate: 0.001
2024-11-05 04:11:57,664 - INFO - [diffusion][Epoch 11292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:57,665 - INFO - [diffusion][Epoch 11293] Epoch 11294/12000
2024-11-05 04:12:00,616 - INFO - [diffusion][Epoch 11293] diffusion training Loss: 0.060261016711592674
2024-11-05 04:12:00,618 - INFO - [diffusion][Epoch 11293] diffusion learning rate: 0.001
2024-11-05 04:12:00,620 - INFO - [diffusion][Epoch 11293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:00,621 - INFO - [diffusion][Epoch 11294] Epoch 11295/12000
2024-11-05 04:12:03,705 - INFO - [diffusion][Epoch 11294] diffusion training Loss: 0.06207804847508669
2024-11-05 04:12:03,707 - INFO - [diffusion][Epoch 11294] diffusion learning rate: 0.001
2024-11-05 04:12:03,709 - INFO - [diffusion][Epoch 11294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:03,712 - INFO - [diffusion][Epoch 11295] Epoch 11296/12000
2024-11-05 04:12:06,652 - INFO - [diffusion][Epoch 11295] diffusion training Loss: 0.06287579610943794
2024-11-05 04:12:06,654 - INFO - [diffusion][Epoch 11295] diffusion learning rate: 0.001
2024-11-05 04:12:06,656 - INFO - [diffusion][Epoch 11295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:06,657 - INFO - [diffusion][Epoch 11296] Epoch 11297/12000
2024-11-05 04:12:10,021 - INFO - [diffusion][Epoch 11296] diffusion training Loss: 0.05262814834713936
2024-11-05 04:12:10,023 - INFO - [diffusion][Epoch 11296] diffusion learning rate: 0.001
2024-11-05 04:12:10,025 - INFO - [diffusion][Epoch 11296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:10,026 - INFO - [diffusion][Epoch 11297] Epoch 11298/12000
2024-11-05 04:12:12,866 - INFO - [diffusion][Epoch 11297] diffusion training Loss: 0.06166035123169422
2024-11-05 04:12:12,867 - INFO - [diffusion][Epoch 11297] diffusion learning rate: 0.001
2024-11-05 04:12:12,869 - INFO - [diffusion][Epoch 11297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:12,871 - INFO - [diffusion][Epoch 11298] Epoch 11299/12000
2024-11-05 04:12:16,100 - INFO - [diffusion][Epoch 11298] diffusion training Loss: 0.05547398887574673
2024-11-05 04:12:16,102 - INFO - [diffusion][Epoch 11298] diffusion learning rate: 0.001
2024-11-05 04:12:16,104 - INFO - [diffusion][Epoch 11298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:16,105 - INFO - [diffusion][Epoch 11299] Epoch 11300/12000
2024-11-05 04:12:19,056 - INFO - [diffusion][Epoch 11299] diffusion training Loss: 0.056753684766590595
2024-11-05 04:12:19,058 - INFO - [diffusion][Epoch 11299] diffusion learning rate: 0.001
2024-11-05 04:12:19,060 - INFO - [diffusion][Epoch 11299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:19,061 - INFO - [diffusion][Epoch 11300] Epoch 11301/12000
2024-11-05 04:12:22,472 - INFO - [diffusion][Epoch 11300] diffusion training Loss: 0.06443917006254196
2024-11-05 04:12:22,474 - INFO - [diffusion][Epoch 11300] diffusion learning rate: 0.001
2024-11-05 04:12:22,476 - INFO - [diffusion][Epoch 11300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:22,478 - INFO - [diffusion][Epoch 11301] Epoch 11302/12000
2024-11-05 04:12:25,334 - INFO - [diffusion][Epoch 11301] diffusion training Loss: 0.059449377469718456
2024-11-05 04:12:25,336 - INFO - [diffusion][Epoch 11301] diffusion learning rate: 0.001
2024-11-05 04:12:25,337 - INFO - [diffusion][Epoch 11301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:25,339 - INFO - [diffusion][Epoch 11302] Epoch 11303/12000
2024-11-05 04:12:28,297 - INFO - [diffusion][Epoch 11302] diffusion training Loss: 0.057023948058485985
2024-11-05 04:12:28,299 - INFO - [diffusion][Epoch 11302] diffusion learning rate: 0.001
2024-11-05 04:12:28,301 - INFO - [diffusion][Epoch 11302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:28,302 - INFO - [diffusion][Epoch 11303] Epoch 11304/12000
2024-11-05 04:12:31,242 - INFO - [diffusion][Epoch 11303] diffusion training Loss: 0.057455929927527905
2024-11-05 04:12:31,244 - INFO - [diffusion][Epoch 11303] diffusion learning rate: 0.001
2024-11-05 04:12:31,246 - INFO - [diffusion][Epoch 11303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:31,247 - INFO - [diffusion][Epoch 11304] Epoch 11305/12000
2024-11-05 04:12:34,153 - INFO - [diffusion][Epoch 11304] diffusion training Loss: 0.05519807804375887
2024-11-05 04:12:34,155 - INFO - [diffusion][Epoch 11304] diffusion learning rate: 0.001
2024-11-05 04:12:34,157 - INFO - [diffusion][Epoch 11304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:34,158 - INFO - [diffusion][Epoch 11305] Epoch 11306/12000
2024-11-05 04:12:38,035 - INFO - [diffusion][Epoch 11305] diffusion training Loss: 0.059379576705396175
2024-11-05 04:12:38,037 - INFO - [diffusion][Epoch 11305] diffusion learning rate: 0.001
2024-11-05 04:12:38,039 - INFO - [diffusion][Epoch 11305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:38,040 - INFO - [diffusion][Epoch 11306] Epoch 11307/12000
2024-11-05 04:12:41,431 - INFO - [diffusion][Epoch 11306] diffusion training Loss: 0.058143217116594315
2024-11-05 04:12:41,433 - INFO - [diffusion][Epoch 11306] diffusion learning rate: 0.001
2024-11-05 04:12:41,435 - INFO - [diffusion][Epoch 11306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:41,437 - INFO - [diffusion][Epoch 11307] Epoch 11308/12000
2024-11-05 04:12:44,362 - INFO - [diffusion][Epoch 11307] diffusion training Loss: 0.05682518891990185
2024-11-05 04:12:44,364 - INFO - [diffusion][Epoch 11307] diffusion learning rate: 0.001
2024-11-05 04:12:44,366 - INFO - [diffusion][Epoch 11307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:44,367 - INFO - [diffusion][Epoch 11308] Epoch 11309/12000
2024-11-05 04:12:48,210 - INFO - [diffusion][Epoch 11308] diffusion training Loss: 0.0579681433737278
2024-11-05 04:12:48,212 - INFO - [diffusion][Epoch 11308] diffusion learning rate: 0.001
2024-11-05 04:12:48,214 - INFO - [diffusion][Epoch 11308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:48,216 - INFO - [diffusion][Epoch 11309] Epoch 11310/12000
2024-11-05 04:12:52,274 - INFO - [diffusion][Epoch 11309] diffusion training Loss: 0.059312582947313786
2024-11-05 04:12:52,276 - INFO - [diffusion][Epoch 11309] diffusion learning rate: 0.001
2024-11-05 04:12:52,278 - INFO - [diffusion][Epoch 11309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:52,279 - INFO - [diffusion][Epoch 11310] Epoch 11311/12000
2024-11-05 04:12:55,459 - INFO - [diffusion][Epoch 11310] diffusion training Loss: 0.05863999202847481
2024-11-05 04:12:55,461 - INFO - [diffusion][Epoch 11310] diffusion learning rate: 0.001
2024-11-05 04:12:55,463 - INFO - [diffusion][Epoch 11310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:55,464 - INFO - [diffusion][Epoch 11311] Epoch 11312/12000
2024-11-05 04:12:59,131 - INFO - [diffusion][Epoch 11311] diffusion training Loss: 0.05574526358395815
2024-11-05 04:12:59,133 - INFO - [diffusion][Epoch 11311] diffusion learning rate: 0.001
2024-11-05 04:12:59,135 - INFO - [diffusion][Epoch 11311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:59,137 - INFO - [diffusion][Epoch 11312] Epoch 11313/12000
2024-11-05 04:13:01,967 - INFO - [diffusion][Epoch 11312] diffusion training Loss: 0.05721490457653999
2024-11-05 04:13:01,970 - INFO - [diffusion][Epoch 11312] diffusion learning rate: 0.001
2024-11-05 04:13:01,972 - INFO - [diffusion][Epoch 11312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:01,973 - INFO - [diffusion][Epoch 11313] Epoch 11314/12000
2024-11-05 04:13:05,568 - INFO - [diffusion][Epoch 11313] diffusion training Loss: 0.06251235119998455
2024-11-05 04:13:05,570 - INFO - [diffusion][Epoch 11313] diffusion learning rate: 0.001
2024-11-05 04:13:05,572 - INFO - [diffusion][Epoch 11313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:05,574 - INFO - [diffusion][Epoch 11314] Epoch 11315/12000
2024-11-05 04:13:09,663 - INFO - [diffusion][Epoch 11314] diffusion training Loss: 0.060651494190096855
2024-11-05 04:13:09,666 - INFO - [diffusion][Epoch 11314] diffusion learning rate: 0.001
2024-11-05 04:13:09,669 - INFO - [diffusion][Epoch 11314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:09,670 - INFO - [diffusion][Epoch 11315] Epoch 11316/12000
2024-11-05 04:13:13,132 - INFO - [diffusion][Epoch 11315] diffusion training Loss: 0.05841024499386549
2024-11-05 04:13:13,136 - INFO - [diffusion][Epoch 11315] diffusion learning rate: 0.001
2024-11-05 04:13:13,163 - INFO - [diffusion][Epoch 11315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:13,165 - INFO - [diffusion][Epoch 11316] Epoch 11317/12000
2024-11-05 04:13:16,592 - INFO - [diffusion][Epoch 11316] diffusion training Loss: 0.058948367834091187
2024-11-05 04:13:16,594 - INFO - [diffusion][Epoch 11316] diffusion learning rate: 0.001
2024-11-05 04:13:16,596 - INFO - [diffusion][Epoch 11316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:16,597 - INFO - [diffusion][Epoch 11317] Epoch 11318/12000
2024-11-05 04:13:20,076 - INFO - [diffusion][Epoch 11317] diffusion training Loss: 0.061729755252599716
2024-11-05 04:13:20,078 - INFO - [diffusion][Epoch 11317] diffusion learning rate: 0.001
2024-11-05 04:13:20,080 - INFO - [diffusion][Epoch 11317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:20,081 - INFO - [diffusion][Epoch 11318] Epoch 11319/12000
2024-11-05 04:13:23,486 - INFO - [diffusion][Epoch 11318] diffusion training Loss: 0.060636987909674644
2024-11-05 04:13:23,488 - INFO - [diffusion][Epoch 11318] diffusion learning rate: 0.001
2024-11-05 04:13:23,490 - INFO - [diffusion][Epoch 11318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:23,492 - INFO - [diffusion][Epoch 11319] Epoch 11320/12000
2024-11-05 04:13:26,846 - INFO - [diffusion][Epoch 11319] diffusion training Loss: 0.06106021348387003
2024-11-05 04:13:26,848 - INFO - [diffusion][Epoch 11319] diffusion learning rate: 0.001
2024-11-05 04:13:26,850 - INFO - [diffusion][Epoch 11319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:26,851 - INFO - [diffusion][Epoch 11320] Epoch 11321/12000
2024-11-05 04:13:30,199 - INFO - [diffusion][Epoch 11320] diffusion training Loss: 0.05734971072524786
2024-11-05 04:13:30,202 - INFO - [diffusion][Epoch 11320] diffusion learning rate: 0.001
2024-11-05 04:13:30,204 - INFO - [diffusion][Epoch 11320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:30,205 - INFO - [diffusion][Epoch 11321] Epoch 11322/12000
2024-11-05 04:13:33,215 - INFO - [diffusion][Epoch 11321] diffusion training Loss: 0.06183773931115866
2024-11-05 04:13:33,216 - INFO - [diffusion][Epoch 11321] diffusion learning rate: 0.001
2024-11-05 04:13:33,218 - INFO - [diffusion][Epoch 11321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:33,219 - INFO - [diffusion][Epoch 11322] Epoch 11323/12000
2024-11-05 04:13:36,049 - INFO - [diffusion][Epoch 11322] diffusion training Loss: 0.06108558177947998
2024-11-05 04:13:36,051 - INFO - [diffusion][Epoch 11322] diffusion learning rate: 0.001
2024-11-05 04:13:36,053 - INFO - [diffusion][Epoch 11322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:36,054 - INFO - [diffusion][Epoch 11323] Epoch 11324/12000
2024-11-05 04:13:39,255 - INFO - [diffusion][Epoch 11323] diffusion training Loss: 0.06015350017696619
2024-11-05 04:13:39,257 - INFO - [diffusion][Epoch 11323] diffusion learning rate: 0.001
2024-11-05 04:13:39,259 - INFO - [diffusion][Epoch 11323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:39,260 - INFO - [diffusion][Epoch 11324] Epoch 11325/12000
2024-11-05 04:13:42,215 - INFO - [diffusion][Epoch 11324] diffusion training Loss: 0.06383225508034229
2024-11-05 04:13:42,218 - INFO - [diffusion][Epoch 11324] diffusion learning rate: 0.001
2024-11-05 04:13:42,220 - INFO - [diffusion][Epoch 11324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:42,221 - INFO - [diffusion][Epoch 11325] Epoch 11326/12000
2024-11-05 04:13:45,548 - INFO - [diffusion][Epoch 11325] diffusion training Loss: 0.06307131052017212
2024-11-05 04:13:45,550 - INFO - [diffusion][Epoch 11325] diffusion learning rate: 0.001
2024-11-05 04:13:45,551 - INFO - [diffusion][Epoch 11325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:45,553 - INFO - [diffusion][Epoch 11326] Epoch 11327/12000
2024-11-05 04:13:48,328 - INFO - [diffusion][Epoch 11326] diffusion training Loss: 0.06460783071815968
2024-11-05 04:13:48,330 - INFO - [diffusion][Epoch 11326] diffusion learning rate: 0.001
2024-11-05 04:13:48,332 - INFO - [diffusion][Epoch 11326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:48,333 - INFO - [diffusion][Epoch 11327] Epoch 11328/12000
2024-11-05 04:13:51,536 - INFO - [diffusion][Epoch 11327] diffusion training Loss: 0.05528520978987217
2024-11-05 04:13:51,538 - INFO - [diffusion][Epoch 11327] diffusion learning rate: 0.001
2024-11-05 04:13:51,541 - INFO - [diffusion][Epoch 11327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:51,543 - INFO - [diffusion][Epoch 11328] Epoch 11329/12000
2024-11-05 04:13:54,431 - INFO - [diffusion][Epoch 11328] diffusion training Loss: 0.059560890309512615
2024-11-05 04:13:54,434 - INFO - [diffusion][Epoch 11328] diffusion learning rate: 0.001
2024-11-05 04:13:54,436 - INFO - [diffusion][Epoch 11328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:54,437 - INFO - [diffusion][Epoch 11329] Epoch 11330/12000
2024-11-05 04:13:57,518 - INFO - [diffusion][Epoch 11329] diffusion training Loss: 0.059079804457724094
2024-11-05 04:13:57,520 - INFO - [diffusion][Epoch 11329] diffusion learning rate: 0.001
2024-11-05 04:13:57,522 - INFO - [diffusion][Epoch 11329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:57,523 - INFO - [diffusion][Epoch 11330] Epoch 11331/12000
2024-11-05 04:14:00,646 - INFO - [diffusion][Epoch 11330] diffusion training Loss: 0.05984699819236994
2024-11-05 04:14:00,648 - INFO - [diffusion][Epoch 11330] diffusion learning rate: 0.001
2024-11-05 04:14:00,650 - INFO - [diffusion][Epoch 11330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:00,651 - INFO - [diffusion][Epoch 11331] Epoch 11332/12000
2024-11-05 04:14:03,730 - INFO - [diffusion][Epoch 11331] diffusion training Loss: 0.061395020224153996
2024-11-05 04:14:03,732 - INFO - [diffusion][Epoch 11331] diffusion learning rate: 0.001
2024-11-05 04:14:03,734 - INFO - [diffusion][Epoch 11331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:03,736 - INFO - [diffusion][Epoch 11332] Epoch 11333/12000
2024-11-05 04:14:06,703 - INFO - [diffusion][Epoch 11332] diffusion training Loss: 0.0578170670196414
2024-11-05 04:14:06,705 - INFO - [diffusion][Epoch 11332] diffusion learning rate: 0.001
2024-11-05 04:14:06,707 - INFO - [diffusion][Epoch 11332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:06,708 - INFO - [diffusion][Epoch 11333] Epoch 11334/12000
2024-11-05 04:14:09,725 - INFO - [diffusion][Epoch 11333] diffusion training Loss: 0.05571548081934452
2024-11-05 04:14:09,728 - INFO - [diffusion][Epoch 11333] diffusion learning rate: 0.001
2024-11-05 04:14:09,729 - INFO - [diffusion][Epoch 11333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:09,731 - INFO - [diffusion][Epoch 11334] Epoch 11335/12000
2024-11-05 04:14:12,644 - INFO - [diffusion][Epoch 11334] diffusion training Loss: 0.054028105922043324
2024-11-05 04:14:12,646 - INFO - [diffusion][Epoch 11334] diffusion learning rate: 0.001
2024-11-05 04:14:12,648 - INFO - [diffusion][Epoch 11334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:12,649 - INFO - [diffusion][Epoch 11335] Epoch 11336/12000
2024-11-05 04:14:15,991 - INFO - [diffusion][Epoch 11335] diffusion training Loss: 0.06281258352100849
2024-11-05 04:14:15,993 - INFO - [diffusion][Epoch 11335] diffusion learning rate: 0.001
2024-11-05 04:14:15,995 - INFO - [diffusion][Epoch 11335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:15,996 - INFO - [diffusion][Epoch 11336] Epoch 11337/12000
2024-11-05 04:14:18,877 - INFO - [diffusion][Epoch 11336] diffusion training Loss: 0.0565316965803504
2024-11-05 04:14:18,879 - INFO - [diffusion][Epoch 11336] diffusion learning rate: 0.001
2024-11-05 04:14:18,881 - INFO - [diffusion][Epoch 11336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:18,882 - INFO - [diffusion][Epoch 11337] Epoch 11338/12000
2024-11-05 04:14:22,295 - INFO - [diffusion][Epoch 11337] diffusion training Loss: 0.05728275701403618
2024-11-05 04:14:22,298 - INFO - [diffusion][Epoch 11337] diffusion learning rate: 0.001
2024-11-05 04:14:22,300 - INFO - [diffusion][Epoch 11337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:22,302 - INFO - [diffusion][Epoch 11338] Epoch 11339/12000
2024-11-05 04:14:25,227 - INFO - [diffusion][Epoch 11338] diffusion training Loss: 0.05464701820164919
2024-11-05 04:14:25,229 - INFO - [diffusion][Epoch 11338] diffusion learning rate: 0.001
2024-11-05 04:14:25,231 - INFO - [diffusion][Epoch 11338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:25,232 - INFO - [diffusion][Epoch 11339] Epoch 11340/12000
2024-11-05 04:14:28,857 - INFO - [diffusion][Epoch 11339] diffusion training Loss: 0.05448987428098917
2024-11-05 04:14:28,860 - INFO - [diffusion][Epoch 11339] diffusion learning rate: 0.001
2024-11-05 04:14:28,862 - INFO - [diffusion][Epoch 11339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:28,863 - INFO - [diffusion][Epoch 11340] Epoch 11341/12000
2024-11-05 04:14:33,024 - INFO - [diffusion][Epoch 11340] diffusion training Loss: 0.05749564245343208
2024-11-05 04:14:33,026 - INFO - [diffusion][Epoch 11340] diffusion learning rate: 0.001
2024-11-05 04:14:33,028 - INFO - [diffusion][Epoch 11340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:33,029 - INFO - [diffusion][Epoch 11341] Epoch 11342/12000
2024-11-05 04:14:35,899 - INFO - [diffusion][Epoch 11341] diffusion training Loss: 0.058990548364818096
2024-11-05 04:14:35,901 - INFO - [diffusion][Epoch 11341] diffusion learning rate: 0.001
2024-11-05 04:14:35,903 - INFO - [diffusion][Epoch 11341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:35,904 - INFO - [diffusion][Epoch 11342] Epoch 11343/12000
2024-11-05 04:14:39,092 - INFO - [diffusion][Epoch 11342] diffusion training Loss: 0.05566428788006306
2024-11-05 04:14:39,094 - INFO - [diffusion][Epoch 11342] diffusion learning rate: 0.001
2024-11-05 04:14:39,114 - INFO - [diffusion][Epoch 11342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:39,115 - INFO - [diffusion][Epoch 11343] Epoch 11344/12000
2024-11-05 04:14:43,080 - INFO - [diffusion][Epoch 11343] diffusion training Loss: 0.05765043385326862
2024-11-05 04:14:43,082 - INFO - [diffusion][Epoch 11343] diffusion learning rate: 0.001
2024-11-05 04:14:43,084 - INFO - [diffusion][Epoch 11343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:43,085 - INFO - [diffusion][Epoch 11344] Epoch 11345/12000
2024-11-05 04:14:46,026 - INFO - [diffusion][Epoch 11344] diffusion training Loss: 0.05921116657555103
2024-11-05 04:14:46,028 - INFO - [diffusion][Epoch 11344] diffusion learning rate: 0.001
2024-11-05 04:14:46,029 - INFO - [diffusion][Epoch 11344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:46,031 - INFO - [diffusion][Epoch 11345] Epoch 11346/12000
2024-11-05 04:14:48,915 - INFO - [diffusion][Epoch 11345] diffusion training Loss: 0.0531889284029603
2024-11-05 04:14:48,917 - INFO - [diffusion][Epoch 11345] diffusion learning rate: 0.001
2024-11-05 04:14:48,920 - INFO - [diffusion][Epoch 11345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:48,922 - INFO - [diffusion][Epoch 11346] Epoch 11347/12000
2024-11-05 04:14:52,342 - INFO - [diffusion][Epoch 11346] diffusion training Loss: 0.055601960979402065
2024-11-05 04:14:52,344 - INFO - [diffusion][Epoch 11346] diffusion learning rate: 0.001
2024-11-05 04:14:52,346 - INFO - [diffusion][Epoch 11346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:52,347 - INFO - [diffusion][Epoch 11347] Epoch 11348/12000
2024-11-05 04:14:55,197 - INFO - [diffusion][Epoch 11347] diffusion training Loss: 0.058990975841879845
2024-11-05 04:14:55,198 - INFO - [diffusion][Epoch 11347] diffusion learning rate: 0.001
2024-11-05 04:14:55,200 - INFO - [diffusion][Epoch 11347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:55,201 - INFO - [diffusion][Epoch 11348] Epoch 11349/12000
2024-11-05 04:14:59,012 - INFO - [diffusion][Epoch 11348] diffusion training Loss: 0.059221173636615276
2024-11-05 04:14:59,014 - INFO - [diffusion][Epoch 11348] diffusion learning rate: 0.001
2024-11-05 04:14:59,016 - INFO - [diffusion][Epoch 11348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:59,017 - INFO - [diffusion][Epoch 11349] Epoch 11350/12000
2024-11-05 04:15:02,508 - INFO - [diffusion][Epoch 11349] diffusion training Loss: 0.06133488565683365
2024-11-05 04:15:02,510 - INFO - [diffusion][Epoch 11349] diffusion learning rate: 0.001
2024-11-05 04:15:02,512 - INFO - [diffusion][Epoch 11349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:02,513 - INFO - [diffusion][Epoch 11350] Epoch 11351/12000
2024-11-05 04:15:05,899 - INFO - [diffusion][Epoch 11350] diffusion training Loss: 0.054425597190856934
2024-11-05 04:15:05,901 - INFO - [diffusion][Epoch 11350] diffusion learning rate: 0.001
2024-11-05 04:15:05,903 - INFO - [diffusion][Epoch 11350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:05,904 - INFO - [diffusion][Epoch 11351] Epoch 11352/12000
2024-11-05 04:15:09,628 - INFO - [diffusion][Epoch 11351] diffusion training Loss: 0.05925185140222311
2024-11-05 04:15:09,630 - INFO - [diffusion][Epoch 11351] diffusion learning rate: 0.001
2024-11-05 04:15:09,632 - INFO - [diffusion][Epoch 11351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:09,634 - INFO - [diffusion][Epoch 11352] Epoch 11353/12000
2024-11-05 04:15:13,098 - INFO - [diffusion][Epoch 11352] diffusion training Loss: 0.06065721157938242
2024-11-05 04:15:13,101 - INFO - [diffusion][Epoch 11352] diffusion learning rate: 0.001
2024-11-05 04:15:13,103 - INFO - [diffusion][Epoch 11352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:13,104 - INFO - [diffusion][Epoch 11353] Epoch 11354/12000
2024-11-05 04:15:16,540 - INFO - [diffusion][Epoch 11353] diffusion training Loss: 0.05444637220352888
2024-11-05 04:15:16,542 - INFO - [diffusion][Epoch 11353] diffusion learning rate: 0.001
2024-11-05 04:15:16,544 - INFO - [diffusion][Epoch 11353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:16,546 - INFO - [diffusion][Epoch 11354] Epoch 11355/12000
2024-11-05 04:15:19,990 - INFO - [diffusion][Epoch 11354] diffusion training Loss: 0.05880672391504049
2024-11-05 04:15:19,992 - INFO - [diffusion][Epoch 11354] diffusion learning rate: 0.001
2024-11-05 04:15:19,993 - INFO - [diffusion][Epoch 11354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:19,995 - INFO - [diffusion][Epoch 11355] Epoch 11356/12000
2024-11-05 04:15:23,409 - INFO - [diffusion][Epoch 11355] diffusion training Loss: 0.05862457770854235
2024-11-05 04:15:23,411 - INFO - [diffusion][Epoch 11355] diffusion learning rate: 0.001
2024-11-05 04:15:23,413 - INFO - [diffusion][Epoch 11355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:23,414 - INFO - [diffusion][Epoch 11356] Epoch 11357/12000
2024-11-05 04:15:26,232 - INFO - [diffusion][Epoch 11356] diffusion training Loss: 0.05594244785606861
2024-11-05 04:15:26,234 - INFO - [diffusion][Epoch 11356] diffusion learning rate: 0.001
2024-11-05 04:15:26,236 - INFO - [diffusion][Epoch 11356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:26,237 - INFO - [diffusion][Epoch 11357] Epoch 11358/12000
2024-11-05 04:15:29,173 - INFO - [diffusion][Epoch 11357] diffusion training Loss: 0.05588801018893719
2024-11-05 04:15:29,175 - INFO - [diffusion][Epoch 11357] diffusion learning rate: 0.001
2024-11-05 04:15:29,177 - INFO - [diffusion][Epoch 11357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:29,178 - INFO - [diffusion][Epoch 11358] Epoch 11359/12000
2024-11-05 04:15:32,081 - INFO - [diffusion][Epoch 11358] diffusion training Loss: 0.0547670004889369
2024-11-05 04:15:32,083 - INFO - [diffusion][Epoch 11358] diffusion learning rate: 0.001
2024-11-05 04:15:32,085 - INFO - [diffusion][Epoch 11358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:32,086 - INFO - [diffusion][Epoch 11359] Epoch 11360/12000
2024-11-05 04:15:34,978 - INFO - [diffusion][Epoch 11359] diffusion training Loss: 0.060149576514959335
2024-11-05 04:15:34,980 - INFO - [diffusion][Epoch 11359] diffusion learning rate: 0.001
2024-11-05 04:15:34,982 - INFO - [diffusion][Epoch 11359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:34,984 - INFO - [diffusion][Epoch 11360] Epoch 11361/12000
2024-11-05 04:15:38,087 - INFO - [diffusion][Epoch 11360] diffusion training Loss: 0.06000074744224548
2024-11-05 04:15:38,090 - INFO - [diffusion][Epoch 11360] diffusion learning rate: 0.001
2024-11-05 04:15:38,092 - INFO - [diffusion][Epoch 11360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:38,094 - INFO - [diffusion][Epoch 11361] Epoch 11362/12000
2024-11-05 04:15:40,934 - INFO - [diffusion][Epoch 11361] diffusion training Loss: 0.05516855884343386
2024-11-05 04:15:40,936 - INFO - [diffusion][Epoch 11361] diffusion learning rate: 0.001
2024-11-05 04:15:40,937 - INFO - [diffusion][Epoch 11361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:40,939 - INFO - [diffusion][Epoch 11362] Epoch 11363/12000
2024-11-05 04:15:43,819 - INFO - [diffusion][Epoch 11362] diffusion training Loss: 0.058339535258710384
2024-11-05 04:15:43,821 - INFO - [diffusion][Epoch 11362] diffusion learning rate: 0.001
2024-11-05 04:15:43,823 - INFO - [diffusion][Epoch 11362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:43,824 - INFO - [diffusion][Epoch 11363] Epoch 11364/12000
2024-11-05 04:15:46,740 - INFO - [diffusion][Epoch 11363] diffusion training Loss: 0.06389327440410852
2024-11-05 04:15:46,741 - INFO - [diffusion][Epoch 11363] diffusion learning rate: 0.001
2024-11-05 04:15:46,743 - INFO - [diffusion][Epoch 11363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:46,744 - INFO - [diffusion][Epoch 11364] Epoch 11365/12000
2024-11-05 04:15:49,835 - INFO - [diffusion][Epoch 11364] diffusion training Loss: 0.05989451054483652
2024-11-05 04:15:49,837 - INFO - [diffusion][Epoch 11364] diffusion learning rate: 0.001
2024-11-05 04:15:49,839 - INFO - [diffusion][Epoch 11364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:49,840 - INFO - [diffusion][Epoch 11365] Epoch 11366/12000
2024-11-05 04:15:52,648 - INFO - [diffusion][Epoch 11365] diffusion training Loss: 0.056826142594218254
2024-11-05 04:15:52,650 - INFO - [diffusion][Epoch 11365] diffusion learning rate: 0.001
2024-11-05 04:15:52,652 - INFO - [diffusion][Epoch 11365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:52,654 - INFO - [diffusion][Epoch 11366] Epoch 11367/12000
2024-11-05 04:15:55,758 - INFO - [diffusion][Epoch 11366] diffusion training Loss: 0.05363774858415127
2024-11-05 04:15:55,760 - INFO - [diffusion][Epoch 11366] diffusion learning rate: 0.001
2024-11-05 04:15:55,762 - INFO - [diffusion][Epoch 11366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:55,763 - INFO - [diffusion][Epoch 11367] Epoch 11368/12000
2024-11-05 04:15:58,515 - INFO - [diffusion][Epoch 11367] diffusion training Loss: 0.054343623109161854
2024-11-05 04:15:58,517 - INFO - [diffusion][Epoch 11367] diffusion learning rate: 0.001
2024-11-05 04:15:58,519 - INFO - [diffusion][Epoch 11367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:58,520 - INFO - [diffusion][Epoch 11368] Epoch 11369/12000
2024-11-05 04:16:01,439 - INFO - [diffusion][Epoch 11368] diffusion training Loss: 0.05834513623267412
2024-11-05 04:16:01,441 - INFO - [diffusion][Epoch 11368] diffusion learning rate: 0.001
2024-11-05 04:16:01,443 - INFO - [diffusion][Epoch 11368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:01,444 - INFO - [diffusion][Epoch 11369] Epoch 11370/12000
2024-11-05 04:16:04,333 - INFO - [diffusion][Epoch 11369] diffusion training Loss: 0.05502043291926384
2024-11-05 04:16:04,335 - INFO - [diffusion][Epoch 11369] diffusion learning rate: 0.001
2024-11-05 04:16:04,337 - INFO - [diffusion][Epoch 11369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:04,338 - INFO - [diffusion][Epoch 11370] Epoch 11371/12000
2024-11-05 04:16:07,346 - INFO - [diffusion][Epoch 11370] diffusion training Loss: 0.05319729819893837
2024-11-05 04:16:07,349 - INFO - [diffusion][Epoch 11370] diffusion learning rate: 0.001
2024-11-05 04:16:07,351 - INFO - [diffusion][Epoch 11370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:07,352 - INFO - [diffusion][Epoch 11371] Epoch 11372/12000
2024-11-05 04:16:10,207 - INFO - [diffusion][Epoch 11371] diffusion training Loss: 0.05996690224856138
2024-11-05 04:16:10,209 - INFO - [diffusion][Epoch 11371] diffusion learning rate: 0.001
2024-11-05 04:16:10,211 - INFO - [diffusion][Epoch 11371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:10,212 - INFO - [diffusion][Epoch 11372] Epoch 11373/12000
2024-11-05 04:16:13,955 - INFO - [diffusion][Epoch 11372] diffusion training Loss: 0.05734769068658352
2024-11-05 04:16:13,957 - INFO - [diffusion][Epoch 11372] diffusion learning rate: 0.001
2024-11-05 04:16:13,986 - INFO - [diffusion][Epoch 11372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:13,987 - INFO - [diffusion][Epoch 11373] Epoch 11374/12000
2024-11-05 04:16:16,945 - INFO - [diffusion][Epoch 11373] diffusion training Loss: 0.05397216975688934
2024-11-05 04:16:16,947 - INFO - [diffusion][Epoch 11373] diffusion learning rate: 0.001
2024-11-05 04:16:16,949 - INFO - [diffusion][Epoch 11373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:16,950 - INFO - [diffusion][Epoch 11374] Epoch 11375/12000
2024-11-05 04:16:21,101 - INFO - [diffusion][Epoch 11374] diffusion training Loss: 0.061206153593957424
2024-11-05 04:16:21,103 - INFO - [diffusion][Epoch 11374] diffusion learning rate: 0.001
2024-11-05 04:16:21,104 - INFO - [diffusion][Epoch 11374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:21,106 - INFO - [diffusion][Epoch 11375] Epoch 11376/12000
2024-11-05 04:16:24,490 - INFO - [diffusion][Epoch 11375] diffusion training Loss: 0.05704030767083168
2024-11-05 04:16:24,492 - INFO - [diffusion][Epoch 11375] diffusion learning rate: 0.001
2024-11-05 04:16:24,494 - INFO - [diffusion][Epoch 11375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:24,495 - INFO - [diffusion][Epoch 11376] Epoch 11377/12000
2024-11-05 04:16:27,406 - INFO - [diffusion][Epoch 11376] diffusion training Loss: 0.058631702326238155
2024-11-05 04:16:27,407 - INFO - [diffusion][Epoch 11376] diffusion learning rate: 0.001
2024-11-05 04:16:27,409 - INFO - [diffusion][Epoch 11376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:27,411 - INFO - [diffusion][Epoch 11377] Epoch 11378/12000
2024-11-05 04:16:31,569 - INFO - [diffusion][Epoch 11377] diffusion training Loss: 0.057885452173650265
2024-11-05 04:16:31,571 - INFO - [diffusion][Epoch 11377] diffusion learning rate: 0.001
2024-11-05 04:16:31,573 - INFO - [diffusion][Epoch 11377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:31,574 - INFO - [diffusion][Epoch 11378] Epoch 11379/12000
2024-11-05 04:16:35,012 - INFO - [diffusion][Epoch 11378] diffusion training Loss: 0.05903219152241945
2024-11-05 04:16:35,014 - INFO - [diffusion][Epoch 11378] diffusion learning rate: 0.001
2024-11-05 04:16:35,016 - INFO - [diffusion][Epoch 11378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:35,017 - INFO - [diffusion][Epoch 11379] Epoch 11380/12000
2024-11-05 04:16:37,944 - INFO - [diffusion][Epoch 11379] diffusion training Loss: 0.06238391809165478
2024-11-05 04:16:37,946 - INFO - [diffusion][Epoch 11379] diffusion learning rate: 0.001
2024-11-05 04:16:37,948 - INFO - [diffusion][Epoch 11379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:37,949 - INFO - [diffusion][Epoch 11380] Epoch 11381/12000
2024-11-05 04:16:41,470 - INFO - [diffusion][Epoch 11380] diffusion training Loss: 0.05398297309875488
2024-11-05 04:16:41,472 - INFO - [diffusion][Epoch 11380] diffusion learning rate: 0.001
2024-11-05 04:16:41,474 - INFO - [diffusion][Epoch 11380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:41,475 - INFO - [diffusion][Epoch 11381] Epoch 11382/12000
2024-11-05 04:16:44,271 - INFO - [diffusion][Epoch 11381] diffusion training Loss: 0.0652132024988532
2024-11-05 04:16:44,274 - INFO - [diffusion][Epoch 11381] diffusion learning rate: 0.001
2024-11-05 04:16:44,276 - INFO - [diffusion][Epoch 11381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:44,277 - INFO - [diffusion][Epoch 11382] Epoch 11383/12000
2024-11-05 04:16:47,855 - INFO - [diffusion][Epoch 11382] diffusion training Loss: 0.05402502417564392
2024-11-05 04:16:47,857 - INFO - [diffusion][Epoch 11382] diffusion learning rate: 0.001
2024-11-05 04:16:47,881 - INFO - [diffusion][Epoch 11382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:47,882 - INFO - [diffusion][Epoch 11383] Epoch 11384/12000
2024-11-05 04:16:51,994 - INFO - [diffusion][Epoch 11383] diffusion training Loss: 0.05999777466058731
2024-11-05 04:16:51,997 - INFO - [diffusion][Epoch 11383] diffusion learning rate: 0.001
2024-11-05 04:16:51,998 - INFO - [diffusion][Epoch 11383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:52,000 - INFO - [diffusion][Epoch 11384] Epoch 11385/12000
2024-11-05 04:16:55,400 - INFO - [diffusion][Epoch 11384] diffusion training Loss: 0.061417498625814915
2024-11-05 04:16:55,402 - INFO - [diffusion][Epoch 11384] diffusion learning rate: 0.001
2024-11-05 04:16:55,404 - INFO - [diffusion][Epoch 11384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:55,405 - INFO - [diffusion][Epoch 11385] Epoch 11386/12000
2024-11-05 04:16:58,841 - INFO - [diffusion][Epoch 11385] diffusion training Loss: 0.060153950937092304
2024-11-05 04:16:58,844 - INFO - [diffusion][Epoch 11385] diffusion learning rate: 0.001
2024-11-05 04:16:58,846 - INFO - [diffusion][Epoch 11385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:58,847 - INFO - [diffusion][Epoch 11386] Epoch 11387/12000
2024-11-05 04:17:02,231 - INFO - [diffusion][Epoch 11386] diffusion training Loss: 0.058137343265116215
2024-11-05 04:17:02,233 - INFO - [diffusion][Epoch 11386] diffusion learning rate: 0.001
2024-11-05 04:17:02,235 - INFO - [diffusion][Epoch 11386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:02,236 - INFO - [diffusion][Epoch 11387] Epoch 11388/12000
2024-11-05 04:17:05,698 - INFO - [diffusion][Epoch 11387] diffusion training Loss: 0.05778904724866152
2024-11-05 04:17:05,700 - INFO - [diffusion][Epoch 11387] diffusion learning rate: 0.001
2024-11-05 04:17:05,702 - INFO - [diffusion][Epoch 11387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:05,704 - INFO - [diffusion][Epoch 11388] Epoch 11389/12000
2024-11-05 04:17:09,242 - INFO - [diffusion][Epoch 11388] diffusion training Loss: 0.05757574550807476
2024-11-05 04:17:09,245 - INFO - [diffusion][Epoch 11388] diffusion learning rate: 0.001
2024-11-05 04:17:09,247 - INFO - [diffusion][Epoch 11388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:09,248 - INFO - [diffusion][Epoch 11389] Epoch 11390/12000
2024-11-05 04:17:12,762 - INFO - [diffusion][Epoch 11389] diffusion training Loss: 0.059693058021366596
2024-11-05 04:17:12,817 - INFO - [diffusion][Epoch 11389] diffusion learning rate: 0.001
2024-11-05 04:17:12,820 - INFO - [diffusion][Epoch 11389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:12,822 - INFO - [diffusion][Epoch 11390] Epoch 11391/12000
2024-11-05 04:17:15,647 - INFO - [diffusion][Epoch 11390] diffusion training Loss: 0.0584664149209857
2024-11-05 04:17:15,649 - INFO - [diffusion][Epoch 11390] diffusion learning rate: 0.001
2024-11-05 04:17:15,651 - INFO - [diffusion][Epoch 11390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:15,653 - INFO - [diffusion][Epoch 11391] Epoch 11392/12000
2024-11-05 04:17:18,555 - INFO - [diffusion][Epoch 11391] diffusion training Loss: 0.05942705739289522
2024-11-05 04:17:18,557 - INFO - [diffusion][Epoch 11391] diffusion learning rate: 0.001
2024-11-05 04:17:18,558 - INFO - [diffusion][Epoch 11391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:18,560 - INFO - [diffusion][Epoch 11392] Epoch 11393/12000
2024-11-05 04:17:21,701 - INFO - [diffusion][Epoch 11392] diffusion training Loss: 0.05928885005414486
2024-11-05 04:17:21,703 - INFO - [diffusion][Epoch 11392] diffusion learning rate: 0.001
2024-11-05 04:17:21,705 - INFO - [diffusion][Epoch 11392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:21,708 - INFO - [diffusion][Epoch 11393] Epoch 11394/12000
2024-11-05 04:17:24,633 - INFO - [diffusion][Epoch 11393] diffusion training Loss: 0.056669156067073345
2024-11-05 04:17:24,635 - INFO - [diffusion][Epoch 11393] diffusion learning rate: 0.001
2024-11-05 04:17:24,637 - INFO - [diffusion][Epoch 11393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:24,638 - INFO - [diffusion][Epoch 11394] Epoch 11395/12000
2024-11-05 04:17:27,976 - INFO - [diffusion][Epoch 11394] diffusion training Loss: 0.05706064496189356
2024-11-05 04:17:27,979 - INFO - [diffusion][Epoch 11394] diffusion learning rate: 0.001
2024-11-05 04:17:27,982 - INFO - [diffusion][Epoch 11394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:27,984 - INFO - [diffusion][Epoch 11395] Epoch 11396/12000
2024-11-05 04:17:30,859 - INFO - [diffusion][Epoch 11395] diffusion training Loss: 0.0563837606459856
2024-11-05 04:17:30,861 - INFO - [diffusion][Epoch 11395] diffusion learning rate: 0.001
2024-11-05 04:17:30,862 - INFO - [diffusion][Epoch 11395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:30,864 - INFO - [diffusion][Epoch 11396] Epoch 11397/12000
2024-11-05 04:17:33,649 - INFO - [diffusion][Epoch 11396] diffusion training Loss: 0.05479831714183092
2024-11-05 04:17:33,651 - INFO - [diffusion][Epoch 11396] diffusion learning rate: 0.001
2024-11-05 04:17:33,653 - INFO - [diffusion][Epoch 11396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:33,654 - INFO - [diffusion][Epoch 11397] Epoch 11398/12000
2024-11-05 04:17:36,784 - INFO - [diffusion][Epoch 11397] diffusion training Loss: 0.05768025107681751
2024-11-05 04:17:36,786 - INFO - [diffusion][Epoch 11397] diffusion learning rate: 0.001
2024-11-05 04:17:36,788 - INFO - [diffusion][Epoch 11397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:36,789 - INFO - [diffusion][Epoch 11398] Epoch 11399/12000
2024-11-05 04:17:39,666 - INFO - [diffusion][Epoch 11398] diffusion training Loss: 0.06527830846607685
2024-11-05 04:17:39,668 - INFO - [diffusion][Epoch 11398] diffusion learning rate: 0.001
2024-11-05 04:17:39,670 - INFO - [diffusion][Epoch 11398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:39,671 - INFO - [diffusion][Epoch 11399] Epoch 11400/12000
2024-11-05 04:17:42,909 - INFO - [diffusion][Epoch 11399] diffusion training Loss: 0.060521190986037254
2024-11-05 04:17:42,911 - INFO - [diffusion][Epoch 11399] diffusion learning rate: 0.001
2024-11-05 04:17:42,912 - INFO - [diffusion][Epoch 11399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:42,914 - INFO - [diffusion][Epoch 11400] Epoch 11401/12000
2024-11-05 04:17:45,634 - INFO - [diffusion][Epoch 11400] diffusion training Loss: 0.06513029895722866
2024-11-05 04:17:45,636 - INFO - [diffusion][Epoch 11400] diffusion learning rate: 0.001
2024-11-05 04:17:45,638 - INFO - [diffusion][Epoch 11400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:45,639 - INFO - [diffusion][Epoch 11401] Epoch 11402/12000
2024-11-05 04:17:48,534 - INFO - [diffusion][Epoch 11401] diffusion training Loss: 0.06031040754169226
2024-11-05 04:17:48,536 - INFO - [diffusion][Epoch 11401] diffusion learning rate: 0.001
2024-11-05 04:17:48,538 - INFO - [diffusion][Epoch 11401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:48,539 - INFO - [diffusion][Epoch 11402] Epoch 11403/12000
2024-11-05 04:17:51,772 - INFO - [diffusion][Epoch 11402] diffusion training Loss: 0.06071343366056681
2024-11-05 04:17:51,774 - INFO - [diffusion][Epoch 11402] diffusion learning rate: 0.001
2024-11-05 04:17:51,776 - INFO - [diffusion][Epoch 11402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:51,777 - INFO - [diffusion][Epoch 11403] Epoch 11404/12000
2024-11-05 04:17:54,538 - INFO - [diffusion][Epoch 11403] diffusion training Loss: 0.054396115243434906
2024-11-05 04:17:54,540 - INFO - [diffusion][Epoch 11403] diffusion learning rate: 0.001
2024-11-05 04:17:54,542 - INFO - [diffusion][Epoch 11403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:54,543 - INFO - [diffusion][Epoch 11404] Epoch 11405/12000
2024-11-05 04:17:58,021 - INFO - [diffusion][Epoch 11404] diffusion training Loss: 0.05605953838676214
2024-11-05 04:17:58,023 - INFO - [diffusion][Epoch 11404] diffusion learning rate: 0.001
2024-11-05 04:17:58,025 - INFO - [diffusion][Epoch 11404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:58,026 - INFO - [diffusion][Epoch 11405] Epoch 11406/12000
2024-11-05 04:18:00,932 - INFO - [diffusion][Epoch 11405] diffusion training Loss: 0.059055862948298454
2024-11-05 04:18:00,934 - INFO - [diffusion][Epoch 11405] diffusion learning rate: 0.001
2024-11-05 04:18:00,936 - INFO - [diffusion][Epoch 11405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:00,937 - INFO - [diffusion][Epoch 11406] Epoch 11407/12000
2024-11-05 04:18:04,266 - INFO - [diffusion][Epoch 11406] diffusion training Loss: 0.052556213922798634
2024-11-05 04:18:04,268 - INFO - [diffusion][Epoch 11406] diffusion learning rate: 0.001
2024-11-05 04:18:04,270 - INFO - [diffusion][Epoch 11406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:04,271 - INFO - [diffusion][Epoch 11407] Epoch 11408/12000
2024-11-05 04:18:07,167 - INFO - [diffusion][Epoch 11407] diffusion training Loss: 0.05917395371943712
2024-11-05 04:18:07,169 - INFO - [diffusion][Epoch 11407] diffusion learning rate: 0.001
2024-11-05 04:18:07,171 - INFO - [diffusion][Epoch 11407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:07,173 - INFO - [diffusion][Epoch 11408] Epoch 11409/12000
2024-11-05 04:18:10,378 - INFO - [diffusion][Epoch 11408] diffusion training Loss: 0.05988730303943157
2024-11-05 04:18:10,380 - INFO - [diffusion][Epoch 11408] diffusion learning rate: 0.001
2024-11-05 04:18:10,382 - INFO - [diffusion][Epoch 11408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:10,383 - INFO - [diffusion][Epoch 11409] Epoch 11410/12000
2024-11-05 04:18:14,079 - INFO - [diffusion][Epoch 11409] diffusion training Loss: 0.058018739335238934
2024-11-05 04:18:14,081 - INFO - [diffusion][Epoch 11409] diffusion learning rate: 0.001
2024-11-05 04:18:14,083 - INFO - [diffusion][Epoch 11409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:14,084 - INFO - [diffusion][Epoch 11410] Epoch 11411/12000
2024-11-05 04:18:17,013 - INFO - [diffusion][Epoch 11410] diffusion training Loss: 0.05874762963503599
2024-11-05 04:18:17,015 - INFO - [diffusion][Epoch 11410] diffusion learning rate: 0.001
2024-11-05 04:18:17,017 - INFO - [diffusion][Epoch 11410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:17,019 - INFO - [diffusion][Epoch 11411] Epoch 11412/12000
2024-11-05 04:18:19,934 - INFO - [diffusion][Epoch 11411] diffusion training Loss: 0.05749738682061434
2024-11-05 04:18:19,936 - INFO - [diffusion][Epoch 11411] diffusion learning rate: 0.001
2024-11-05 04:18:19,938 - INFO - [diffusion][Epoch 11411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:19,940 - INFO - [diffusion][Epoch 11412] Epoch 11413/12000
2024-11-05 04:18:24,412 - INFO - [diffusion][Epoch 11412] diffusion training Loss: 0.06278929207473993
2024-11-05 04:18:24,414 - INFO - [diffusion][Epoch 11412] diffusion learning rate: 0.001
2024-11-05 04:18:24,416 - INFO - [diffusion][Epoch 11412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:24,418 - INFO - [diffusion][Epoch 11413] Epoch 11414/12000
2024-11-05 04:18:27,647 - INFO - [diffusion][Epoch 11413] diffusion training Loss: 0.057377444580197334
2024-11-05 04:18:27,649 - INFO - [diffusion][Epoch 11413] diffusion learning rate: 0.001
2024-11-05 04:18:27,651 - INFO - [diffusion][Epoch 11413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:27,652 - INFO - [diffusion][Epoch 11414] Epoch 11415/12000
2024-11-05 04:18:30,791 - INFO - [diffusion][Epoch 11414] diffusion training Loss: 0.05878772772848606
2024-11-05 04:18:30,793 - INFO - [diffusion][Epoch 11414] diffusion learning rate: 0.001
2024-11-05 04:18:30,794 - INFO - [diffusion][Epoch 11414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:30,795 - INFO - [diffusion][Epoch 11415] Epoch 11416/12000
2024-11-05 04:18:34,265 - INFO - [diffusion][Epoch 11415] diffusion training Loss: 0.05989821068942547
2024-11-05 04:18:34,267 - INFO - [diffusion][Epoch 11415] diffusion learning rate: 0.001
2024-11-05 04:18:34,269 - INFO - [diffusion][Epoch 11415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:34,270 - INFO - [diffusion][Epoch 11416] Epoch 11417/12000
2024-11-05 04:18:37,186 - INFO - [diffusion][Epoch 11416] diffusion training Loss: 0.053057994693517685
2024-11-05 04:18:37,188 - INFO - [diffusion][Epoch 11416] diffusion learning rate: 0.001
2024-11-05 04:18:37,190 - INFO - [diffusion][Epoch 11416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:37,191 - INFO - [diffusion][Epoch 11417] Epoch 11418/12000
2024-11-05 04:18:41,016 - INFO - [diffusion][Epoch 11417] diffusion training Loss: 0.055461280047893524
2024-11-05 04:18:41,018 - INFO - [diffusion][Epoch 11417] diffusion learning rate: 0.001
2024-11-05 04:18:41,020 - INFO - [diffusion][Epoch 11417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:41,021 - INFO - [diffusion][Epoch 11418] Epoch 11419/12000
2024-11-05 04:18:44,539 - INFO - [diffusion][Epoch 11418] diffusion training Loss: 0.0570236761122942
2024-11-05 04:18:44,541 - INFO - [diffusion][Epoch 11418] diffusion learning rate: 0.001
2024-11-05 04:18:44,542 - INFO - [diffusion][Epoch 11418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:44,544 - INFO - [diffusion][Epoch 11419] Epoch 11420/12000
2024-11-05 04:18:48,011 - INFO - [diffusion][Epoch 11419] diffusion training Loss: 0.05547854024916887
2024-11-05 04:18:48,013 - INFO - [diffusion][Epoch 11419] diffusion learning rate: 0.001
2024-11-05 04:18:48,015 - INFO - [diffusion][Epoch 11419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:48,016 - INFO - [diffusion][Epoch 11420] Epoch 11421/12000
2024-11-05 04:18:51,494 - INFO - [diffusion][Epoch 11420] diffusion training Loss: 0.06379834190011024
2024-11-05 04:18:51,497 - INFO - [diffusion][Epoch 11420] diffusion learning rate: 0.001
2024-11-05 04:18:51,498 - INFO - [diffusion][Epoch 11420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:51,499 - INFO - [diffusion][Epoch 11421] Epoch 11422/12000
2024-11-05 04:18:54,933 - INFO - [diffusion][Epoch 11421] diffusion training Loss: 0.0570349358022213
2024-11-05 04:18:54,935 - INFO - [diffusion][Epoch 11421] diffusion learning rate: 0.001
2024-11-05 04:18:54,936 - INFO - [diffusion][Epoch 11421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:54,938 - INFO - [diffusion][Epoch 11422] Epoch 11423/12000
2024-11-05 04:18:58,454 - INFO - [diffusion][Epoch 11422] diffusion training Loss: 0.06235146429389715
2024-11-05 04:18:58,456 - INFO - [diffusion][Epoch 11422] diffusion learning rate: 0.001
2024-11-05 04:18:58,458 - INFO - [diffusion][Epoch 11422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:58,460 - INFO - [diffusion][Epoch 11423] Epoch 11424/12000
2024-11-05 04:19:01,989 - INFO - [diffusion][Epoch 11423] diffusion training Loss: 0.050198644399642944
2024-11-05 04:19:01,992 - INFO - [diffusion][Epoch 11423] diffusion learning rate: 0.001
2024-11-05 04:19:01,994 - INFO - [diffusion][Epoch 11423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:01,996 - INFO - [diffusion][Epoch 11424] Epoch 11425/12000
2024-11-05 04:19:05,395 - INFO - [diffusion][Epoch 11424] diffusion training Loss: 0.05806293152272701
2024-11-05 04:19:05,398 - INFO - [diffusion][Epoch 11424] diffusion learning rate: 0.001
2024-11-05 04:19:05,400 - INFO - [diffusion][Epoch 11424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:05,401 - INFO - [diffusion][Epoch 11425] Epoch 11426/12000
2024-11-05 04:19:08,262 - INFO - [diffusion][Epoch 11425] diffusion training Loss: 0.05507747270166874
2024-11-05 04:19:08,264 - INFO - [diffusion][Epoch 11425] diffusion learning rate: 0.001
2024-11-05 04:19:08,265 - INFO - [diffusion][Epoch 11425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:08,267 - INFO - [diffusion][Epoch 11426] Epoch 11427/12000
2024-11-05 04:19:11,641 - INFO - [diffusion][Epoch 11426] diffusion training Loss: 0.05584538076072931
2024-11-05 04:19:11,643 - INFO - [diffusion][Epoch 11426] diffusion learning rate: 0.001
2024-11-05 04:19:11,645 - INFO - [diffusion][Epoch 11426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:11,646 - INFO - [diffusion][Epoch 11427] Epoch 11428/12000
2024-11-05 04:19:14,529 - INFO - [diffusion][Epoch 11427] diffusion training Loss: 0.05672069173306227
2024-11-05 04:19:14,531 - INFO - [diffusion][Epoch 11427] diffusion learning rate: 0.001
2024-11-05 04:19:14,533 - INFO - [diffusion][Epoch 11427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:14,534 - INFO - [diffusion][Epoch 11428] Epoch 11429/12000
2024-11-05 04:19:17,415 - INFO - [diffusion][Epoch 11428] diffusion training Loss: 0.058046141639351845
2024-11-05 04:19:17,417 - INFO - [diffusion][Epoch 11428] diffusion learning rate: 0.001
2024-11-05 04:19:17,419 - INFO - [diffusion][Epoch 11428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:17,420 - INFO - [diffusion][Epoch 11429] Epoch 11430/12000
2024-11-05 04:19:20,253 - INFO - [diffusion][Epoch 11429] diffusion training Loss: 0.058967833407223225
2024-11-05 04:19:20,255 - INFO - [diffusion][Epoch 11429] diffusion learning rate: 0.001
2024-11-05 04:19:20,257 - INFO - [diffusion][Epoch 11429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:20,259 - INFO - [diffusion][Epoch 11430] Epoch 11431/12000
2024-11-05 04:19:23,177 - INFO - [diffusion][Epoch 11430] diffusion training Loss: 0.06321481708437204
2024-11-05 04:19:23,179 - INFO - [diffusion][Epoch 11430] diffusion learning rate: 0.001
2024-11-05 04:19:23,181 - INFO - [diffusion][Epoch 11430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:23,182 - INFO - [diffusion][Epoch 11431] Epoch 11432/12000
2024-11-05 04:19:26,074 - INFO - [diffusion][Epoch 11431] diffusion training Loss: 0.051281729713082314
2024-11-05 04:19:26,076 - INFO - [diffusion][Epoch 11431] diffusion learning rate: 0.001
2024-11-05 04:19:26,104 - INFO - [diffusion][Epoch 11431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:26,105 - INFO - [diffusion][Epoch 11432] Epoch 11433/12000
2024-11-05 04:19:29,030 - INFO - [diffusion][Epoch 11432] diffusion training Loss: 0.058679526671767235
2024-11-05 04:19:29,032 - INFO - [diffusion][Epoch 11432] diffusion learning rate: 0.001
2024-11-05 04:19:29,034 - INFO - [diffusion][Epoch 11432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:29,035 - INFO - [diffusion][Epoch 11433] Epoch 11434/12000
2024-11-05 04:19:31,919 - INFO - [diffusion][Epoch 11433] diffusion training Loss: 0.05649884883314371
2024-11-05 04:19:31,921 - INFO - [diffusion][Epoch 11433] diffusion learning rate: 0.001
2024-11-05 04:19:31,948 - INFO - [diffusion][Epoch 11433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:31,950 - INFO - [diffusion][Epoch 11434] Epoch 11435/12000
2024-11-05 04:19:34,896 - INFO - [diffusion][Epoch 11434] diffusion training Loss: 0.05947697162628174
2024-11-05 04:19:34,898 - INFO - [diffusion][Epoch 11434] diffusion learning rate: 0.001
2024-11-05 04:19:34,900 - INFO - [diffusion][Epoch 11434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:34,901 - INFO - [diffusion][Epoch 11435] Epoch 11436/12000
2024-11-05 04:19:38,079 - INFO - [diffusion][Epoch 11435] diffusion training Loss: 0.0562784718349576
2024-11-05 04:19:38,081 - INFO - [diffusion][Epoch 11435] diffusion learning rate: 0.001
2024-11-05 04:19:38,083 - INFO - [diffusion][Epoch 11435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:38,084 - INFO - [diffusion][Epoch 11436] Epoch 11437/12000
2024-11-05 04:19:40,958 - INFO - [diffusion][Epoch 11436] diffusion training Loss: 0.057003121823072433
2024-11-05 04:19:40,960 - INFO - [diffusion][Epoch 11436] diffusion learning rate: 0.001
2024-11-05 04:19:40,962 - INFO - [diffusion][Epoch 11436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:40,963 - INFO - [diffusion][Epoch 11437] Epoch 11438/12000
2024-11-05 04:19:43,828 - INFO - [diffusion][Epoch 11437] diffusion training Loss: 0.057524651288986206
2024-11-05 04:19:43,830 - INFO - [diffusion][Epoch 11437] diffusion learning rate: 0.001
2024-11-05 04:19:43,832 - INFO - [diffusion][Epoch 11437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:43,833 - INFO - [diffusion][Epoch 11438] Epoch 11439/12000
2024-11-05 04:19:47,274 - INFO - [diffusion][Epoch 11438] diffusion training Loss: 0.057107807137072086
2024-11-05 04:19:47,276 - INFO - [diffusion][Epoch 11438] diffusion learning rate: 0.001
2024-11-05 04:19:47,277 - INFO - [diffusion][Epoch 11438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:47,279 - INFO - [diffusion][Epoch 11439] Epoch 11440/12000
2024-11-05 04:19:50,162 - INFO - [diffusion][Epoch 11439] diffusion training Loss: 0.061580623500049114
2024-11-05 04:19:50,164 - INFO - [diffusion][Epoch 11439] diffusion learning rate: 0.001
2024-11-05 04:19:50,166 - INFO - [diffusion][Epoch 11439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:50,167 - INFO - [diffusion][Epoch 11440] Epoch 11441/12000
2024-11-05 04:19:53,487 - INFO - [diffusion][Epoch 11440] diffusion training Loss: 0.05519479047507048
2024-11-05 04:19:53,489 - INFO - [diffusion][Epoch 11440] diffusion learning rate: 0.001
2024-11-05 04:19:53,491 - INFO - [diffusion][Epoch 11440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:53,492 - INFO - [diffusion][Epoch 11441] Epoch 11442/12000
2024-11-05 04:19:56,432 - INFO - [diffusion][Epoch 11441] diffusion training Loss: 0.06141545530408621
2024-11-05 04:19:56,434 - INFO - [diffusion][Epoch 11441] diffusion learning rate: 0.001
2024-11-05 04:19:56,436 - INFO - [diffusion][Epoch 11441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:56,438 - INFO - [diffusion][Epoch 11442] Epoch 11443/12000
2024-11-05 04:19:59,396 - INFO - [diffusion][Epoch 11442] diffusion training Loss: 0.0617718230932951
2024-11-05 04:19:59,398 - INFO - [diffusion][Epoch 11442] diffusion learning rate: 0.001
2024-11-05 04:19:59,401 - INFO - [diffusion][Epoch 11442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:59,402 - INFO - [diffusion][Epoch 11443] Epoch 11444/12000
2024-11-05 04:20:03,991 - INFO - [diffusion][Epoch 11443] diffusion training Loss: 0.05767460446804762
2024-11-05 04:20:03,993 - INFO - [diffusion][Epoch 11443] diffusion learning rate: 0.001
2024-11-05 04:20:03,995 - INFO - [diffusion][Epoch 11443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:03,996 - INFO - [diffusion][Epoch 11444] Epoch 11445/12000
2024-11-05 04:20:06,859 - INFO - [diffusion][Epoch 11444] diffusion training Loss: 0.0600808784365654
2024-11-05 04:20:06,861 - INFO - [diffusion][Epoch 11444] diffusion learning rate: 0.001
2024-11-05 04:20:06,863 - INFO - [diffusion][Epoch 11444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:06,864 - INFO - [diffusion][Epoch 11445] Epoch 11446/12000
2024-11-05 04:20:09,691 - INFO - [diffusion][Epoch 11445] diffusion training Loss: 0.05771091114729643
2024-11-05 04:20:09,694 - INFO - [diffusion][Epoch 11445] diffusion learning rate: 0.001
2024-11-05 04:20:09,696 - INFO - [diffusion][Epoch 11445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:09,698 - INFO - [diffusion][Epoch 11446] Epoch 11447/12000
2024-11-05 04:20:13,942 - INFO - [diffusion][Epoch 11446] diffusion training Loss: 0.06008681748062372
2024-11-05 04:20:13,944 - INFO - [diffusion][Epoch 11446] diffusion learning rate: 0.001
2024-11-05 04:20:13,946 - INFO - [diffusion][Epoch 11446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:13,947 - INFO - [diffusion][Epoch 11447] Epoch 11448/12000
2024-11-05 04:20:17,439 - INFO - [diffusion][Epoch 11447] diffusion training Loss: 0.061156826093792915
2024-11-05 04:20:17,441 - INFO - [diffusion][Epoch 11447] diffusion learning rate: 0.001
2024-11-05 04:20:17,443 - INFO - [diffusion][Epoch 11447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:17,444 - INFO - [diffusion][Epoch 11448] Epoch 11449/12000
2024-11-05 04:20:20,367 - INFO - [diffusion][Epoch 11448] diffusion training Loss: 0.055315591394901276
2024-11-05 04:20:20,370 - INFO - [diffusion][Epoch 11448] diffusion learning rate: 0.001
2024-11-05 04:20:20,371 - INFO - [diffusion][Epoch 11448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:20,373 - INFO - [diffusion][Epoch 11449] Epoch 11450/12000
2024-11-05 04:20:23,889 - INFO - [diffusion][Epoch 11449] diffusion training Loss: 0.056458087638020515
2024-11-05 04:20:23,891 - INFO - [diffusion][Epoch 11449] diffusion learning rate: 0.001
2024-11-05 04:20:23,893 - INFO - [diffusion][Epoch 11449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:23,894 - INFO - [diffusion][Epoch 11450] Epoch 11451/12000
2024-11-05 04:20:26,653 - INFO - [diffusion][Epoch 11450] diffusion training Loss: 0.05542308837175369
2024-11-05 04:20:26,655 - INFO - [diffusion][Epoch 11450] diffusion learning rate: 0.001
2024-11-05 04:20:26,657 - INFO - [diffusion][Epoch 11450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:26,659 - INFO - [diffusion][Epoch 11451] Epoch 11452/12000
2024-11-05 04:20:30,371 - INFO - [diffusion][Epoch 11451] diffusion training Loss: 0.05867449566721916
2024-11-05 04:20:30,373 - INFO - [diffusion][Epoch 11451] diffusion learning rate: 0.001
2024-11-05 04:20:30,375 - INFO - [diffusion][Epoch 11451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:30,377 - INFO - [diffusion][Epoch 11452] Epoch 11453/12000
2024-11-05 04:20:33,259 - INFO - [diffusion][Epoch 11452] diffusion training Loss: 0.05707188602536917
2024-11-05 04:20:33,261 - INFO - [diffusion][Epoch 11452] diffusion learning rate: 0.001
2024-11-05 04:20:33,263 - INFO - [diffusion][Epoch 11452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:33,264 - INFO - [diffusion][Epoch 11453] Epoch 11454/12000
2024-11-05 04:20:36,149 - INFO - [diffusion][Epoch 11453] diffusion training Loss: 0.06331414729356766
2024-11-05 04:20:36,151 - INFO - [diffusion][Epoch 11453] diffusion learning rate: 0.001
2024-11-05 04:20:36,153 - INFO - [diffusion][Epoch 11453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:36,154 - INFO - [diffusion][Epoch 11454] Epoch 11455/12000
2024-11-05 04:20:39,093 - INFO - [diffusion][Epoch 11454] diffusion training Loss: 0.0650331424549222
2024-11-05 04:20:39,095 - INFO - [diffusion][Epoch 11454] diffusion learning rate: 0.001
2024-11-05 04:20:39,097 - INFO - [diffusion][Epoch 11454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:39,098 - INFO - [diffusion][Epoch 11455] Epoch 11456/12000
2024-11-05 04:20:41,948 - INFO - [diffusion][Epoch 11455] diffusion training Loss: 0.053268554620444775
2024-11-05 04:20:41,950 - INFO - [diffusion][Epoch 11455] diffusion learning rate: 0.001
2024-11-05 04:20:41,952 - INFO - [diffusion][Epoch 11455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:41,953 - INFO - [diffusion][Epoch 11456] Epoch 11457/12000
2024-11-05 04:20:45,098 - INFO - [diffusion][Epoch 11456] diffusion training Loss: 0.05777887161821127
2024-11-05 04:20:45,100 - INFO - [diffusion][Epoch 11456] diffusion learning rate: 0.001
2024-11-05 04:20:45,102 - INFO - [diffusion][Epoch 11456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:45,104 - INFO - [diffusion][Epoch 11457] Epoch 11458/12000
2024-11-05 04:20:48,009 - INFO - [diffusion][Epoch 11457] diffusion training Loss: 0.05479972530156374
2024-11-05 04:20:48,011 - INFO - [diffusion][Epoch 11457] diffusion learning rate: 0.001
2024-11-05 04:20:48,013 - INFO - [diffusion][Epoch 11457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:48,014 - INFO - [diffusion][Epoch 11458] Epoch 11459/12000
2024-11-05 04:20:50,885 - INFO - [diffusion][Epoch 11458] diffusion training Loss: 0.058037273585796356
2024-11-05 04:20:50,887 - INFO - [diffusion][Epoch 11458] diffusion learning rate: 0.001
2024-11-05 04:20:50,889 - INFO - [diffusion][Epoch 11458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:50,890 - INFO - [diffusion][Epoch 11459] Epoch 11460/12000
2024-11-05 04:20:53,782 - INFO - [diffusion][Epoch 11459] diffusion training Loss: 0.057321841828525066
2024-11-05 04:20:53,783 - INFO - [diffusion][Epoch 11459] diffusion learning rate: 0.001
2024-11-05 04:20:53,785 - INFO - [diffusion][Epoch 11459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:53,787 - INFO - [diffusion][Epoch 11460] Epoch 11461/12000
2024-11-05 04:20:56,753 - INFO - [diffusion][Epoch 11460] diffusion training Loss: 0.05514565762132406
2024-11-05 04:20:56,755 - INFO - [diffusion][Epoch 11460] diffusion learning rate: 0.001
2024-11-05 04:20:56,757 - INFO - [diffusion][Epoch 11460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:56,758 - INFO - [diffusion][Epoch 11461] Epoch 11462/12000
2024-11-05 04:20:59,626 - INFO - [diffusion][Epoch 11461] diffusion training Loss: 0.05936801340430975
2024-11-05 04:20:59,629 - INFO - [diffusion][Epoch 11461] diffusion learning rate: 0.001
2024-11-05 04:20:59,630 - INFO - [diffusion][Epoch 11461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:59,632 - INFO - [diffusion][Epoch 11462] Epoch 11463/12000
2024-11-05 04:21:02,462 - INFO - [diffusion][Epoch 11462] diffusion training Loss: 0.05945809185504913
2024-11-05 04:21:02,464 - INFO - [diffusion][Epoch 11462] diffusion learning rate: 0.001
2024-11-05 04:21:02,466 - INFO - [diffusion][Epoch 11462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:02,467 - INFO - [diffusion][Epoch 11463] Epoch 11464/12000
2024-11-05 04:21:05,379 - INFO - [diffusion][Epoch 11463] diffusion training Loss: 0.06569928582757711
2024-11-05 04:21:05,381 - INFO - [diffusion][Epoch 11463] diffusion learning rate: 0.001
2024-11-05 04:21:05,383 - INFO - [diffusion][Epoch 11463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:05,384 - INFO - [diffusion][Epoch 11464] Epoch 11465/12000
2024-11-05 04:21:08,225 - INFO - [diffusion][Epoch 11464] diffusion training Loss: 0.06277955416589975
2024-11-05 04:21:08,227 - INFO - [diffusion][Epoch 11464] diffusion learning rate: 0.001
2024-11-05 04:21:08,229 - INFO - [diffusion][Epoch 11464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:08,231 - INFO - [diffusion][Epoch 11465] Epoch 11466/12000
2024-11-05 04:21:11,019 - INFO - [diffusion][Epoch 11465] diffusion training Loss: 0.0645076259970665
2024-11-05 04:21:11,021 - INFO - [diffusion][Epoch 11465] diffusion learning rate: 0.001
2024-11-05 04:21:11,023 - INFO - [diffusion][Epoch 11465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:11,024 - INFO - [diffusion][Epoch 11466] Epoch 11467/12000
2024-11-05 04:21:13,924 - INFO - [diffusion][Epoch 11466] diffusion training Loss: 0.060580761171877384
2024-11-05 04:21:13,926 - INFO - [diffusion][Epoch 11466] diffusion learning rate: 0.001
2024-11-05 04:21:13,928 - INFO - [diffusion][Epoch 11466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:13,929 - INFO - [diffusion][Epoch 11467] Epoch 11468/12000
2024-11-05 04:21:16,810 - INFO - [diffusion][Epoch 11467] diffusion training Loss: 0.058799357153475285
2024-11-05 04:21:16,812 - INFO - [diffusion][Epoch 11467] diffusion learning rate: 0.001
2024-11-05 04:21:16,814 - INFO - [diffusion][Epoch 11467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:16,815 - INFO - [diffusion][Epoch 11468] Epoch 11469/12000
2024-11-05 04:21:19,665 - INFO - [diffusion][Epoch 11468] diffusion training Loss: 0.05390320438891649
2024-11-05 04:21:19,667 - INFO - [diffusion][Epoch 11468] diffusion learning rate: 0.001
2024-11-05 04:21:19,669 - INFO - [diffusion][Epoch 11468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:19,670 - INFO - [diffusion][Epoch 11469] Epoch 11470/12000
2024-11-05 04:21:22,538 - INFO - [diffusion][Epoch 11469] diffusion training Loss: 0.06089490745216608
2024-11-05 04:21:22,540 - INFO - [diffusion][Epoch 11469] diffusion learning rate: 0.001
2024-11-05 04:21:22,542 - INFO - [diffusion][Epoch 11469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:22,544 - INFO - [diffusion][Epoch 11470] Epoch 11471/12000
2024-11-05 04:21:25,283 - INFO - [diffusion][Epoch 11470] diffusion training Loss: 0.05754366610199213
2024-11-05 04:21:25,285 - INFO - [diffusion][Epoch 11470] diffusion learning rate: 0.001
2024-11-05 04:21:25,287 - INFO - [diffusion][Epoch 11470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:25,288 - INFO - [diffusion][Epoch 11471] Epoch 11472/12000
2024-11-05 04:21:28,037 - INFO - [diffusion][Epoch 11471] diffusion training Loss: 0.05709422007203102
2024-11-05 04:21:28,039 - INFO - [diffusion][Epoch 11471] diffusion learning rate: 0.001
2024-11-05 04:21:28,041 - INFO - [diffusion][Epoch 11471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:28,042 - INFO - [diffusion][Epoch 11472] Epoch 11473/12000
2024-11-05 04:21:30,795 - INFO - [diffusion][Epoch 11472] diffusion training Loss: 0.061689674854278564
2024-11-05 04:21:30,798 - INFO - [diffusion][Epoch 11472] diffusion learning rate: 0.001
2024-11-05 04:21:30,800 - INFO - [diffusion][Epoch 11472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:30,801 - INFO - [diffusion][Epoch 11473] Epoch 11474/12000
2024-11-05 04:21:33,577 - INFO - [diffusion][Epoch 11473] diffusion training Loss: 0.05537715461105108
2024-11-05 04:21:33,579 - INFO - [diffusion][Epoch 11473] diffusion learning rate: 0.001
2024-11-05 04:21:33,581 - INFO - [diffusion][Epoch 11473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:33,582 - INFO - [diffusion][Epoch 11474] Epoch 11475/12000
2024-11-05 04:21:36,399 - INFO - [diffusion][Epoch 11474] diffusion training Loss: 0.05971977207809687
2024-11-05 04:21:36,401 - INFO - [diffusion][Epoch 11474] diffusion learning rate: 0.001
2024-11-05 04:21:36,403 - INFO - [diffusion][Epoch 11474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:36,404 - INFO - [diffusion][Epoch 11475] Epoch 11476/12000
2024-11-05 04:21:39,200 - INFO - [diffusion][Epoch 11475] diffusion training Loss: 0.06304903607815504
2024-11-05 04:21:39,202 - INFO - [diffusion][Epoch 11475] diffusion learning rate: 0.001
2024-11-05 04:21:39,204 - INFO - [diffusion][Epoch 11475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:39,205 - INFO - [diffusion][Epoch 11476] Epoch 11477/12000
2024-11-05 04:21:42,623 - INFO - [diffusion][Epoch 11476] diffusion training Loss: 0.06219108775258064
2024-11-05 04:21:42,625 - INFO - [diffusion][Epoch 11476] diffusion learning rate: 0.001
2024-11-05 04:21:42,627 - INFO - [diffusion][Epoch 11476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:42,628 - INFO - [diffusion][Epoch 11477] Epoch 11478/12000
2024-11-05 04:21:45,502 - INFO - [diffusion][Epoch 11477] diffusion training Loss: 0.054958922788500786
2024-11-05 04:21:45,504 - INFO - [diffusion][Epoch 11477] diffusion learning rate: 0.001
2024-11-05 04:21:45,506 - INFO - [diffusion][Epoch 11477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:45,507 - INFO - [diffusion][Epoch 11478] Epoch 11479/12000
2024-11-05 04:21:48,255 - INFO - [diffusion][Epoch 11478] diffusion training Loss: 0.06258970405906439
2024-11-05 04:21:48,257 - INFO - [diffusion][Epoch 11478] diffusion learning rate: 0.001
2024-11-05 04:21:48,259 - INFO - [diffusion][Epoch 11478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:48,260 - INFO - [diffusion][Epoch 11479] Epoch 11480/12000
2024-11-05 04:21:51,041 - INFO - [diffusion][Epoch 11479] diffusion training Loss: 0.057899235747754574
2024-11-05 04:21:51,043 - INFO - [diffusion][Epoch 11479] diffusion learning rate: 0.001
2024-11-05 04:21:51,070 - INFO - [diffusion][Epoch 11479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:51,072 - INFO - [diffusion][Epoch 11480] Epoch 11481/12000
2024-11-05 04:21:53,857 - INFO - [diffusion][Epoch 11480] diffusion training Loss: 0.059748572297394276
2024-11-05 04:21:53,859 - INFO - [diffusion][Epoch 11480] diffusion learning rate: 0.001
2024-11-05 04:21:53,861 - INFO - [diffusion][Epoch 11480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:53,862 - INFO - [diffusion][Epoch 11481] Epoch 11482/12000
2024-11-05 04:21:56,671 - INFO - [diffusion][Epoch 11481] diffusion training Loss: 0.05843097064644098
2024-11-05 04:21:56,673 - INFO - [diffusion][Epoch 11481] diffusion learning rate: 0.001
2024-11-05 04:21:56,701 - INFO - [diffusion][Epoch 11481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:56,702 - INFO - [diffusion][Epoch 11482] Epoch 11483/12000
2024-11-05 04:21:59,527 - INFO - [diffusion][Epoch 11482] diffusion training Loss: 0.061629461124539375
2024-11-05 04:21:59,529 - INFO - [diffusion][Epoch 11482] diffusion learning rate: 0.001
2024-11-05 04:21:59,531 - INFO - [diffusion][Epoch 11482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:59,532 - INFO - [diffusion][Epoch 11483] Epoch 11484/12000
2024-11-05 04:22:02,377 - INFO - [diffusion][Epoch 11483] diffusion training Loss: 0.05729215778410435
2024-11-05 04:22:02,380 - INFO - [diffusion][Epoch 11483] diffusion learning rate: 0.001
2024-11-05 04:22:02,381 - INFO - [diffusion][Epoch 11483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:02,383 - INFO - [diffusion][Epoch 11484] Epoch 11485/12000
2024-11-05 04:22:05,318 - INFO - [diffusion][Epoch 11484] diffusion training Loss: 0.0616451408714056
2024-11-05 04:22:05,320 - INFO - [diffusion][Epoch 11484] diffusion learning rate: 0.001
2024-11-05 04:22:05,322 - INFO - [diffusion][Epoch 11484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:05,323 - INFO - [diffusion][Epoch 11485] Epoch 11486/12000
2024-11-05 04:22:08,059 - INFO - [diffusion][Epoch 11485] diffusion training Loss: 0.05629394203424454
2024-11-05 04:22:08,061 - INFO - [diffusion][Epoch 11485] diffusion learning rate: 0.001
2024-11-05 04:22:08,063 - INFO - [diffusion][Epoch 11485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:08,064 - INFO - [diffusion][Epoch 11486] Epoch 11487/12000
2024-11-05 04:22:10,922 - INFO - [diffusion][Epoch 11486] diffusion training Loss: 0.058627210557460785
2024-11-05 04:22:10,924 - INFO - [diffusion][Epoch 11486] diffusion learning rate: 0.001
2024-11-05 04:22:10,926 - INFO - [diffusion][Epoch 11486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:10,927 - INFO - [diffusion][Epoch 11487] Epoch 11488/12000
2024-11-05 04:22:13,791 - INFO - [diffusion][Epoch 11487] diffusion training Loss: 0.06110650114715099
2024-11-05 04:22:13,792 - INFO - [diffusion][Epoch 11487] diffusion learning rate: 0.001
2024-11-05 04:22:13,794 - INFO - [diffusion][Epoch 11487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:13,795 - INFO - [diffusion][Epoch 11488] Epoch 11489/12000
2024-11-05 04:22:16,654 - INFO - [diffusion][Epoch 11488] diffusion training Loss: 0.05698103457689285
2024-11-05 04:22:16,657 - INFO - [diffusion][Epoch 11488] diffusion learning rate: 0.001
2024-11-05 04:22:16,658 - INFO - [diffusion][Epoch 11488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:16,659 - INFO - [diffusion][Epoch 11489] Epoch 11490/12000
2024-11-05 04:22:19,368 - INFO - [diffusion][Epoch 11489] diffusion training Loss: 0.05626736022531986
2024-11-05 04:22:19,370 - INFO - [diffusion][Epoch 11489] diffusion learning rate: 0.001
2024-11-05 04:22:19,371 - INFO - [diffusion][Epoch 11489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:19,373 - INFO - [diffusion][Epoch 11490] Epoch 11491/12000
2024-11-05 04:22:22,140 - INFO - [diffusion][Epoch 11490] diffusion training Loss: 0.05800746474415064
2024-11-05 04:22:22,142 - INFO - [diffusion][Epoch 11490] diffusion learning rate: 0.001
2024-11-05 04:22:22,144 - INFO - [diffusion][Epoch 11490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:22,145 - INFO - [diffusion][Epoch 11491] Epoch 11492/12000
2024-11-05 04:22:25,051 - INFO - [diffusion][Epoch 11491] diffusion training Loss: 0.05801223777234554
2024-11-05 04:22:25,053 - INFO - [diffusion][Epoch 11491] diffusion learning rate: 0.001
2024-11-05 04:22:25,055 - INFO - [diffusion][Epoch 11491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:25,056 - INFO - [diffusion][Epoch 11492] Epoch 11493/12000
2024-11-05 04:22:27,945 - INFO - [diffusion][Epoch 11492] diffusion training Loss: 0.061142053455114365
2024-11-05 04:22:27,946 - INFO - [diffusion][Epoch 11492] diffusion learning rate: 0.001
2024-11-05 04:22:27,948 - INFO - [diffusion][Epoch 11492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:27,950 - INFO - [diffusion][Epoch 11493] Epoch 11494/12000
2024-11-05 04:22:30,814 - INFO - [diffusion][Epoch 11493] diffusion training Loss: 0.05669240280985832
2024-11-05 04:22:30,817 - INFO - [diffusion][Epoch 11493] diffusion learning rate: 0.001
2024-11-05 04:22:30,819 - INFO - [diffusion][Epoch 11493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:30,820 - INFO - [diffusion][Epoch 11494] Epoch 11495/12000
2024-11-05 04:22:33,678 - INFO - [diffusion][Epoch 11494] diffusion training Loss: 0.060454110614955425
2024-11-05 04:22:33,680 - INFO - [diffusion][Epoch 11494] diffusion learning rate: 0.001
2024-11-05 04:22:33,682 - INFO - [diffusion][Epoch 11494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:33,683 - INFO - [diffusion][Epoch 11495] Epoch 11496/12000
2024-11-05 04:22:36,539 - INFO - [diffusion][Epoch 11495] diffusion training Loss: 0.05997557379305363
2024-11-05 04:22:36,541 - INFO - [diffusion][Epoch 11495] diffusion learning rate: 0.001
2024-11-05 04:22:36,543 - INFO - [diffusion][Epoch 11495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:36,544 - INFO - [diffusion][Epoch 11496] Epoch 11497/12000
2024-11-05 04:22:39,451 - INFO - [diffusion][Epoch 11496] diffusion training Loss: 0.06431017629802227
2024-11-05 04:22:39,453 - INFO - [diffusion][Epoch 11496] diffusion learning rate: 0.001
2024-11-05 04:22:39,455 - INFO - [diffusion][Epoch 11496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:39,456 - INFO - [diffusion][Epoch 11497] Epoch 11498/12000
2024-11-05 04:22:42,405 - INFO - [diffusion][Epoch 11497] diffusion training Loss: 0.05638557765632868
2024-11-05 04:22:42,406 - INFO - [diffusion][Epoch 11497] diffusion learning rate: 0.001
2024-11-05 04:22:42,408 - INFO - [diffusion][Epoch 11497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:42,409 - INFO - [diffusion][Epoch 11498] Epoch 11499/12000
2024-11-05 04:22:45,224 - INFO - [diffusion][Epoch 11498] diffusion training Loss: 0.061208851635456085
2024-11-05 04:22:45,226 - INFO - [diffusion][Epoch 11498] diffusion learning rate: 0.001
2024-11-05 04:22:45,228 - INFO - [diffusion][Epoch 11498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:45,230 - INFO - [diffusion][Epoch 11499] Epoch 11500/12000
2024-11-05 04:22:48,079 - INFO - [diffusion][Epoch 11499] diffusion training Loss: 0.06185938697308302
2024-11-05 04:22:48,081 - INFO - [diffusion][Epoch 11499] diffusion learning rate: 0.001
2024-11-05 04:22:48,082 - INFO - [diffusion][Epoch 11499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:48,084 - INFO - [diffusion][Epoch 11500] Epoch 11501/12000
2024-11-05 04:22:50,974 - INFO - [diffusion][Epoch 11500] diffusion training Loss: 0.056651396676898
2024-11-05 04:22:50,977 - INFO - [diffusion][Epoch 11500] diffusion learning rate: 0.001
2024-11-05 04:22:50,979 - INFO - [diffusion][Epoch 11500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:50,981 - INFO - [diffusion][Epoch 11501] Epoch 11502/12000
2024-11-05 04:22:53,905 - INFO - [diffusion][Epoch 11501] diffusion training Loss: 0.057940926402807236
2024-11-05 04:22:53,907 - INFO - [diffusion][Epoch 11501] diffusion learning rate: 0.001
2024-11-05 04:22:53,934 - INFO - [diffusion][Epoch 11501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:53,936 - INFO - [diffusion][Epoch 11502] Epoch 11503/12000
2024-11-05 04:22:56,699 - INFO - [diffusion][Epoch 11502] diffusion training Loss: 0.0520973764359951
2024-11-05 04:22:56,701 - INFO - [diffusion][Epoch 11502] diffusion learning rate: 0.001
2024-11-05 04:22:56,703 - INFO - [diffusion][Epoch 11502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:56,704 - INFO - [diffusion][Epoch 11503] Epoch 11504/12000
2024-11-05 04:22:59,476 - INFO - [diffusion][Epoch 11503] diffusion training Loss: 0.05241253599524498
2024-11-05 04:22:59,478 - INFO - [diffusion][Epoch 11503] diffusion learning rate: 0.001
2024-11-05 04:22:59,480 - INFO - [diffusion][Epoch 11503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:59,482 - INFO - [diffusion][Epoch 11504] Epoch 11505/12000
2024-11-05 04:23:02,322 - INFO - [diffusion][Epoch 11504] diffusion training Loss: 0.05372766871005297
2024-11-05 04:23:02,325 - INFO - [diffusion][Epoch 11504] diffusion learning rate: 0.001
2024-11-05 04:23:02,326 - INFO - [diffusion][Epoch 11504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:02,328 - INFO - [diffusion][Epoch 11505] Epoch 11506/12000
2024-11-05 04:23:05,086 - INFO - [diffusion][Epoch 11505] diffusion training Loss: 0.05530215334147215
2024-11-05 04:23:05,088 - INFO - [diffusion][Epoch 11505] diffusion learning rate: 0.001
2024-11-05 04:23:05,090 - INFO - [diffusion][Epoch 11505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:05,092 - INFO - [diffusion][Epoch 11506] Epoch 11507/12000
2024-11-05 04:23:07,882 - INFO - [diffusion][Epoch 11506] diffusion training Loss: 0.05758003983646631
2024-11-05 04:23:07,884 - INFO - [diffusion][Epoch 11506] diffusion learning rate: 0.001
2024-11-05 04:23:07,885 - INFO - [diffusion][Epoch 11506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:07,887 - INFO - [diffusion][Epoch 11507] Epoch 11508/12000
2024-11-05 04:23:10,692 - INFO - [diffusion][Epoch 11507] diffusion training Loss: 0.05739763379096985
2024-11-05 04:23:10,694 - INFO - [diffusion][Epoch 11507] diffusion learning rate: 0.001
2024-11-05 04:23:10,696 - INFO - [diffusion][Epoch 11507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:10,697 - INFO - [diffusion][Epoch 11508] Epoch 11509/12000
2024-11-05 04:23:13,557 - INFO - [diffusion][Epoch 11508] diffusion training Loss: 0.057208940386772156
2024-11-05 04:23:13,559 - INFO - [diffusion][Epoch 11508] diffusion learning rate: 0.001
2024-11-05 04:23:13,560 - INFO - [diffusion][Epoch 11508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:13,562 - INFO - [diffusion][Epoch 11509] Epoch 11510/12000
2024-11-05 04:23:16,439 - INFO - [diffusion][Epoch 11509] diffusion training Loss: 0.058125135488808155
2024-11-05 04:23:16,441 - INFO - [diffusion][Epoch 11509] diffusion learning rate: 0.001
2024-11-05 04:23:16,442 - INFO - [diffusion][Epoch 11509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:16,443 - INFO - [diffusion][Epoch 11510] Epoch 11511/12000
2024-11-05 04:23:19,319 - INFO - [diffusion][Epoch 11510] diffusion training Loss: 0.05309597589075565
2024-11-05 04:23:19,321 - INFO - [diffusion][Epoch 11510] diffusion learning rate: 0.001
2024-11-05 04:23:19,322 - INFO - [diffusion][Epoch 11510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:19,324 - INFO - [diffusion][Epoch 11511] Epoch 11512/12000
2024-11-05 04:23:22,198 - INFO - [diffusion][Epoch 11511] diffusion training Loss: 0.054687852039933205
2024-11-05 04:23:22,200 - INFO - [diffusion][Epoch 11511] diffusion learning rate: 0.001
2024-11-05 04:23:22,202 - INFO - [diffusion][Epoch 11511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:22,203 - INFO - [diffusion][Epoch 11512] Epoch 11513/12000
2024-11-05 04:23:24,965 - INFO - [diffusion][Epoch 11512] diffusion training Loss: 0.05611690133810043
2024-11-05 04:23:24,967 - INFO - [diffusion][Epoch 11512] diffusion learning rate: 0.001
2024-11-05 04:23:24,969 - INFO - [diffusion][Epoch 11512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:24,970 - INFO - [diffusion][Epoch 11513] Epoch 11514/12000
2024-11-05 04:23:27,732 - INFO - [diffusion][Epoch 11513] diffusion training Loss: 0.05641193874180317
2024-11-05 04:23:27,734 - INFO - [diffusion][Epoch 11513] diffusion learning rate: 0.001
2024-11-05 04:23:27,736 - INFO - [diffusion][Epoch 11513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:27,737 - INFO - [diffusion][Epoch 11514] Epoch 11515/12000
2024-11-05 04:23:30,559 - INFO - [diffusion][Epoch 11514] diffusion training Loss: 0.05605168268084526
2024-11-05 04:23:30,562 - INFO - [diffusion][Epoch 11514] diffusion learning rate: 0.001
2024-11-05 04:23:30,564 - INFO - [diffusion][Epoch 11514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:30,566 - INFO - [diffusion][Epoch 11515] Epoch 11516/12000
2024-11-05 04:23:33,341 - INFO - [diffusion][Epoch 11515] diffusion training Loss: 0.06321033276617527
2024-11-05 04:23:33,343 - INFO - [diffusion][Epoch 11515] diffusion learning rate: 0.001
2024-11-05 04:23:33,344 - INFO - [diffusion][Epoch 11515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:33,346 - INFO - [diffusion][Epoch 11516] Epoch 11517/12000
2024-11-05 04:23:36,146 - INFO - [diffusion][Epoch 11516] diffusion training Loss: 0.05331096053123474
2024-11-05 04:23:36,147 - INFO - [diffusion][Epoch 11516] diffusion learning rate: 0.001
2024-11-05 04:23:36,149 - INFO - [diffusion][Epoch 11516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:36,150 - INFO - [diffusion][Epoch 11517] Epoch 11518/12000
2024-11-05 04:23:39,035 - INFO - [diffusion][Epoch 11517] diffusion training Loss: 0.06403328757733107
2024-11-05 04:23:39,037 - INFO - [diffusion][Epoch 11517] diffusion learning rate: 0.001
2024-11-05 04:23:39,039 - INFO - [diffusion][Epoch 11517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:39,040 - INFO - [diffusion][Epoch 11518] Epoch 11519/12000
2024-11-05 04:23:42,525 - INFO - [diffusion][Epoch 11518] diffusion training Loss: 0.05827593896538019
2024-11-05 04:23:42,527 - INFO - [diffusion][Epoch 11518] diffusion learning rate: 0.001
2024-11-05 04:23:42,529 - INFO - [diffusion][Epoch 11518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:42,530 - INFO - [diffusion][Epoch 11519] Epoch 11520/12000
2024-11-05 04:23:45,441 - INFO - [diffusion][Epoch 11519] diffusion training Loss: 0.05668327026069164
2024-11-05 04:23:45,443 - INFO - [diffusion][Epoch 11519] diffusion learning rate: 0.001
2024-11-05 04:23:45,445 - INFO - [diffusion][Epoch 11519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:45,448 - INFO - [diffusion][Epoch 11520] Epoch 11521/12000
2024-11-05 04:23:48,383 - INFO - [diffusion][Epoch 11520] diffusion training Loss: 0.05935013107955456
2024-11-05 04:23:48,384 - INFO - [diffusion][Epoch 11520] diffusion learning rate: 0.001
2024-11-05 04:23:48,386 - INFO - [diffusion][Epoch 11520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:48,388 - INFO - [diffusion][Epoch 11521] Epoch 11522/12000
2024-11-05 04:23:51,297 - INFO - [diffusion][Epoch 11521] diffusion training Loss: 0.0666326405480504
2024-11-05 04:23:51,299 - INFO - [diffusion][Epoch 11521] diffusion learning rate: 0.001
2024-11-05 04:23:51,301 - INFO - [diffusion][Epoch 11521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:51,302 - INFO - [diffusion][Epoch 11522] Epoch 11523/12000
2024-11-05 04:23:54,250 - INFO - [diffusion][Epoch 11522] diffusion training Loss: 0.061582667753100395
2024-11-05 04:23:54,252 - INFO - [diffusion][Epoch 11522] diffusion learning rate: 0.001
2024-11-05 04:23:54,254 - INFO - [diffusion][Epoch 11522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:54,255 - INFO - [diffusion][Epoch 11523] Epoch 11524/12000
2024-11-05 04:23:57,129 - INFO - [diffusion][Epoch 11523] diffusion training Loss: 0.06259952299296856
2024-11-05 04:23:57,131 - INFO - [diffusion][Epoch 11523] diffusion learning rate: 0.001
2024-11-05 04:23:57,133 - INFO - [diffusion][Epoch 11523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:57,134 - INFO - [diffusion][Epoch 11524] Epoch 11525/12000
2024-11-05 04:24:00,049 - INFO - [diffusion][Epoch 11524] diffusion training Loss: 0.06029289960861206
2024-11-05 04:24:00,051 - INFO - [diffusion][Epoch 11524] diffusion learning rate: 0.001
2024-11-05 04:24:00,053 - INFO - [diffusion][Epoch 11524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:00,054 - INFO - [diffusion][Epoch 11525] Epoch 11526/12000
2024-11-05 04:24:02,784 - INFO - [diffusion][Epoch 11525] diffusion training Loss: 0.055199471302330494
2024-11-05 04:24:02,786 - INFO - [diffusion][Epoch 11525] diffusion learning rate: 0.001
2024-11-05 04:24:02,788 - INFO - [diffusion][Epoch 11525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:02,789 - INFO - [diffusion][Epoch 11526] Epoch 11527/12000
2024-11-05 04:24:05,646 - INFO - [diffusion][Epoch 11526] diffusion training Loss: 0.058202474378049374
2024-11-05 04:24:05,648 - INFO - [diffusion][Epoch 11526] diffusion learning rate: 0.001
2024-11-05 04:24:05,699 - INFO - [diffusion][Epoch 11526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:05,700 - INFO - [diffusion][Epoch 11527] Epoch 11528/12000
2024-11-05 04:24:08,460 - INFO - [diffusion][Epoch 11527] diffusion training Loss: 0.05696247145533562
2024-11-05 04:24:08,462 - INFO - [diffusion][Epoch 11527] diffusion learning rate: 0.001
2024-11-05 04:24:08,464 - INFO - [diffusion][Epoch 11527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:08,465 - INFO - [diffusion][Epoch 11528] Epoch 11529/12000
2024-11-05 04:24:11,242 - INFO - [diffusion][Epoch 11528] diffusion training Loss: 0.05477429833263159
2024-11-05 04:24:11,244 - INFO - [diffusion][Epoch 11528] diffusion learning rate: 0.001
2024-11-05 04:24:11,268 - INFO - [diffusion][Epoch 11528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:11,270 - INFO - [diffusion][Epoch 11529] Epoch 11530/12000
2024-11-05 04:24:13,936 - INFO - [diffusion][Epoch 11529] diffusion training Loss: 0.06213168613612652
2024-11-05 04:24:13,938 - INFO - [diffusion][Epoch 11529] diffusion learning rate: 0.001
2024-11-05 04:24:13,940 - INFO - [diffusion][Epoch 11529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:13,941 - INFO - [diffusion][Epoch 11530] Epoch 11531/12000
2024-11-05 04:24:16,806 - INFO - [diffusion][Epoch 11530] diffusion training Loss: 0.061794279143214226
2024-11-05 04:24:16,808 - INFO - [diffusion][Epoch 11530] diffusion learning rate: 0.001
2024-11-05 04:24:16,810 - INFO - [diffusion][Epoch 11530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:16,811 - INFO - [diffusion][Epoch 11531] Epoch 11532/12000
2024-11-05 04:24:19,696 - INFO - [diffusion][Epoch 11531] diffusion training Loss: 0.06028202548623085
2024-11-05 04:24:19,698 - INFO - [diffusion][Epoch 11531] diffusion learning rate: 0.001
2024-11-05 04:24:19,700 - INFO - [diffusion][Epoch 11531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:19,701 - INFO - [diffusion][Epoch 11532] Epoch 11533/12000
2024-11-05 04:24:22,523 - INFO - [diffusion][Epoch 11532] diffusion training Loss: 0.05926052667200565
2024-11-05 04:24:22,525 - INFO - [diffusion][Epoch 11532] diffusion learning rate: 0.001
2024-11-05 04:24:22,527 - INFO - [diffusion][Epoch 11532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:22,528 - INFO - [diffusion][Epoch 11533] Epoch 11534/12000
2024-11-05 04:24:25,418 - INFO - [diffusion][Epoch 11533] diffusion training Loss: 0.05863587465137243
2024-11-05 04:24:25,420 - INFO - [diffusion][Epoch 11533] diffusion learning rate: 0.001
2024-11-05 04:24:25,421 - INFO - [diffusion][Epoch 11533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:25,423 - INFO - [diffusion][Epoch 11534] Epoch 11535/12000
2024-11-05 04:24:28,311 - INFO - [diffusion][Epoch 11534] diffusion training Loss: 0.057455115020275116
2024-11-05 04:24:28,313 - INFO - [diffusion][Epoch 11534] diffusion learning rate: 0.001
2024-11-05 04:24:28,315 - INFO - [diffusion][Epoch 11534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:28,316 - INFO - [diffusion][Epoch 11535] Epoch 11536/12000
2024-11-05 04:24:31,183 - INFO - [diffusion][Epoch 11535] diffusion training Loss: 0.053909347392618656
2024-11-05 04:24:31,185 - INFO - [diffusion][Epoch 11535] diffusion learning rate: 0.001
2024-11-05 04:24:31,187 - INFO - [diffusion][Epoch 11535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:31,188 - INFO - [diffusion][Epoch 11536] Epoch 11537/12000
2024-11-05 04:24:34,106 - INFO - [diffusion][Epoch 11536] diffusion training Loss: 0.05501606408506632
2024-11-05 04:24:34,109 - INFO - [diffusion][Epoch 11536] diffusion learning rate: 0.001
2024-11-05 04:24:34,111 - INFO - [diffusion][Epoch 11536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:34,113 - INFO - [diffusion][Epoch 11537] Epoch 11538/12000
2024-11-05 04:24:37,029 - INFO - [diffusion][Epoch 11537] diffusion training Loss: 0.055274869315326214
2024-11-05 04:24:37,032 - INFO - [diffusion][Epoch 11537] diffusion learning rate: 0.001
2024-11-05 04:24:37,034 - INFO - [diffusion][Epoch 11537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:37,035 - INFO - [diffusion][Epoch 11538] Epoch 11539/12000
2024-11-05 04:24:39,941 - INFO - [diffusion][Epoch 11538] diffusion training Loss: 0.06237470358610153
2024-11-05 04:24:39,943 - INFO - [diffusion][Epoch 11538] diffusion learning rate: 0.001
2024-11-05 04:24:39,944 - INFO - [diffusion][Epoch 11538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:39,946 - INFO - [diffusion][Epoch 11539] Epoch 11540/12000
2024-11-05 04:24:42,795 - INFO - [diffusion][Epoch 11539] diffusion training Loss: 0.05879224371165037
2024-11-05 04:24:42,797 - INFO - [diffusion][Epoch 11539] diffusion learning rate: 0.001
2024-11-05 04:24:42,799 - INFO - [diffusion][Epoch 11539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:42,800 - INFO - [diffusion][Epoch 11540] Epoch 11541/12000
2024-11-05 04:24:46,073 - INFO - [diffusion][Epoch 11540] diffusion training Loss: 0.06029635854065418
2024-11-05 04:24:46,075 - INFO - [diffusion][Epoch 11540] diffusion learning rate: 0.001
2024-11-05 04:24:46,076 - INFO - [diffusion][Epoch 11540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:46,077 - INFO - [diffusion][Epoch 11541] Epoch 11542/12000
2024-11-05 04:24:48,886 - INFO - [diffusion][Epoch 11541] diffusion training Loss: 0.05550721101462841
2024-11-05 04:24:48,888 - INFO - [diffusion][Epoch 11541] diffusion learning rate: 0.001
2024-11-05 04:24:48,889 - INFO - [diffusion][Epoch 11541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:48,891 - INFO - [diffusion][Epoch 11542] Epoch 11543/12000
2024-11-05 04:24:51,777 - INFO - [diffusion][Epoch 11542] diffusion training Loss: 0.054732466116547585
2024-11-05 04:24:51,779 - INFO - [diffusion][Epoch 11542] diffusion learning rate: 0.001
2024-11-05 04:24:51,781 - INFO - [diffusion][Epoch 11542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:51,782 - INFO - [diffusion][Epoch 11543] Epoch 11544/12000
2024-11-05 04:24:54,641 - INFO - [diffusion][Epoch 11543] diffusion training Loss: 0.051674664951860905
2024-11-05 04:24:54,643 - INFO - [diffusion][Epoch 11543] diffusion learning rate: 0.001
2024-11-05 04:24:54,685 - INFO - [diffusion][Epoch 11543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:54,686 - INFO - [diffusion][Epoch 11544] Epoch 11545/12000
2024-11-05 04:24:57,420 - INFO - [diffusion][Epoch 11544] diffusion training Loss: 0.0612198943272233
2024-11-05 04:24:57,422 - INFO - [diffusion][Epoch 11544] diffusion learning rate: 0.001
2024-11-05 04:24:57,424 - INFO - [diffusion][Epoch 11544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:57,425 - INFO - [diffusion][Epoch 11545] Epoch 11546/12000
2024-11-05 04:25:00,305 - INFO - [diffusion][Epoch 11545] diffusion training Loss: 0.058541358448565006
2024-11-05 04:25:00,308 - INFO - [diffusion][Epoch 11545] diffusion learning rate: 0.001
2024-11-05 04:25:00,310 - INFO - [diffusion][Epoch 11545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:00,311 - INFO - [diffusion][Epoch 11546] Epoch 11547/12000
2024-11-05 04:25:03,216 - INFO - [diffusion][Epoch 11546] diffusion training Loss: 0.05662514269351959
2024-11-05 04:25:03,218 - INFO - [diffusion][Epoch 11546] diffusion learning rate: 0.001
2024-11-05 04:25:03,220 - INFO - [diffusion][Epoch 11546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:03,221 - INFO - [diffusion][Epoch 11547] Epoch 11548/12000
2024-11-05 04:25:06,097 - INFO - [diffusion][Epoch 11547] diffusion training Loss: 0.05452701449394226
2024-11-05 04:25:06,099 - INFO - [diffusion][Epoch 11547] diffusion learning rate: 0.001
2024-11-05 04:25:06,101 - INFO - [diffusion][Epoch 11547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:06,102 - INFO - [diffusion][Epoch 11548] Epoch 11549/12000
2024-11-05 04:25:09,030 - INFO - [diffusion][Epoch 11548] diffusion training Loss: 0.0568240312859416
2024-11-05 04:25:09,032 - INFO - [diffusion][Epoch 11548] diffusion learning rate: 0.001
2024-11-05 04:25:09,034 - INFO - [diffusion][Epoch 11548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:09,035 - INFO - [diffusion][Epoch 11549] Epoch 11550/12000
2024-11-05 04:25:11,954 - INFO - [diffusion][Epoch 11549] diffusion training Loss: 0.05242322199046612
2024-11-05 04:25:11,956 - INFO - [diffusion][Epoch 11549] diffusion learning rate: 0.001
2024-11-05 04:25:11,958 - INFO - [diffusion][Epoch 11549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:11,959 - INFO - [diffusion][Epoch 11550] Epoch 11551/12000
2024-11-05 04:25:14,710 - INFO - [diffusion][Epoch 11550] diffusion training Loss: 0.05681957770138979
2024-11-05 04:25:14,713 - INFO - [diffusion][Epoch 11550] diffusion learning rate: 0.001
2024-11-05 04:25:14,716 - INFO - [diffusion][Epoch 11550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:14,718 - INFO - [diffusion][Epoch 11551] Epoch 11552/12000
2024-11-05 04:25:17,584 - INFO - [diffusion][Epoch 11551] diffusion training Loss: 0.051734015345573425
2024-11-05 04:25:17,586 - INFO - [diffusion][Epoch 11551] diffusion learning rate: 0.001
2024-11-05 04:25:17,588 - INFO - [diffusion][Epoch 11551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:17,589 - INFO - [diffusion][Epoch 11552] Epoch 11553/12000
2024-11-05 04:25:20,387 - INFO - [diffusion][Epoch 11552] diffusion training Loss: 0.05520408507436514
2024-11-05 04:25:20,389 - INFO - [diffusion][Epoch 11552] diffusion learning rate: 0.001
2024-11-05 04:25:20,391 - INFO - [diffusion][Epoch 11552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:20,393 - INFO - [diffusion][Epoch 11553] Epoch 11554/12000
2024-11-05 04:25:23,184 - INFO - [diffusion][Epoch 11553] diffusion training Loss: 0.060747611336410046
2024-11-05 04:25:23,186 - INFO - [diffusion][Epoch 11553] diffusion learning rate: 0.001
2024-11-05 04:25:23,188 - INFO - [diffusion][Epoch 11553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:23,189 - INFO - [diffusion][Epoch 11554] Epoch 11555/12000
2024-11-05 04:25:25,900 - INFO - [diffusion][Epoch 11554] diffusion training Loss: 0.06018113624304533
2024-11-05 04:25:25,902 - INFO - [diffusion][Epoch 11554] diffusion learning rate: 0.001
2024-11-05 04:25:25,904 - INFO - [diffusion][Epoch 11554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:25,905 - INFO - [diffusion][Epoch 11555] Epoch 11556/12000
2024-11-05 04:25:28,602 - INFO - [diffusion][Epoch 11555] diffusion training Loss: 0.055464744567871094
2024-11-05 04:25:28,604 - INFO - [diffusion][Epoch 11555] diffusion learning rate: 0.001
2024-11-05 04:25:28,606 - INFO - [diffusion][Epoch 11555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:28,607 - INFO - [diffusion][Epoch 11556] Epoch 11557/12000
2024-11-05 04:25:31,450 - INFO - [diffusion][Epoch 11556] diffusion training Loss: 0.05661313235759735
2024-11-05 04:25:31,452 - INFO - [diffusion][Epoch 11556] diffusion learning rate: 0.001
2024-11-05 04:25:31,453 - INFO - [diffusion][Epoch 11556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:31,455 - INFO - [diffusion][Epoch 11557] Epoch 11558/12000
2024-11-05 04:25:34,289 - INFO - [diffusion][Epoch 11557] diffusion training Loss: 0.05501773674041033
2024-11-05 04:25:34,291 - INFO - [diffusion][Epoch 11557] diffusion learning rate: 0.001
2024-11-05 04:25:34,293 - INFO - [diffusion][Epoch 11557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:34,294 - INFO - [diffusion][Epoch 11558] Epoch 11559/12000
2024-11-05 04:25:37,217 - INFO - [diffusion][Epoch 11558] diffusion training Loss: 0.057167571038007736
2024-11-05 04:25:37,219 - INFO - [diffusion][Epoch 11558] diffusion learning rate: 0.001
2024-11-05 04:25:37,221 - INFO - [diffusion][Epoch 11558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:37,222 - INFO - [diffusion][Epoch 11559] Epoch 11560/12000
2024-11-05 04:25:40,049 - INFO - [diffusion][Epoch 11559] diffusion training Loss: 0.05805750843137503
2024-11-05 04:25:40,083 - INFO - [diffusion][Epoch 11559] diffusion learning rate: 0.001
2024-11-05 04:25:40,084 - INFO - [diffusion][Epoch 11559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:40,086 - INFO - [diffusion][Epoch 11560] Epoch 11561/12000
2024-11-05 04:25:43,566 - INFO - [diffusion][Epoch 11560] diffusion training Loss: 0.05931466165930033
2024-11-05 04:25:43,568 - INFO - [diffusion][Epoch 11560] diffusion learning rate: 0.001
2024-11-05 04:25:43,569 - INFO - [diffusion][Epoch 11560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:43,571 - INFO - [diffusion][Epoch 11561] Epoch 11562/12000
2024-11-05 04:25:46,427 - INFO - [diffusion][Epoch 11561] diffusion training Loss: 0.0574503131210804
2024-11-05 04:25:46,430 - INFO - [diffusion][Epoch 11561] diffusion learning rate: 0.001
2024-11-05 04:25:46,433 - INFO - [diffusion][Epoch 11561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:46,434 - INFO - [diffusion][Epoch 11562] Epoch 11563/12000
2024-11-05 04:25:49,336 - INFO - [diffusion][Epoch 11562] diffusion training Loss: 0.060891393572092056
2024-11-05 04:25:49,338 - INFO - [diffusion][Epoch 11562] diffusion learning rate: 0.001
2024-11-05 04:25:49,340 - INFO - [diffusion][Epoch 11562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:49,342 - INFO - [diffusion][Epoch 11563] Epoch 11564/12000
2024-11-05 04:25:52,221 - INFO - [diffusion][Epoch 11563] diffusion training Loss: 0.05996452271938324
2024-11-05 04:25:52,223 - INFO - [diffusion][Epoch 11563] diffusion learning rate: 0.001
2024-11-05 04:25:52,225 - INFO - [diffusion][Epoch 11563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:52,226 - INFO - [diffusion][Epoch 11564] Epoch 11565/12000
2024-11-05 04:25:55,051 - INFO - [diffusion][Epoch 11564] diffusion training Loss: 0.052534534595906734
2024-11-05 04:25:55,053 - INFO - [diffusion][Epoch 11564] diffusion learning rate: 0.001
2024-11-05 04:25:55,054 - INFO - [diffusion][Epoch 11564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:55,056 - INFO - [diffusion][Epoch 11565] Epoch 11566/12000
2024-11-05 04:25:57,905 - INFO - [diffusion][Epoch 11565] diffusion training Loss: 0.06031484995037317
2024-11-05 04:25:57,907 - INFO - [diffusion][Epoch 11565] diffusion learning rate: 0.001
2024-11-05 04:25:57,908 - INFO - [diffusion][Epoch 11565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:57,910 - INFO - [diffusion][Epoch 11566] Epoch 11567/12000
2024-11-05 04:26:00,801 - INFO - [diffusion][Epoch 11566] diffusion training Loss: 0.05942330323159695
2024-11-05 04:26:00,803 - INFO - [diffusion][Epoch 11566] diffusion learning rate: 0.001
2024-11-05 04:26:00,804 - INFO - [diffusion][Epoch 11566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:00,805 - INFO - [diffusion][Epoch 11567] Epoch 11568/12000
2024-11-05 04:26:03,699 - INFO - [diffusion][Epoch 11567] diffusion training Loss: 0.055495018139481544
2024-11-05 04:26:03,701 - INFO - [diffusion][Epoch 11567] diffusion learning rate: 0.001
2024-11-05 04:26:03,703 - INFO - [diffusion][Epoch 11567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:03,704 - INFO - [diffusion][Epoch 11568] Epoch 11569/12000
2024-11-05 04:26:06,482 - INFO - [diffusion][Epoch 11568] diffusion training Loss: 0.05432656407356262
2024-11-05 04:26:06,484 - INFO - [diffusion][Epoch 11568] diffusion learning rate: 0.001
2024-11-05 04:26:06,485 - INFO - [diffusion][Epoch 11568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:06,487 - INFO - [diffusion][Epoch 11569] Epoch 11570/12000
2024-11-05 04:26:09,349 - INFO - [diffusion][Epoch 11569] diffusion training Loss: 0.06153583247214556
2024-11-05 04:26:09,351 - INFO - [diffusion][Epoch 11569] diffusion learning rate: 0.001
2024-11-05 04:26:09,353 - INFO - [diffusion][Epoch 11569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:09,354 - INFO - [diffusion][Epoch 11570] Epoch 11571/12000
2024-11-05 04:26:12,174 - INFO - [diffusion][Epoch 11570] diffusion training Loss: 0.062149420380592346
2024-11-05 04:26:12,405 - INFO - [diffusion][Epoch 11570] diffusion learning rate: 0.001
2024-11-05 04:26:12,407 - INFO - [diffusion][Epoch 11570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:12,409 - INFO - [diffusion][Epoch 11571] Epoch 11572/12000
2024-11-05 04:26:15,297 - INFO - [diffusion][Epoch 11571] diffusion training Loss: 0.06039403285831213
2024-11-05 04:26:15,299 - INFO - [diffusion][Epoch 11571] diffusion learning rate: 0.001
2024-11-05 04:26:15,301 - INFO - [diffusion][Epoch 11571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:15,302 - INFO - [diffusion][Epoch 11572] Epoch 11573/12000
2024-11-05 04:26:18,047 - INFO - [diffusion][Epoch 11572] diffusion training Loss: 0.057840229012072086
2024-11-05 04:26:18,049 - INFO - [diffusion][Epoch 11572] diffusion learning rate: 0.001
2024-11-05 04:26:18,050 - INFO - [diffusion][Epoch 11572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:18,051 - INFO - [diffusion][Epoch 11573] Epoch 11574/12000
2024-11-05 04:26:20,870 - INFO - [diffusion][Epoch 11573] diffusion training Loss: 0.05570504814386368
2024-11-05 04:26:20,872 - INFO - [diffusion][Epoch 11573] diffusion learning rate: 0.001
2024-11-05 04:26:20,874 - INFO - [diffusion][Epoch 11573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:20,875 - INFO - [diffusion][Epoch 11574] Epoch 11575/12000
2024-11-05 04:26:23,695 - INFO - [diffusion][Epoch 11574] diffusion training Loss: 0.057348557747900486
2024-11-05 04:26:23,697 - INFO - [diffusion][Epoch 11574] diffusion learning rate: 0.001
2024-11-05 04:26:23,699 - INFO - [diffusion][Epoch 11574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:23,700 - INFO - [diffusion][Epoch 11575] Epoch 11576/12000
2024-11-05 04:26:26,455 - INFO - [diffusion][Epoch 11575] diffusion training Loss: 0.058630846440792084
2024-11-05 04:26:26,457 - INFO - [diffusion][Epoch 11575] diffusion learning rate: 0.001
2024-11-05 04:26:26,458 - INFO - [diffusion][Epoch 11575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:26,459 - INFO - [diffusion][Epoch 11576] Epoch 11577/12000
2024-11-05 04:26:29,292 - INFO - [diffusion][Epoch 11576] diffusion training Loss: 0.05639342963695526
2024-11-05 04:26:29,294 - INFO - [diffusion][Epoch 11576] diffusion learning rate: 0.001
2024-11-05 04:26:29,296 - INFO - [diffusion][Epoch 11576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:29,297 - INFO - [diffusion][Epoch 11577] Epoch 11578/12000
2024-11-05 04:26:32,006 - INFO - [diffusion][Epoch 11577] diffusion training Loss: 0.05729534663259983
2024-11-05 04:26:32,008 - INFO - [diffusion][Epoch 11577] diffusion learning rate: 0.001
2024-11-05 04:26:32,009 - INFO - [diffusion][Epoch 11577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:32,011 - INFO - [diffusion][Epoch 11578] Epoch 11579/12000
2024-11-05 04:26:34,745 - INFO - [diffusion][Epoch 11578] diffusion training Loss: 0.05962209217250347
2024-11-05 04:26:34,748 - INFO - [diffusion][Epoch 11578] diffusion learning rate: 0.001
2024-11-05 04:26:34,750 - INFO - [diffusion][Epoch 11578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:34,751 - INFO - [diffusion][Epoch 11579] Epoch 11580/12000
2024-11-05 04:26:37,715 - INFO - [diffusion][Epoch 11579] diffusion training Loss: 0.05530232563614845
2024-11-05 04:26:37,717 - INFO - [diffusion][Epoch 11579] diffusion learning rate: 0.001
2024-11-05 04:26:37,719 - INFO - [diffusion][Epoch 11579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:37,720 - INFO - [diffusion][Epoch 11580] Epoch 11581/12000
2024-11-05 04:26:40,775 - INFO - [diffusion][Epoch 11580] diffusion training Loss: 0.058135101571679115
2024-11-05 04:26:40,777 - INFO - [diffusion][Epoch 11580] diffusion learning rate: 0.001
2024-11-05 04:26:40,779 - INFO - [diffusion][Epoch 11580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:40,780 - INFO - [diffusion][Epoch 11581] Epoch 11582/12000
2024-11-05 04:26:43,577 - INFO - [diffusion][Epoch 11581] diffusion training Loss: 0.05607265327125788
2024-11-05 04:26:43,579 - INFO - [diffusion][Epoch 11581] diffusion learning rate: 0.001
2024-11-05 04:26:43,581 - INFO - [diffusion][Epoch 11581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:43,582 - INFO - [diffusion][Epoch 11582] Epoch 11583/12000
2024-11-05 04:26:46,467 - INFO - [diffusion][Epoch 11582] diffusion training Loss: 0.052920738235116005
2024-11-05 04:26:46,468 - INFO - [diffusion][Epoch 11582] diffusion learning rate: 0.001
2024-11-05 04:26:46,470 - INFO - [diffusion][Epoch 11582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:46,471 - INFO - [diffusion][Epoch 11583] Epoch 11584/12000
2024-11-05 04:26:49,232 - INFO - [diffusion][Epoch 11583] diffusion training Loss: 0.05515761859714985
2024-11-05 04:26:49,234 - INFO - [diffusion][Epoch 11583] diffusion learning rate: 0.001
2024-11-05 04:26:49,236 - INFO - [diffusion][Epoch 11583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:49,237 - INFO - [diffusion][Epoch 11584] Epoch 11585/12000
2024-11-05 04:26:52,047 - INFO - [diffusion][Epoch 11584] diffusion training Loss: 0.0578229958191514
2024-11-05 04:26:52,049 - INFO - [diffusion][Epoch 11584] diffusion learning rate: 0.001
2024-11-05 04:26:52,050 - INFO - [diffusion][Epoch 11584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:52,052 - INFO - [diffusion][Epoch 11585] Epoch 11586/12000
2024-11-05 04:26:54,686 - INFO - [diffusion][Epoch 11585] diffusion training Loss: 0.05754125490784645
2024-11-05 04:26:54,688 - INFO - [diffusion][Epoch 11585] diffusion learning rate: 0.001
2024-11-05 04:26:54,690 - INFO - [diffusion][Epoch 11585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:54,691 - INFO - [diffusion][Epoch 11586] Epoch 11587/12000
2024-11-05 04:26:57,555 - INFO - [diffusion][Epoch 11586] diffusion training Loss: 0.057625492103397846
2024-11-05 04:26:57,557 - INFO - [diffusion][Epoch 11586] diffusion learning rate: 0.001
2024-11-05 04:26:57,558 - INFO - [diffusion][Epoch 11586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:57,560 - INFO - [diffusion][Epoch 11587] Epoch 11588/12000
2024-11-05 04:27:00,367 - INFO - [diffusion][Epoch 11587] diffusion training Loss: 0.05920984596014023
2024-11-05 04:27:00,370 - INFO - [diffusion][Epoch 11587] diffusion learning rate: 0.001
2024-11-05 04:27:00,372 - INFO - [diffusion][Epoch 11587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:00,374 - INFO - [diffusion][Epoch 11588] Epoch 11589/12000
2024-11-05 04:27:03,185 - INFO - [diffusion][Epoch 11588] diffusion training Loss: 0.058539146557450294
2024-11-05 04:27:03,187 - INFO - [diffusion][Epoch 11588] diffusion learning rate: 0.001
2024-11-05 04:27:03,189 - INFO - [diffusion][Epoch 11588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:03,190 - INFO - [diffusion][Epoch 11589] Epoch 11590/12000
2024-11-05 04:27:06,208 - INFO - [diffusion][Epoch 11589] diffusion training Loss: 0.06135759316384792
2024-11-05 04:27:06,211 - INFO - [diffusion][Epoch 11589] diffusion learning rate: 0.001
2024-11-05 04:27:06,213 - INFO - [diffusion][Epoch 11589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:06,215 - INFO - [diffusion][Epoch 11590] Epoch 11591/12000
2024-11-05 04:27:09,060 - INFO - [diffusion][Epoch 11590] diffusion training Loss: 0.05568168219178915
2024-11-05 04:27:09,062 - INFO - [diffusion][Epoch 11590] diffusion learning rate: 0.001
2024-11-05 04:27:09,064 - INFO - [diffusion][Epoch 11590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:09,065 - INFO - [diffusion][Epoch 11591] Epoch 11592/12000
2024-11-05 04:27:11,921 - INFO - [diffusion][Epoch 11591] diffusion training Loss: 0.05653470568358898
2024-11-05 04:27:11,923 - INFO - [diffusion][Epoch 11591] diffusion learning rate: 0.001
2024-11-05 04:27:11,925 - INFO - [diffusion][Epoch 11591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:11,927 - INFO - [diffusion][Epoch 11592] Epoch 11593/12000
2024-11-05 04:27:14,700 - INFO - [diffusion][Epoch 11592] diffusion training Loss: 0.05263167526572943
2024-11-05 04:27:14,702 - INFO - [diffusion][Epoch 11592] diffusion learning rate: 0.001
2024-11-05 04:27:14,704 - INFO - [diffusion][Epoch 11592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:14,705 - INFO - [diffusion][Epoch 11593] Epoch 11594/12000
2024-11-05 04:27:17,435 - INFO - [diffusion][Epoch 11593] diffusion training Loss: 0.06143835932016373
2024-11-05 04:27:17,437 - INFO - [diffusion][Epoch 11593] diffusion learning rate: 0.001
2024-11-05 04:27:17,438 - INFO - [diffusion][Epoch 11593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:17,439 - INFO - [diffusion][Epoch 11594] Epoch 11595/12000
2024-11-05 04:27:20,240 - INFO - [diffusion][Epoch 11594] diffusion training Loss: 0.054914822801947594
2024-11-05 04:27:20,242 - INFO - [diffusion][Epoch 11594] diffusion learning rate: 0.001
2024-11-05 04:27:20,244 - INFO - [diffusion][Epoch 11594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:20,245 - INFO - [diffusion][Epoch 11595] Epoch 11596/12000
2024-11-05 04:27:22,885 - INFO - [diffusion][Epoch 11595] diffusion training Loss: 0.06260538473725319
2024-11-05 04:27:22,887 - INFO - [diffusion][Epoch 11595] diffusion learning rate: 0.001
2024-11-05 04:27:22,891 - INFO - [diffusion][Epoch 11595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:22,892 - INFO - [diffusion][Epoch 11596] Epoch 11597/12000
2024-11-05 04:27:25,729 - INFO - [diffusion][Epoch 11596] diffusion training Loss: 0.05370204243808985
2024-11-05 04:27:25,731 - INFO - [diffusion][Epoch 11596] diffusion learning rate: 0.001
2024-11-05 04:27:25,733 - INFO - [diffusion][Epoch 11596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:25,734 - INFO - [diffusion][Epoch 11597] Epoch 11598/12000
2024-11-05 04:27:28,486 - INFO - [diffusion][Epoch 11597] diffusion training Loss: 0.06078676879405975
2024-11-05 04:27:28,488 - INFO - [diffusion][Epoch 11597] diffusion learning rate: 0.001
2024-11-05 04:27:28,490 - INFO - [diffusion][Epoch 11597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:28,491 - INFO - [diffusion][Epoch 11598] Epoch 11599/12000
2024-11-05 04:27:31,236 - INFO - [diffusion][Epoch 11598] diffusion training Loss: 0.054283248260617256
2024-11-05 04:27:31,238 - INFO - [diffusion][Epoch 11598] diffusion learning rate: 0.001
2024-11-05 04:27:31,240 - INFO - [diffusion][Epoch 11598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:31,241 - INFO - [diffusion][Epoch 11599] Epoch 11600/12000
2024-11-05 04:27:34,000 - INFO - [diffusion][Epoch 11599] diffusion training Loss: 0.061072349548339844
2024-11-05 04:27:34,002 - INFO - [diffusion][Epoch 11599] diffusion learning rate: 0.001
2024-11-05 04:27:34,004 - INFO - [diffusion][Epoch 11599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:34,005 - INFO - [diffusion][Epoch 11600] Epoch 11601/12000
2024-11-05 04:27:36,853 - INFO - [diffusion][Epoch 11600] diffusion training Loss: 0.05956420209258795
2024-11-05 04:27:36,855 - INFO - [diffusion][Epoch 11600] diffusion learning rate: 0.001
2024-11-05 04:27:36,857 - INFO - [diffusion][Epoch 11600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:36,858 - INFO - [diffusion][Epoch 11601] Epoch 11602/12000
2024-11-05 04:27:39,854 - INFO - [diffusion][Epoch 11601] diffusion training Loss: 0.0600365586578846
2024-11-05 04:27:39,856 - INFO - [diffusion][Epoch 11601] diffusion learning rate: 0.001
2024-11-05 04:27:39,858 - INFO - [diffusion][Epoch 11601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:39,860 - INFO - [diffusion][Epoch 11602] Epoch 11603/12000
2024-11-05 04:27:43,272 - INFO - [diffusion][Epoch 11602] diffusion training Loss: 0.056394695304334164
2024-11-05 04:27:43,274 - INFO - [diffusion][Epoch 11602] diffusion learning rate: 0.001
2024-11-05 04:27:43,275 - INFO - [diffusion][Epoch 11602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:43,277 - INFO - [diffusion][Epoch 11603] Epoch 11604/12000
2024-11-05 04:27:46,158 - INFO - [diffusion][Epoch 11603] diffusion training Loss: 0.05791474040597677
2024-11-05 04:27:46,160 - INFO - [diffusion][Epoch 11603] diffusion learning rate: 0.001
2024-11-05 04:27:46,162 - INFO - [diffusion][Epoch 11603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:46,164 - INFO - [diffusion][Epoch 11604] Epoch 11605/12000
2024-11-05 04:27:48,986 - INFO - [diffusion][Epoch 11604] diffusion training Loss: 0.05365815758705139
2024-11-05 04:27:48,987 - INFO - [diffusion][Epoch 11604] diffusion learning rate: 0.001
2024-11-05 04:27:48,989 - INFO - [diffusion][Epoch 11604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:48,991 - INFO - [diffusion][Epoch 11605] Epoch 11606/12000
2024-11-05 04:27:51,854 - INFO - [diffusion][Epoch 11605] diffusion training Loss: 0.061969504691660404
2024-11-05 04:27:51,856 - INFO - [diffusion][Epoch 11605] diffusion learning rate: 0.001
2024-11-05 04:27:51,858 - INFO - [diffusion][Epoch 11605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:51,859 - INFO - [diffusion][Epoch 11606] Epoch 11607/12000
2024-11-05 04:27:54,775 - INFO - [diffusion][Epoch 11606] diffusion training Loss: 0.05584141705185175
2024-11-05 04:27:54,776 - INFO - [diffusion][Epoch 11606] diffusion learning rate: 0.001
2024-11-05 04:27:54,778 - INFO - [diffusion][Epoch 11606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:54,780 - INFO - [diffusion][Epoch 11607] Epoch 11608/12000
2024-11-05 04:27:57,635 - INFO - [diffusion][Epoch 11607] diffusion training Loss: 0.05963450390845537
2024-11-05 04:27:57,637 - INFO - [diffusion][Epoch 11607] diffusion learning rate: 0.001
2024-11-05 04:27:57,639 - INFO - [diffusion][Epoch 11607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:57,640 - INFO - [diffusion][Epoch 11608] Epoch 11609/12000
2024-11-05 04:28:00,483 - INFO - [diffusion][Epoch 11608] diffusion training Loss: 0.05277865380048752
2024-11-05 04:28:00,485 - INFO - [diffusion][Epoch 11608] diffusion learning rate: 0.001
2024-11-05 04:28:00,486 - INFO - [diffusion][Epoch 11608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:00,488 - INFO - [diffusion][Epoch 11609] Epoch 11610/12000
2024-11-05 04:28:03,246 - INFO - [diffusion][Epoch 11609] diffusion training Loss: 0.059187366627156734
2024-11-05 04:28:03,248 - INFO - [diffusion][Epoch 11609] diffusion learning rate: 0.001
2024-11-05 04:28:03,250 - INFO - [diffusion][Epoch 11609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:03,251 - INFO - [diffusion][Epoch 11610] Epoch 11611/12000
2024-11-05 04:28:06,136 - INFO - [diffusion][Epoch 11610] diffusion training Loss: 0.05696895532310009
2024-11-05 04:28:06,138 - INFO - [diffusion][Epoch 11610] diffusion learning rate: 0.001
2024-11-05 04:28:06,140 - INFO - [diffusion][Epoch 11610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:06,141 - INFO - [diffusion][Epoch 11611] Epoch 11612/12000
2024-11-05 04:28:09,030 - INFO - [diffusion][Epoch 11611] diffusion training Loss: 0.058855283074080944
2024-11-05 04:28:09,032 - INFO - [diffusion][Epoch 11611] diffusion learning rate: 0.001
2024-11-05 04:28:09,034 - INFO - [diffusion][Epoch 11611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:09,035 - INFO - [diffusion][Epoch 11612] Epoch 11613/12000
2024-11-05 04:28:11,844 - INFO - [diffusion][Epoch 11612] diffusion training Loss: 0.05788747686892748
2024-11-05 04:28:11,847 - INFO - [diffusion][Epoch 11612] diffusion learning rate: 0.001
2024-11-05 04:28:11,849 - INFO - [diffusion][Epoch 11612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:11,851 - INFO - [diffusion][Epoch 11613] Epoch 11614/12000
2024-11-05 04:28:14,619 - INFO - [diffusion][Epoch 11613] diffusion training Loss: 0.0610901890322566
2024-11-05 04:28:14,621 - INFO - [diffusion][Epoch 11613] diffusion learning rate: 0.001
2024-11-05 04:28:14,623 - INFO - [diffusion][Epoch 11613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:14,625 - INFO - [diffusion][Epoch 11614] Epoch 11615/12000
2024-11-05 04:28:17,385 - INFO - [diffusion][Epoch 11614] diffusion training Loss: 0.056274959817528725
2024-11-05 04:28:17,387 - INFO - [diffusion][Epoch 11614] diffusion learning rate: 0.001
2024-11-05 04:28:17,389 - INFO - [diffusion][Epoch 11614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:17,390 - INFO - [diffusion][Epoch 11615] Epoch 11616/12000
2024-11-05 04:28:20,171 - INFO - [diffusion][Epoch 11615] diffusion training Loss: 0.05481511168181896
2024-11-05 04:28:20,173 - INFO - [diffusion][Epoch 11615] diffusion learning rate: 0.001
2024-11-05 04:28:20,201 - INFO - [diffusion][Epoch 11615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:20,203 - INFO - [diffusion][Epoch 11616] Epoch 11617/12000
2024-11-05 04:28:23,078 - INFO - [diffusion][Epoch 11616] diffusion training Loss: 0.0596591355279088
2024-11-05 04:28:23,080 - INFO - [diffusion][Epoch 11616] diffusion learning rate: 0.001
2024-11-05 04:28:23,082 - INFO - [diffusion][Epoch 11616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:23,083 - INFO - [diffusion][Epoch 11617] Epoch 11618/12000
2024-11-05 04:28:25,917 - INFO - [diffusion][Epoch 11617] diffusion training Loss: 0.06150451861321926
2024-11-05 04:28:25,919 - INFO - [diffusion][Epoch 11617] diffusion learning rate: 0.001
2024-11-05 04:28:25,921 - INFO - [diffusion][Epoch 11617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:25,923 - INFO - [diffusion][Epoch 11618] Epoch 11619/12000
2024-11-05 04:28:28,843 - INFO - [diffusion][Epoch 11618] diffusion training Loss: 0.05362336430698633
2024-11-05 04:28:28,845 - INFO - [diffusion][Epoch 11618] diffusion learning rate: 0.001
2024-11-05 04:28:28,847 - INFO - [diffusion][Epoch 11618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:28,848 - INFO - [diffusion][Epoch 11619] Epoch 11620/12000
2024-11-05 04:28:31,724 - INFO - [diffusion][Epoch 11619] diffusion training Loss: 0.05341279786080122
2024-11-05 04:28:31,726 - INFO - [diffusion][Epoch 11619] diffusion learning rate: 0.001
2024-11-05 04:28:31,728 - INFO - [diffusion][Epoch 11619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:31,729 - INFO - [diffusion][Epoch 11620] Epoch 11621/12000
2024-11-05 04:28:34,571 - INFO - [diffusion][Epoch 11620] diffusion training Loss: 0.058038363233208656
2024-11-05 04:28:34,573 - INFO - [diffusion][Epoch 11620] diffusion learning rate: 0.001
2024-11-05 04:28:34,575 - INFO - [diffusion][Epoch 11620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:34,576 - INFO - [diffusion][Epoch 11621] Epoch 11622/12000
2024-11-05 04:28:37,657 - INFO - [diffusion][Epoch 11621] diffusion training Loss: 0.051379104144871235
2024-11-05 04:28:37,659 - INFO - [diffusion][Epoch 11621] diffusion learning rate: 0.001
2024-11-05 04:28:37,661 - INFO - [diffusion][Epoch 11621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:37,662 - INFO - [diffusion][Epoch 11622] Epoch 11623/12000
2024-11-05 04:28:40,569 - INFO - [diffusion][Epoch 11622] diffusion training Loss: 0.059079479426145554
2024-11-05 04:28:40,571 - INFO - [diffusion][Epoch 11622] diffusion learning rate: 0.001
2024-11-05 04:28:40,573 - INFO - [diffusion][Epoch 11622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:40,575 - INFO - [diffusion][Epoch 11623] Epoch 11624/12000
2024-11-05 04:28:43,339 - INFO - [diffusion][Epoch 11623] diffusion training Loss: 0.05718483589589596
2024-11-05 04:28:43,341 - INFO - [diffusion][Epoch 11623] diffusion learning rate: 0.001
2024-11-05 04:28:43,343 - INFO - [diffusion][Epoch 11623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:43,344 - INFO - [diffusion][Epoch 11624] Epoch 11625/12000
2024-11-05 04:28:46,163 - INFO - [diffusion][Epoch 11624] diffusion training Loss: 0.05570958647876978
2024-11-05 04:28:46,165 - INFO - [diffusion][Epoch 11624] diffusion learning rate: 0.001
2024-11-05 04:28:46,167 - INFO - [diffusion][Epoch 11624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:46,169 - INFO - [diffusion][Epoch 11625] Epoch 11626/12000
2024-11-05 04:28:49,070 - INFO - [diffusion][Epoch 11625] diffusion training Loss: 0.051478915847837925
2024-11-05 04:28:49,072 - INFO - [diffusion][Epoch 11625] diffusion learning rate: 0.001
2024-11-05 04:28:49,074 - INFO - [diffusion][Epoch 11625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:49,075 - INFO - [diffusion][Epoch 11626] Epoch 11627/12000
2024-11-05 04:28:51,927 - INFO - [diffusion][Epoch 11626] diffusion training Loss: 0.06126820016652346
2024-11-05 04:28:51,929 - INFO - [diffusion][Epoch 11626] diffusion learning rate: 0.001
2024-11-05 04:28:51,930 - INFO - [diffusion][Epoch 11626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:51,932 - INFO - [diffusion][Epoch 11627] Epoch 11628/12000
2024-11-05 04:28:54,802 - INFO - [diffusion][Epoch 11627] diffusion training Loss: 0.055610181763768196
2024-11-05 04:28:54,804 - INFO - [diffusion][Epoch 11627] diffusion learning rate: 0.001
2024-11-05 04:28:54,806 - INFO - [diffusion][Epoch 11627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:54,807 - INFO - [diffusion][Epoch 11628] Epoch 11629/12000
2024-11-05 04:28:57,644 - INFO - [diffusion][Epoch 11628] diffusion training Loss: 0.06179699022322893
2024-11-05 04:28:57,646 - INFO - [diffusion][Epoch 11628] diffusion learning rate: 0.001
2024-11-05 04:28:57,647 - INFO - [diffusion][Epoch 11628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:57,649 - INFO - [diffusion][Epoch 11629] Epoch 11630/12000
2024-11-05 04:29:00,400 - INFO - [diffusion][Epoch 11629] diffusion training Loss: 0.06015998497605324
2024-11-05 04:29:00,402 - INFO - [diffusion][Epoch 11629] diffusion learning rate: 0.001
2024-11-05 04:29:00,404 - INFO - [diffusion][Epoch 11629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:00,405 - INFO - [diffusion][Epoch 11630] Epoch 11631/12000
2024-11-05 04:29:03,270 - INFO - [diffusion][Epoch 11630] diffusion training Loss: 0.058178579434752464
2024-11-05 04:29:03,272 - INFO - [diffusion][Epoch 11630] diffusion learning rate: 0.001
2024-11-05 04:29:03,274 - INFO - [diffusion][Epoch 11630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:03,275 - INFO - [diffusion][Epoch 11631] Epoch 11632/12000
2024-11-05 04:29:06,178 - INFO - [diffusion][Epoch 11631] diffusion training Loss: 0.0598885640501976
2024-11-05 04:29:06,180 - INFO - [diffusion][Epoch 11631] diffusion learning rate: 0.001
2024-11-05 04:29:06,182 - INFO - [diffusion][Epoch 11631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:06,183 - INFO - [diffusion][Epoch 11632] Epoch 11633/12000
2024-11-05 04:29:08,924 - INFO - [diffusion][Epoch 11632] diffusion training Loss: 0.05773110222071409
2024-11-05 04:29:08,926 - INFO - [diffusion][Epoch 11632] diffusion learning rate: 0.001
2024-11-05 04:29:08,928 - INFO - [diffusion][Epoch 11632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:08,929 - INFO - [diffusion][Epoch 11633] Epoch 11634/12000
2024-11-05 04:29:11,651 - INFO - [diffusion][Epoch 11633] diffusion training Loss: 0.06030367221683264
2024-11-05 04:29:11,653 - INFO - [diffusion][Epoch 11633] diffusion learning rate: 0.001
2024-11-05 04:29:11,654 - INFO - [diffusion][Epoch 11633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:11,656 - INFO - [diffusion][Epoch 11634] Epoch 11635/12000
2024-11-05 04:29:14,592 - INFO - [diffusion][Epoch 11634] diffusion training Loss: 0.06365424674004316
2024-11-05 04:29:14,594 - INFO - [diffusion][Epoch 11634] diffusion learning rate: 0.001
2024-11-05 04:29:14,596 - INFO - [diffusion][Epoch 11634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:14,597 - INFO - [diffusion][Epoch 11635] Epoch 11636/12000
2024-11-05 04:29:17,403 - INFO - [diffusion][Epoch 11635] diffusion training Loss: 0.0533803291618824
2024-11-05 04:29:17,405 - INFO - [diffusion][Epoch 11635] diffusion learning rate: 0.001
2024-11-05 04:29:17,407 - INFO - [diffusion][Epoch 11635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:17,408 - INFO - [diffusion][Epoch 11636] Epoch 11637/12000
2024-11-05 04:29:20,135 - INFO - [diffusion][Epoch 11636] diffusion training Loss: 0.0545831173658371
2024-11-05 04:29:20,137 - INFO - [diffusion][Epoch 11636] diffusion learning rate: 0.001
2024-11-05 04:29:20,139 - INFO - [diffusion][Epoch 11636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:20,140 - INFO - [diffusion][Epoch 11637] Epoch 11638/12000
2024-11-05 04:29:22,867 - INFO - [diffusion][Epoch 11637] diffusion training Loss: 0.05777519103139639
2024-11-05 04:29:22,869 - INFO - [diffusion][Epoch 11637] diffusion learning rate: 0.001
2024-11-05 04:29:22,871 - INFO - [diffusion][Epoch 11637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:22,872 - INFO - [diffusion][Epoch 11638] Epoch 11639/12000
2024-11-05 04:29:25,588 - INFO - [diffusion][Epoch 11638] diffusion training Loss: 0.061643379740417004
2024-11-05 04:29:25,590 - INFO - [diffusion][Epoch 11638] diffusion learning rate: 0.001
2024-11-05 04:29:25,593 - INFO - [diffusion][Epoch 11638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:25,594 - INFO - [diffusion][Epoch 11639] Epoch 11640/12000
2024-11-05 04:29:28,369 - INFO - [diffusion][Epoch 11639] diffusion training Loss: 0.061476134695112705
2024-11-05 04:29:28,372 - INFO - [diffusion][Epoch 11639] diffusion learning rate: 0.001
2024-11-05 04:29:28,373 - INFO - [diffusion][Epoch 11639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:28,375 - INFO - [diffusion][Epoch 11640] Epoch 11641/12000
2024-11-05 04:29:31,168 - INFO - [diffusion][Epoch 11640] diffusion training Loss: 0.05326865706592798
2024-11-05 04:29:31,170 - INFO - [diffusion][Epoch 11640] diffusion learning rate: 0.001
2024-11-05 04:29:31,172 - INFO - [diffusion][Epoch 11640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:31,173 - INFO - [diffusion][Epoch 11641] Epoch 11642/12000
2024-11-05 04:29:34,254 - INFO - [diffusion][Epoch 11641] diffusion training Loss: 0.06062823720276356
2024-11-05 04:29:34,256 - INFO - [diffusion][Epoch 11641] diffusion learning rate: 0.001
2024-11-05 04:29:34,257 - INFO - [diffusion][Epoch 11641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:34,259 - INFO - [diffusion][Epoch 11642] Epoch 11643/12000
2024-11-05 04:29:36,983 - INFO - [diffusion][Epoch 11642] diffusion training Loss: 0.056566390208899975
2024-11-05 04:29:36,985 - INFO - [diffusion][Epoch 11642] diffusion learning rate: 0.001
2024-11-05 04:29:36,987 - INFO - [diffusion][Epoch 11642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:36,988 - INFO - [diffusion][Epoch 11643] Epoch 11644/12000
2024-11-05 04:29:39,612 - INFO - [diffusion][Epoch 11643] diffusion training Loss: 0.05414988845586777
2024-11-05 04:29:39,614 - INFO - [diffusion][Epoch 11643] diffusion learning rate: 0.001
2024-11-05 04:29:39,616 - INFO - [diffusion][Epoch 11643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:39,617 - INFO - [diffusion][Epoch 11644] Epoch 11645/12000
2024-11-05 04:29:42,221 - INFO - [diffusion][Epoch 11644] diffusion training Loss: 0.06235277559608221
2024-11-05 04:29:42,223 - INFO - [diffusion][Epoch 11644] diffusion learning rate: 0.001
2024-11-05 04:29:42,224 - INFO - [diffusion][Epoch 11644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:42,225 - INFO - [diffusion][Epoch 11645] Epoch 11646/12000
2024-11-05 04:29:44,864 - INFO - [diffusion][Epoch 11645] diffusion training Loss: 0.062356043606996536
2024-11-05 04:29:44,866 - INFO - [diffusion][Epoch 11645] diffusion learning rate: 0.001
2024-11-05 04:29:44,868 - INFO - [diffusion][Epoch 11645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:44,869 - INFO - [diffusion][Epoch 11646] Epoch 11647/12000
2024-11-05 04:29:47,712 - INFO - [diffusion][Epoch 11646] diffusion training Loss: 0.06230248883366585
2024-11-05 04:29:47,714 - INFO - [diffusion][Epoch 11646] diffusion learning rate: 0.001
2024-11-05 04:29:47,716 - INFO - [diffusion][Epoch 11646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:47,718 - INFO - [diffusion][Epoch 11647] Epoch 11648/12000
2024-11-05 04:29:50,496 - INFO - [diffusion][Epoch 11647] diffusion training Loss: 0.0554704898968339
2024-11-05 04:29:50,498 - INFO - [diffusion][Epoch 11647] diffusion learning rate: 0.001
2024-11-05 04:29:50,499 - INFO - [diffusion][Epoch 11647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:50,501 - INFO - [diffusion][Epoch 11648] Epoch 11649/12000
2024-11-05 04:29:53,188 - INFO - [diffusion][Epoch 11648] diffusion training Loss: 0.061569470912218094
2024-11-05 04:29:53,190 - INFO - [diffusion][Epoch 11648] diffusion learning rate: 0.001
2024-11-05 04:29:53,192 - INFO - [diffusion][Epoch 11648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:53,193 - INFO - [diffusion][Epoch 11649] Epoch 11650/12000
2024-11-05 04:29:55,916 - INFO - [diffusion][Epoch 11649] diffusion training Loss: 0.0629876060411334
2024-11-05 04:29:55,918 - INFO - [diffusion][Epoch 11649] diffusion learning rate: 0.001
2024-11-05 04:29:55,920 - INFO - [diffusion][Epoch 11649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:55,921 - INFO - [diffusion][Epoch 11650] Epoch 11651/12000
2024-11-05 04:29:58,613 - INFO - [diffusion][Epoch 11650] diffusion training Loss: 0.057202055118978024
2024-11-05 04:29:58,617 - INFO - [diffusion][Epoch 11650] diffusion learning rate: 0.001
2024-11-05 04:29:58,618 - INFO - [diffusion][Epoch 11650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:58,620 - INFO - [diffusion][Epoch 11651] Epoch 11652/12000
2024-11-05 04:30:01,254 - INFO - [diffusion][Epoch 11651] diffusion training Loss: 0.05985450092703104
2024-11-05 04:30:01,256 - INFO - [diffusion][Epoch 11651] diffusion learning rate: 0.001
2024-11-05 04:30:01,257 - INFO - [diffusion][Epoch 11651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:01,259 - INFO - [diffusion][Epoch 11652] Epoch 11653/12000
2024-11-05 04:30:04,138 - INFO - [diffusion][Epoch 11652] diffusion training Loss: 0.05596396140754223
2024-11-05 04:30:04,140 - INFO - [diffusion][Epoch 11652] diffusion learning rate: 0.001
2024-11-05 04:30:04,142 - INFO - [diffusion][Epoch 11652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:04,143 - INFO - [diffusion][Epoch 11653] Epoch 11654/12000
2024-11-05 04:30:07,038 - INFO - [diffusion][Epoch 11653] diffusion training Loss: 0.05628478527069092
2024-11-05 04:30:07,040 - INFO - [diffusion][Epoch 11653] diffusion learning rate: 0.001
2024-11-05 04:30:07,042 - INFO - [diffusion][Epoch 11653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:07,043 - INFO - [diffusion][Epoch 11654] Epoch 11655/12000
2024-11-05 04:30:09,834 - INFO - [diffusion][Epoch 11654] diffusion training Loss: 0.05434248596429825
2024-11-05 04:30:09,836 - INFO - [diffusion][Epoch 11654] diffusion learning rate: 0.001
2024-11-05 04:30:09,885 - INFO - [diffusion][Epoch 11654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:09,886 - INFO - [diffusion][Epoch 11655] Epoch 11656/12000
2024-11-05 04:30:12,579 - INFO - [diffusion][Epoch 11655] diffusion training Loss: 0.05347908195108175
2024-11-05 04:30:12,582 - INFO - [diffusion][Epoch 11655] diffusion learning rate: 0.001
2024-11-05 04:30:12,583 - INFO - [diffusion][Epoch 11655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:12,585 - INFO - [diffusion][Epoch 11656] Epoch 11657/12000
2024-11-05 04:30:15,262 - INFO - [diffusion][Epoch 11656] diffusion training Loss: 0.06291325762867928
2024-11-05 04:30:15,264 - INFO - [diffusion][Epoch 11656] diffusion learning rate: 0.001
2024-11-05 04:30:15,267 - INFO - [diffusion][Epoch 11656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:15,268 - INFO - [diffusion][Epoch 11657] Epoch 11658/12000
2024-11-05 04:30:18,126 - INFO - [diffusion][Epoch 11657] diffusion training Loss: 0.05503010284155607
2024-11-05 04:30:18,128 - INFO - [diffusion][Epoch 11657] diffusion learning rate: 0.001
2024-11-05 04:30:18,130 - INFO - [diffusion][Epoch 11657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:18,131 - INFO - [diffusion][Epoch 11658] Epoch 11659/12000
2024-11-05 04:30:20,955 - INFO - [diffusion][Epoch 11658] diffusion training Loss: 0.059052975848317146
2024-11-05 04:30:20,957 - INFO - [diffusion][Epoch 11658] diffusion learning rate: 0.001
2024-11-05 04:30:20,958 - INFO - [diffusion][Epoch 11658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:20,960 - INFO - [diffusion][Epoch 11659] Epoch 11660/12000
2024-11-05 04:30:23,872 - INFO - [diffusion][Epoch 11659] diffusion training Loss: 0.06285272818058729
2024-11-05 04:30:23,874 - INFO - [diffusion][Epoch 11659] diffusion learning rate: 0.001
2024-11-05 04:30:23,876 - INFO - [diffusion][Epoch 11659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:23,877 - INFO - [diffusion][Epoch 11660] Epoch 11661/12000
2024-11-05 04:30:26,670 - INFO - [diffusion][Epoch 11660] diffusion training Loss: 0.05767512787133455
2024-11-05 04:30:26,672 - INFO - [diffusion][Epoch 11660] diffusion learning rate: 0.001
2024-11-05 04:30:26,674 - INFO - [diffusion][Epoch 11660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:26,675 - INFO - [diffusion][Epoch 11661] Epoch 11662/12000
2024-11-05 04:30:29,540 - INFO - [diffusion][Epoch 11661] diffusion training Loss: 0.06161772646009922
2024-11-05 04:30:29,542 - INFO - [diffusion][Epoch 11661] diffusion learning rate: 0.001
2024-11-05 04:30:29,546 - INFO - [diffusion][Epoch 11661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:29,547 - INFO - [diffusion][Epoch 11662] Epoch 11663/12000
2024-11-05 04:30:32,841 - INFO - [diffusion][Epoch 11662] diffusion training Loss: 0.06160372402518988
2024-11-05 04:30:32,843 - INFO - [diffusion][Epoch 11662] diffusion learning rate: 0.001
2024-11-05 04:30:32,845 - INFO - [diffusion][Epoch 11662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:32,846 - INFO - [diffusion][Epoch 11663] Epoch 11664/12000
2024-11-05 04:30:35,529 - INFO - [diffusion][Epoch 11663] diffusion training Loss: 0.059983174316585064
2024-11-05 04:30:35,531 - INFO - [diffusion][Epoch 11663] diffusion learning rate: 0.001
2024-11-05 04:30:35,533 - INFO - [diffusion][Epoch 11663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:35,535 - INFO - [diffusion][Epoch 11664] Epoch 11665/12000
2024-11-05 04:30:38,369 - INFO - [diffusion][Epoch 11664] diffusion training Loss: 0.051832977682352066
2024-11-05 04:30:38,371 - INFO - [diffusion][Epoch 11664] diffusion learning rate: 0.001
2024-11-05 04:30:38,373 - INFO - [diffusion][Epoch 11664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:38,374 - INFO - [diffusion][Epoch 11665] Epoch 11666/12000
2024-11-05 04:30:41,244 - INFO - [diffusion][Epoch 11665] diffusion training Loss: 0.052831342443823814
2024-11-05 04:30:41,246 - INFO - [diffusion][Epoch 11665] diffusion learning rate: 0.001
2024-11-05 04:30:41,248 - INFO - [diffusion][Epoch 11665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:41,249 - INFO - [diffusion][Epoch 11666] Epoch 11667/12000
2024-11-05 04:30:44,116 - INFO - [diffusion][Epoch 11666] diffusion training Loss: 0.05427050311118364
2024-11-05 04:30:44,118 - INFO - [diffusion][Epoch 11666] diffusion learning rate: 0.001
2024-11-05 04:30:44,119 - INFO - [diffusion][Epoch 11666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:44,121 - INFO - [diffusion][Epoch 11667] Epoch 11668/12000
2024-11-05 04:30:46,985 - INFO - [diffusion][Epoch 11667] diffusion training Loss: 0.05411246884614229
2024-11-05 04:30:46,987 - INFO - [diffusion][Epoch 11667] diffusion learning rate: 0.001
2024-11-05 04:30:47,038 - INFO - [diffusion][Epoch 11667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:47,039 - INFO - [diffusion][Epoch 11668] Epoch 11669/12000
2024-11-05 04:30:49,892 - INFO - [diffusion][Epoch 11668] diffusion training Loss: 0.056825222447514534
2024-11-05 04:30:49,894 - INFO - [diffusion][Epoch 11668] diffusion learning rate: 0.001
2024-11-05 04:30:49,896 - INFO - [diffusion][Epoch 11668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:49,898 - INFO - [diffusion][Epoch 11669] Epoch 11670/12000
2024-11-05 04:30:52,674 - INFO - [diffusion][Epoch 11669] diffusion training Loss: 0.05945348273962736
2024-11-05 04:30:52,677 - INFO - [diffusion][Epoch 11669] diffusion learning rate: 0.001
2024-11-05 04:30:52,704 - INFO - [diffusion][Epoch 11669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:52,706 - INFO - [diffusion][Epoch 11670] Epoch 11671/12000
2024-11-05 04:30:55,626 - INFO - [diffusion][Epoch 11670] diffusion training Loss: 0.05952470004558563
2024-11-05 04:30:55,628 - INFO - [diffusion][Epoch 11670] diffusion learning rate: 0.001
2024-11-05 04:30:55,630 - INFO - [diffusion][Epoch 11670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:55,631 - INFO - [diffusion][Epoch 11671] Epoch 11672/12000
2024-11-05 04:30:58,408 - INFO - [diffusion][Epoch 11671] diffusion training Loss: 0.05942760594189167
2024-11-05 04:30:58,409 - INFO - [diffusion][Epoch 11671] diffusion learning rate: 0.001
2024-11-05 04:30:58,411 - INFO - [diffusion][Epoch 11671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:58,412 - INFO - [diffusion][Epoch 11672] Epoch 11673/12000
2024-11-05 04:31:01,142 - INFO - [diffusion][Epoch 11672] diffusion training Loss: 0.062276845797896385
2024-11-05 04:31:01,144 - INFO - [diffusion][Epoch 11672] diffusion learning rate: 0.001
2024-11-05 04:31:01,146 - INFO - [diffusion][Epoch 11672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:01,148 - INFO - [diffusion][Epoch 11673] Epoch 11674/12000
2024-11-05 04:31:03,966 - INFO - [diffusion][Epoch 11673] diffusion training Loss: 0.05439368821680546
2024-11-05 04:31:03,968 - INFO - [diffusion][Epoch 11673] diffusion learning rate: 0.001
2024-11-05 04:31:03,970 - INFO - [diffusion][Epoch 11673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:03,971 - INFO - [diffusion][Epoch 11674] Epoch 11675/12000
2024-11-05 04:31:06,727 - INFO - [diffusion][Epoch 11674] diffusion training Loss: 0.056496988981962204
2024-11-05 04:31:06,729 - INFO - [diffusion][Epoch 11674] diffusion learning rate: 0.001
2024-11-05 04:31:06,731 - INFO - [diffusion][Epoch 11674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:06,732 - INFO - [diffusion][Epoch 11675] Epoch 11676/12000
2024-11-05 04:31:09,508 - INFO - [diffusion][Epoch 11675] diffusion training Loss: 0.061150699853897095
2024-11-05 04:31:09,510 - INFO - [diffusion][Epoch 11675] diffusion learning rate: 0.001
2024-11-05 04:31:09,512 - INFO - [diffusion][Epoch 11675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:09,513 - INFO - [diffusion][Epoch 11676] Epoch 11677/12000
2024-11-05 04:31:12,354 - INFO - [diffusion][Epoch 11676] diffusion training Loss: 0.05507943406701088
2024-11-05 04:31:12,401 - INFO - [diffusion][Epoch 11676] diffusion learning rate: 0.001
2024-11-05 04:31:12,403 - INFO - [diffusion][Epoch 11676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:12,404 - INFO - [diffusion][Epoch 11677] Epoch 11678/12000
2024-11-05 04:31:15,186 - INFO - [diffusion][Epoch 11677] diffusion training Loss: 0.05783391650766134
2024-11-05 04:31:15,188 - INFO - [diffusion][Epoch 11677] diffusion learning rate: 0.001
2024-11-05 04:31:15,190 - INFO - [diffusion][Epoch 11677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:15,191 - INFO - [diffusion][Epoch 11678] Epoch 11679/12000
2024-11-05 04:31:17,963 - INFO - [diffusion][Epoch 11678] diffusion training Loss: 0.06298208516091108
2024-11-05 04:31:17,965 - INFO - [diffusion][Epoch 11678] diffusion learning rate: 0.001
2024-11-05 04:31:17,967 - INFO - [diffusion][Epoch 11678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:17,968 - INFO - [diffusion][Epoch 11679] Epoch 11680/12000
2024-11-05 04:31:20,889 - INFO - [diffusion][Epoch 11679] diffusion training Loss: 0.060041834600269794
2024-11-05 04:31:20,891 - INFO - [diffusion][Epoch 11679] diffusion learning rate: 0.001
2024-11-05 04:31:20,893 - INFO - [diffusion][Epoch 11679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:20,894 - INFO - [diffusion][Epoch 11680] Epoch 11681/12000
2024-11-05 04:31:23,698 - INFO - [diffusion][Epoch 11680] diffusion training Loss: 0.05646769516170025
2024-11-05 04:31:23,700 - INFO - [diffusion][Epoch 11680] diffusion learning rate: 0.001
2024-11-05 04:31:23,702 - INFO - [diffusion][Epoch 11680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:23,703 - INFO - [diffusion][Epoch 11681] Epoch 11682/12000
2024-11-05 04:31:26,435 - INFO - [diffusion][Epoch 11681] diffusion training Loss: 0.059109972789883614
2024-11-05 04:31:26,437 - INFO - [diffusion][Epoch 11681] diffusion learning rate: 0.001
2024-11-05 04:31:26,439 - INFO - [diffusion][Epoch 11681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:26,440 - INFO - [diffusion][Epoch 11682] Epoch 11683/12000
2024-11-05 04:31:29,540 - INFO - [diffusion][Epoch 11682] diffusion training Loss: 0.05980624072253704
2024-11-05 04:31:29,542 - INFO - [diffusion][Epoch 11682] diffusion learning rate: 0.001
2024-11-05 04:31:29,544 - INFO - [diffusion][Epoch 11682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:29,545 - INFO - [diffusion][Epoch 11683] Epoch 11684/12000
2024-11-05 04:31:32,401 - INFO - [diffusion][Epoch 11683] diffusion training Loss: 0.05523446574807167
2024-11-05 04:31:32,403 - INFO - [diffusion][Epoch 11683] diffusion learning rate: 0.001
2024-11-05 04:31:32,405 - INFO - [diffusion][Epoch 11683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:32,406 - INFO - [diffusion][Epoch 11684] Epoch 11685/12000
2024-11-05 04:31:35,176 - INFO - [diffusion][Epoch 11684] diffusion training Loss: 0.058625463396310806
2024-11-05 04:31:35,178 - INFO - [diffusion][Epoch 11684] diffusion learning rate: 0.001
2024-11-05 04:31:35,179 - INFO - [diffusion][Epoch 11684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:35,181 - INFO - [diffusion][Epoch 11685] Epoch 11686/12000
2024-11-05 04:31:38,047 - INFO - [diffusion][Epoch 11685] diffusion training Loss: 0.05322589725255966
2024-11-05 04:31:38,049 - INFO - [diffusion][Epoch 11685] diffusion learning rate: 0.001
2024-11-05 04:31:38,051 - INFO - [diffusion][Epoch 11685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:38,052 - INFO - [diffusion][Epoch 11686] Epoch 11687/12000
2024-11-05 04:31:40,954 - INFO - [diffusion][Epoch 11686] diffusion training Loss: 0.055037631653249264
2024-11-05 04:31:40,957 - INFO - [diffusion][Epoch 11686] diffusion learning rate: 0.001
2024-11-05 04:31:40,959 - INFO - [diffusion][Epoch 11686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:40,960 - INFO - [diffusion][Epoch 11687] Epoch 11688/12000
2024-11-05 04:31:43,806 - INFO - [diffusion][Epoch 11687] diffusion training Loss: 0.056317581795156
2024-11-05 04:31:43,808 - INFO - [diffusion][Epoch 11687] diffusion learning rate: 0.001
2024-11-05 04:31:43,810 - INFO - [diffusion][Epoch 11687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:43,811 - INFO - [diffusion][Epoch 11688] Epoch 11689/12000
2024-11-05 04:31:46,637 - INFO - [diffusion][Epoch 11688] diffusion training Loss: 0.05506844073534012
2024-11-05 04:31:46,638 - INFO - [diffusion][Epoch 11688] diffusion learning rate: 0.001
2024-11-05 04:31:46,640 - INFO - [diffusion][Epoch 11688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:46,641 - INFO - [diffusion][Epoch 11689] Epoch 11690/12000
2024-11-05 04:31:49,402 - INFO - [diffusion][Epoch 11689] diffusion training Loss: 0.05786425247788429
2024-11-05 04:31:49,404 - INFO - [diffusion][Epoch 11689] diffusion learning rate: 0.001
2024-11-05 04:31:49,406 - INFO - [diffusion][Epoch 11689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:49,407 - INFO - [diffusion][Epoch 11690] Epoch 11691/12000
2024-11-05 04:31:52,153 - INFO - [diffusion][Epoch 11690] diffusion training Loss: 0.05942270904779434
2024-11-05 04:31:52,155 - INFO - [diffusion][Epoch 11690] diffusion learning rate: 0.001
2024-11-05 04:31:52,156 - INFO - [diffusion][Epoch 11690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:52,157 - INFO - [diffusion][Epoch 11691] Epoch 11692/12000
2024-11-05 04:31:54,939 - INFO - [diffusion][Epoch 11691] diffusion training Loss: 0.05705735273659229
2024-11-05 04:31:54,941 - INFO - [diffusion][Epoch 11691] diffusion learning rate: 0.001
2024-11-05 04:31:54,942 - INFO - [diffusion][Epoch 11691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:54,943 - INFO - [diffusion][Epoch 11692] Epoch 11693/12000
2024-11-05 04:31:57,820 - INFO - [diffusion][Epoch 11692] diffusion training Loss: 0.05982516799122095
2024-11-05 04:31:57,822 - INFO - [diffusion][Epoch 11692] diffusion learning rate: 0.001
2024-11-05 04:31:57,823 - INFO - [diffusion][Epoch 11692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:57,825 - INFO - [diffusion][Epoch 11693] Epoch 11694/12000
2024-11-05 04:32:00,657 - INFO - [diffusion][Epoch 11693] diffusion training Loss: 0.056089166551828384
2024-11-05 04:32:00,659 - INFO - [diffusion][Epoch 11693] diffusion learning rate: 0.001
2024-11-05 04:32:00,661 - INFO - [diffusion][Epoch 11693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:00,662 - INFO - [diffusion][Epoch 11694] Epoch 11695/12000
2024-11-05 04:32:03,463 - INFO - [diffusion][Epoch 11694] diffusion training Loss: 0.058357263915240765
2024-11-05 04:32:03,464 - INFO - [diffusion][Epoch 11694] diffusion learning rate: 0.001
2024-11-05 04:32:03,466 - INFO - [diffusion][Epoch 11694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:03,467 - INFO - [diffusion][Epoch 11695] Epoch 11696/12000
2024-11-05 04:32:06,310 - INFO - [diffusion][Epoch 11695] diffusion training Loss: 0.06530856341123581
2024-11-05 04:32:06,313 - INFO - [diffusion][Epoch 11695] diffusion learning rate: 0.001
2024-11-05 04:32:06,315 - INFO - [diffusion][Epoch 11695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:06,317 - INFO - [diffusion][Epoch 11696] Epoch 11697/12000
2024-11-05 04:32:09,056 - INFO - [diffusion][Epoch 11696] diffusion training Loss: 0.061521051451563835
2024-11-05 04:32:09,057 - INFO - [diffusion][Epoch 11696] diffusion learning rate: 0.001
2024-11-05 04:32:09,060 - INFO - [diffusion][Epoch 11696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:09,061 - INFO - [diffusion][Epoch 11697] Epoch 11698/12000
2024-11-05 04:32:11,900 - INFO - [diffusion][Epoch 11697] diffusion training Loss: 0.05749405641108751
2024-11-05 04:32:11,902 - INFO - [diffusion][Epoch 11697] diffusion learning rate: 0.001
2024-11-05 04:32:11,903 - INFO - [diffusion][Epoch 11697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:11,905 - INFO - [diffusion][Epoch 11698] Epoch 11699/12000
2024-11-05 04:32:14,800 - INFO - [diffusion][Epoch 11698] diffusion training Loss: 0.06255076639354229
2024-11-05 04:32:14,801 - INFO - [diffusion][Epoch 11698] diffusion learning rate: 0.001
2024-11-05 04:32:14,803 - INFO - [diffusion][Epoch 11698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:14,804 - INFO - [diffusion][Epoch 11699] Epoch 11700/12000
2024-11-05 04:32:17,660 - INFO - [diffusion][Epoch 11699] diffusion training Loss: 0.05529601126909256
2024-11-05 04:32:17,662 - INFO - [diffusion][Epoch 11699] diffusion learning rate: 0.001
2024-11-05 04:32:17,663 - INFO - [diffusion][Epoch 11699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:17,664 - INFO - [diffusion][Epoch 11700] Epoch 11701/12000
2024-11-05 04:32:20,404 - INFO - [diffusion][Epoch 11700] diffusion training Loss: 0.06084676273167133
2024-11-05 04:32:20,405 - INFO - [diffusion][Epoch 11700] diffusion learning rate: 0.001
2024-11-05 04:32:20,407 - INFO - [diffusion][Epoch 11700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:20,408 - INFO - [diffusion][Epoch 11701] Epoch 11702/12000
2024-11-05 04:32:23,199 - INFO - [diffusion][Epoch 11701] diffusion training Loss: 0.05917505919933319
2024-11-05 04:32:23,201 - INFO - [diffusion][Epoch 11701] diffusion learning rate: 0.001
2024-11-05 04:32:23,203 - INFO - [diffusion][Epoch 11701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:23,204 - INFO - [diffusion][Epoch 11702] Epoch 11703/12000
2024-11-05 04:32:25,985 - INFO - [diffusion][Epoch 11702] diffusion training Loss: 0.05886377766728401
2024-11-05 04:32:25,987 - INFO - [diffusion][Epoch 11702] diffusion learning rate: 0.001
2024-11-05 04:32:25,989 - INFO - [diffusion][Epoch 11702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:25,990 - INFO - [diffusion][Epoch 11703] Epoch 11704/12000
2024-11-05 04:32:28,747 - INFO - [diffusion][Epoch 11703] diffusion training Loss: 0.05985523946583271
2024-11-05 04:32:28,749 - INFO - [diffusion][Epoch 11703] diffusion learning rate: 0.001
2024-11-05 04:32:28,772 - INFO - [diffusion][Epoch 11703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:28,774 - INFO - [diffusion][Epoch 11704] Epoch 11705/12000
2024-11-05 04:32:31,748 - INFO - [diffusion][Epoch 11704] diffusion training Loss: 0.053894941695034504
2024-11-05 04:32:31,750 - INFO - [diffusion][Epoch 11704] diffusion learning rate: 0.001
2024-11-05 04:32:31,751 - INFO - [diffusion][Epoch 11704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:31,753 - INFO - [diffusion][Epoch 11705] Epoch 11706/12000
2024-11-05 04:32:34,513 - INFO - [diffusion][Epoch 11705] diffusion training Loss: 0.05945989303290844
2024-11-05 04:32:34,517 - INFO - [diffusion][Epoch 11705] diffusion learning rate: 0.001
2024-11-05 04:32:34,519 - INFO - [diffusion][Epoch 11705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:34,521 - INFO - [diffusion][Epoch 11706] Epoch 11707/12000
2024-11-05 04:32:37,376 - INFO - [diffusion][Epoch 11706] diffusion training Loss: 0.06321088783442974
2024-11-05 04:32:37,378 - INFO - [diffusion][Epoch 11706] diffusion learning rate: 0.001
2024-11-05 04:32:37,380 - INFO - [diffusion][Epoch 11706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:37,381 - INFO - [diffusion][Epoch 11707] Epoch 11708/12000
2024-11-05 04:32:40,270 - INFO - [diffusion][Epoch 11707] diffusion training Loss: 0.05825286731123924
2024-11-05 04:32:40,272 - INFO - [diffusion][Epoch 11707] diffusion learning rate: 0.001
2024-11-05 04:32:40,274 - INFO - [diffusion][Epoch 11707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:40,275 - INFO - [diffusion][Epoch 11708] Epoch 11709/12000
2024-11-05 04:32:42,934 - INFO - [diffusion][Epoch 11708] diffusion training Loss: 0.05682425852864981
2024-11-05 04:32:42,937 - INFO - [diffusion][Epoch 11708] diffusion learning rate: 0.001
2024-11-05 04:32:42,938 - INFO - [diffusion][Epoch 11708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:42,939 - INFO - [diffusion][Epoch 11709] Epoch 11710/12000
2024-11-05 04:32:45,566 - INFO - [diffusion][Epoch 11709] diffusion training Loss: 0.05848655477166176
2024-11-05 04:32:45,569 - INFO - [diffusion][Epoch 11709] diffusion learning rate: 0.001
2024-11-05 04:32:45,570 - INFO - [diffusion][Epoch 11709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:45,572 - INFO - [diffusion][Epoch 11710] Epoch 11711/12000
2024-11-05 04:32:48,457 - INFO - [diffusion][Epoch 11710] diffusion training Loss: 0.049831745214760303
2024-11-05 04:32:48,459 - INFO - [diffusion][Epoch 11710] diffusion learning rate: 0.001
2024-11-05 04:32:48,461 - INFO - [diffusion][Epoch 11710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:48,462 - INFO - [diffusion][Epoch 11711] Epoch 11712/12000
2024-11-05 04:32:51,373 - INFO - [diffusion][Epoch 11711] diffusion training Loss: 0.055807613767683506
2024-11-05 04:32:51,375 - INFO - [diffusion][Epoch 11711] diffusion learning rate: 0.001
2024-11-05 04:32:51,376 - INFO - [diffusion][Epoch 11711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:51,378 - INFO - [diffusion][Epoch 11712] Epoch 11713/12000
2024-11-05 04:32:54,111 - INFO - [diffusion][Epoch 11712] diffusion training Loss: 0.055446526035666466
2024-11-05 04:32:54,112 - INFO - [diffusion][Epoch 11712] diffusion learning rate: 0.001
2024-11-05 04:32:54,114 - INFO - [diffusion][Epoch 11712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:54,115 - INFO - [diffusion][Epoch 11713] Epoch 11714/12000
2024-11-05 04:32:56,945 - INFO - [diffusion][Epoch 11713] diffusion training Loss: 0.061521923169493675
2024-11-05 04:32:56,947 - INFO - [diffusion][Epoch 11713] diffusion learning rate: 0.001
2024-11-05 04:32:56,949 - INFO - [diffusion][Epoch 11713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:56,950 - INFO - [diffusion][Epoch 11714] Epoch 11715/12000
2024-11-05 04:32:59,813 - INFO - [diffusion][Epoch 11714] diffusion training Loss: 0.05801409110426903
2024-11-05 04:32:59,815 - INFO - [diffusion][Epoch 11714] diffusion learning rate: 0.001
2024-11-05 04:32:59,817 - INFO - [diffusion][Epoch 11714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:59,818 - INFO - [diffusion][Epoch 11715] Epoch 11716/12000
2024-11-05 04:33:02,679 - INFO - [diffusion][Epoch 11715] diffusion training Loss: 0.056324380449950695
2024-11-05 04:33:02,681 - INFO - [diffusion][Epoch 11715] diffusion learning rate: 0.001
2024-11-05 04:33:02,682 - INFO - [diffusion][Epoch 11715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:02,684 - INFO - [diffusion][Epoch 11716] Epoch 11717/12000
2024-11-05 04:33:05,588 - INFO - [diffusion][Epoch 11716] diffusion training Loss: 0.052984265610575676
2024-11-05 04:33:05,590 - INFO - [diffusion][Epoch 11716] diffusion learning rate: 0.001
2024-11-05 04:33:05,591 - INFO - [diffusion][Epoch 11716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:05,593 - INFO - [diffusion][Epoch 11717] Epoch 11718/12000
2024-11-05 04:33:08,452 - INFO - [diffusion][Epoch 11717] diffusion training Loss: 0.0592774972319603
2024-11-05 04:33:08,454 - INFO - [diffusion][Epoch 11717] diffusion learning rate: 0.001
2024-11-05 04:33:08,455 - INFO - [diffusion][Epoch 11717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:08,457 - INFO - [diffusion][Epoch 11718] Epoch 11719/12000
2024-11-05 04:33:11,278 - INFO - [diffusion][Epoch 11718] diffusion training Loss: 0.05362658575177193
2024-11-05 04:33:11,280 - INFO - [diffusion][Epoch 11718] diffusion learning rate: 0.001
2024-11-05 04:33:11,282 - INFO - [diffusion][Epoch 11718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:11,283 - INFO - [diffusion][Epoch 11719] Epoch 11720/12000
2024-11-05 04:33:14,138 - INFO - [diffusion][Epoch 11719] diffusion training Loss: 0.05441131163388491
2024-11-05 04:33:14,140 - INFO - [diffusion][Epoch 11719] diffusion learning rate: 0.001
2024-11-05 04:33:14,142 - INFO - [diffusion][Epoch 11719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:14,143 - INFO - [diffusion][Epoch 11720] Epoch 11721/12000
2024-11-05 04:33:17,021 - INFO - [diffusion][Epoch 11720] diffusion training Loss: 0.05784990545362234
2024-11-05 04:33:17,023 - INFO - [diffusion][Epoch 11720] diffusion learning rate: 0.001
2024-11-05 04:33:17,025 - INFO - [diffusion][Epoch 11720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:17,026 - INFO - [diffusion][Epoch 11721] Epoch 11722/12000
2024-11-05 04:33:19,961 - INFO - [diffusion][Epoch 11721] diffusion training Loss: 0.05728145223110914
2024-11-05 04:33:19,963 - INFO - [diffusion][Epoch 11721] diffusion learning rate: 0.001
2024-11-05 04:33:19,965 - INFO - [diffusion][Epoch 11721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:19,966 - INFO - [diffusion][Epoch 11722] Epoch 11723/12000
2024-11-05 04:33:22,877 - INFO - [diffusion][Epoch 11722] diffusion training Loss: 0.060553863644599915
2024-11-05 04:33:22,878 - INFO - [diffusion][Epoch 11722] diffusion learning rate: 0.001
2024-11-05 04:33:22,880 - INFO - [diffusion][Epoch 11722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:22,881 - INFO - [diffusion][Epoch 11723] Epoch 11724/12000
2024-11-05 04:33:25,737 - INFO - [diffusion][Epoch 11723] diffusion training Loss: 0.057125973515212536
2024-11-05 04:33:25,739 - INFO - [diffusion][Epoch 11723] diffusion learning rate: 0.001
2024-11-05 04:33:25,741 - INFO - [diffusion][Epoch 11723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:25,742 - INFO - [diffusion][Epoch 11724] Epoch 11725/12000
2024-11-05 04:33:28,648 - INFO - [diffusion][Epoch 11724] diffusion training Loss: 0.05517532117664814
2024-11-05 04:33:28,649 - INFO - [diffusion][Epoch 11724] diffusion learning rate: 0.001
2024-11-05 04:33:28,651 - INFO - [diffusion][Epoch 11724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:28,652 - INFO - [diffusion][Epoch 11725] Epoch 11726/12000
2024-11-05 04:33:31,749 - INFO - [diffusion][Epoch 11725] diffusion training Loss: 0.056516687385737896
2024-11-05 04:33:31,751 - INFO - [diffusion][Epoch 11725] diffusion learning rate: 0.001
2024-11-05 04:33:31,752 - INFO - [diffusion][Epoch 11725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:31,754 - INFO - [diffusion][Epoch 11726] Epoch 11727/12000
2024-11-05 04:33:34,581 - INFO - [diffusion][Epoch 11726] diffusion training Loss: 0.057814087718725204
2024-11-05 04:33:34,583 - INFO - [diffusion][Epoch 11726] diffusion learning rate: 0.001
2024-11-05 04:33:34,585 - INFO - [diffusion][Epoch 11726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:34,586 - INFO - [diffusion][Epoch 11727] Epoch 11728/12000
2024-11-05 04:33:37,505 - INFO - [diffusion][Epoch 11727] diffusion training Loss: 0.05729901045560837
2024-11-05 04:33:37,507 - INFO - [diffusion][Epoch 11727] diffusion learning rate: 0.001
2024-11-05 04:33:37,509 - INFO - [diffusion][Epoch 11727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:37,510 - INFO - [diffusion][Epoch 11728] Epoch 11729/12000
2024-11-05 04:33:40,421 - INFO - [diffusion][Epoch 11728] diffusion training Loss: 0.05827848706394434
2024-11-05 04:33:40,423 - INFO - [diffusion][Epoch 11728] diffusion learning rate: 0.001
2024-11-05 04:33:40,425 - INFO - [diffusion][Epoch 11728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:40,426 - INFO - [diffusion][Epoch 11729] Epoch 11730/12000
2024-11-05 04:33:43,345 - INFO - [diffusion][Epoch 11729] diffusion training Loss: 0.05327446013689041
2024-11-05 04:33:43,348 - INFO - [diffusion][Epoch 11729] diffusion learning rate: 0.001
2024-11-05 04:33:43,349 - INFO - [diffusion][Epoch 11729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:43,351 - INFO - [diffusion][Epoch 11730] Epoch 11731/12000
2024-11-05 04:33:46,221 - INFO - [diffusion][Epoch 11730] diffusion training Loss: 0.061238192953169346
2024-11-05 04:33:46,223 - INFO - [diffusion][Epoch 11730] diffusion learning rate: 0.001
2024-11-05 04:33:46,225 - INFO - [diffusion][Epoch 11730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:46,226 - INFO - [diffusion][Epoch 11731] Epoch 11732/12000
2024-11-05 04:33:49,133 - INFO - [diffusion][Epoch 11731] diffusion training Loss: 0.05749284941703081
2024-11-05 04:33:49,135 - INFO - [diffusion][Epoch 11731] diffusion learning rate: 0.001
2024-11-05 04:33:49,137 - INFO - [diffusion][Epoch 11731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:49,138 - INFO - [diffusion][Epoch 11732] Epoch 11733/12000
2024-11-05 04:33:52,001 - INFO - [diffusion][Epoch 11732] diffusion training Loss: 0.05757472198456526
2024-11-05 04:33:52,003 - INFO - [diffusion][Epoch 11732] diffusion learning rate: 0.001
2024-11-05 04:33:52,005 - INFO - [diffusion][Epoch 11732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:52,006 - INFO - [diffusion][Epoch 11733] Epoch 11734/12000
2024-11-05 04:33:54,908 - INFO - [diffusion][Epoch 11733] diffusion training Loss: 0.06135579291731119
2024-11-05 04:33:54,910 - INFO - [diffusion][Epoch 11733] diffusion learning rate: 0.001
2024-11-05 04:33:54,912 - INFO - [diffusion][Epoch 11733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:54,913 - INFO - [diffusion][Epoch 11734] Epoch 11735/12000
2024-11-05 04:33:57,804 - INFO - [diffusion][Epoch 11734] diffusion training Loss: 0.057158044539391994
2024-11-05 04:33:57,807 - INFO - [diffusion][Epoch 11734] diffusion learning rate: 0.001
2024-11-05 04:33:57,809 - INFO - [diffusion][Epoch 11734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:57,810 - INFO - [diffusion][Epoch 11735] Epoch 11736/12000
2024-11-05 04:34:00,715 - INFO - [diffusion][Epoch 11735] diffusion training Loss: 0.06215731706470251
2024-11-05 04:34:00,717 - INFO - [diffusion][Epoch 11735] diffusion learning rate: 0.001
2024-11-05 04:34:00,719 - INFO - [diffusion][Epoch 11735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:00,720 - INFO - [diffusion][Epoch 11736] Epoch 11737/12000
2024-11-05 04:34:03,610 - INFO - [diffusion][Epoch 11736] diffusion training Loss: 0.06272904854267836
2024-11-05 04:34:03,612 - INFO - [diffusion][Epoch 11736] diffusion learning rate: 0.001
2024-11-05 04:34:03,613 - INFO - [diffusion][Epoch 11736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:03,615 - INFO - [diffusion][Epoch 11737] Epoch 11738/12000
2024-11-05 04:34:06,518 - INFO - [diffusion][Epoch 11737] diffusion training Loss: 0.057767828926444054
2024-11-05 04:34:06,520 - INFO - [diffusion][Epoch 11737] diffusion learning rate: 0.001
2024-11-05 04:34:06,522 - INFO - [diffusion][Epoch 11737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:06,524 - INFO - [diffusion][Epoch 11738] Epoch 11739/12000
2024-11-05 04:34:09,512 - INFO - [diffusion][Epoch 11738] diffusion training Loss: 0.06092632655054331
2024-11-05 04:34:09,515 - INFO - [diffusion][Epoch 11738] diffusion learning rate: 0.001
2024-11-05 04:34:09,520 - INFO - [diffusion][Epoch 11738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:09,522 - INFO - [diffusion][Epoch 11739] Epoch 11740/12000
2024-11-05 04:34:12,442 - INFO - [diffusion][Epoch 11739] diffusion training Loss: 0.055325331166386604
2024-11-05 04:34:12,444 - INFO - [diffusion][Epoch 11739] diffusion learning rate: 0.001
2024-11-05 04:34:12,446 - INFO - [diffusion][Epoch 11739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:12,447 - INFO - [diffusion][Epoch 11740] Epoch 11741/12000
2024-11-05 04:34:15,325 - INFO - [diffusion][Epoch 11740] diffusion training Loss: 0.05207654181867838
2024-11-05 04:34:15,327 - INFO - [diffusion][Epoch 11740] diffusion learning rate: 0.001
2024-11-05 04:34:15,329 - INFO - [diffusion][Epoch 11740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:15,330 - INFO - [diffusion][Epoch 11741] Epoch 11742/12000
2024-11-05 04:34:18,135 - INFO - [diffusion][Epoch 11741] diffusion training Loss: 0.062478347681462765
2024-11-05 04:34:18,137 - INFO - [diffusion][Epoch 11741] diffusion learning rate: 0.001
2024-11-05 04:34:18,139 - INFO - [diffusion][Epoch 11741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:18,140 - INFO - [diffusion][Epoch 11742] Epoch 11743/12000
2024-11-05 04:34:21,049 - INFO - [diffusion][Epoch 11742] diffusion training Loss: 0.05521109141409397
2024-11-05 04:34:21,051 - INFO - [diffusion][Epoch 11742] diffusion learning rate: 0.001
2024-11-05 04:34:21,053 - INFO - [diffusion][Epoch 11742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:21,055 - INFO - [diffusion][Epoch 11743] Epoch 11744/12000
2024-11-05 04:34:23,962 - INFO - [diffusion][Epoch 11743] diffusion training Loss: 0.06464522797614336
2024-11-05 04:34:23,964 - INFO - [diffusion][Epoch 11743] diffusion learning rate: 0.001
2024-11-05 04:34:23,966 - INFO - [diffusion][Epoch 11743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:23,968 - INFO - [diffusion][Epoch 11744] Epoch 11745/12000
2024-11-05 04:34:26,789 - INFO - [diffusion][Epoch 11744] diffusion training Loss: 0.05676389392465353
2024-11-05 04:34:26,791 - INFO - [diffusion][Epoch 11744] diffusion learning rate: 0.001
2024-11-05 04:34:26,793 - INFO - [diffusion][Epoch 11744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:26,795 - INFO - [diffusion][Epoch 11745] Epoch 11746/12000
2024-11-05 04:34:29,688 - INFO - [diffusion][Epoch 11745] diffusion training Loss: 0.057498582638800144
2024-11-05 04:34:29,690 - INFO - [diffusion][Epoch 11745] diffusion learning rate: 0.001
2024-11-05 04:34:29,692 - INFO - [diffusion][Epoch 11745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:29,694 - INFO - [diffusion][Epoch 11746] Epoch 11747/12000
2024-11-05 04:34:32,818 - INFO - [diffusion][Epoch 11746] diffusion training Loss: 0.05642066150903702
2024-11-05 04:34:32,820 - INFO - [diffusion][Epoch 11746] diffusion learning rate: 0.001
2024-11-05 04:34:32,822 - INFO - [diffusion][Epoch 11746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:32,823 - INFO - [diffusion][Epoch 11747] Epoch 11748/12000
2024-11-05 04:34:35,758 - INFO - [diffusion][Epoch 11747] diffusion training Loss: 0.056955739855766296
2024-11-05 04:34:35,761 - INFO - [diffusion][Epoch 11747] diffusion learning rate: 0.001
2024-11-05 04:34:35,763 - INFO - [diffusion][Epoch 11747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:35,764 - INFO - [diffusion][Epoch 11748] Epoch 11749/12000
2024-11-05 04:34:38,720 - INFO - [diffusion][Epoch 11748] diffusion training Loss: 0.054628520272672176
2024-11-05 04:34:38,723 - INFO - [diffusion][Epoch 11748] diffusion learning rate: 0.001
2024-11-05 04:34:38,724 - INFO - [diffusion][Epoch 11748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:38,726 - INFO - [diffusion][Epoch 11749] Epoch 11750/12000
2024-11-05 04:34:41,647 - INFO - [diffusion][Epoch 11749] diffusion training Loss: 0.05696611385792494
2024-11-05 04:34:41,649 - INFO - [diffusion][Epoch 11749] diffusion learning rate: 0.001
2024-11-05 04:34:41,651 - INFO - [diffusion][Epoch 11749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:41,652 - INFO - [diffusion][Epoch 11750] Epoch 11751/12000
2024-11-05 04:34:44,554 - INFO - [diffusion][Epoch 11750] diffusion training Loss: 0.06094702146947384
2024-11-05 04:34:44,557 - INFO - [diffusion][Epoch 11750] diffusion learning rate: 0.001
2024-11-05 04:34:44,559 - INFO - [diffusion][Epoch 11750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:44,560 - INFO - [diffusion][Epoch 11751] Epoch 11752/12000
2024-11-05 04:34:47,463 - INFO - [diffusion][Epoch 11751] diffusion training Loss: 0.05381334386765957
2024-11-05 04:34:47,465 - INFO - [diffusion][Epoch 11751] diffusion learning rate: 0.001
2024-11-05 04:34:47,467 - INFO - [diffusion][Epoch 11751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:47,469 - INFO - [diffusion][Epoch 11752] Epoch 11753/12000
2024-11-05 04:34:50,353 - INFO - [diffusion][Epoch 11752] diffusion training Loss: 0.0595557140186429
2024-11-05 04:34:50,355 - INFO - [diffusion][Epoch 11752] diffusion learning rate: 0.001
2024-11-05 04:34:50,357 - INFO - [diffusion][Epoch 11752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:50,359 - INFO - [diffusion][Epoch 11753] Epoch 11754/12000
2024-11-05 04:34:53,268 - INFO - [diffusion][Epoch 11753] diffusion training Loss: 0.05321425572037697
2024-11-05 04:34:53,270 - INFO - [diffusion][Epoch 11753] diffusion learning rate: 0.001
2024-11-05 04:34:53,272 - INFO - [diffusion][Epoch 11753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:53,273 - INFO - [diffusion][Epoch 11754] Epoch 11755/12000
2024-11-05 04:34:56,145 - INFO - [diffusion][Epoch 11754] diffusion training Loss: 0.062163240276277065
2024-11-05 04:34:56,147 - INFO - [diffusion][Epoch 11754] diffusion learning rate: 0.001
2024-11-05 04:34:56,149 - INFO - [diffusion][Epoch 11754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:56,150 - INFO - [diffusion][Epoch 11755] Epoch 11756/12000
2024-11-05 04:34:59,063 - INFO - [diffusion][Epoch 11755] diffusion training Loss: 0.057381254620850086
2024-11-05 04:34:59,065 - INFO - [diffusion][Epoch 11755] diffusion learning rate: 0.001
2024-11-05 04:34:59,066 - INFO - [diffusion][Epoch 11755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:59,068 - INFO - [diffusion][Epoch 11756] Epoch 11757/12000
2024-11-05 04:35:02,041 - INFO - [diffusion][Epoch 11756] diffusion training Loss: 0.0563732935115695
2024-11-05 04:35:02,043 - INFO - [diffusion][Epoch 11756] diffusion learning rate: 0.001
2024-11-05 04:35:02,045 - INFO - [diffusion][Epoch 11756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:02,046 - INFO - [diffusion][Epoch 11757] Epoch 11758/12000
2024-11-05 04:35:04,951 - INFO - [diffusion][Epoch 11757] diffusion training Loss: 0.05984584707766771
2024-11-05 04:35:04,953 - INFO - [diffusion][Epoch 11757] diffusion learning rate: 0.001
2024-11-05 04:35:04,954 - INFO - [diffusion][Epoch 11757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:04,956 - INFO - [diffusion][Epoch 11758] Epoch 11759/12000
2024-11-05 04:35:07,862 - INFO - [diffusion][Epoch 11758] diffusion training Loss: 0.05789570789784193
2024-11-05 04:35:07,864 - INFO - [diffusion][Epoch 11758] diffusion learning rate: 0.001
2024-11-05 04:35:07,866 - INFO - [diffusion][Epoch 11758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:07,867 - INFO - [diffusion][Epoch 11759] Epoch 11760/12000
2024-11-05 04:35:10,795 - INFO - [diffusion][Epoch 11759] diffusion training Loss: 0.05729730147868395
2024-11-05 04:35:10,797 - INFO - [diffusion][Epoch 11759] diffusion learning rate: 0.001
2024-11-05 04:35:10,799 - INFO - [diffusion][Epoch 11759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:10,801 - INFO - [diffusion][Epoch 11760] Epoch 11761/12000
2024-11-05 04:35:13,667 - INFO - [diffusion][Epoch 11760] diffusion training Loss: 0.06120881158858538
2024-11-05 04:35:13,669 - INFO - [diffusion][Epoch 11760] diffusion learning rate: 0.001
2024-11-05 04:35:13,671 - INFO - [diffusion][Epoch 11760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:13,672 - INFO - [diffusion][Epoch 11761] Epoch 11762/12000
2024-11-05 04:35:16,619 - INFO - [diffusion][Epoch 11761] diffusion training Loss: 0.058860281482338905
2024-11-05 04:35:16,621 - INFO - [diffusion][Epoch 11761] diffusion learning rate: 0.001
2024-11-05 04:35:16,623 - INFO - [diffusion][Epoch 11761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:16,624 - INFO - [diffusion][Epoch 11762] Epoch 11763/12000
2024-11-05 04:35:19,551 - INFO - [diffusion][Epoch 11762] diffusion training Loss: 0.05988426320254803
2024-11-05 04:35:19,553 - INFO - [diffusion][Epoch 11762] diffusion learning rate: 0.001
2024-11-05 04:35:19,555 - INFO - [diffusion][Epoch 11762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:19,556 - INFO - [diffusion][Epoch 11763] Epoch 11764/12000
2024-11-05 04:35:22,419 - INFO - [diffusion][Epoch 11763] diffusion training Loss: 0.057930825278162956
2024-11-05 04:35:22,421 - INFO - [diffusion][Epoch 11763] diffusion learning rate: 0.001
2024-11-05 04:35:22,423 - INFO - [diffusion][Epoch 11763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:22,425 - INFO - [diffusion][Epoch 11764] Epoch 11765/12000
2024-11-05 04:35:25,322 - INFO - [diffusion][Epoch 11764] diffusion training Loss: 0.056602565571665764
2024-11-05 04:35:25,324 - INFO - [diffusion][Epoch 11764] diffusion learning rate: 0.001
2024-11-05 04:35:25,326 - INFO - [diffusion][Epoch 11764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:25,327 - INFO - [diffusion][Epoch 11765] Epoch 11766/12000
2024-11-05 04:35:28,207 - INFO - [diffusion][Epoch 11765] diffusion training Loss: 0.05326574295759201
2024-11-05 04:35:28,209 - INFO - [diffusion][Epoch 11765] diffusion learning rate: 0.001
2024-11-05 04:35:28,211 - INFO - [diffusion][Epoch 11765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:28,212 - INFO - [diffusion][Epoch 11766] Epoch 11767/12000
2024-11-05 04:35:31,114 - INFO - [diffusion][Epoch 11766] diffusion training Loss: 0.060000572353601456
2024-11-05 04:35:31,116 - INFO - [diffusion][Epoch 11766] diffusion learning rate: 0.001
2024-11-05 04:35:31,118 - INFO - [diffusion][Epoch 11766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:31,119 - INFO - [diffusion][Epoch 11767] Epoch 11768/12000
2024-11-05 04:35:34,264 - INFO - [diffusion][Epoch 11767] diffusion training Loss: 0.05570070818066597
2024-11-05 04:35:34,266 - INFO - [diffusion][Epoch 11767] diffusion learning rate: 0.001
2024-11-05 04:35:34,268 - INFO - [diffusion][Epoch 11767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:34,269 - INFO - [diffusion][Epoch 11768] Epoch 11769/12000
2024-11-05 04:35:37,072 - INFO - [diffusion][Epoch 11768] diffusion training Loss: 0.05591082386672497
2024-11-05 04:35:37,074 - INFO - [diffusion][Epoch 11768] diffusion learning rate: 0.001
2024-11-05 04:35:37,076 - INFO - [diffusion][Epoch 11768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:37,077 - INFO - [diffusion][Epoch 11769] Epoch 11770/12000
2024-11-05 04:35:39,946 - INFO - [diffusion][Epoch 11769] diffusion training Loss: 0.05613353196531534
2024-11-05 04:35:39,948 - INFO - [diffusion][Epoch 11769] diffusion learning rate: 0.001
2024-11-05 04:35:39,950 - INFO - [diffusion][Epoch 11769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:39,951 - INFO - [diffusion][Epoch 11770] Epoch 11771/12000
2024-11-05 04:35:42,866 - INFO - [diffusion][Epoch 11770] diffusion training Loss: 0.06024136487394571
2024-11-05 04:35:42,868 - INFO - [diffusion][Epoch 11770] diffusion learning rate: 0.001
2024-11-05 04:35:42,870 - INFO - [diffusion][Epoch 11770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:42,871 - INFO - [diffusion][Epoch 11771] Epoch 11772/12000
2024-11-05 04:35:45,781 - INFO - [diffusion][Epoch 11771] diffusion training Loss: 0.055003722198307514
2024-11-05 04:35:45,784 - INFO - [diffusion][Epoch 11771] diffusion learning rate: 0.001
2024-11-05 04:35:45,786 - INFO - [diffusion][Epoch 11771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:45,787 - INFO - [diffusion][Epoch 11772] Epoch 11773/12000
2024-11-05 04:35:48,740 - INFO - [diffusion][Epoch 11772] diffusion training Loss: 0.05815380718559027
2024-11-05 04:35:48,742 - INFO - [diffusion][Epoch 11772] diffusion learning rate: 0.001
2024-11-05 04:35:48,744 - INFO - [diffusion][Epoch 11772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:48,745 - INFO - [diffusion][Epoch 11773] Epoch 11774/12000
2024-11-05 04:35:51,624 - INFO - [diffusion][Epoch 11773] diffusion training Loss: 0.05573843326419592
2024-11-05 04:35:51,627 - INFO - [diffusion][Epoch 11773] diffusion learning rate: 0.001
2024-11-05 04:35:51,628 - INFO - [diffusion][Epoch 11773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:51,630 - INFO - [diffusion][Epoch 11774] Epoch 11775/12000
2024-11-05 04:35:54,506 - INFO - [diffusion][Epoch 11774] diffusion training Loss: 0.055601755157113075
2024-11-05 04:35:54,509 - INFO - [diffusion][Epoch 11774] diffusion learning rate: 0.001
2024-11-05 04:35:54,510 - INFO - [diffusion][Epoch 11774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:54,512 - INFO - [diffusion][Epoch 11775] Epoch 11776/12000
2024-11-05 04:35:57,386 - INFO - [diffusion][Epoch 11775] diffusion training Loss: 0.06065553333610296
2024-11-05 04:35:57,388 - INFO - [diffusion][Epoch 11775] diffusion learning rate: 0.001
2024-11-05 04:35:57,390 - INFO - [diffusion][Epoch 11775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:57,391 - INFO - [diffusion][Epoch 11776] Epoch 11777/12000
2024-11-05 04:36:00,288 - INFO - [diffusion][Epoch 11776] diffusion training Loss: 0.05692966282367706
2024-11-05 04:36:00,290 - INFO - [diffusion][Epoch 11776] diffusion learning rate: 0.001
2024-11-05 04:36:00,292 - INFO - [diffusion][Epoch 11776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:00,293 - INFO - [diffusion][Epoch 11777] Epoch 11778/12000
2024-11-05 04:36:03,178 - INFO - [diffusion][Epoch 11777] diffusion training Loss: 0.057466703467071056
2024-11-05 04:36:03,180 - INFO - [diffusion][Epoch 11777] diffusion learning rate: 0.001
2024-11-05 04:36:03,182 - INFO - [diffusion][Epoch 11777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:03,183 - INFO - [diffusion][Epoch 11778] Epoch 11779/12000
2024-11-05 04:36:06,085 - INFO - [diffusion][Epoch 11778] diffusion training Loss: 0.06060041859745979
2024-11-05 04:36:06,087 - INFO - [diffusion][Epoch 11778] diffusion learning rate: 0.001
2024-11-05 04:36:06,089 - INFO - [diffusion][Epoch 11778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:06,090 - INFO - [diffusion][Epoch 11779] Epoch 11780/12000
2024-11-05 04:36:09,033 - INFO - [diffusion][Epoch 11779] diffusion training Loss: 0.05639862548559904
2024-11-05 04:36:09,035 - INFO - [diffusion][Epoch 11779] diffusion learning rate: 0.001
2024-11-05 04:36:09,037 - INFO - [diffusion][Epoch 11779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:09,038 - INFO - [diffusion][Epoch 11780] Epoch 11781/12000
2024-11-05 04:36:11,972 - INFO - [diffusion][Epoch 11780] diffusion training Loss: 0.05498198512941599
2024-11-05 04:36:11,974 - INFO - [diffusion][Epoch 11780] diffusion learning rate: 0.001
2024-11-05 04:36:11,975 - INFO - [diffusion][Epoch 11780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:11,977 - INFO - [diffusion][Epoch 11781] Epoch 11782/12000
2024-11-05 04:36:14,885 - INFO - [diffusion][Epoch 11781] diffusion training Loss: 0.060356255620718
2024-11-05 04:36:14,888 - INFO - [diffusion][Epoch 11781] diffusion learning rate: 0.001
2024-11-05 04:36:14,889 - INFO - [diffusion][Epoch 11781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:14,891 - INFO - [diffusion][Epoch 11782] Epoch 11783/12000
2024-11-05 04:36:17,712 - INFO - [diffusion][Epoch 11782] diffusion training Loss: 0.05460038036108017
2024-11-05 04:36:17,714 - INFO - [diffusion][Epoch 11782] diffusion learning rate: 0.001
2024-11-05 04:36:17,716 - INFO - [diffusion][Epoch 11782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:17,717 - INFO - [diffusion][Epoch 11783] Epoch 11784/12000
2024-11-05 04:36:20,436 - INFO - [diffusion][Epoch 11783] diffusion training Loss: 0.06053268164396286
2024-11-05 04:36:20,438 - INFO - [diffusion][Epoch 11783] diffusion learning rate: 0.001
2024-11-05 04:36:20,440 - INFO - [diffusion][Epoch 11783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:20,441 - INFO - [diffusion][Epoch 11784] Epoch 11785/12000
2024-11-05 04:36:23,302 - INFO - [diffusion][Epoch 11784] diffusion training Loss: 0.061113141477108
2024-11-05 04:36:23,305 - INFO - [diffusion][Epoch 11784] diffusion learning rate: 0.001
2024-11-05 04:36:23,308 - INFO - [diffusion][Epoch 11784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:23,309 - INFO - [diffusion][Epoch 11785] Epoch 11786/12000
2024-11-05 04:36:26,215 - INFO - [diffusion][Epoch 11785] diffusion training Loss: 0.054664671421051025
2024-11-05 04:36:26,217 - INFO - [diffusion][Epoch 11785] diffusion learning rate: 0.001
2024-11-05 04:36:26,219 - INFO - [diffusion][Epoch 11785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:26,223 - INFO - [diffusion][Epoch 11786] Epoch 11787/12000
2024-11-05 04:36:29,271 - INFO - [diffusion][Epoch 11786] diffusion training Loss: 0.060766637325286865
2024-11-05 04:36:29,274 - INFO - [diffusion][Epoch 11786] diffusion learning rate: 0.001
2024-11-05 04:36:29,275 - INFO - [diffusion][Epoch 11786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:29,277 - INFO - [diffusion][Epoch 11787] Epoch 11788/12000
2024-11-05 04:36:32,148 - INFO - [diffusion][Epoch 11787] diffusion training Loss: 0.05945664457976818
2024-11-05 04:36:32,150 - INFO - [diffusion][Epoch 11787] diffusion learning rate: 0.001
2024-11-05 04:36:32,152 - INFO - [diffusion][Epoch 11787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:32,153 - INFO - [diffusion][Epoch 11788] Epoch 11789/12000
2024-11-05 04:36:34,959 - INFO - [diffusion][Epoch 11788] diffusion training Loss: 0.05362930800765753
2024-11-05 04:36:34,961 - INFO - [diffusion][Epoch 11788] diffusion learning rate: 0.001
2024-11-05 04:36:34,963 - INFO - [diffusion][Epoch 11788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:34,964 - INFO - [diffusion][Epoch 11789] Epoch 11790/12000
2024-11-05 04:36:37,754 - INFO - [diffusion][Epoch 11789] diffusion training Loss: 0.05841971840709448
2024-11-05 04:36:37,756 - INFO - [diffusion][Epoch 11789] diffusion learning rate: 0.001
2024-11-05 04:36:37,757 - INFO - [diffusion][Epoch 11789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:37,759 - INFO - [diffusion][Epoch 11790] Epoch 11791/12000
2024-11-05 04:36:40,595 - INFO - [diffusion][Epoch 11790] diffusion training Loss: 0.06190161593258381
2024-11-05 04:36:40,597 - INFO - [diffusion][Epoch 11790] diffusion learning rate: 0.001
2024-11-05 04:36:40,599 - INFO - [diffusion][Epoch 11790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:40,600 - INFO - [diffusion][Epoch 11791] Epoch 11792/12000
2024-11-05 04:36:43,507 - INFO - [diffusion][Epoch 11791] diffusion training Loss: 0.057017071172595024
2024-11-05 04:36:43,509 - INFO - [diffusion][Epoch 11791] diffusion learning rate: 0.001
2024-11-05 04:36:43,510 - INFO - [diffusion][Epoch 11791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:43,512 - INFO - [diffusion][Epoch 11792] Epoch 11793/12000
2024-11-05 04:36:46,355 - INFO - [diffusion][Epoch 11792] diffusion training Loss: 0.057260435074567795
2024-11-05 04:36:46,359 - INFO - [diffusion][Epoch 11792] diffusion learning rate: 0.001
2024-11-05 04:36:46,361 - INFO - [diffusion][Epoch 11792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:46,363 - INFO - [diffusion][Epoch 11793] Epoch 11794/12000
2024-11-05 04:36:49,146 - INFO - [diffusion][Epoch 11793] diffusion training Loss: 0.05713838804513216
2024-11-05 04:36:49,148 - INFO - [diffusion][Epoch 11793] diffusion learning rate: 0.001
2024-11-05 04:36:49,150 - INFO - [diffusion][Epoch 11793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:49,151 - INFO - [diffusion][Epoch 11794] Epoch 11795/12000
2024-11-05 04:36:51,848 - INFO - [diffusion][Epoch 11794] diffusion training Loss: 0.05140512902289629
2024-11-05 04:36:51,850 - INFO - [diffusion][Epoch 11794] diffusion learning rate: 0.001
2024-11-05 04:36:51,852 - INFO - [diffusion][Epoch 11794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:51,853 - INFO - [diffusion][Epoch 11795] Epoch 11796/12000
2024-11-05 04:36:54,609 - INFO - [diffusion][Epoch 11795] diffusion training Loss: 0.054105366580188274
2024-11-05 04:36:54,611 - INFO - [diffusion][Epoch 11795] diffusion learning rate: 0.001
2024-11-05 04:36:54,613 - INFO - [diffusion][Epoch 11795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:54,615 - INFO - [diffusion][Epoch 11796] Epoch 11797/12000
2024-11-05 04:36:57,432 - INFO - [diffusion][Epoch 11796] diffusion training Loss: 0.057337647303938866
2024-11-05 04:36:57,434 - INFO - [diffusion][Epoch 11796] diffusion learning rate: 0.001
2024-11-05 04:36:57,436 - INFO - [diffusion][Epoch 11796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:57,437 - INFO - [diffusion][Epoch 11797] Epoch 11798/12000
2024-11-05 04:37:00,232 - INFO - [diffusion][Epoch 11797] diffusion training Loss: 0.06000778451561928
2024-11-05 04:37:00,234 - INFO - [diffusion][Epoch 11797] diffusion learning rate: 0.001
2024-11-05 04:37:00,236 - INFO - [diffusion][Epoch 11797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:00,237 - INFO - [diffusion][Epoch 11798] Epoch 11799/12000
2024-11-05 04:37:02,981 - INFO - [diffusion][Epoch 11798] diffusion training Loss: 0.05443515069782734
2024-11-05 04:37:02,983 - INFO - [diffusion][Epoch 11798] diffusion learning rate: 0.001
2024-11-05 04:37:02,984 - INFO - [diffusion][Epoch 11798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:02,986 - INFO - [diffusion][Epoch 11799] Epoch 11800/12000
2024-11-05 04:37:05,696 - INFO - [diffusion][Epoch 11799] diffusion training Loss: 0.06340950448065996
2024-11-05 04:37:05,698 - INFO - [diffusion][Epoch 11799] diffusion learning rate: 0.001
2024-11-05 04:37:05,700 - INFO - [diffusion][Epoch 11799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:05,701 - INFO - [diffusion][Epoch 11800] Epoch 11801/12000
2024-11-05 04:37:08,400 - INFO - [diffusion][Epoch 11800] diffusion training Loss: 0.056257519870996475
2024-11-05 04:37:08,402 - INFO - [diffusion][Epoch 11800] diffusion learning rate: 0.001
2024-11-05 04:37:08,404 - INFO - [diffusion][Epoch 11800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:08,405 - INFO - [diffusion][Epoch 11801] Epoch 11802/12000
2024-11-05 04:37:11,306 - INFO - [diffusion][Epoch 11801] diffusion training Loss: 0.05348130036145449
2024-11-05 04:37:11,308 - INFO - [diffusion][Epoch 11801] diffusion learning rate: 0.001
2024-11-05 04:37:11,309 - INFO - [diffusion][Epoch 11801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:11,311 - INFO - [diffusion][Epoch 11802] Epoch 11803/12000
2024-11-05 04:37:14,158 - INFO - [diffusion][Epoch 11802] diffusion training Loss: 0.0582524836063385
2024-11-05 04:37:14,161 - INFO - [diffusion][Epoch 11802] diffusion learning rate: 0.001
2024-11-05 04:37:14,163 - INFO - [diffusion][Epoch 11802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:14,165 - INFO - [diffusion][Epoch 11803] Epoch 11804/12000
2024-11-05 04:37:17,000 - INFO - [diffusion][Epoch 11803] diffusion training Loss: 0.05638744588941336
2024-11-05 04:37:17,002 - INFO - [diffusion][Epoch 11803] diffusion learning rate: 0.001
2024-11-05 04:37:17,004 - INFO - [diffusion][Epoch 11803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:17,005 - INFO - [diffusion][Epoch 11804] Epoch 11805/12000
2024-11-05 04:37:19,903 - INFO - [diffusion][Epoch 11804] diffusion training Loss: 0.061645688489079475
2024-11-05 04:37:19,905 - INFO - [diffusion][Epoch 11804] diffusion learning rate: 0.001
2024-11-05 04:37:19,907 - INFO - [diffusion][Epoch 11804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:19,908 - INFO - [diffusion][Epoch 11805] Epoch 11806/12000
2024-11-05 04:37:22,690 - INFO - [diffusion][Epoch 11805] diffusion training Loss: 0.06168091483414173
2024-11-05 04:37:22,692 - INFO - [diffusion][Epoch 11805] diffusion learning rate: 0.001
2024-11-05 04:37:22,694 - INFO - [diffusion][Epoch 11805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:22,695 - INFO - [diffusion][Epoch 11806] Epoch 11807/12000
2024-11-05 04:37:25,468 - INFO - [diffusion][Epoch 11806] diffusion training Loss: 0.05953746289014816
2024-11-05 04:37:25,470 - INFO - [diffusion][Epoch 11806] diffusion learning rate: 0.001
2024-11-05 04:37:25,472 - INFO - [diffusion][Epoch 11806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:25,473 - INFO - [diffusion][Epoch 11807] Epoch 11808/12000
2024-11-05 04:37:28,532 - INFO - [diffusion][Epoch 11807] diffusion training Loss: 0.06109797302633524
2024-11-05 04:37:28,534 - INFO - [diffusion][Epoch 11807] diffusion learning rate: 0.001
2024-11-05 04:37:28,536 - INFO - [diffusion][Epoch 11807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:28,537 - INFO - [diffusion][Epoch 11808] Epoch 11809/12000
2024-11-05 04:37:31,428 - INFO - [diffusion][Epoch 11808] diffusion training Loss: 0.058189962059259415
2024-11-05 04:37:31,431 - INFO - [diffusion][Epoch 11808] diffusion learning rate: 0.001
2024-11-05 04:37:31,433 - INFO - [diffusion][Epoch 11808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:31,434 - INFO - [diffusion][Epoch 11809] Epoch 11810/12000
2024-11-05 04:37:34,211 - INFO - [diffusion][Epoch 11809] diffusion training Loss: 0.05476429685950279
2024-11-05 04:37:34,213 - INFO - [diffusion][Epoch 11809] diffusion learning rate: 0.001
2024-11-05 04:37:34,216 - INFO - [diffusion][Epoch 11809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:34,217 - INFO - [diffusion][Epoch 11810] Epoch 11811/12000
2024-11-05 04:37:36,955 - INFO - [diffusion][Epoch 11810] diffusion training Loss: 0.05819978658109903
2024-11-05 04:37:36,958 - INFO - [diffusion][Epoch 11810] diffusion learning rate: 0.001
2024-11-05 04:37:36,989 - INFO - [diffusion][Epoch 11810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:36,991 - INFO - [diffusion][Epoch 11811] Epoch 11812/12000
2024-11-05 04:37:39,657 - INFO - [diffusion][Epoch 11811] diffusion training Loss: 0.05334499850869179
2024-11-05 04:37:39,660 - INFO - [diffusion][Epoch 11811] diffusion learning rate: 0.001
2024-11-05 04:37:39,662 - INFO - [diffusion][Epoch 11811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:39,664 - INFO - [diffusion][Epoch 11812] Epoch 11813/12000
2024-11-05 04:37:42,349 - INFO - [diffusion][Epoch 11812] diffusion training Loss: 0.05802350863814354
2024-11-05 04:37:42,352 - INFO - [diffusion][Epoch 11812] diffusion learning rate: 0.001
2024-11-05 04:37:42,354 - INFO - [diffusion][Epoch 11812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:42,356 - INFO - [diffusion][Epoch 11813] Epoch 11814/12000
2024-11-05 04:37:45,135 - INFO - [diffusion][Epoch 11813] diffusion training Loss: 0.055739243514835835
2024-11-05 04:37:45,137 - INFO - [diffusion][Epoch 11813] diffusion learning rate: 0.001
2024-11-05 04:37:45,139 - INFO - [diffusion][Epoch 11813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:45,140 - INFO - [diffusion][Epoch 11814] Epoch 11815/12000
2024-11-05 04:37:47,984 - INFO - [diffusion][Epoch 11814] diffusion training Loss: 0.06444498244673014
2024-11-05 04:37:47,987 - INFO - [diffusion][Epoch 11814] diffusion learning rate: 0.001
2024-11-05 04:37:47,989 - INFO - [diffusion][Epoch 11814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:47,990 - INFO - [diffusion][Epoch 11815] Epoch 11816/12000
2024-11-05 04:37:50,703 - INFO - [diffusion][Epoch 11815] diffusion training Loss: 0.0551586402580142
2024-11-05 04:37:50,705 - INFO - [diffusion][Epoch 11815] diffusion learning rate: 0.001
2024-11-05 04:37:50,707 - INFO - [diffusion][Epoch 11815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:50,708 - INFO - [diffusion][Epoch 11816] Epoch 11817/12000
2024-11-05 04:37:53,625 - INFO - [diffusion][Epoch 11816] diffusion training Loss: 0.058085668832063675
2024-11-05 04:37:53,627 - INFO - [diffusion][Epoch 11816] diffusion learning rate: 0.001
2024-11-05 04:37:53,628 - INFO - [diffusion][Epoch 11816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:53,630 - INFO - [diffusion][Epoch 11817] Epoch 11818/12000
2024-11-05 04:37:56,493 - INFO - [diffusion][Epoch 11817] diffusion training Loss: 0.0627226373180747
2024-11-05 04:37:56,494 - INFO - [diffusion][Epoch 11817] diffusion learning rate: 0.001
2024-11-05 04:37:56,496 - INFO - [diffusion][Epoch 11817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:56,497 - INFO - [diffusion][Epoch 11818] Epoch 11819/12000
2024-11-05 04:37:59,231 - INFO - [diffusion][Epoch 11818] diffusion training Loss: 0.05737520381808281
2024-11-05 04:37:59,233 - INFO - [diffusion][Epoch 11818] diffusion learning rate: 0.001
2024-11-05 04:37:59,234 - INFO - [diffusion][Epoch 11818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:59,236 - INFO - [diffusion][Epoch 11819] Epoch 11820/12000
2024-11-05 04:38:02,054 - INFO - [diffusion][Epoch 11819] diffusion training Loss: 0.05864804610610008
2024-11-05 04:38:02,056 - INFO - [diffusion][Epoch 11819] diffusion learning rate: 0.001
2024-11-05 04:38:02,058 - INFO - [diffusion][Epoch 11819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:02,059 - INFO - [diffusion][Epoch 11820] Epoch 11821/12000
2024-11-05 04:38:04,866 - INFO - [diffusion][Epoch 11820] diffusion training Loss: 0.05988345202058554
2024-11-05 04:38:04,868 - INFO - [diffusion][Epoch 11820] diffusion learning rate: 0.001
2024-11-05 04:38:04,870 - INFO - [diffusion][Epoch 11820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:04,872 - INFO - [diffusion][Epoch 11821] Epoch 11822/12000
2024-11-05 04:38:07,780 - INFO - [diffusion][Epoch 11821] diffusion training Loss: 0.05568337719887495
2024-11-05 04:38:07,782 - INFO - [diffusion][Epoch 11821] diffusion learning rate: 0.001
2024-11-05 04:38:07,783 - INFO - [diffusion][Epoch 11821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:07,784 - INFO - [diffusion][Epoch 11822] Epoch 11823/12000
2024-11-05 04:38:10,593 - INFO - [diffusion][Epoch 11822] diffusion training Loss: 0.059902080334722996
2024-11-05 04:38:10,594 - INFO - [diffusion][Epoch 11822] diffusion learning rate: 0.001
2024-11-05 04:38:10,596 - INFO - [diffusion][Epoch 11822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:10,597 - INFO - [diffusion][Epoch 11823] Epoch 11824/12000
2024-11-05 04:38:13,513 - INFO - [diffusion][Epoch 11823] diffusion training Loss: 0.06280457880347967
2024-11-05 04:38:13,515 - INFO - [diffusion][Epoch 11823] diffusion learning rate: 0.001
2024-11-05 04:38:13,516 - INFO - [diffusion][Epoch 11823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:13,518 - INFO - [diffusion][Epoch 11824] Epoch 11825/12000
2024-11-05 04:38:16,439 - INFO - [diffusion][Epoch 11824] diffusion training Loss: 0.06059548910707235
2024-11-05 04:38:16,441 - INFO - [diffusion][Epoch 11824] diffusion learning rate: 0.001
2024-11-05 04:38:16,443 - INFO - [diffusion][Epoch 11824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:16,444 - INFO - [diffusion][Epoch 11825] Epoch 11826/12000
2024-11-05 04:38:19,314 - INFO - [diffusion][Epoch 11825] diffusion training Loss: 0.06012256350368261
2024-11-05 04:38:19,316 - INFO - [diffusion][Epoch 11825] diffusion learning rate: 0.001
2024-11-05 04:38:19,369 - INFO - [diffusion][Epoch 11825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:19,371 - INFO - [diffusion][Epoch 11826] Epoch 11827/12000
2024-11-05 04:38:22,181 - INFO - [diffusion][Epoch 11826] diffusion training Loss: 0.058194478042423725
2024-11-05 04:38:22,183 - INFO - [diffusion][Epoch 11826] diffusion learning rate: 0.001
2024-11-05 04:38:22,185 - INFO - [diffusion][Epoch 11826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:22,186 - INFO - [diffusion][Epoch 11827] Epoch 11828/12000
2024-11-05 04:38:25,689 - INFO - [diffusion][Epoch 11827] diffusion training Loss: 0.05562504939734936
2024-11-05 04:38:25,691 - INFO - [diffusion][Epoch 11827] diffusion learning rate: 0.001
2024-11-05 04:38:25,693 - INFO - [diffusion][Epoch 11827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:25,695 - INFO - [diffusion][Epoch 11828] Epoch 11829/12000
2024-11-05 04:38:28,606 - INFO - [diffusion][Epoch 11828] diffusion training Loss: 0.05434957332909107
2024-11-05 04:38:28,608 - INFO - [diffusion][Epoch 11828] diffusion learning rate: 0.001
2024-11-05 04:38:28,609 - INFO - [diffusion][Epoch 11828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:28,611 - INFO - [diffusion][Epoch 11829] Epoch 11830/12000
2024-11-05 04:38:31,491 - INFO - [diffusion][Epoch 11829] diffusion training Loss: 0.06034427508711815
2024-11-05 04:38:31,494 - INFO - [diffusion][Epoch 11829] diffusion learning rate: 0.001
2024-11-05 04:38:31,496 - INFO - [diffusion][Epoch 11829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:31,498 - INFO - [diffusion][Epoch 11830] Epoch 11831/12000
2024-11-05 04:38:34,380 - INFO - [diffusion][Epoch 11830] diffusion training Loss: 0.05810331366956234
2024-11-05 04:38:34,382 - INFO - [diffusion][Epoch 11830] diffusion learning rate: 0.001
2024-11-05 04:38:34,384 - INFO - [diffusion][Epoch 11830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:34,385 - INFO - [diffusion][Epoch 11831] Epoch 11832/12000
2024-11-05 04:38:37,323 - INFO - [diffusion][Epoch 11831] diffusion training Loss: 0.05887565761804581
2024-11-05 04:38:37,325 - INFO - [diffusion][Epoch 11831] diffusion learning rate: 0.001
2024-11-05 04:38:37,327 - INFO - [diffusion][Epoch 11831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:37,328 - INFO - [diffusion][Epoch 11832] Epoch 11833/12000
2024-11-05 04:38:40,190 - INFO - [diffusion][Epoch 11832] diffusion training Loss: 0.05539205204695463
2024-11-05 04:38:40,192 - INFO - [diffusion][Epoch 11832] diffusion learning rate: 0.001
2024-11-05 04:38:40,194 - INFO - [diffusion][Epoch 11832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:40,195 - INFO - [diffusion][Epoch 11833] Epoch 11834/12000
2024-11-05 04:38:43,091 - INFO - [diffusion][Epoch 11833] diffusion training Loss: 0.05646457429975271
2024-11-05 04:38:43,093 - INFO - [diffusion][Epoch 11833] diffusion learning rate: 0.001
2024-11-05 04:38:43,095 - INFO - [diffusion][Epoch 11833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:43,096 - INFO - [diffusion][Epoch 11834] Epoch 11835/12000
2024-11-05 04:38:45,985 - INFO - [diffusion][Epoch 11834] diffusion training Loss: 0.06279737129807472
2024-11-05 04:38:45,999 - INFO - [diffusion][Epoch 11834] diffusion learning rate: 0.001
2024-11-05 04:38:46,001 - INFO - [diffusion][Epoch 11834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:46,002 - INFO - [diffusion][Epoch 11835] Epoch 11836/12000
2024-11-05 04:38:48,915 - INFO - [diffusion][Epoch 11835] diffusion training Loss: 0.054641108959913254
2024-11-05 04:38:48,918 - INFO - [diffusion][Epoch 11835] diffusion learning rate: 0.001
2024-11-05 04:38:48,920 - INFO - [diffusion][Epoch 11835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:48,921 - INFO - [diffusion][Epoch 11836] Epoch 11837/12000
2024-11-05 04:38:51,832 - INFO - [diffusion][Epoch 11836] diffusion training Loss: 0.05729059502482414
2024-11-05 04:38:51,834 - INFO - [diffusion][Epoch 11836] diffusion learning rate: 0.001
2024-11-05 04:38:51,835 - INFO - [diffusion][Epoch 11836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:51,837 - INFO - [diffusion][Epoch 11837] Epoch 11838/12000
2024-11-05 04:38:54,734 - INFO - [diffusion][Epoch 11837] diffusion training Loss: 0.06037140265107155
2024-11-05 04:38:54,736 - INFO - [diffusion][Epoch 11837] diffusion learning rate: 0.001
2024-11-05 04:38:54,738 - INFO - [diffusion][Epoch 11837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:54,739 - INFO - [diffusion][Epoch 11838] Epoch 11839/12000
2024-11-05 04:38:57,659 - INFO - [diffusion][Epoch 11838] diffusion training Loss: 0.05755160376429558
2024-11-05 04:38:57,661 - INFO - [diffusion][Epoch 11838] diffusion learning rate: 0.001
2024-11-05 04:38:57,663 - INFO - [diffusion][Epoch 11838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:57,664 - INFO - [diffusion][Epoch 11839] Epoch 11840/12000
2024-11-05 04:39:00,562 - INFO - [diffusion][Epoch 11839] diffusion training Loss: 0.06216690223664045
2024-11-05 04:39:00,564 - INFO - [diffusion][Epoch 11839] diffusion learning rate: 0.001
2024-11-05 04:39:00,566 - INFO - [diffusion][Epoch 11839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:00,567 - INFO - [diffusion][Epoch 11840] Epoch 11841/12000
2024-11-05 04:39:03,381 - INFO - [diffusion][Epoch 11840] diffusion training Loss: 0.055542140267789364
2024-11-05 04:39:03,383 - INFO - [diffusion][Epoch 11840] diffusion learning rate: 0.001
2024-11-05 04:39:03,385 - INFO - [diffusion][Epoch 11840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:03,386 - INFO - [diffusion][Epoch 11841] Epoch 11842/12000
2024-11-05 04:39:06,311 - INFO - [diffusion][Epoch 11841] diffusion training Loss: 0.06019137520343065
2024-11-05 04:39:06,313 - INFO - [diffusion][Epoch 11841] diffusion learning rate: 0.001
2024-11-05 04:39:06,314 - INFO - [diffusion][Epoch 11841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:06,316 - INFO - [diffusion][Epoch 11842] Epoch 11843/12000
2024-11-05 04:39:09,159 - INFO - [diffusion][Epoch 11842] diffusion training Loss: 0.05374602694064379
2024-11-05 04:39:09,162 - INFO - [diffusion][Epoch 11842] diffusion learning rate: 0.001
2024-11-05 04:39:09,163 - INFO - [diffusion][Epoch 11842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:09,164 - INFO - [diffusion][Epoch 11843] Epoch 11844/12000
2024-11-05 04:39:12,034 - INFO - [diffusion][Epoch 11843] diffusion training Loss: 0.05196180008351803
2024-11-05 04:39:12,036 - INFO - [diffusion][Epoch 11843] diffusion learning rate: 0.001
2024-11-05 04:39:12,037 - INFO - [diffusion][Epoch 11843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:12,039 - INFO - [diffusion][Epoch 11844] Epoch 11845/12000
2024-11-05 04:39:14,905 - INFO - [diffusion][Epoch 11844] diffusion training Loss: 0.05865455511957407
2024-11-05 04:39:14,907 - INFO - [diffusion][Epoch 11844] diffusion learning rate: 0.001
2024-11-05 04:39:14,908 - INFO - [diffusion][Epoch 11844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:14,910 - INFO - [diffusion][Epoch 11845] Epoch 11846/12000
2024-11-05 04:39:17,912 - INFO - [diffusion][Epoch 11845] diffusion training Loss: 0.06034101638942957
2024-11-05 04:39:17,915 - INFO - [diffusion][Epoch 11845] diffusion learning rate: 0.001
2024-11-05 04:39:17,917 - INFO - [diffusion][Epoch 11845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:17,918 - INFO - [diffusion][Epoch 11846] Epoch 11847/12000
2024-11-05 04:39:20,857 - INFO - [diffusion][Epoch 11846] diffusion training Loss: 0.057767114602029324
2024-11-05 04:39:21,011 - INFO - [diffusion][Epoch 11846] diffusion learning rate: 0.001
2024-11-05 04:39:21,148 - INFO - [diffusion][Epoch 11846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:21,237 - INFO - [diffusion][Epoch 11847] Epoch 11848/12000
2024-11-05 04:39:24,438 - INFO - [diffusion][Epoch 11847] diffusion training Loss: 0.060410626232624054
2024-11-05 04:39:24,440 - INFO - [diffusion][Epoch 11847] diffusion learning rate: 0.001
2024-11-05 04:39:24,442 - INFO - [diffusion][Epoch 11847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:24,443 - INFO - [diffusion][Epoch 11848] Epoch 11849/12000
2024-11-05 04:39:27,323 - INFO - [diffusion][Epoch 11848] diffusion training Loss: 0.05596555396914482
2024-11-05 04:39:27,325 - INFO - [diffusion][Epoch 11848] diffusion learning rate: 0.001
2024-11-05 04:39:27,327 - INFO - [diffusion][Epoch 11848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:27,328 - INFO - [diffusion][Epoch 11849] Epoch 11850/12000
2024-11-05 04:39:30,204 - INFO - [diffusion][Epoch 11849] diffusion training Loss: 0.06300304550677538
2024-11-05 04:39:30,206 - INFO - [diffusion][Epoch 11849] diffusion learning rate: 0.001
2024-11-05 04:39:30,208 - INFO - [diffusion][Epoch 11849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:30,209 - INFO - [diffusion][Epoch 11850] Epoch 11851/12000
2024-11-05 04:39:33,263 - INFO - [diffusion][Epoch 11850] diffusion training Loss: 0.05471106618642807
2024-11-05 04:39:33,265 - INFO - [diffusion][Epoch 11850] diffusion learning rate: 0.001
2024-11-05 04:39:33,267 - INFO - [diffusion][Epoch 11850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:33,269 - INFO - [diffusion][Epoch 11851] Epoch 11852/12000
2024-11-05 04:39:36,181 - INFO - [diffusion][Epoch 11851] diffusion training Loss: 0.059653800912201405
2024-11-05 04:39:36,183 - INFO - [diffusion][Epoch 11851] diffusion learning rate: 0.001
2024-11-05 04:39:36,185 - INFO - [diffusion][Epoch 11851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:36,186 - INFO - [diffusion][Epoch 11852] Epoch 11853/12000
2024-11-05 04:39:39,070 - INFO - [diffusion][Epoch 11852] diffusion training Loss: 0.056721399538218975
2024-11-05 04:39:39,072 - INFO - [diffusion][Epoch 11852] diffusion learning rate: 0.001
2024-11-05 04:39:39,074 - INFO - [diffusion][Epoch 11852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:39,075 - INFO - [diffusion][Epoch 11853] Epoch 11854/12000
2024-11-05 04:39:41,943 - INFO - [diffusion][Epoch 11853] diffusion training Loss: 0.05632408428937197
2024-11-05 04:39:41,944 - INFO - [diffusion][Epoch 11853] diffusion learning rate: 0.001
2024-11-05 04:39:41,946 - INFO - [diffusion][Epoch 11853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:41,947 - INFO - [diffusion][Epoch 11854] Epoch 11855/12000
2024-11-05 04:39:44,849 - INFO - [diffusion][Epoch 11854] diffusion training Loss: 0.059089500457048416
2024-11-05 04:39:44,852 - INFO - [diffusion][Epoch 11854] diffusion learning rate: 0.001
2024-11-05 04:39:44,853 - INFO - [diffusion][Epoch 11854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:44,855 - INFO - [diffusion][Epoch 11855] Epoch 11856/12000
2024-11-05 04:39:47,758 - INFO - [diffusion][Epoch 11855] diffusion training Loss: 0.058732641860842705
2024-11-05 04:39:47,760 - INFO - [diffusion][Epoch 11855] diffusion learning rate: 0.001
2024-11-05 04:39:47,762 - INFO - [diffusion][Epoch 11855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:47,763 - INFO - [diffusion][Epoch 11856] Epoch 11857/12000
2024-11-05 04:39:50,677 - INFO - [diffusion][Epoch 11856] diffusion training Loss: 0.06084632966667414
2024-11-05 04:39:50,680 - INFO - [diffusion][Epoch 11856] diffusion learning rate: 0.001
2024-11-05 04:39:50,682 - INFO - [diffusion][Epoch 11856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:50,683 - INFO - [diffusion][Epoch 11857] Epoch 11858/12000
2024-11-05 04:39:53,528 - INFO - [diffusion][Epoch 11857] diffusion training Loss: 0.06000274792313576
2024-11-05 04:39:53,531 - INFO - [diffusion][Epoch 11857] diffusion learning rate: 0.001
2024-11-05 04:39:53,532 - INFO - [diffusion][Epoch 11857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:53,533 - INFO - [diffusion][Epoch 11858] Epoch 11859/12000
2024-11-05 04:39:56,414 - INFO - [diffusion][Epoch 11858] diffusion training Loss: 0.05578981712460518
2024-11-05 04:39:56,415 - INFO - [diffusion][Epoch 11858] diffusion learning rate: 0.001
2024-11-05 04:39:56,417 - INFO - [diffusion][Epoch 11858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:56,418 - INFO - [diffusion][Epoch 11859] Epoch 11860/12000
2024-11-05 04:39:59,349 - INFO - [diffusion][Epoch 11859] diffusion training Loss: 0.056082542054355145
2024-11-05 04:39:59,351 - INFO - [diffusion][Epoch 11859] diffusion learning rate: 0.001
2024-11-05 04:39:59,353 - INFO - [diffusion][Epoch 11859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:59,354 - INFO - [diffusion][Epoch 11860] Epoch 11861/12000
2024-11-05 04:40:02,265 - INFO - [diffusion][Epoch 11860] diffusion training Loss: 0.05921149253845215
2024-11-05 04:40:02,267 - INFO - [diffusion][Epoch 11860] diffusion learning rate: 0.001
2024-11-05 04:40:02,269 - INFO - [diffusion][Epoch 11860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:02,270 - INFO - [diffusion][Epoch 11861] Epoch 11862/12000
2024-11-05 04:40:05,091 - INFO - [diffusion][Epoch 11861] diffusion training Loss: 0.058108980767428875
2024-11-05 04:40:05,093 - INFO - [diffusion][Epoch 11861] diffusion learning rate: 0.001
2024-11-05 04:40:05,094 - INFO - [diffusion][Epoch 11861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:05,096 - INFO - [diffusion][Epoch 11862] Epoch 11863/12000
2024-11-05 04:40:07,976 - INFO - [diffusion][Epoch 11862] diffusion training Loss: 0.054127830080688
2024-11-05 04:40:07,978 - INFO - [diffusion][Epoch 11862] diffusion learning rate: 0.001
2024-11-05 04:40:07,980 - INFO - [diffusion][Epoch 11862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:07,981 - INFO - [diffusion][Epoch 11863] Epoch 11864/12000
2024-11-05 04:40:10,908 - INFO - [diffusion][Epoch 11863] diffusion training Loss: 0.0588209368288517
2024-11-05 04:40:10,910 - INFO - [diffusion][Epoch 11863] diffusion learning rate: 0.001
2024-11-05 04:40:10,912 - INFO - [diffusion][Epoch 11863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:10,913 - INFO - [diffusion][Epoch 11864] Epoch 11865/12000
2024-11-05 04:40:13,788 - INFO - [diffusion][Epoch 11864] diffusion training Loss: 0.055305564776062965
2024-11-05 04:40:13,790 - INFO - [diffusion][Epoch 11864] diffusion learning rate: 0.001
2024-11-05 04:40:13,792 - INFO - [diffusion][Epoch 11864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:13,793 - INFO - [diffusion][Epoch 11865] Epoch 11866/12000
2024-11-05 04:40:16,676 - INFO - [diffusion][Epoch 11865] diffusion training Loss: 0.058904385194182396
2024-11-05 04:40:16,678 - INFO - [diffusion][Epoch 11865] diffusion learning rate: 0.001
2024-11-05 04:40:16,680 - INFO - [diffusion][Epoch 11865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:16,681 - INFO - [diffusion][Epoch 11866] Epoch 11867/12000
2024-11-05 04:40:19,594 - INFO - [diffusion][Epoch 11866] diffusion training Loss: 0.06027981545776129
2024-11-05 04:40:19,596 - INFO - [diffusion][Epoch 11866] diffusion learning rate: 0.001
2024-11-05 04:40:19,598 - INFO - [diffusion][Epoch 11866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:19,599 - INFO - [diffusion][Epoch 11867] Epoch 11868/12000
2024-11-05 04:40:22,430 - INFO - [diffusion][Epoch 11867] diffusion training Loss: 0.060430197045207024
2024-11-05 04:40:22,431 - INFO - [diffusion][Epoch 11867] diffusion learning rate: 0.001
2024-11-05 04:40:22,433 - INFO - [diffusion][Epoch 11867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:22,434 - INFO - [diffusion][Epoch 11868] Epoch 11869/12000
2024-11-05 04:40:25,825 - INFO - [diffusion][Epoch 11868] diffusion training Loss: 0.05950529221445322
2024-11-05 04:40:25,827 - INFO - [diffusion][Epoch 11868] diffusion learning rate: 0.001
2024-11-05 04:40:25,829 - INFO - [diffusion][Epoch 11868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:25,830 - INFO - [diffusion][Epoch 11869] Epoch 11870/12000
2024-11-05 04:40:28,702 - INFO - [diffusion][Epoch 11869] diffusion training Loss: 0.05778220761567354
2024-11-05 04:40:28,704 - INFO - [diffusion][Epoch 11869] diffusion learning rate: 0.001
2024-11-05 04:40:28,706 - INFO - [diffusion][Epoch 11869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:28,707 - INFO - [diffusion][Epoch 11870] Epoch 11871/12000
2024-11-05 04:40:31,635 - INFO - [diffusion][Epoch 11870] diffusion training Loss: 0.0570296011865139
2024-11-05 04:40:31,637 - INFO - [diffusion][Epoch 11870] diffusion learning rate: 0.001
2024-11-05 04:40:31,639 - INFO - [diffusion][Epoch 11870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:31,641 - INFO - [diffusion][Epoch 11871] Epoch 11872/12000
2024-11-05 04:40:34,507 - INFO - [diffusion][Epoch 11871] diffusion training Loss: 0.05592117831110954
2024-11-05 04:40:34,509 - INFO - [diffusion][Epoch 11871] diffusion learning rate: 0.001
2024-11-05 04:40:34,536 - INFO - [diffusion][Epoch 11871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:34,537 - INFO - [diffusion][Epoch 11872] Epoch 11873/12000
2024-11-05 04:40:37,414 - INFO - [diffusion][Epoch 11872] diffusion training Loss: 0.058549391105771065
2024-11-05 04:40:37,416 - INFO - [diffusion][Epoch 11872] diffusion learning rate: 0.001
2024-11-05 04:40:37,418 - INFO - [diffusion][Epoch 11872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:37,419 - INFO - [diffusion][Epoch 11873] Epoch 11874/12000
2024-11-05 04:40:40,278 - INFO - [diffusion][Epoch 11873] diffusion training Loss: 0.05764880497008562
2024-11-05 04:40:40,280 - INFO - [diffusion][Epoch 11873] diffusion learning rate: 0.001
2024-11-05 04:40:40,282 - INFO - [diffusion][Epoch 11873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:40,283 - INFO - [diffusion][Epoch 11874] Epoch 11875/12000
2024-11-05 04:40:43,182 - INFO - [diffusion][Epoch 11874] diffusion training Loss: 0.057959177531301975
2024-11-05 04:40:43,184 - INFO - [diffusion][Epoch 11874] diffusion learning rate: 0.001
2024-11-05 04:40:43,186 - INFO - [diffusion][Epoch 11874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:43,187 - INFO - [diffusion][Epoch 11875] Epoch 11876/12000
2024-11-05 04:40:46,059 - INFO - [diffusion][Epoch 11875] diffusion training Loss: 0.05636487156152725
2024-11-05 04:40:46,060 - INFO - [diffusion][Epoch 11875] diffusion learning rate: 0.001
2024-11-05 04:40:46,062 - INFO - [diffusion][Epoch 11875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:46,064 - INFO - [diffusion][Epoch 11876] Epoch 11877/12000
2024-11-05 04:40:48,955 - INFO - [diffusion][Epoch 11876] diffusion training Loss: 0.05669233202934265
2024-11-05 04:40:48,957 - INFO - [diffusion][Epoch 11876] diffusion learning rate: 0.001
2024-11-05 04:40:48,959 - INFO - [diffusion][Epoch 11876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:48,960 - INFO - [diffusion][Epoch 11877] Epoch 11878/12000
2024-11-05 04:40:51,809 - INFO - [diffusion][Epoch 11877] diffusion training Loss: 0.05381241627037525
2024-11-05 04:40:51,812 - INFO - [diffusion][Epoch 11877] diffusion learning rate: 0.001
2024-11-05 04:40:51,813 - INFO - [diffusion][Epoch 11877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:51,815 - INFO - [diffusion][Epoch 11878] Epoch 11879/12000
2024-11-05 04:40:54,653 - INFO - [diffusion][Epoch 11878] diffusion training Loss: 0.05543048493564129
2024-11-05 04:40:54,655 - INFO - [diffusion][Epoch 11878] diffusion learning rate: 0.001
2024-11-05 04:40:54,657 - INFO - [diffusion][Epoch 11878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:54,658 - INFO - [diffusion][Epoch 11879] Epoch 11880/12000
2024-11-05 04:40:57,507 - INFO - [diffusion][Epoch 11879] diffusion training Loss: 0.05773373693227768
2024-11-05 04:40:57,509 - INFO - [diffusion][Epoch 11879] diffusion learning rate: 0.001
2024-11-05 04:40:57,538 - INFO - [diffusion][Epoch 11879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:57,540 - INFO - [diffusion][Epoch 11880] Epoch 11881/12000
2024-11-05 04:41:00,410 - INFO - [diffusion][Epoch 11880] diffusion training Loss: 0.05800064746290445
2024-11-05 04:41:00,412 - INFO - [diffusion][Epoch 11880] diffusion learning rate: 0.001
2024-11-05 04:41:00,413 - INFO - [diffusion][Epoch 11880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:00,415 - INFO - [diffusion][Epoch 11881] Epoch 11882/12000
2024-11-05 04:41:03,292 - INFO - [diffusion][Epoch 11881] diffusion training Loss: 0.05435585882514715
2024-11-05 04:41:03,294 - INFO - [diffusion][Epoch 11881] diffusion learning rate: 0.001
2024-11-05 04:41:03,296 - INFO - [diffusion][Epoch 11881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:03,297 - INFO - [diffusion][Epoch 11882] Epoch 11883/12000
2024-11-05 04:41:06,194 - INFO - [diffusion][Epoch 11882] diffusion training Loss: 0.05738107394427061
2024-11-05 04:41:06,196 - INFO - [diffusion][Epoch 11882] diffusion learning rate: 0.001
2024-11-05 04:41:06,197 - INFO - [diffusion][Epoch 11882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:06,199 - INFO - [diffusion][Epoch 11883] Epoch 11884/12000
2024-11-05 04:41:09,077 - INFO - [diffusion][Epoch 11883] diffusion training Loss: 0.05317882355302572
2024-11-05 04:41:09,079 - INFO - [diffusion][Epoch 11883] diffusion learning rate: 0.001
2024-11-05 04:41:09,081 - INFO - [diffusion][Epoch 11883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:09,082 - INFO - [diffusion][Epoch 11884] Epoch 11885/12000
2024-11-05 04:41:11,970 - INFO - [diffusion][Epoch 11884] diffusion training Loss: 0.0573582099750638
2024-11-05 04:41:11,972 - INFO - [diffusion][Epoch 11884] diffusion learning rate: 0.001
2024-11-05 04:41:11,974 - INFO - [diffusion][Epoch 11884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:11,975 - INFO - [diffusion][Epoch 11885] Epoch 11886/12000
2024-11-05 04:41:14,836 - INFO - [diffusion][Epoch 11885] diffusion training Loss: 0.05285712890326977
2024-11-05 04:41:14,838 - INFO - [diffusion][Epoch 11885] diffusion learning rate: 0.001
2024-11-05 04:41:14,840 - INFO - [diffusion][Epoch 11885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:14,842 - INFO - [diffusion][Epoch 11886] Epoch 11887/12000
2024-11-05 04:41:17,763 - INFO - [diffusion][Epoch 11886] diffusion training Loss: 0.05311430897563696
2024-11-05 04:41:17,765 - INFO - [diffusion][Epoch 11886] diffusion learning rate: 0.001
2024-11-05 04:41:17,767 - INFO - [diffusion][Epoch 11886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:17,769 - INFO - [diffusion][Epoch 11887] Epoch 11888/12000
2024-11-05 04:41:20,644 - INFO - [diffusion][Epoch 11887] diffusion training Loss: 0.0577776450663805
2024-11-05 04:41:20,646 - INFO - [diffusion][Epoch 11887] diffusion learning rate: 0.001
2024-11-05 04:41:20,648 - INFO - [diffusion][Epoch 11887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:20,649 - INFO - [diffusion][Epoch 11888] Epoch 11889/12000
2024-11-05 04:41:23,749 - INFO - [diffusion][Epoch 11888] diffusion training Loss: 0.0596756162121892
2024-11-05 04:41:23,751 - INFO - [diffusion][Epoch 11888] diffusion learning rate: 0.001
2024-11-05 04:41:23,752 - INFO - [diffusion][Epoch 11888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:23,754 - INFO - [diffusion][Epoch 11889] Epoch 11890/12000
2024-11-05 04:41:26,616 - INFO - [diffusion][Epoch 11889] diffusion training Loss: 0.05789609905332327
2024-11-05 04:41:26,617 - INFO - [diffusion][Epoch 11889] diffusion learning rate: 0.001
2024-11-05 04:41:26,619 - INFO - [diffusion][Epoch 11889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:26,621 - INFO - [diffusion][Epoch 11890] Epoch 11891/12000
2024-11-05 04:41:29,476 - INFO - [diffusion][Epoch 11890] diffusion training Loss: 0.05398073326796293
2024-11-05 04:41:29,478 - INFO - [diffusion][Epoch 11890] diffusion learning rate: 0.001
2024-11-05 04:41:29,479 - INFO - [diffusion][Epoch 11890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:29,481 - INFO - [diffusion][Epoch 11891] Epoch 11892/12000
2024-11-05 04:41:32,241 - INFO - [diffusion][Epoch 11891] diffusion training Loss: 0.057683007791638374
2024-11-05 04:41:32,243 - INFO - [diffusion][Epoch 11891] diffusion learning rate: 0.001
2024-11-05 04:41:32,244 - INFO - [diffusion][Epoch 11891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:32,246 - INFO - [diffusion][Epoch 11892] Epoch 11893/12000
2024-11-05 04:41:35,122 - INFO - [diffusion][Epoch 11892] diffusion training Loss: 0.05776144564151764
2024-11-05 04:41:35,124 - INFO - [diffusion][Epoch 11892] diffusion learning rate: 0.001
2024-11-05 04:41:35,126 - INFO - [diffusion][Epoch 11892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:35,128 - INFO - [diffusion][Epoch 11893] Epoch 11894/12000
2024-11-05 04:41:38,003 - INFO - [diffusion][Epoch 11893] diffusion training Loss: 0.05511628556996584
2024-11-05 04:41:38,005 - INFO - [diffusion][Epoch 11893] diffusion learning rate: 0.001
2024-11-05 04:41:38,006 - INFO - [diffusion][Epoch 11893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:38,008 - INFO - [diffusion][Epoch 11894] Epoch 11895/12000
2024-11-05 04:41:40,809 - INFO - [diffusion][Epoch 11894] diffusion training Loss: 0.05931020621210337
2024-11-05 04:41:40,811 - INFO - [diffusion][Epoch 11894] diffusion learning rate: 0.001
2024-11-05 04:41:40,812 - INFO - [diffusion][Epoch 11894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:40,814 - INFO - [diffusion][Epoch 11895] Epoch 11896/12000
2024-11-05 04:41:43,776 - INFO - [diffusion][Epoch 11895] diffusion training Loss: 0.059856473468244076
2024-11-05 04:41:43,778 - INFO - [diffusion][Epoch 11895] diffusion learning rate: 0.001
2024-11-05 04:41:43,780 - INFO - [diffusion][Epoch 11895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:43,781 - INFO - [diffusion][Epoch 11896] Epoch 11897/12000
2024-11-05 04:41:46,721 - INFO - [diffusion][Epoch 11896] diffusion training Loss: 0.052662323229014874
2024-11-05 04:41:46,723 - INFO - [diffusion][Epoch 11896] diffusion learning rate: 0.001
2024-11-05 04:41:46,724 - INFO - [diffusion][Epoch 11896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:46,726 - INFO - [diffusion][Epoch 11897] Epoch 11898/12000
2024-11-05 04:41:49,571 - INFO - [diffusion][Epoch 11897] diffusion training Loss: 0.05730330944061279
2024-11-05 04:41:49,573 - INFO - [diffusion][Epoch 11897] diffusion learning rate: 0.001
2024-11-05 04:41:49,575 - INFO - [diffusion][Epoch 11897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:49,577 - INFO - [diffusion][Epoch 11898] Epoch 11899/12000
2024-11-05 04:41:52,467 - INFO - [diffusion][Epoch 11898] diffusion training Loss: 0.054544473066926
2024-11-05 04:41:52,470 - INFO - [diffusion][Epoch 11898] diffusion learning rate: 0.001
2024-11-05 04:41:52,472 - INFO - [diffusion][Epoch 11898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:52,473 - INFO - [diffusion][Epoch 11899] Epoch 11900/12000
2024-11-05 04:41:55,338 - INFO - [diffusion][Epoch 11899] diffusion training Loss: 0.05652867071330547
2024-11-05 04:41:55,340 - INFO - [diffusion][Epoch 11899] diffusion learning rate: 0.001
2024-11-05 04:41:55,342 - INFO - [diffusion][Epoch 11899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:55,344 - INFO - [diffusion][Epoch 11900] Epoch 11901/12000
2024-11-05 04:41:58,198 - INFO - [diffusion][Epoch 11900] diffusion training Loss: 0.051627740263938904
2024-11-05 04:41:58,200 - INFO - [diffusion][Epoch 11900] diffusion learning rate: 0.001
2024-11-05 04:41:58,220 - INFO - [diffusion][Epoch 11900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:58,222 - INFO - [diffusion][Epoch 11901] Epoch 11902/12000
2024-11-05 04:42:01,089 - INFO - [diffusion][Epoch 11901] diffusion training Loss: 0.05775707680732012
2024-11-05 04:42:01,091 - INFO - [diffusion][Epoch 11901] diffusion learning rate: 0.001
2024-11-05 04:42:01,093 - INFO - [diffusion][Epoch 11901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:01,094 - INFO - [diffusion][Epoch 11902] Epoch 11903/12000
2024-11-05 04:42:04,005 - INFO - [diffusion][Epoch 11902] diffusion training Loss: 0.05468233581632376
2024-11-05 04:42:04,007 - INFO - [diffusion][Epoch 11902] diffusion learning rate: 0.001
2024-11-05 04:42:04,009 - INFO - [diffusion][Epoch 11902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:04,010 - INFO - [diffusion][Epoch 11903] Epoch 11904/12000
2024-11-05 04:42:06,906 - INFO - [diffusion][Epoch 11903] diffusion training Loss: 0.05600410979241133
2024-11-05 04:42:06,908 - INFO - [diffusion][Epoch 11903] diffusion learning rate: 0.001
2024-11-05 04:42:06,910 - INFO - [diffusion][Epoch 11903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:06,912 - INFO - [diffusion][Epoch 11904] Epoch 11905/12000
2024-11-05 04:42:09,865 - INFO - [diffusion][Epoch 11904] diffusion training Loss: 0.05700798146426678
2024-11-05 04:42:09,867 - INFO - [diffusion][Epoch 11904] diffusion learning rate: 0.001
2024-11-05 04:42:09,869 - INFO - [diffusion][Epoch 11904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:09,871 - INFO - [diffusion][Epoch 11905] Epoch 11906/12000
2024-11-05 04:42:12,761 - INFO - [diffusion][Epoch 11905] diffusion training Loss: 0.057683068327605724
2024-11-05 04:42:12,764 - INFO - [diffusion][Epoch 11905] diffusion learning rate: 0.001
2024-11-05 04:42:12,766 - INFO - [diffusion][Epoch 11905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:12,767 - INFO - [diffusion][Epoch 11906] Epoch 11907/12000
2024-11-05 04:42:15,638 - INFO - [diffusion][Epoch 11906] diffusion training Loss: 0.057730477303266525
2024-11-05 04:42:15,640 - INFO - [diffusion][Epoch 11906] diffusion learning rate: 0.001
2024-11-05 04:42:15,642 - INFO - [diffusion][Epoch 11906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:15,643 - INFO - [diffusion][Epoch 11907] Epoch 11908/12000
2024-11-05 04:42:18,589 - INFO - [diffusion][Epoch 11907] diffusion training Loss: 0.06044054124504328
2024-11-05 04:42:18,591 - INFO - [diffusion][Epoch 11907] diffusion learning rate: 0.001
2024-11-05 04:42:18,593 - INFO - [diffusion][Epoch 11907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:18,595 - INFO - [diffusion][Epoch 11908] Epoch 11909/12000
2024-11-05 04:42:21,462 - INFO - [diffusion][Epoch 11908] diffusion training Loss: 0.059336903505027294
2024-11-05 04:42:21,464 - INFO - [diffusion][Epoch 11908] diffusion learning rate: 0.001
2024-11-05 04:42:21,466 - INFO - [diffusion][Epoch 11908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:21,467 - INFO - [diffusion][Epoch 11909] Epoch 11910/12000
2024-11-05 04:42:24,646 - INFO - [diffusion][Epoch 11909] diffusion training Loss: 0.053447083570063114
2024-11-05 04:42:24,648 - INFO - [diffusion][Epoch 11909] diffusion learning rate: 0.001
2024-11-05 04:42:24,652 - INFO - [diffusion][Epoch 11909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:24,654 - INFO - [diffusion][Epoch 11910] Epoch 11911/12000
2024-11-05 04:42:27,562 - INFO - [diffusion][Epoch 11910] diffusion training Loss: 0.05792515445500612
2024-11-05 04:42:27,565 - INFO - [diffusion][Epoch 11910] diffusion learning rate: 0.001
2024-11-05 04:42:27,567 - INFO - [diffusion][Epoch 11910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:27,568 - INFO - [diffusion][Epoch 11911] Epoch 11912/12000
2024-11-05 04:42:30,494 - INFO - [diffusion][Epoch 11911] diffusion training Loss: 0.05808612052351236
2024-11-05 04:42:30,496 - INFO - [diffusion][Epoch 11911] diffusion learning rate: 0.001
2024-11-05 04:42:30,498 - INFO - [diffusion][Epoch 11911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:30,499 - INFO - [diffusion][Epoch 11912] Epoch 11913/12000
2024-11-05 04:42:33,412 - INFO - [diffusion][Epoch 11912] diffusion training Loss: 0.055949811823666096
2024-11-05 04:42:33,413 - INFO - [diffusion][Epoch 11912] diffusion learning rate: 0.001
2024-11-05 04:42:33,415 - INFO - [diffusion][Epoch 11912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:33,416 - INFO - [diffusion][Epoch 11913] Epoch 11914/12000
2024-11-05 04:42:36,277 - INFO - [diffusion][Epoch 11913] diffusion training Loss: 0.05620064586400986
2024-11-05 04:42:36,279 - INFO - [diffusion][Epoch 11913] diffusion learning rate: 0.001
2024-11-05 04:42:36,280 - INFO - [diffusion][Epoch 11913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:36,282 - INFO - [diffusion][Epoch 11914] Epoch 11915/12000
2024-11-05 04:42:39,170 - INFO - [diffusion][Epoch 11914] diffusion training Loss: 0.056272316724061966
2024-11-05 04:42:39,172 - INFO - [diffusion][Epoch 11914] diffusion learning rate: 0.001
2024-11-05 04:42:39,173 - INFO - [diffusion][Epoch 11914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:39,175 - INFO - [diffusion][Epoch 11915] Epoch 11916/12000
2024-11-05 04:42:42,089 - INFO - [diffusion][Epoch 11915] diffusion training Loss: 0.05906348396092653
2024-11-05 04:42:42,091 - INFO - [diffusion][Epoch 11915] diffusion learning rate: 0.001
2024-11-05 04:42:42,093 - INFO - [diffusion][Epoch 11915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:42,094 - INFO - [diffusion][Epoch 11916] Epoch 11917/12000
2024-11-05 04:42:44,988 - INFO - [diffusion][Epoch 11916] diffusion training Loss: 0.057425133883953094
2024-11-05 04:42:44,990 - INFO - [diffusion][Epoch 11916] diffusion learning rate: 0.001
2024-11-05 04:42:44,991 - INFO - [diffusion][Epoch 11916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:44,993 - INFO - [diffusion][Epoch 11917] Epoch 11918/12000
2024-11-05 04:42:47,821 - INFO - [diffusion][Epoch 11917] diffusion training Loss: 0.059712900780141354
2024-11-05 04:42:47,823 - INFO - [diffusion][Epoch 11917] diffusion learning rate: 0.001
2024-11-05 04:42:47,825 - INFO - [diffusion][Epoch 11917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:47,826 - INFO - [diffusion][Epoch 11918] Epoch 11919/12000
2024-11-05 04:42:50,728 - INFO - [diffusion][Epoch 11918] diffusion training Loss: 0.056672148406505585
2024-11-05 04:42:50,730 - INFO - [diffusion][Epoch 11918] diffusion learning rate: 0.001
2024-11-05 04:42:50,731 - INFO - [diffusion][Epoch 11918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:50,733 - INFO - [diffusion][Epoch 11919] Epoch 11920/12000
2024-11-05 04:42:53,696 - INFO - [diffusion][Epoch 11919] diffusion training Loss: 0.05517670884728432
2024-11-05 04:42:53,699 - INFO - [diffusion][Epoch 11919] diffusion learning rate: 0.001
2024-11-05 04:42:53,701 - INFO - [diffusion][Epoch 11919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:53,702 - INFO - [diffusion][Epoch 11920] Epoch 11921/12000
2024-11-05 04:42:56,576 - INFO - [diffusion][Epoch 11920] diffusion training Loss: 0.05687976721674204
2024-11-05 04:42:56,578 - INFO - [diffusion][Epoch 11920] diffusion learning rate: 0.001
2024-11-05 04:42:56,580 - INFO - [diffusion][Epoch 11920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:56,581 - INFO - [diffusion][Epoch 11921] Epoch 11922/12000
2024-11-05 04:42:59,509 - INFO - [diffusion][Epoch 11921] diffusion training Loss: 0.061250499449670315
2024-11-05 04:42:59,511 - INFO - [diffusion][Epoch 11921] diffusion learning rate: 0.001
2024-11-05 04:42:59,513 - INFO - [diffusion][Epoch 11921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:59,514 - INFO - [diffusion][Epoch 11922] Epoch 11923/12000
2024-11-05 04:43:02,449 - INFO - [diffusion][Epoch 11922] diffusion training Loss: 0.05436588544398546
2024-11-05 04:43:02,451 - INFO - [diffusion][Epoch 11922] diffusion learning rate: 0.001
2024-11-05 04:43:02,453 - INFO - [diffusion][Epoch 11922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:02,454 - INFO - [diffusion][Epoch 11923] Epoch 11924/12000
2024-11-05 04:43:05,328 - INFO - [diffusion][Epoch 11923] diffusion training Loss: 0.05558212846517563
2024-11-05 04:43:05,330 - INFO - [diffusion][Epoch 11923] diffusion learning rate: 0.001
2024-11-05 04:43:05,332 - INFO - [diffusion][Epoch 11923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:05,333 - INFO - [diffusion][Epoch 11924] Epoch 11925/12000
2024-11-05 04:43:08,259 - INFO - [diffusion][Epoch 11924] diffusion training Loss: 0.06031188275665045
2024-11-05 04:43:08,261 - INFO - [diffusion][Epoch 11924] diffusion learning rate: 0.001
2024-11-05 04:43:08,263 - INFO - [diffusion][Epoch 11924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:08,264 - INFO - [diffusion][Epoch 11925] Epoch 11926/12000
2024-11-05 04:43:11,152 - INFO - [diffusion][Epoch 11925] diffusion training Loss: 0.06096716411411762
2024-11-05 04:43:11,154 - INFO - [diffusion][Epoch 11925] diffusion learning rate: 0.001
2024-11-05 04:43:11,156 - INFO - [diffusion][Epoch 11925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:11,158 - INFO - [diffusion][Epoch 11926] Epoch 11927/12000
2024-11-05 04:43:14,101 - INFO - [diffusion][Epoch 11926] diffusion training Loss: 0.06028507463634014
2024-11-05 04:43:14,103 - INFO - [diffusion][Epoch 11926] diffusion learning rate: 0.001
2024-11-05 04:43:14,105 - INFO - [diffusion][Epoch 11926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:14,106 - INFO - [diffusion][Epoch 11927] Epoch 11928/12000
2024-11-05 04:43:16,989 - INFO - [diffusion][Epoch 11927] diffusion training Loss: 0.06538207456469536
2024-11-05 04:43:16,991 - INFO - [diffusion][Epoch 11927] diffusion learning rate: 0.001
2024-11-05 04:43:16,993 - INFO - [diffusion][Epoch 11927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:16,994 - INFO - [diffusion][Epoch 11928] Epoch 11929/12000
2024-11-05 04:43:19,856 - INFO - [diffusion][Epoch 11928] diffusion training Loss: 0.062010833993554115
2024-11-05 04:43:19,858 - INFO - [diffusion][Epoch 11928] diffusion learning rate: 0.001
2024-11-05 04:43:19,859 - INFO - [diffusion][Epoch 11928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:19,861 - INFO - [diffusion][Epoch 11929] Epoch 11930/12000
2024-11-05 04:43:22,743 - INFO - [diffusion][Epoch 11929] diffusion training Loss: 0.054637775756418705
2024-11-05 04:43:22,745 - INFO - [diffusion][Epoch 11929] diffusion learning rate: 0.001
2024-11-05 04:43:22,747 - INFO - [diffusion][Epoch 11929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:22,748 - INFO - [diffusion][Epoch 11930] Epoch 11931/12000
2024-11-05 04:43:25,926 - INFO - [diffusion][Epoch 11930] diffusion training Loss: 0.05868288595229387
2024-11-05 04:43:25,928 - INFO - [diffusion][Epoch 11930] diffusion learning rate: 0.001
2024-11-05 04:43:25,930 - INFO - [diffusion][Epoch 11930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:25,931 - INFO - [diffusion][Epoch 11931] Epoch 11932/12000
2024-11-05 04:43:28,641 - INFO - [diffusion][Epoch 11931] diffusion training Loss: 0.05927966348826885
2024-11-05 04:43:28,643 - INFO - [diffusion][Epoch 11931] diffusion learning rate: 0.001
2024-11-05 04:43:28,645 - INFO - [diffusion][Epoch 11931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:28,646 - INFO - [diffusion][Epoch 11932] Epoch 11933/12000
2024-11-05 04:43:31,543 - INFO - [diffusion][Epoch 11932] diffusion training Loss: 0.05799803789705038
2024-11-05 04:43:31,545 - INFO - [diffusion][Epoch 11932] diffusion learning rate: 0.001
2024-11-05 04:43:31,547 - INFO - [diffusion][Epoch 11932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:31,548 - INFO - [diffusion][Epoch 11933] Epoch 11934/12000
2024-11-05 04:43:34,249 - INFO - [diffusion][Epoch 11933] diffusion training Loss: 0.05068813730031252
2024-11-05 04:43:34,251 - INFO - [diffusion][Epoch 11933] diffusion learning rate: 0.001
2024-11-05 04:43:34,253 - INFO - [diffusion][Epoch 11933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:34,254 - INFO - [diffusion][Epoch 11934] Epoch 11935/12000
2024-11-05 04:43:37,141 - INFO - [diffusion][Epoch 11934] diffusion training Loss: 0.05726516246795654
2024-11-05 04:43:37,143 - INFO - [diffusion][Epoch 11934] diffusion learning rate: 0.001
2024-11-05 04:43:37,145 - INFO - [diffusion][Epoch 11934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:37,147 - INFO - [diffusion][Epoch 11935] Epoch 11936/12000
2024-11-05 04:43:39,920 - INFO - [diffusion][Epoch 11935] diffusion training Loss: 0.0565427765250206
2024-11-05 04:43:39,922 - INFO - [diffusion][Epoch 11935] diffusion learning rate: 0.001
2024-11-05 04:43:39,924 - INFO - [diffusion][Epoch 11935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:39,925 - INFO - [diffusion][Epoch 11936] Epoch 11937/12000
2024-11-05 04:43:42,856 - INFO - [diffusion][Epoch 11936] diffusion training Loss: 0.055462989024817944
2024-11-05 04:43:42,858 - INFO - [diffusion][Epoch 11936] diffusion learning rate: 0.001
2024-11-05 04:43:42,860 - INFO - [diffusion][Epoch 11936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:42,861 - INFO - [diffusion][Epoch 11937] Epoch 11938/12000
2024-11-05 04:43:45,741 - INFO - [diffusion][Epoch 11937] diffusion training Loss: 0.057271975092589855
2024-11-05 04:43:45,743 - INFO - [diffusion][Epoch 11937] diffusion learning rate: 0.001
2024-11-05 04:43:45,744 - INFO - [diffusion][Epoch 11937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:45,746 - INFO - [diffusion][Epoch 11938] Epoch 11939/12000
2024-11-05 04:43:48,562 - INFO - [diffusion][Epoch 11938] diffusion training Loss: 0.062229475006461143
2024-11-05 04:43:48,564 - INFO - [diffusion][Epoch 11938] diffusion learning rate: 0.001
2024-11-05 04:43:48,566 - INFO - [diffusion][Epoch 11938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:48,567 - INFO - [diffusion][Epoch 11939] Epoch 11940/12000
2024-11-05 04:43:51,463 - INFO - [diffusion][Epoch 11939] diffusion training Loss: 0.05610113311558962
2024-11-05 04:43:51,465 - INFO - [diffusion][Epoch 11939] diffusion learning rate: 0.001
2024-11-05 04:43:51,467 - INFO - [diffusion][Epoch 11939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:51,468 - INFO - [diffusion][Epoch 11940] Epoch 11941/12000
2024-11-05 04:43:54,320 - INFO - [diffusion][Epoch 11940] diffusion training Loss: 0.05643452890217304
2024-11-05 04:43:54,323 - INFO - [diffusion][Epoch 11940] diffusion learning rate: 0.001
2024-11-05 04:43:54,325 - INFO - [diffusion][Epoch 11940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:54,326 - INFO - [diffusion][Epoch 11941] Epoch 11942/12000
2024-11-05 04:43:57,236 - INFO - [diffusion][Epoch 11941] diffusion training Loss: 0.05497595574706793
2024-11-05 04:43:57,239 - INFO - [diffusion][Epoch 11941] diffusion learning rate: 0.001
2024-11-05 04:43:57,242 - INFO - [diffusion][Epoch 11941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:57,243 - INFO - [diffusion][Epoch 11942] Epoch 11943/12000
2024-11-05 04:44:00,057 - INFO - [diffusion][Epoch 11942] diffusion training Loss: 0.06020989641547203
2024-11-05 04:44:00,059 - INFO - [diffusion][Epoch 11942] diffusion learning rate: 0.001
2024-11-05 04:44:00,061 - INFO - [diffusion][Epoch 11942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:00,062 - INFO - [diffusion][Epoch 11943] Epoch 11944/12000
2024-11-05 04:44:02,973 - INFO - [diffusion][Epoch 11943] diffusion training Loss: 0.05535533558577299
2024-11-05 04:44:02,975 - INFO - [diffusion][Epoch 11943] diffusion learning rate: 0.001
2024-11-05 04:44:02,977 - INFO - [diffusion][Epoch 11943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:02,978 - INFO - [diffusion][Epoch 11944] Epoch 11945/12000
2024-11-05 04:44:05,869 - INFO - [diffusion][Epoch 11944] diffusion training Loss: 0.06236523948609829
2024-11-05 04:44:05,871 - INFO - [diffusion][Epoch 11944] diffusion learning rate: 0.001
2024-11-05 04:44:05,873 - INFO - [diffusion][Epoch 11944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:05,874 - INFO - [diffusion][Epoch 11945] Epoch 11946/12000
2024-11-05 04:44:08,652 - INFO - [diffusion][Epoch 11945] diffusion training Loss: 0.05358884111046791
2024-11-05 04:44:08,654 - INFO - [diffusion][Epoch 11945] diffusion learning rate: 0.001
2024-11-05 04:44:08,656 - INFO - [diffusion][Epoch 11945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:08,657 - INFO - [diffusion][Epoch 11946] Epoch 11947/12000
2024-11-05 04:44:11,496 - INFO - [diffusion][Epoch 11946] diffusion training Loss: 0.0532823521643877
2024-11-05 04:44:11,498 - INFO - [diffusion][Epoch 11946] diffusion learning rate: 0.001
2024-11-05 04:44:11,499 - INFO - [diffusion][Epoch 11946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:11,501 - INFO - [diffusion][Epoch 11947] Epoch 11948/12000
2024-11-05 04:44:14,405 - INFO - [diffusion][Epoch 11947] diffusion training Loss: 0.05992279574275017
2024-11-05 04:44:14,407 - INFO - [diffusion][Epoch 11947] diffusion learning rate: 0.001
2024-11-05 04:44:14,409 - INFO - [diffusion][Epoch 11947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:14,410 - INFO - [diffusion][Epoch 11948] Epoch 11949/12000
2024-11-05 04:44:17,257 - INFO - [diffusion][Epoch 11948] diffusion training Loss: 0.056633057072758675
2024-11-05 04:44:17,259 - INFO - [diffusion][Epoch 11948] diffusion learning rate: 0.001
2024-11-05 04:44:17,261 - INFO - [diffusion][Epoch 11948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:17,262 - INFO - [diffusion][Epoch 11949] Epoch 11950/12000
2024-11-05 04:44:20,192 - INFO - [diffusion][Epoch 11949] diffusion training Loss: 0.05753031466156244
2024-11-05 04:44:20,194 - INFO - [diffusion][Epoch 11949] diffusion learning rate: 0.001
2024-11-05 04:44:20,195 - INFO - [diffusion][Epoch 11949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:20,197 - INFO - [diffusion][Epoch 11950] Epoch 11951/12000
2024-11-05 04:44:23,121 - INFO - [diffusion][Epoch 11950] diffusion training Loss: 0.062030223198235035
2024-11-05 04:44:23,123 - INFO - [diffusion][Epoch 11950] diffusion learning rate: 0.001
2024-11-05 04:44:23,125 - INFO - [diffusion][Epoch 11950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:23,126 - INFO - [diffusion][Epoch 11951] Epoch 11952/12000
2024-11-05 04:44:26,527 - INFO - [diffusion][Epoch 11951] diffusion training Loss: 0.054227777756750584
2024-11-05 04:44:26,529 - INFO - [diffusion][Epoch 11951] diffusion learning rate: 0.001
2024-11-05 04:44:26,531 - INFO - [diffusion][Epoch 11951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:26,533 - INFO - [diffusion][Epoch 11952] Epoch 11953/12000
2024-11-05 04:44:29,422 - INFO - [diffusion][Epoch 11952] diffusion training Loss: 0.05699463561177254
2024-11-05 04:44:29,424 - INFO - [diffusion][Epoch 11952] diffusion learning rate: 0.001
2024-11-05 04:44:29,426 - INFO - [diffusion][Epoch 11952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:29,427 - INFO - [diffusion][Epoch 11953] Epoch 11954/12000
2024-11-05 04:44:32,339 - INFO - [diffusion][Epoch 11953] diffusion training Loss: 0.061895073391497135
2024-11-05 04:44:32,341 - INFO - [diffusion][Epoch 11953] diffusion learning rate: 0.001
2024-11-05 04:44:32,343 - INFO - [diffusion][Epoch 11953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:32,344 - INFO - [diffusion][Epoch 11954] Epoch 11955/12000
2024-11-05 04:44:35,124 - INFO - [diffusion][Epoch 11954] diffusion training Loss: 0.062088836915791035
2024-11-05 04:44:35,127 - INFO - [diffusion][Epoch 11954] diffusion learning rate: 0.001
2024-11-05 04:44:35,129 - INFO - [diffusion][Epoch 11954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:35,130 - INFO - [diffusion][Epoch 11955] Epoch 11956/12000
2024-11-05 04:44:37,995 - INFO - [diffusion][Epoch 11955] diffusion training Loss: 0.058016082271933556
2024-11-05 04:44:37,997 - INFO - [diffusion][Epoch 11955] diffusion learning rate: 0.001
2024-11-05 04:44:37,999 - INFO - [diffusion][Epoch 11955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:38,000 - INFO - [diffusion][Epoch 11956] Epoch 11957/12000
2024-11-05 04:44:40,957 - INFO - [diffusion][Epoch 11956] diffusion training Loss: 0.05787844769656658
2024-11-05 04:44:40,959 - INFO - [diffusion][Epoch 11956] diffusion learning rate: 0.001
2024-11-05 04:44:40,961 - INFO - [diffusion][Epoch 11956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:40,962 - INFO - [diffusion][Epoch 11957] Epoch 11958/12000
2024-11-05 04:44:43,877 - INFO - [diffusion][Epoch 11957] diffusion training Loss: 0.06040216702967882
2024-11-05 04:44:43,880 - INFO - [diffusion][Epoch 11957] diffusion learning rate: 0.001
2024-11-05 04:44:43,882 - INFO - [diffusion][Epoch 11957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:43,883 - INFO - [diffusion][Epoch 11958] Epoch 11959/12000
2024-11-05 04:44:46,759 - INFO - [diffusion][Epoch 11958] diffusion training Loss: 0.05841616913676262
2024-11-05 04:44:46,761 - INFO - [diffusion][Epoch 11958] diffusion learning rate: 0.001
2024-11-05 04:44:46,763 - INFO - [diffusion][Epoch 11958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:46,764 - INFO - [diffusion][Epoch 11959] Epoch 11960/12000
2024-11-05 04:44:49,657 - INFO - [diffusion][Epoch 11959] diffusion training Loss: 0.05507184751331806
2024-11-05 04:44:49,659 - INFO - [diffusion][Epoch 11959] diffusion learning rate: 0.001
2024-11-05 04:44:49,661 - INFO - [diffusion][Epoch 11959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:49,663 - INFO - [diffusion][Epoch 11960] Epoch 11961/12000
2024-11-05 04:44:52,564 - INFO - [diffusion][Epoch 11960] diffusion training Loss: 0.054428355768322945
2024-11-05 04:44:52,566 - INFO - [diffusion][Epoch 11960] diffusion learning rate: 0.001
2024-11-05 04:44:52,598 - INFO - [diffusion][Epoch 11960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:52,599 - INFO - [diffusion][Epoch 11961] Epoch 11962/12000
2024-11-05 04:44:55,511 - INFO - [diffusion][Epoch 11961] diffusion training Loss: 0.06069726776331663
2024-11-05 04:44:55,515 - INFO - [diffusion][Epoch 11961] diffusion learning rate: 0.001
2024-11-05 04:44:55,517 - INFO - [diffusion][Epoch 11961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:55,518 - INFO - [diffusion][Epoch 11962] Epoch 11963/12000
2024-11-05 04:44:58,282 - INFO - [diffusion][Epoch 11962] diffusion training Loss: 0.057413590140640736
2024-11-05 04:44:58,284 - INFO - [diffusion][Epoch 11962] diffusion learning rate: 0.001
2024-11-05 04:44:58,286 - INFO - [diffusion][Epoch 11962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:58,287 - INFO - [diffusion][Epoch 11963] Epoch 11964/12000
2024-11-05 04:45:01,203 - INFO - [diffusion][Epoch 11963] diffusion training Loss: 0.061300311237573624
2024-11-05 04:45:01,205 - INFO - [diffusion][Epoch 11963] diffusion learning rate: 0.001
2024-11-05 04:45:01,207 - INFO - [diffusion][Epoch 11963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:01,208 - INFO - [diffusion][Epoch 11964] Epoch 11965/12000
2024-11-05 04:45:04,108 - INFO - [diffusion][Epoch 11964] diffusion training Loss: 0.05485881678760052
2024-11-05 04:45:04,110 - INFO - [diffusion][Epoch 11964] diffusion learning rate: 0.001
2024-11-05 04:45:04,112 - INFO - [diffusion][Epoch 11964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:04,114 - INFO - [diffusion][Epoch 11965] Epoch 11966/12000
2024-11-05 04:45:07,020 - INFO - [diffusion][Epoch 11965] diffusion training Loss: 0.05671633221209049
2024-11-05 04:45:07,022 - INFO - [diffusion][Epoch 11965] diffusion learning rate: 0.001
2024-11-05 04:45:07,024 - INFO - [diffusion][Epoch 11965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:07,025 - INFO - [diffusion][Epoch 11966] Epoch 11967/12000
2024-11-05 04:45:09,962 - INFO - [diffusion][Epoch 11966] diffusion training Loss: 0.05256631504744291
2024-11-05 04:45:09,964 - INFO - [diffusion][Epoch 11966] diffusion learning rate: 0.001
2024-11-05 04:45:09,966 - INFO - [diffusion][Epoch 11966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:09,967 - INFO - [diffusion][Epoch 11967] Epoch 11968/12000
2024-11-05 04:45:12,907 - INFO - [diffusion][Epoch 11967] diffusion training Loss: 0.052331370301544666
2024-11-05 04:45:12,909 - INFO - [diffusion][Epoch 11967] diffusion learning rate: 0.001
2024-11-05 04:45:12,910 - INFO - [diffusion][Epoch 11967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:12,912 - INFO - [diffusion][Epoch 11968] Epoch 11969/12000
2024-11-05 04:45:15,812 - INFO - [diffusion][Epoch 11968] diffusion training Loss: 0.05639167409390211
2024-11-05 04:45:15,814 - INFO - [diffusion][Epoch 11968] diffusion learning rate: 0.001
2024-11-05 04:45:15,815 - INFO - [diffusion][Epoch 11968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:15,817 - INFO - [diffusion][Epoch 11969] Epoch 11970/12000
2024-11-05 04:45:18,729 - INFO - [diffusion][Epoch 11969] diffusion training Loss: 0.05585172213613987
2024-11-05 04:45:18,731 - INFO - [diffusion][Epoch 11969] diffusion learning rate: 0.001
2024-11-05 04:45:18,733 - INFO - [diffusion][Epoch 11969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:18,734 - INFO - [diffusion][Epoch 11970] Epoch 11971/12000
2024-11-05 04:45:21,610 - INFO - [diffusion][Epoch 11970] diffusion training Loss: 0.056467861868441105
2024-11-05 04:45:21,612 - INFO - [diffusion][Epoch 11970] diffusion learning rate: 0.001
2024-11-05 04:45:21,614 - INFO - [diffusion][Epoch 11970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:21,615 - INFO - [diffusion][Epoch 11971] Epoch 11972/12000
2024-11-05 04:45:24,499 - INFO - [diffusion][Epoch 11971] diffusion training Loss: 0.06177021563053131
2024-11-05 04:45:24,501 - INFO - [diffusion][Epoch 11971] diffusion learning rate: 0.001
2024-11-05 04:45:24,503 - INFO - [diffusion][Epoch 11971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:24,504 - INFO - [diffusion][Epoch 11972] Epoch 11973/12000
2024-11-05 04:45:27,952 - INFO - [diffusion][Epoch 11972] diffusion training Loss: 0.05720445420593023
2024-11-05 04:45:27,954 - INFO - [diffusion][Epoch 11972] diffusion learning rate: 0.001
2024-11-05 04:45:27,956 - INFO - [diffusion][Epoch 11972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:27,958 - INFO - [diffusion][Epoch 11973] Epoch 11974/12000
2024-11-05 04:45:30,702 - INFO - [diffusion][Epoch 11973] diffusion training Loss: 0.06133711524307728
2024-11-05 04:45:30,704 - INFO - [diffusion][Epoch 11973] diffusion learning rate: 0.001
2024-11-05 04:45:30,706 - INFO - [diffusion][Epoch 11973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:30,707 - INFO - [diffusion][Epoch 11974] Epoch 11975/12000
2024-11-05 04:45:33,588 - INFO - [diffusion][Epoch 11974] diffusion training Loss: 0.05847198702394962
2024-11-05 04:45:33,590 - INFO - [diffusion][Epoch 11974] diffusion learning rate: 0.001
2024-11-05 04:45:33,592 - INFO - [diffusion][Epoch 11974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:33,593 - INFO - [diffusion][Epoch 11975] Epoch 11976/12000
2024-11-05 04:45:36,479 - INFO - [diffusion][Epoch 11975] diffusion training Loss: 0.05586227308958769
2024-11-05 04:45:36,481 - INFO - [diffusion][Epoch 11975] diffusion learning rate: 0.001
2024-11-05 04:45:36,483 - INFO - [diffusion][Epoch 11975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:36,484 - INFO - [diffusion][Epoch 11976] Epoch 11977/12000
2024-11-05 04:45:39,353 - INFO - [diffusion][Epoch 11976] diffusion training Loss: 0.05762303154915571
2024-11-05 04:45:39,355 - INFO - [diffusion][Epoch 11976] diffusion learning rate: 0.001
2024-11-05 04:45:39,357 - INFO - [diffusion][Epoch 11976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:39,358 - INFO - [diffusion][Epoch 11977] Epoch 11978/12000
2024-11-05 04:45:42,304 - INFO - [diffusion][Epoch 11977] diffusion training Loss: 0.06072451267391443
2024-11-05 04:45:42,306 - INFO - [diffusion][Epoch 11977] diffusion learning rate: 0.001
2024-11-05 04:45:42,308 - INFO - [diffusion][Epoch 11977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:42,309 - INFO - [diffusion][Epoch 11978] Epoch 11979/12000
2024-11-05 04:45:45,226 - INFO - [diffusion][Epoch 11978] diffusion training Loss: 0.05881838873028755
2024-11-05 04:45:45,229 - INFO - [diffusion][Epoch 11978] diffusion learning rate: 0.001
2024-11-05 04:45:45,231 - INFO - [diffusion][Epoch 11978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:45,232 - INFO - [diffusion][Epoch 11979] Epoch 11980/12000
2024-11-05 04:45:48,105 - INFO - [diffusion][Epoch 11979] diffusion training Loss: 0.05652236007153988
2024-11-05 04:45:48,107 - INFO - [diffusion][Epoch 11979] diffusion learning rate: 0.001
2024-11-05 04:45:48,109 - INFO - [diffusion][Epoch 11979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:48,110 - INFO - [diffusion][Epoch 11980] Epoch 11981/12000
2024-11-05 04:45:50,969 - INFO - [diffusion][Epoch 11980] diffusion training Loss: 0.05548724625259638
2024-11-05 04:45:50,971 - INFO - [diffusion][Epoch 11980] diffusion learning rate: 0.001
2024-11-05 04:45:50,973 - INFO - [diffusion][Epoch 11980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:50,974 - INFO - [diffusion][Epoch 11981] Epoch 11982/12000
2024-11-05 04:45:53,891 - INFO - [diffusion][Epoch 11981] diffusion training Loss: 0.057085832580924034
2024-11-05 04:45:53,893 - INFO - [diffusion][Epoch 11981] diffusion learning rate: 0.001
2024-11-05 04:45:53,921 - INFO - [diffusion][Epoch 11981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:53,922 - INFO - [diffusion][Epoch 11982] Epoch 11983/12000
2024-11-05 04:45:56,830 - INFO - [diffusion][Epoch 11982] diffusion training Loss: 0.059503739699721336
2024-11-05 04:45:56,832 - INFO - [diffusion][Epoch 11982] diffusion learning rate: 0.001
2024-11-05 04:45:56,834 - INFO - [diffusion][Epoch 11982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:56,836 - INFO - [diffusion][Epoch 11983] Epoch 11984/12000
2024-11-05 04:45:59,725 - INFO - [diffusion][Epoch 11983] diffusion training Loss: 0.06267343554645777
2024-11-05 04:45:59,727 - INFO - [diffusion][Epoch 11983] diffusion learning rate: 0.001
2024-11-05 04:45:59,729 - INFO - [diffusion][Epoch 11983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:59,730 - INFO - [diffusion][Epoch 11984] Epoch 11985/12000
2024-11-05 04:46:02,625 - INFO - [diffusion][Epoch 11984] diffusion training Loss: 0.05987662263214588
2024-11-05 04:46:02,627 - INFO - [diffusion][Epoch 11984] diffusion learning rate: 0.001
2024-11-05 04:46:02,628 - INFO - [diffusion][Epoch 11984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:02,630 - INFO - [diffusion][Epoch 11985] Epoch 11986/12000
2024-11-05 04:46:05,521 - INFO - [diffusion][Epoch 11985] diffusion training Loss: 0.06026077643036842
2024-11-05 04:46:05,590 - INFO - [diffusion][Epoch 11985] diffusion learning rate: 0.001
2024-11-05 04:46:05,592 - INFO - [diffusion][Epoch 11985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:05,594 - INFO - [diffusion][Epoch 11986] Epoch 11987/12000
2024-11-05 04:46:08,477 - INFO - [diffusion][Epoch 11986] diffusion training Loss: 0.06475038081407547
2024-11-05 04:46:08,479 - INFO - [diffusion][Epoch 11986] diffusion learning rate: 0.001
2024-11-05 04:46:08,481 - INFO - [diffusion][Epoch 11986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:08,483 - INFO - [diffusion][Epoch 11987] Epoch 11988/12000
2024-11-05 04:46:11,423 - INFO - [diffusion][Epoch 11987] diffusion training Loss: 0.05565608199685812
2024-11-05 04:46:11,425 - INFO - [diffusion][Epoch 11987] diffusion learning rate: 0.001
2024-11-05 04:46:11,427 - INFO - [diffusion][Epoch 11987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:11,428 - INFO - [diffusion][Epoch 11988] Epoch 11989/12000
2024-11-05 04:46:14,313 - INFO - [diffusion][Epoch 11988] diffusion training Loss: 0.060104476287961006
2024-11-05 04:46:14,316 - INFO - [diffusion][Epoch 11988] diffusion learning rate: 0.001
2024-11-05 04:46:14,318 - INFO - [diffusion][Epoch 11988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:14,319 - INFO - [diffusion][Epoch 11989] Epoch 11990/12000
2024-11-05 04:46:16,994 - INFO - [diffusion][Epoch 11989] diffusion training Loss: 0.05451149772852659
2024-11-05 04:46:16,996 - INFO - [diffusion][Epoch 11989] diffusion learning rate: 0.001
2024-11-05 04:46:17,024 - INFO - [diffusion][Epoch 11989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:17,026 - INFO - [diffusion][Epoch 11990] Epoch 11991/12000
2024-11-05 04:46:19,888 - INFO - [diffusion][Epoch 11990] diffusion training Loss: 0.057700141333043575
2024-11-05 04:46:19,890 - INFO - [diffusion][Epoch 11990] diffusion learning rate: 0.001
[INFO|configuration_utils.py:728] 2024-11-05 04:46:35,550 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:35,552 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:46:35,612 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:35,613 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:35,618 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:35,618 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:35,618 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:35,618 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:35,618 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:35,618 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:46:35,618 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:35,619 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:46:35,783 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:46:37,030 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:46:37,030 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:46:37,893 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:46:37,894 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:46:37,896 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:46:37,896 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:46:37,896 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
[INFO|training_args.py:1902] 2024-11-05 04:46:42,507 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:46:42,508 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:46:42,509 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:46:45,464 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:45,465 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:46:45,512 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:45,513 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:45,515 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:45,515 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:45,515 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:45,515 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:45,515 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:45,515 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:46:45,515 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:45,516 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:46:45,667 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:46:46,608 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:46:46,609 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:46:47,419 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:46:47,421 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:46:47,424 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:46:47,425 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:46:47,425 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]
{'eval_loss': 0.18751734495162964, 'eval_accuracy': 0.9369266055045872, 'eval_runtime': 4.548, 'eval_samples_per_second': 191.732, 'eval_steps_per_second': 0.22}
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:04.54
  eval_samples            =        872
  eval_samples_per_second =    191.732
  eval_steps_per_second   =       0.22
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.51it/s]
[INFO|training_args.py:1902] 2024-11-05 04:46:48,561 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:46:48,561 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:46:48,562 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:46:51,428 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:51,429 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:46:51,481 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:51,482 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:51,484 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:51,484 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:51,484 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:51,484 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:51,484 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:51,484 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:46:51,484 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:51,485 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:46:51,610 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:46:52,552 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:46:52,553 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:46:53,215 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:46:53,217 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:46:53,220 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:46:53,221 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:46:53,221 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    813.761
  eval_steps_per_second   =      0.933
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.99it/s]
[INFO|training_args.py:1902] 2024-11-05 04:46:53,727 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:46:53,727 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:46:53,728 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:46:56,465 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:56,466 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:46:56,515 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:56,516 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:56,518 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:56,518 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:56,518 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:56,518 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:56,518 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:46:56,518 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:46:56,519 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:46:56,520 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:46:56,650 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:46:57,887 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:46:57,887 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:46:58,727 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:46:58,729 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:46:58,731 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:46:58,731 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:46:58,731 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =      0.647
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =     622.52
  eval_steps_per_second   =      2.247
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.17it/s]
[INFO|training_args.py:1902] 2024-11-05 04:46:59,205 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:46:59,205 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:46:59,206 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:01,895 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:01,896 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:01,949 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:01,950 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:01,952 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:01,952 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:01,952 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:01,952 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:01,952 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:01,952 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:01,953 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:01,954 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:02,083 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:03,038 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:03,038 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:03,769 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:03,772 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:03,774 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:03,774 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:47:03,775 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.639
  eval_loss               =     0.6473
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    662.742
  eval_steps_per_second   =      2.393
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.36it/s]
[INFO|training_args.py:1902] 2024-11-05 04:47:04,412 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:04,412 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:04,413 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:07,569 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:07,570 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:07,624 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:07,625 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:07,627 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:07,627 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:07,627 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:07,627 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:07,627 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:07,627 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:07,627 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:07,628 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:07,774 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:08,663 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:08,663 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:09,314 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:09,315 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:09,317 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:09,317 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:47:09,317 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.58
  eval_samples            =        408
  eval_samples_per_second =    702.036
  eval_steps_per_second   =      1.721
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.17it/s]
[INFO|training_args.py:1902] 2024-11-05 04:47:09,854 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:09,854 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:09,855 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:13,626 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:13,628 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:13,693 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:13,694 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:13,696 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:13,696 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:13,696 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:13,696 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:13,696 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:13,696 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:13,697 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:13,698 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:13,840 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:14,794 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:14,794 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:15,432 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:15,434 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:15,436 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:15,436 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:47:15,436 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8235
  eval_combined_score     =     0.8486
  eval_f1                 =     0.8737
  eval_loss               =     0.4188
  eval_runtime            = 0:00:00.47
  eval_samples            =        408
  eval_samples_per_second =    856.327
  eval_steps_per_second   =      2.099
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.59it/s]
[INFO|training_args.py:1902] 2024-11-05 04:47:17,006 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:17,006 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:17,007 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:19,903 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:19,904 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:19,953 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:19,954 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:19,955 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:19,956 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:19,956 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:19,956 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:19,956 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:19,956 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:19,956 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:19,957 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:20,066 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:20,981 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:20,981 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:22,841 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:22,843 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:22,845 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:22,846 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:47:22,846 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.51
  eval_samples              =       1043
  eval_samples_per_second   =    689.279
  eval_steps_per_second     =      0.661
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

100% 1/1 [00:00<00:00, 30.96it/s]
[INFO|training_args.py:1902] 2024-11-05 04:47:24,131 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:24,131 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:24,132 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:26,961 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:26,962 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:27,015 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:27,017 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:27,028 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:27,028 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:27,028 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:27,028 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:27,028 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:27,028 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:27,029 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:27,030 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:27,154 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:28,095 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:28,095 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:28,710 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:28,712 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:28,714 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:28,715 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:47:28,715 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4793
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    849.721
  eval_steps_per_second     =      0.815
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.08s/it]
100% 3/3 [00:03<00:00,  1.12s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 04:47:34,924 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:34,925 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:34,926 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:37,742 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:37,743 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:37,791 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:37,792 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:37,796 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:37,796 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:37,796 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:37,796 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:37,796 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:37,796 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:37,797 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:37,797 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:37,932 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:39,199 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:39,199 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:39,951 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:39,953 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:39,955 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:39,955 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:47:39,955 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.14
  eval_samples            =       5463
  eval_samples_per_second =    889.513
  eval_steps_per_second   =      0.488
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.01it/s]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
[INFO|training_args.py:1902] 2024-11-05 04:47:46,049 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:46,049 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:46,050 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:48,893 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:48,894 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:48,939 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:48,940 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:48,942 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:48,942 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:48,942 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:48,942 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:48,942 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:48,942 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:48,943 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:48,944 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:49,071 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:50,039 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:50,039 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:50,709 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:50,711 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:50,714 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:50,714 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:47:50,714 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.03
  eval_samples            =       5463
  eval_samples_per_second =    905.952
  eval_steps_per_second   =      0.498
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.80it/s]
[INFO|training_args.py:1902] 2024-11-05 04:47:52,467 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:47:52,467 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:47:52,468 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:47:55,101 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:55,102 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:47:55,163 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:55,164 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:55,166 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:55,166 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:55,166 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:55,166 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:55,166 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:47:55,166 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:47:55,167 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:47:55,167 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:47:55,285 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:47:56,230 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:47:56,230 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:47:56,896 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:47:56,898 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:47:56,901 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:47:56,901 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:47:56,901 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =      0.635
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.69
  eval_samples            =       1500
  eval_samples_per_second =    886.903
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.591
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.97it/s]
2024-11-05 04:47:59,014 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:47:59,016 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models rte: 0.6389891696750902
2024-11-05 04:47:59,017 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models mrpc: 0.8736842105263158
2024-11-05 04:47:59,018 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 04:47:59,019 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 04:47:59,020 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models stsb: 0.86910537080605
2024-11-05 04:47:59,021 - INFO - [diffusion][Epoch 11990] average DIFF reconstruction auto_encoder_models accuracy: 0.7896111472492754
2024-11-05 04:47:59,023 - INFO - [diffusion][Epoch 11990] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6389891696750902), ('mrpc', 0.87215411558669), ('mrpc', 0.8736842105263158), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.86907015199184), ('stsb', 0.86910537080605)]
2024-11-05 04:47:59,024 - INFO - [diffusion][Epoch 11990] ---------------------------------
2024-11-05 04:47:59,133 - INFO - [diffusion][Epoch 11990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:59,135 - INFO - [diffusion][Epoch 11991] Epoch 11992/12000
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6342
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:02.06
  eval_samples            =       1500
  eval_samples_per_second =    727.496
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.485
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:48:02,249 - INFO - [diffusion][Epoch 11991] diffusion training Loss: 0.06435415614396334
2024-11-05 04:48:02,251 - INFO - [diffusion][Epoch 11991] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:48:14,565 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:14,567 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:14,568 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:17,432 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:17,433 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:17,487 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:17,488 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:17,490 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:17,490 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:17,490 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:17,490 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:17,490 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:17,490 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:17,490 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:17,491 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:17,628 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:18,577 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:18,577 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:19,132 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:19,135 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:19,138 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:19,138 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:48:19,138 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.79it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:20,285 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:20,286 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:20,287 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:23,251 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:23,252 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:23,305 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:23,306 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:23,307 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:23,307 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:23,307 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:23,307 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:23,307 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:23,308 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:23,308 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:23,309 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:23,434 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:24,390 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:24,390 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:24,897 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:24,899 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:24,901 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:24,901 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:48:24,901 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.08
  eval_samples            =        872
  eval_samples_per_second =    801.541
  eval_steps_per_second   =      0.919
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.22it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:25,993 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:25,994 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:25,994 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:28,948 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:28,949 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:29,000 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:29,001 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:29,003 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:29,003 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:29,003 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:29,003 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:29,003 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:29,003 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:29,004 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:29,005 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:29,143 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:30,085 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:30,085 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:30,652 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:30,654 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:30,656 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:30,656 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:48:30,656 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.03
  eval_samples            =        872
  eval_samples_per_second =    840.313
  eval_steps_per_second   =      0.964
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.56it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:31,495 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:31,496 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:31,496 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:34,252 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:34,253 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:34,296 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:34,297 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:34,299 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:34,299 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:34,299 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:34,299 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:34,299 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:34,299 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:34,299 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:34,300 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:34,413 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:35,370 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:35,370 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:37,081 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:37,083 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:37,085 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:37,085 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:48:37,085 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.78
  eval_samples            =        277
  eval_samples_per_second =    353.706
  eval_steps_per_second   =      1.277
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.18it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:37,550 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:37,551 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:37,552 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:40,411 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:40,412 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:40,466 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:40,467 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:40,471 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:40,471 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:40,471 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:40,471 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:40,471 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:40,471 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:40,472 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:40,473 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:40,591 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:41,537 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:41,537 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:42,136 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:42,138 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:42,140 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:42,141 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:48:42,141 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.639
  eval_loss               =     0.6474
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    677.309
  eval_steps_per_second   =      2.445
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.79it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:42,769 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:42,769 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:42,770 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:45,619 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:45,620 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:45,670 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:45,671 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:45,673 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:45,673 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:45,673 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:45,673 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:45,673 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:45,673 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:45,673 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:45,674 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:45,807 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:46,757 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:46,758 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:47,396 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:47,399 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:47,401 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:47,401 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:48:47,401 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8235
  eval_combined_score     =     0.8486
  eval_f1                 =     0.8737
  eval_loss               =     0.4187
  eval_runtime            = 0:00:00.57
  eval_samples            =        408
  eval_samples_per_second =    715.224
  eval_steps_per_second   =      1.753
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.24it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:48,022 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:48,022 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:48,023 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:51,076 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:51,077 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:51,127 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:51,128 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:51,130 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:51,130 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:51,130 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:51,130 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:51,130 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:51,130 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:51,131 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:51,132 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:51,259 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:52,216 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:52,216 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:52,801 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:52,803 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:52,806 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:52,806 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:48:52,806 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.55
  eval_samples            =        408
  eval_samples_per_second =    730.571
  eval_steps_per_second   =      1.791
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.72it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:54,071 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:54,071 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:54,072 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:48:56,704 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:56,705 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:48:56,757 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:56,758 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:56,760 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:56,760 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:56,760 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:56,760 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:56,760 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:48:56,760 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:48:56,761 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:48:56,762 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:48:56,895 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:48:57,867 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:48:57,867 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:48:58,510 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:48:58,512 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:48:58,514 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:48:58,514 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:48:58,514 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4795
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    866.307
  eval_steps_per_second     =      0.831
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.38it/s]
[INFO|training_args.py:1902] 2024-11-05 04:48:59,732 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:48:59,733 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:48:59,734 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:49:03,082 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:03,083 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:49:03,137 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:03,138 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:03,140 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:03,140 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:03,140 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:03,140 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:03,140 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:03,140 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:49:03,140 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:03,141 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:49:03,272 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:49:04,202 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:49:04,207 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:49:04,784 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:49:04,786 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:49:04,788 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:49:04,789 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:49:04,789 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.15
  eval_samples              =       1043
  eval_samples_per_second   =    900.137
  eval_steps_per_second     =      0.863
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

 67% 2/3 [00:02<00:01,  1.19s/it]
100% 3/3 [00:03<00:00,  1.20s/it]
100% 3/3 [00:03<00:00,  1.22s/it]
[INFO|training_args.py:1902] 2024-11-05 04:49:11,063 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:49:11,064 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:49:11,065 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:49:19,555 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:19,556 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:49:19,607 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:19,608 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:19,610 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:19,610 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:19,610 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:19,610 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:19,610 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:19,610 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:49:19,611 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:19,612 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:49:19,748 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:49:20,694 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:49:20,695 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:49:21,310 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:49:21,313 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:49:21,315 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:49:21,316 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:49:21,316 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.20
  eval_samples            =       5463
  eval_samples_per_second =    880.358
  eval_steps_per_second   =      0.483
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.04s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
[INFO|training_args.py:1902] 2024-11-05 04:49:27,391 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:49:27,392 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:49:27,393 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:49:30,112 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:30,113 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:49:30,167 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:30,168 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:30,170 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:30,170 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:30,170 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:30,170 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:30,170 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:49:30,170 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:49:30,171 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:49:30,171 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:49:30,320 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:49:31,271 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:49:31,272 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:49:32,287 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:49:32,289 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:49:32,291 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:49:32,291 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:49:32,292 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.00
  eval_samples            =       5463
  eval_samples_per_second =    909.919
  eval_steps_per_second   =        0.5
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.71it/s]
[INFO|training_args.py:1902] 2024-11-05 04:49:34,079 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:49:34,080 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:49:34,081 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:07,908 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:07,909 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:07,968 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:07,969 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:07,972 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:07,972 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:07,972 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:07,972 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:07,973 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:07,973 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:07,973 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:07,974 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:08,068 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:50:08,985 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:50:08,986 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:50:09,605 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:50:09,607 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:50:09,609 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:50:09,609 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:50:09,609 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6346
  eval_pearson            =      0.869
  eval_runtime            = 0:00:01.71
  eval_samples            =       1500
  eval_samples_per_second =     873.27
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.582
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.88it/s]
2024-11-05 04:50:11,431 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:50:11,433 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models rte: 0.6389891696750902
2024-11-05 04:50:11,434 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models mrpc: 0.8736842105263158
2024-11-05 04:50:11,435 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 04:50:11,436 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 04:50:11,437 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models stsb: 0.8691190838700404
2024-11-05 04:50:11,438 - INFO - [diffusion][Epoch 11991] average DIFF reconstruction auto_encoder_models accuracy: 0.7896098665049028
2024-11-05 04:50:11,440 - INFO - [diffusion][Epoch 11991] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6389891696750902), ('mrpc', 0.8736842105263158), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690410699953763), ('stsb', 0.8691190838700404)]
2024-11-05 04:50:11,441 - INFO - [diffusion][Epoch 11991] ---------------------------------
2024-11-05 04:50:11,571 - INFO - [diffusion][Epoch 11991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:11,573 - INFO - [diffusion][Epoch 11992] Epoch 11993/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6345
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.76
  eval_samples            =       1500
  eval_samples_per_second =    850.371
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.567
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:50:14,513 - INFO - [diffusion][Epoch 11992] diffusion training Loss: 0.05413228180259466
2024-11-05 04:50:14,515 - INFO - [diffusion][Epoch 11992] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:50:27,050 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:50:27,051 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:50:27,052 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:30,081 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:30,082 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:30,136 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:30,137 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:30,139 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:30,139 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:30,139 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:30,139 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:30,139 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:30,139 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:30,139 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:30,140 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:30,276 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:50:31,246 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:50:31,246 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:50:31,928 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:50:31,930 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:50:31,933 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:50:31,933 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:50:31,933 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.93it/s]
[INFO|training_args.py:1902] 2024-11-05 04:50:32,997 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:50:32,998 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:50:32,999 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:35,981 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:35,982 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:36,070 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:36,071 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:36,073 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:36,073 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:36,073 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:36,073 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:36,073 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:36,073 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:36,073 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:36,074 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:36,199 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:50:37,155 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:50:37,155 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:50:37,791 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:50:37,793 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:50:37,796 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:50:37,796 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:50:37,796 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.00
  eval_samples            =        872
  eval_samples_per_second =    865.642
  eval_steps_per_second   =      0.993
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.16it/s]
[INFO|training_args.py:1902] 2024-11-05 04:50:39,242 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:50:39,242 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:50:39,243 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:42,041 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:42,042 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:42,101 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:42,102 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:42,104 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:42,104 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:42,104 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:42,104 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:42,104 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:42,104 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:42,104 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:42,105 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:42,234 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:50:43,161 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:50:43,162 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:50:43,784 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:50:43,786 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:50:43,788 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:50:43,788 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:50:43,788 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.38
  eval_samples            =        872
  eval_samples_per_second =    630.886
  eval_steps_per_second   =      0.723
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 50.34it/s]
[INFO|training_args.py:1902] 2024-11-05 04:50:44,241 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:50:44,241 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:50:44,242 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:47,425 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:47,427 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:47,480 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:47,481 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:47,483 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:47,484 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:47,484 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:47,484 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:47,484 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:47,484 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:47,485 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:47,486 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:47,595 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:50:48,497 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:50:48,498 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:50:49,149 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:50:49,152 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:50:49,155 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:50:49,155 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:50:49,155 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.39
  eval_samples            =        277
  eval_samples_per_second =     699.79
  eval_steps_per_second   =      2.526
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.48it/s]
[INFO|training_args.py:1902] 2024-11-05 04:50:49,649 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:50:49,650 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:50:49,650 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:53,669 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:53,671 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:53,719 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:53,720 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:53,722 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:53,722 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:53,722 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:53,722 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:53,722 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:53,722 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:53,722 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:53,723 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:53,851 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:50:54,801 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:50:54,801 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:50:55,346 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:50:55,348 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:50:55,351 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:50:55,351 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:50:55,351 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6473
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =    642.781
  eval_steps_per_second   =      2.321
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.57it/s]
[INFO|training_args.py:1902] 2024-11-05 04:50:56,300 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:50:56,301 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:50:56,301 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:50:59,337 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:59,340 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:50:59,384 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:59,385 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:59,387 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:59,387 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:59,387 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:59,388 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:59,388 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:50:59,388 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:50:59,388 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:50:59,389 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:50:59,523 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:00,454 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:00,454 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:01,078 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:01,080 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:01,083 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:01,083 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:51:01,083 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.88
  eval_samples            =        408
  eval_samples_per_second =    460.157
  eval_steps_per_second   =      1.128
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.87it/s]
[INFO|training_args.py:1902] 2024-11-05 04:51:01,710 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:51:01,711 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:51:01,712 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:51:04,761 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:04,762 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:51:04,810 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:04,811 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:04,813 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:04,813 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:04,813 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:04,813 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:04,813 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:04,813 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:51:04,814 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:04,815 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:51:04,931 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:05,901 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:05,901 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:07,394 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:07,396 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:07,399 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:07,399 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:51:07,399 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.56
  eval_samples            =        408
  eval_samples_per_second =    720.423
  eval_steps_per_second   =      1.766
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.63it/s]
[INFO|training_args.py:1902] 2024-11-05 04:51:08,700 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:51:08,700 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:51:08,701 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:51:11,834 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:11,835 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:51:11,904 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:11,905 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:11,908 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:11,908 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:11,908 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:11,908 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:11,908 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:11,909 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:51:11,909 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:11,910 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:51:12,033 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:12,991 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:12,991 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:13,599 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:13,601 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:13,604 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:13,604 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:51:13,604 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.23
  eval_samples              =       1043
  eval_samples_per_second   =      846.1
  eval_steps_per_second     =      0.811
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

100% 1/1 [00:00<00:00, 32.52it/s]
[INFO|training_args.py:1902] 2024-11-05 04:51:14,858 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:51:14,858 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:51:14,859 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:51:17,667 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:17,669 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:51:17,771 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:17,772 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:17,774 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:17,774 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:17,774 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:17,774 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:17,774 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:17,774 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:51:17,774 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:17,775 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:51:17,920 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:18,830 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:18,831 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:19,798 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:19,800 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:19,803 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:19,803 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:51:19,803 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.19
  eval_samples              =       1043
  eval_samples_per_second   =    872.847
  eval_steps_per_second     =      0.837
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 04:51:25,925 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:51:25,925 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:51:25,926 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:51:29,214 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:29,215 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:51:29,262 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:29,263 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:29,265 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:29,265 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:29,265 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:29,265 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:29,265 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:29,265 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:51:29,265 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:29,266 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:51:29,408 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:30,353 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:30,353 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:30,919 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:30,920 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:30,922 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:30,922 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:51:30,922 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.05
  eval_samples            =       5463
  eval_samples_per_second =    902.028
  eval_steps_per_second   =      0.495
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.07s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 04:51:36,912 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:51:36,913 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:51:36,914 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:51:39,901 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:39,903 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:51:39,961 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:39,962 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:39,964 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:39,964 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:39,964 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:39,964 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:39,964 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:39,964 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:51:39,965 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:39,965 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:51:40,092 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:41,299 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:41,299 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:41,922 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:41,924 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:41,927 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:41,927 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:51:41,927 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.91
  eval_samples            =       5463
  eval_samples_per_second =    922.991
  eval_steps_per_second   =      0.507
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.29it/s]
[INFO|training_args.py:1902] 2024-11-05 04:51:43,679 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:51:43,680 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:51:43,680 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:51:46,931 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:46,932 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:51:46,982 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:46,983 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:46,985 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:46,985 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:46,986 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:46,986 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:46,986 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:51:46,986 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:51:46,986 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:51:46,987 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:51:47,120 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:51:48,104 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:51:48,105 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:51:48,685 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:51:48,687 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:51:48,690 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:51:48,690 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:51:48,690 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6341
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.69
  eval_samples            =       1500
  eval_samples_per_second =    882.736
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.588
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.69it/s]
2024-11-05 04:51:50,479 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:51:50,481 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models rte: 0.6353790613718412
2024-11-05 04:51:50,482 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models mrpc: 0.87215411558669
2024-11-05 04:51:50,483 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 04:51:50,484 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 04:51:50,485 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models stsb: 0.8691054526807902
2024-11-05 04:51:50,487 - INFO - [diffusion][Epoch 11992] average DIFF reconstruction auto_encoder_models accuracy: 0.789182553929261
2024-11-05 04:51:50,488 - INFO - [diffusion][Epoch 11992] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6353790613718412), ('mrpc', 0.87215411558669), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690671535198011), ('stsb', 0.8691054526807902)]
2024-11-05 04:51:50,489 - INFO - [diffusion][Epoch 11992] ---------------------------------
2024-11-05 04:51:50,617 - INFO - [diffusion][Epoch 11992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:50,619 - INFO - [diffusion][Epoch 11993] Epoch 11994/12000
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6347
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.73
  eval_samples            =       1500
  eval_samples_per_second =    866.995
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.578
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:51:53,610 - INFO - [diffusion][Epoch 11993] diffusion training Loss: 0.045421427115797997
2024-11-05 04:51:53,612 - INFO - [diffusion][Epoch 11993] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:52:05,845 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:05,846 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:05,847 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:09,846 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:09,847 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:09,896 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:09,897 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:09,899 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:09,900 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:09,900 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:09,900 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:09,900 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:09,900 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:09,900 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:09,901 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:10,060 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:11,055 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:11,055 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:11,649 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:11,651 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:11,653 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:11,653 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:52:11,653 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.78it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:12,793 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:12,793 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:12,794 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:15,650 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:15,651 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:15,699 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:15,700 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:15,704 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:15,704 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:15,704 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:15,704 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:15,704 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:15,704 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:15,704 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:15,705 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:15,819 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:17,099 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:17,100 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:17,625 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:17,627 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:17,629 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:17,629 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:52:17,629 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    808.228
  eval_steps_per_second   =      0.927
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.74it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:18,733 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:18,733 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:18,734 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:21,360 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:21,361 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:21,408 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:21,409 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:21,410 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:21,410 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:21,410 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:21,411 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:21,411 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:21,411 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:21,411 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:21,412 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:21,545 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:22,504 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:22,504 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:23,101 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:23,103 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:23,106 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:23,106 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:52:23,106 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.04
  eval_samples            =        872
  eval_samples_per_second =    836.218
  eval_steps_per_second   =      0.959
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.76it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:23,581 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:23,581 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:23,582 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:26,035 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:26,036 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:26,088 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:26,089 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:26,091 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:26,091 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:26,091 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:26,091 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:26,091 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:26,091 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:26,092 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:26,093 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:26,214 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:27,195 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:27,195 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:27,846 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:27,848 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:27,850 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:27,850 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:52:27,850 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6467
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    673.642
  eval_steps_per_second   =      2.432
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.73it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:28,382 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:28,382 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:28,383 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:30,991 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:30,992 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:31,045 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:31,046 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:31,048 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:31,048 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:31,048 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:31,048 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:31,048 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:31,048 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:31,049 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:31,050 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:31,187 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:32,134 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:32,135 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:32,782 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:32,784 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:32,786 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:32,786 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:52:32,786 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6471
  eval_runtime            = 0:00:00.46
  eval_samples            =        277
  eval_samples_per_second =    589.492
  eval_steps_per_second   =      2.128
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.44it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:33,691 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:33,691 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:33,692 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:36,373 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:36,374 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:36,425 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:36,426 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:36,428 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:36,428 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:36,428 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:36,428 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:36,428 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:36,429 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:36,429 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:36,430 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:36,534 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:37,443 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:37,444 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:37,997 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:37,999 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:38,001 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:38,001 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:52:38,001 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8235
  eval_combined_score     =     0.8486
  eval_f1                 =     0.8737
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.84
  eval_samples            =        408
  eval_samples_per_second =    481.169
  eval_steps_per_second   =      1.179
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 25.76it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:38,673 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:38,673 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:38,674 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:52:41,449 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:41,450 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:52:41,502 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:41,503 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:41,505 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:41,505 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:41,505 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:41,505 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:41,505 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:52:41,505 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:52:41,505 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:52:41,506 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:52:41,626 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:52:42,563 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:52:42,564 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:52:43,174 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:52:43,176 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:52:43,179 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:52:43,179 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:52:43,179 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.59
  eval_samples            =        408
  eval_samples_per_second =    683.386
  eval_steps_per_second   =      1.675
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.16it/s]
[INFO|training_args.py:1902] 2024-11-05 04:52:44,452 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:52:44,453 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:52:44,454 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:56:47,519 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:56:47,520 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:56:47,571 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:56:47,572 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:47,576 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:47,576 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:47,576 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:47,576 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:47,576 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:47,576 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:56:47,576 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:56:47,577 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:56:47,713 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:56:48,672 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:56:48,672 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:56:49,450 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:56:49,452 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:56:49,455 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:56:49,455 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:56:49,455 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4795
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    863.036
  eval_steps_per_second     =      0.827
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.68it/s]
[INFO|training_args.py:1902] 2024-11-05 04:56:51,039 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:56:51,039 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:56:51,040 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:56:53,963 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:56:53,964 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:56:54,009 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:56:54,010 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:54,012 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:54,012 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:54,012 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:54,012 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:54,012 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:56:54,012 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:56:54,013 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:56:54,014 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:56:54,137 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:56:55,088 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:56:55,088 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:56:55,653 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:56:55,655 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:56:55,658 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:56:55,658 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:56:55,658 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.51
  eval_samples              =       1043
  eval_samples_per_second   =    688.443
  eval_steps_per_second     =       0.66
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.04s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
[INFO|training_args.py:1902] 2024-11-05 04:57:01,811 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:01,811 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:01,812 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:57:05,382 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:05,383 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:57:05,557 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:05,558 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:05,560 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:05,560 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:05,560 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:05,560 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:05,560 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:05,560 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:57:05,560 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:05,561 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:57:05,686 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:57:06,669 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:57:06,669 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:57:07,398 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:57:07,400 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:57:07,403 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:57:07,403 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:57:07,403 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.08
  eval_samples            =       5463
  eval_samples_per_second =    897.243
  eval_steps_per_second   =      0.493
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.02it/s]
100% 3/3 [00:03<00:00,  1.18s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
[INFO|training_args.py:1902] 2024-11-05 04:57:13,525 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:13,526 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:13,526 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:57:16,513 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:16,514 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:57:16,560 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:16,561 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:16,563 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:16,563 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:16,563 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:16,563 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:16,563 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:16,563 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:57:16,564 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:16,565 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:57:16,693 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:57:17,678 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:57:17,678 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:57:18,278 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:57:18,280 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:57:18,284 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:57:18,284 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:57:18,284 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.06
  eval_samples            =       5463
  eval_samples_per_second =    901.264
  eval_steps_per_second   =      0.495
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.90it/s]
[INFO|training_args.py:1902] 2024-11-05 04:57:19,987 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:19,987 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:19,988 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:57:23,118 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:23,119 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:57:23,163 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:23,164 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:23,166 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:23,166 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:23,166 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:23,166 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:23,166 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:23,166 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:57:23,167 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:23,168 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:57:23,296 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:57:24,256 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:57:24,257 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:57:24,933 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:57:24,935 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:57:24,938 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:57:24,938 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:57:24,938 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6343
  eval_pearson            =      0.869
  eval_runtime            = 0:00:01.65
  eval_samples            =       1500
  eval_samples_per_second =    906.582
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.604
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.35it/s]
2024-11-05 04:57:26,714 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:57:26,716 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models rte: 0.6353790613718412
2024-11-05 04:57:26,717 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models mrpc: 0.8736842105263158
2024-11-05 04:57:26,718 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 04:57:26,719 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 04:57:26,720 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models stsb: 0.8691297725439346
2024-11-05 04:57:26,721 - INFO - [diffusion][Epoch 11993] average DIFF reconstruction auto_encoder_models accuracy: 0.7893103389911564
2024-11-05 04:57:26,722 - INFO - [diffusion][Epoch 11993] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6353790613718412), ('mrpc', 0.8736842105263158), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690461594597774), ('stsb', 0.8691297725439346)]
2024-11-05 04:57:26,723 - INFO - [diffusion][Epoch 11993] ---------------------------------
2024-11-05 04:57:26,841 - INFO - [diffusion][Epoch 11993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:26,845 - INFO - [diffusion][Epoch 11994] Epoch 11995/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6339
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.73
  eval_samples            =       1500
  eval_samples_per_second =    864.672
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.576
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:57:29,878 - INFO - [diffusion][Epoch 11994] diffusion training Loss: 0.03937519621104002
2024-11-05 04:57:29,879 - INFO - [diffusion][Epoch 11994] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:57:42,328 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:42,330 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:42,331 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:57:45,419 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:45,420 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:57:45,472 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:45,473 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:45,475 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:45,475 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:45,475 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:45,475 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:45,475 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:45,475 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:57:45,476 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:45,477 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:57:45,616 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:57:46,575 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:57:46,576 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:57:47,225 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:57:47,228 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:57:47,230 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:57:47,230 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:57:47,230 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.51it/s]
[INFO|training_args.py:1902] 2024-11-05 04:57:48,584 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:48,585 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:48,585 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:57:51,632 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:51,633 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:57:51,680 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:51,681 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:51,685 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:51,685 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:51,685 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:51,685 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:51,685 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:51,685 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:57:51,685 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:51,686 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:57:51,804 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:57:52,748 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:57:52,748 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:57:53,507 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:57:53,509 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:57:53,511 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:57:53,512 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:57:53,512 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.29
  eval_samples            =        872
  eval_samples_per_second =    672.454
  eval_steps_per_second   =      0.771
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.73it/s]
[INFO|training_args.py:1902] 2024-11-05 04:57:54,590 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:54,591 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:54,591 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:57:57,268 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:57,269 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:57:57,322 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:57,323 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:57,325 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:57,325 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:57,325 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:57,325 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:57,325 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:57:57,325 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:57:57,326 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:57:57,327 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:57:57,461 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:57:58,382 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:57:58,382 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:57:58,988 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:57:58,992 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:57:58,997 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:57:58,997 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:57:58,997 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.01
  eval_samples            =        872
  eval_samples_per_second =    856.346
  eval_steps_per_second   =      0.982
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.68it/s]
[INFO|training_args.py:1902] 2024-11-05 04:57:59,479 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:57:59,479 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:57:59,480 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:02,612 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:02,614 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:02,667 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:02,668 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:02,670 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:02,670 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:02,670 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:02,670 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:02,670 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:02,670 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:02,671 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:02,672 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:02,786 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:03,724 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:03,724 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:04,297 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:04,299 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:04,301 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:04,302 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:58:04,302 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    664.196
  eval_steps_per_second   =      2.398
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.21it/s]
[INFO|training_args.py:1902] 2024-11-05 04:58:04,811 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:04,811 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:04,812 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:08,235 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:08,236 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:08,281 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:08,282 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:08,283 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:08,284 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:08,284 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:08,284 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:08,284 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:08,284 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:08,284 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:08,285 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:08,404 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:09,339 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:09,339 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:09,951 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:09,953 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:09,956 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:09,956 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:58:09,956 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6473
  eval_runtime            = 0:00:00.45
  eval_samples            =        277
  eval_samples_per_second =    612.944
  eval_steps_per_second   =      2.213
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.31it/s]
[INFO|training_args.py:1902] 2024-11-05 04:58:10,657 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:10,657 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:10,658 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:14,822 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:14,823 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:14,882 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:14,883 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:14,885 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:14,885 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:14,885 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:14,885 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:14,885 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:14,885 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:14,886 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:14,887 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:15,005 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:15,962 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:15,962 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:16,629 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:16,631 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:16,633 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:16,633 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:58:16,633 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =      0.419
  eval_runtime            = 0:00:00.65
  eval_samples            =        408
  eval_samples_per_second =    624.379
  eval_steps_per_second   =       1.53
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.43it/s]
[INFO|training_args.py:1902] 2024-11-05 04:58:17,342 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:17,343 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:17,343 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:20,402 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:20,404 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:20,455 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:20,456 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:20,457 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:20,457 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:20,457 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:20,457 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:20,457 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:20,457 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:20,458 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:20,458 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:20,530 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:21,464 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:21,464 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:22,081 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:22,083 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:22,085 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:22,086 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:58:22,086 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =      0.419
  eval_runtime            = 0:00:00.64
  eval_samples            =        408
  eval_samples_per_second =    628.418
  eval_steps_per_second   =       1.54
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.42it/s]
[INFO|training_args.py:1902] 2024-11-05 04:58:23,339 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:23,339 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:23,340 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:26,393 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:26,394 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:26,462 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:26,463 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:26,465 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:26,465 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:26,465 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:26,465 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:26,465 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:26,465 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:26,466 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:26,467 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:26,588 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:27,635 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:27,636 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:28,471 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:28,473 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:28,475 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:28,475 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:58:28,475 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4795
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    867.255
  eval_steps_per_second     =      0.832
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.87it/s]
[INFO|training_args.py:1902] 2024-11-05 04:58:29,739 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:29,739 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:29,740 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:32,942 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:32,943 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:33,107 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:33,108 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:33,110 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:33,110 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:33,110 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:33,110 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:33,110 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:33,111 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:33,111 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:33,112 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:33,224 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:34,159 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:34,159 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:34,788 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:34,790 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:34,792 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:34,793 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:58:34,793 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    850.087
  eval_steps_per_second     =      0.815
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.06it/s]
100% 3/3 [00:03<00:00,  1.05s/it]
100% 3/3 [00:03<00:00,  1.04s/it]
[INFO|training_args.py:1902] 2024-11-05 04:58:40,638 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:40,639 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:40,639 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:43,452 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:43,453 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:43,502 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:43,503 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:43,505 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:43,505 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:43,505 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:43,505 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:43,505 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:43,505 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:43,506 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:43,507 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:43,624 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:44,610 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:44,611 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:45,201 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:45,203 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:45,206 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:45,206 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 04:58:45,206 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.79
  eval_samples            =       5463
  eval_samples_per_second =    942.216
  eval_steps_per_second   =      0.517
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.18s/it]
100% 3/3 [00:03<00:00,  1.22s/it]
100% 3/3 [00:03<00:00,  1.23s/it]
[INFO|training_args.py:1902] 2024-11-05 04:58:51,556 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:51,557 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:51,558 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:58:54,459 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:54,460 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:58:54,508 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:54,509 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:54,512 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:54,512 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:54,512 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:54,512 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:54,512 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:58:54,512 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:58:54,513 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:58:54,514 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:58:54,630 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:58:55,563 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:58:55,563 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:58:56,172 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:58:56,174 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:58:56,177 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:58:56,177 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:58:56,177 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.28
  eval_samples            =       5463
  eval_samples_per_second =    868.739
  eval_steps_per_second   =      0.477
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.16it/s]
[INFO|training_args.py:1902] 2024-11-05 04:58:57,928 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:58:57,929 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:58:57,930 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:01,167 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:01,168 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:01,214 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:01,215 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:01,217 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:01,217 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:01,217 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:01,217 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:01,217 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:01,217 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:01,218 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:01,219 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:01,315 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:02,202 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:02,202 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:02,686 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:02,688 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:02,691 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:02,691 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 04:59:02,691 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6338
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.68
  eval_samples            =       1500
  eval_samples_per_second =    891.949
  eval_spearmanr          =     0.8757
  eval_steps_per_second   =      0.595
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 29.23it/s]
2024-11-05 04:59:04,506 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 04:59:04,508 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models rte: 0.6353790613718412
2024-11-05 04:59:04,509 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models mrpc: 0.87215411558669
2024-11-05 04:59:04,510 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 04:59:04,511 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 04:59:04,512 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models stsb: 0.8691223744814751
2024-11-05 04:59:04,513 - INFO - [diffusion][Epoch 11994] average DIFF reconstruction auto_encoder_models accuracy: 0.7891883917547569
2024-11-05 04:59:04,514 - INFO - [diffusion][Epoch 11994] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6353790613718412), ('mrpc', 0.87215411558669), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8691223744814751), ('stsb', 0.8691202856250678)]
2024-11-05 04:59:04,516 - INFO - [diffusion][Epoch 11994] ---------------------------------
2024-11-05 04:59:04,648 - INFO - [diffusion][Epoch 11994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:04,650 - INFO - [diffusion][Epoch 11995] Epoch 11996/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6347
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.77
  eval_samples            =       1500
  eval_samples_per_second =    844.751
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.563
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 04:59:07,614 - INFO - [diffusion][Epoch 11995] diffusion training Loss: 0.03547848854213953
2024-11-05 04:59:07,616 - INFO - [diffusion][Epoch 11995] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 04:59:19,729 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:19,730 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:19,732 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:23,067 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:23,068 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:23,125 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:23,126 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:23,128 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:23,128 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:23,128 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:23,128 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:23,128 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:23,128 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:23,129 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:23,130 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:23,261 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:24,301 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:24,301 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:24,825 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:24,828 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:24,830 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:24,830 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:59:24,830 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.87it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:25,952 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:25,953 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:25,953 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:28,625 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:28,627 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:28,676 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:28,677 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:28,679 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:28,679 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:28,679 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:28,679 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:28,679 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:28,679 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:28,680 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:28,681 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:28,801 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:29,764 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:29,764 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:30,347 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:30,349 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:30,352 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:30,352 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 04:59:30,352 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.05
  eval_samples            =        872
  eval_samples_per_second =     823.01
  eval_steps_per_second   =      0.944
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.11it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:31,486 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:31,486 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:31,487 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:35,235 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:35,236 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:35,287 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:35,288 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:35,290 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:35,290 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:35,290 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:35,290 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:35,290 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:35,290 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:35,291 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:35,292 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:35,406 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:36,348 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:36,348 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:36,996 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:36,998 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:37,001 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:37,001 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:59:37,001 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =     810.62
  eval_steps_per_second   =       0.93
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.61it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:37,469 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:37,469 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:37,470 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:40,458 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:40,459 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:40,509 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:40,510 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:40,512 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:40,512 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:40,512 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:40,512 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:40,512 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:40,512 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:40,513 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:40,514 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:40,639 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:41,643 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:41,644 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:42,482 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:42,484 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:42,486 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:42,486 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 04:59:42,486 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    674.793
  eval_steps_per_second   =      2.436
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 50.17it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:42,935 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:42,935 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:42,936 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:45,947 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:45,948 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:46,032 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:46,033 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:46,034 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:46,034 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:46,034 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:46,034 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:46,034 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:46,035 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:46,035 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:46,036 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:46,157 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:47,126 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:47,126 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:47,721 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:47,723 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:47,725 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:47,725 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:59:47,725 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6473
  eval_runtime            = 0:00:00.39
  eval_samples            =        277
  eval_samples_per_second =    697.422
  eval_steps_per_second   =      2.518
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.42it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:48,351 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:48,351 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:48,352 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:51,355 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:51,356 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:51,404 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:51,405 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:51,407 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:51,407 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:51,407 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:51,407 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:51,407 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:51,407 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:51,408 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:51,409 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:51,533 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:52,514 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:52,514 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:53,158 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:53,161 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:53,163 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:53,163 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 04:59:53,163 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4191
  eval_runtime            = 0:00:00.57
  eval_samples            =        408
  eval_samples_per_second =    704.702
  eval_steps_per_second   =      1.727
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.19it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:53,817 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:53,817 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:53,818 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 04:59:56,710 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:56,711 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 04:59:56,763 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:56,764 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:56,768 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:56,768 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:56,768 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:56,768 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:56,768 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 04:59:56,768 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 04:59:56,768 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 04:59:56,769 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 04:59:56,901 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 04:59:57,829 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 04:59:57,829 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 04:59:58,399 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 04:59:58,402 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 04:59:58,407 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 04:59:58,407 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 04:59:58,407 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =      0.419
  eval_runtime            = 0:00:00.58
  eval_samples            =        408
  eval_samples_per_second =     693.17
  eval_steps_per_second   =      1.699
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.03it/s]
[INFO|training_args.py:1902] 2024-11-05 04:59:59,800 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 04:59:59,801 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 04:59:59,802 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:00:02,721 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:02,723 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:00:02,777 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:02,778 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:02,780 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:02,780 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:02,780 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:02,780 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:02,780 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:02,780 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:00:02,781 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:02,782 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:00:02,914 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:00:03,867 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:00:03,867 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:00:04,584 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:00:04,587 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:00:04,589 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:00:04,589 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:00:04,589 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4795
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.32
  eval_samples              =       1043
  eval_samples_per_second   =    784.907
  eval_steps_per_second     =      0.753
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.44it/s]
[INFO|training_args.py:1902] 2024-11-05 05:00:05,864 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:00:05,864 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:00:05,865 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:00:08,998 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:08,999 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:00:09,047 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:09,048 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:09,050 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:09,050 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:09,050 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:09,050 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:09,050 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:09,050 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:00:09,051 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:09,052 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:00:09,159 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:00:10,110 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:00:10,111 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:00:10,674 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:00:10,676 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:00:10,678 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:00:10,678 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:00:10,678 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    849.786
  eval_steps_per_second     =      0.815
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.06it/s]
100% 3/3 [00:03<00:00,  1.08s/it]
100% 3/3 [00:03<00:00,  1.07s/it]
[INFO|training_args.py:1902] 2024-11-05 05:00:16,515 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:00:16,515 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:00:16,516 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:00:20,549 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:20,550 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:00:20,599 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:20,600 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:20,601 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:20,601 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:20,602 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:20,602 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:20,602 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:20,602 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:00:20,602 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:20,603 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:00:20,722 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:00:21,658 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:00:21,658 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:00:22,260 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:00:22,263 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:00:22,265 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:00:22,265 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:00:22,265 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.77
  eval_samples            =       5463
  eval_samples_per_second =    945.697
  eval_steps_per_second   =      0.519
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.07s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
[INFO|training_args.py:1902] 2024-11-05 05:00:28,637 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:00:28,637 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:00:28,638 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:00:31,548 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:31,549 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:00:31,596 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:31,597 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:31,599 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:31,599 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:31,599 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:31,599 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:31,599 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:31,599 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:00:31,600 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:31,601 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:00:31,724 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:00:32,694 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:00:32,694 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:00:33,334 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:00:33,336 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:00:33,342 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:00:33,343 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:00:33,343 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.32
  eval_samples            =       5463
  eval_samples_per_second =    863.768
  eval_steps_per_second   =      0.474
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.02it/s]
[INFO|training_args.py:1902] 2024-11-05 05:00:35,092 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:00:35,092 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:00:35,093 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:00:38,076 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:38,077 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:00:38,127 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:38,128 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:38,130 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:38,130 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:38,130 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:38,130 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:38,130 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:00:38,130 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:00:38,131 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:00:38,132 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:00:38,245 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:00:39,155 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:00:39,155 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:00:39,786 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:00:39,788 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:00:39,791 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:00:39,791 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:00:39,791 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6343
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.70
  eval_samples            =       1500
  eval_samples_per_second =    882.181
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.588
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.60it/s]
2024-11-05 05:00:41,465 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 05:00:41,467 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models rte: 0.6353790613718412
2024-11-05 05:00:41,468 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models mrpc: 0.87215411558669
2024-11-05 05:00:41,469 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 05:00:41,470 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 05:00:41,471 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models stsb: 0.8690906678884812
2024-11-05 05:00:41,472 - INFO - [diffusion][Epoch 11995] average DIFF reconstruction auto_encoder_models accuracy: 0.7891820239090998
2024-11-05 05:00:41,473 - INFO - [diffusion][Epoch 11995] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6353790613718412), ('mrpc', 0.87215411558669), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690755780701762), ('stsb', 0.8690906678884812)]
2024-11-05 05:00:41,475 - INFO - [diffusion][Epoch 11995] ---------------------------------
2024-11-05 05:00:41,593 - INFO - [diffusion][Epoch 11995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:41,594 - INFO - [diffusion][Epoch 11996] Epoch 11997/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6345
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.61
  eval_samples            =       1500
  eval_samples_per_second =    927.067
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.618
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 05:00:44,609 - INFO - [diffusion][Epoch 11996] diffusion training Loss: 0.034663135185837746
2024-11-05 05:00:44,611 - INFO - [diffusion][Epoch 11996] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 05:00:56,871 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:00:56,872 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:00:56,874 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:00,995 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:00,996 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:01,052 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:01,054 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:01,057 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:01,057 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:01,057 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:01,057 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:01,057 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:01,057 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:01,058 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:01,059 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:01,189 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:02,172 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:02,172 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:02,830 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:02,832 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:02,835 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:02,835 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:01:02,835 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.67it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:03,974 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:03,975 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:03,976 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:06,898 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:06,899 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:06,948 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:06,949 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:06,951 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:06,951 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:06,951 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:06,951 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:06,951 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:06,951 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:06,952 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:06,952 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:07,084 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:08,016 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:08,021 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:08,629 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:08,631 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:08,634 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:08,634 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:01:08,634 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.08
  eval_samples            =        872
  eval_samples_per_second =    804.561
  eval_steps_per_second   =      0.923
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.99it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:09,761 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:09,762 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:09,762 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:12,456 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:12,457 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:12,508 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:12,509 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:12,511 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:12,511 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:12,511 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:12,511 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:12,511 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:12,511 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:12,512 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:12,513 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:12,648 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:13,576 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:13,576 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:14,096 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:14,098 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:14,100 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:14,100 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:01:14,100 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    814.279
  eval_steps_per_second   =      0.934
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.37it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:14,620 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:14,620 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:14,621 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:17,205 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:17,206 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:17,264 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:17,265 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:17,267 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:17,267 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:17,267 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:17,267 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:17,267 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:17,267 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:17,268 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:17,268 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:17,391 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:18,319 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:18,319 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:19,014 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:19,016 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:19,018 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:19,018 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:01:19,018 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6469
  eval_runtime            = 0:00:00.46
  eval_samples            =        277
  eval_samples_per_second =    600.564
  eval_steps_per_second   =      2.168
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.52it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:19,717 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:19,718 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:19,719 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:22,544 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:22,545 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:22,594 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:22,595 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:22,597 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:22,597 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:22,597 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:22,597 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:22,597 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:22,597 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:22,598 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:22,599 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:22,716 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:23,666 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:23,667 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:24,228 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:24,230 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:24,233 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:24,233 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:01:24,233 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6471
  eval_runtime            = 0:00:00.64
  eval_samples            =        277
  eval_samples_per_second =    432.276
  eval_steps_per_second   =      1.561
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.55it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:24,872 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:24,873 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:24,874 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:27,456 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:27,457 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:27,514 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:27,515 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:27,517 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:27,518 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:27,518 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:27,518 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:27,518 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:27,518 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:27,518 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:27,519 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:27,637 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:28,595 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:28,595 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:29,193 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:29,195 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:29,197 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:29,197 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:01:29,197 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.57
  eval_samples            =        408
  eval_samples_per_second =    705.278
  eval_steps_per_second   =      1.729
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 29.48it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:29,768 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:29,769 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:29,770 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:32,818 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:32,819 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:32,867 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:32,868 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:32,870 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:32,870 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:32,870 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:32,870 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:32,871 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:32,871 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:32,871 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:32,872 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:33,002 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:33,936 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:33,941 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:34,605 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:34,607 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:34,609 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:34,609 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:01:34,609 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.49
  eval_samples            =        408
  eval_samples_per_second =    816.734
  eval_steps_per_second   =      2.002
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.60it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:35,880 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:35,881 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:35,881 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:38,888 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:38,889 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:38,939 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:38,940 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:38,942 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:38,942 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:38,942 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:38,942 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:38,942 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:38,942 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:38,943 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:38,944 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:39,083 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:40,015 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:40,015 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:40,553 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:40,555 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:40,556 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:40,557 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:01:40,557 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.21
  eval_samples              =       1043
  eval_samples_per_second   =    860.596
  eval_steps_per_second     =      0.825
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.16it/s]
[INFO|training_args.py:1902] 2024-11-05 05:01:41,747 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:41,748 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:41,748 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:44,996 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:44,997 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:45,042 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:45,043 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:45,045 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:45,045 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:45,045 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:45,045 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:45,045 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:45,045 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:45,046 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:45,047 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:45,181 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:46,142 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:46,143 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:46,737 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:46,739 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:46,742 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:46,742 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:01:46,742 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.13
  eval_samples              =       1043
  eval_samples_per_second   =    920.124
  eval_steps_per_second     =      0.882
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.01it/s]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
[INFO|training_args.py:1902] 2024-11-05 05:01:52,506 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:01:52,507 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:01:52,507 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:01:55,891 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:55,892 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:01:55,964 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:55,965 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:55,966 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:55,966 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:55,966 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:55,966 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:55,966 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:01:55,966 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:01:55,967 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:01:55,968 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:01:56,089 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:01:56,992 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:01:56,993 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:01:57,584 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:01:57,586 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:01:57,587 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:01:57,587 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:01:57,587 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.69
  eval_samples            =       5463
  eval_samples_per_second =    959.009
  eval_steps_per_second   =      0.527
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.05s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 05:02:03,572 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:03,572 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:03,573 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:06,849 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:06,850 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:06,897 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:06,898 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:06,902 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:06,902 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:06,902 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:06,902 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:06,902 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:06,902 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:06,902 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:06,903 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:07,041 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:08,016 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:08,017 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:02:08,638 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:02:08,641 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:02:08,643 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:02:08,643 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:02:08,643 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.92
  eval_samples            =       5463
  eval_samples_per_second =    922.234
  eval_steps_per_second   =      0.506
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.27it/s]
[INFO|training_args.py:1902] 2024-11-05 05:02:10,446 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:10,446 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:10,447 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:15,025 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:15,026 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:15,098 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:15,099 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:15,101 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:15,101 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:15,101 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:15,101 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:15,101 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:15,101 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:15,101 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:15,102 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:15,215 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:16,152 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:16,152 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:02:16,716 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:02:16,718 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:02:16,720 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:02:16,720 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:02:16,720 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =      0.634
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.73
  eval_samples            =       1500
  eval_samples_per_second =    863.948
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.576
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.36it/s]
2024-11-05 05:02:18,430 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 05:02:18,432 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models rte: 0.6353790613718412
2024-11-05 05:02:18,433 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models mrpc: 0.87215411558669
2024-11-05 05:02:18,434 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 05:02:18,435 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 05:02:18,436 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models stsb: 0.8691516128813391
2024-11-05 05:02:18,437 - INFO - [diffusion][Epoch 11996] average DIFF reconstruction auto_encoder_models accuracy: 0.7894889919090552
2024-11-05 05:02:18,439 - INFO - [diffusion][Epoch 11996] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6353790613718412), ('rte', 0.6353790613718412), ('mrpc', 0.87215411558669), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690881407735337), ('stsb', 0.8691516128813391)]
2024-11-05 05:02:18,440 - INFO - [diffusion][Epoch 11996] ---------------------------------
2024-11-05 05:02:18,562 - INFO - [diffusion][Epoch 11996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:18,563 - INFO - [diffusion][Epoch 11997] Epoch 11998/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6337
  eval_pearson            =     0.8692
  eval_runtime            = 0:00:01.65
  eval_samples            =       1500
  eval_samples_per_second =    905.514
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.604
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 05:02:21,808 - INFO - [diffusion][Epoch 11997] diffusion training Loss: 0.03563082590699196
2024-11-05 05:02:21,810 - INFO - [diffusion][Epoch 11997] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 05:02:34,639 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:34,640 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:34,641 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:37,354 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:37,355 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:37,416 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:37,417 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:37,419 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:37,419 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:37,419 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:37,419 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:37,419 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:37,419 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:37,420 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:37,421 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:37,552 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:38,521 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:38,521 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:02:39,174 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:02:39,176 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:02:39,178 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:02:39,178 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:02:39,178 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.79it/s]
[INFO|training_args.py:1902] 2024-11-05 05:02:40,312 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:40,312 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:40,313 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:43,009 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:43,010 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:43,055 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:43,056 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:43,059 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:43,059 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:43,059 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:43,059 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:43,059 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:43,059 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:43,059 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:43,060 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:43,176 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:44,130 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:44,133 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:02:44,661 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:02:44,663 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:02:44,665 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:02:44,665 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:02:44,665 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.08
  eval_samples            =        872
  eval_samples_per_second =    806.475
  eval_steps_per_second   =      0.925
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.55it/s]
[INFO|training_args.py:1902] 2024-11-05 05:02:45,748 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:45,748 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:45,749 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:48,806 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:48,807 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:48,853 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:48,854 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:48,856 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:48,856 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:48,856 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:48,856 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:48,856 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:48,856 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:48,857 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:48,858 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:49,006 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:49,949 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:49,949 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:02:50,593 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:02:50,595 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:02:50,597 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:02:50,598 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:02:50,598 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.02
  eval_samples            =        872
  eval_samples_per_second =     851.76
  eval_steps_per_second   =      0.977
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.24it/s]
[INFO|training_args.py:1902] 2024-11-05 05:02:51,085 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:51,086 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:51,086 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:53,737 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:53,738 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:53,789 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:53,790 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:53,792 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:53,792 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:53,792 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:53,792 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:53,792 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:53,792 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:53,793 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:53,794 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:53,906 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:54,844 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:54,844 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:02:55,409 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:02:55,411 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:02:55,413 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:02:55,413 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:02:55,413 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6469
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =    642.959
  eval_steps_per_second   =      2.321
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.67it/s]
[INFO|training_args.py:1902] 2024-11-05 05:02:56,125 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:02:56,126 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:02:56,127 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:02:58,875 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:58,876 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:02:58,924 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:58,925 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:58,927 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:58,927 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:58,927 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:58,927 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:58,927 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:02:58,927 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:02:58,928 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:02:58,929 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:02:59,043 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:02:59,975 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:02:59,976 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:00,582 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:00,583 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:00,585 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:00,585 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:03:00,585 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.639
  eval_loss               =     0.6471
  eval_runtime            = 0:00:00.65
  eval_samples            =        277
  eval_samples_per_second =    422.482
  eval_steps_per_second   =      1.525
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.57it/s]
[INFO|training_args.py:1902] 2024-11-05 05:03:01,172 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:01,173 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:01,173 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:04,207 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:04,208 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:04,285 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:04,286 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:04,288 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:04,288 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:04,288 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:04,289 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:04,289 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:04,289 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:04,289 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:04,290 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:04,406 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:05,393 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:05,394 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:05,976 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:05,978 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:05,980 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:05,980 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:03:05,980 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8235
  eval_combined_score     =     0.8488
  eval_f1                 =     0.8741
  eval_loss               =     0.4191
  eval_runtime            = 0:00:00.53
  eval_samples            =        408
  eval_samples_per_second =    769.579
  eval_steps_per_second   =      1.886
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.26it/s]
[INFO|training_args.py:1902] 2024-11-05 05:03:06,632 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:06,632 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:06,633 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:10,362 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:10,363 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:10,411 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:10,412 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:10,416 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:10,416 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:10,416 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:10,416 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:10,416 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:10,416 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:10,416 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:10,417 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:10,542 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:11,451 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:11,451 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:12,143 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:12,145 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:12,147 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:12,147 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:03:12,147 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =      0.419
  eval_runtime            = 0:00:00.59
  eval_samples            =        408
  eval_samples_per_second =    691.227
  eval_steps_per_second   =      1.694
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.44it/s]
[INFO|training_args.py:1902] 2024-11-05 05:03:13,649 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:13,650 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:13,650 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:16,267 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:16,268 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:16,317 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:16,318 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:16,320 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:16,320 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:16,320 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:16,320 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:16,320 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:16,320 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:16,321 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:16,322 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:16,464 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:17,429 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:17,429 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:18,039 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:18,042 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:18,046 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:18,046 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:03:18,046 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.43
  eval_samples              =       1043
  eval_samples_per_second   =    725.294
  eval_steps_per_second     =      0.695
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.15it/s]
[INFO|training_args.py:1902] 2024-11-05 05:03:19,293 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:19,293 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:19,294 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:22,017 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:22,018 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:22,069 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:22,070 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:22,072 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:22,072 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:22,072 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:22,072 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:22,072 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:22,072 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:22,073 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:22,074 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:22,195 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:23,146 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:23,147 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:23,723 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:23,725 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:23,727 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:23,727 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:03:23,727 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.18
  eval_samples              =       1043
  eval_samples_per_second   =    878.559
  eval_steps_per_second     =      0.842
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 05:03:29,843 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:29,843 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:29,844 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:32,269 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:32,271 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:32,317 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:32,318 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:32,320 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:32,320 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:32,320 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:32,320 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:32,320 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:32,320 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:32,320 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:32,321 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:32,440 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:33,344 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:33,344 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:34,162 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:34,165 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:34,167 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:34,167 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:03:34,167 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.05
  eval_samples            =       5463
  eval_samples_per_second =    902.573
  eval_steps_per_second   =      0.496
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.07it/s]
100% 3/3 [00:03<00:00,  1.08s/it]
100% 3/3 [00:03<00:00,  1.06s/it]
[INFO|training_args.py:1902] 2024-11-05 05:03:40,111 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:40,112 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:40,113 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:42,938 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:42,939 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:42,990 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:42,991 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:42,993 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:42,993 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:42,993 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:42,993 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:42,993 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:42,993 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:42,994 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:42,995 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:43,137 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:44,073 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:44,074 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:44,637 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:44,639 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:44,642 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:44,642 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:03:44,642 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.88
  eval_samples            =       5463
  eval_samples_per_second =    928.431
  eval_steps_per_second   =       0.51
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.02it/s]
[INFO|training_args.py:1902] 2024-11-05 05:03:46,270 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:03:46,270 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:03:46,271 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:03:49,166 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:49,168 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:03:49,241 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:49,242 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:49,244 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:49,244 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:49,244 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:49,244 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:49,245 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:03:49,245 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:03:49,245 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:03:49,246 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:03:49,368 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:03:50,337 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:03:50,337 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:03:50,906 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:03:50,908 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:03:50,911 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:03:50,911 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:03:50,911 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6344
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.56
  eval_samples            =       1500
  eval_samples_per_second =    956.627
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.638
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.60it/s]
2024-11-05 05:03:52,709 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 05:03:52,711 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models rte: 0.6389891696750902
2024-11-05 05:03:52,712 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models mrpc: 0.8741258741258742
2024-11-05 05:03:52,717 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 05:03:52,718 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 05:03:52,719 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models stsb: 0.8690979564446125
2024-11-05 05:03:52,720 - INFO - [diffusion][Epoch 11997] average DIFF reconstruction auto_encoder_models accuracy: 0.7896468990866895
2024-11-05 05:03:52,722 - INFO - [diffusion][Epoch 11997] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.6389891696750902), ('mrpc', 0.8741258741258742), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690649248026886), ('stsb', 0.8690979564446125)]
2024-11-05 05:03:52,723 - INFO - [diffusion][Epoch 11997] ---------------------------------
2024-11-05 05:03:52,840 - INFO - [diffusion][Epoch 11997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:52,841 - INFO - [diffusion][Epoch 11998] Epoch 11999/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6348
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.74
  eval_samples            =       1500
  eval_samples_per_second =    858.799
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.573
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 05:03:55,733 - INFO - [diffusion][Epoch 11998] diffusion training Loss: 0.039682790637016296
2024-11-05 05:03:55,734 - INFO - [diffusion][Epoch 11998] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 05:04:08,022 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:08,023 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:08,024 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:11,125 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:11,126 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:11,179 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:11,180 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:11,184 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:11,184 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:11,184 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:11,184 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:11,184 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:11,184 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:11,185 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:11,185 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:11,319 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:12,280 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:12,280 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:13,818 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:13,819 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:13,821 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:13,821 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:04:13,821 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 21.64it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:14,893 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:14,893 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:14,894 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:18,042 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:18,043 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:18,092 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:18,093 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:18,094 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:18,095 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:18,095 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:18,095 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:18,095 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:18,095 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:18,095 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:18,096 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:18,216 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:19,170 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:19,170 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:19,791 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:19,793 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:19,795 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:19,795 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:04:19,795 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.01
  eval_samples            =        872
  eval_samples_per_second =    861.919
  eval_steps_per_second   =      0.988
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.64it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:21,012 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:21,012 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:21,013 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:23,798 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:23,800 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:23,857 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:23,859 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:23,861 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:23,861 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:23,861 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:23,861 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:23,861 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:23,861 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:23,862 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:23,862 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:23,984 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:24,937 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:24,938 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:25,460 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:25,462 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:25,464 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:25,465 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:04:25,465 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.15
  eval_samples            =        872
  eval_samples_per_second =    755.766
  eval_steps_per_second   =      0.867
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.37it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:25,941 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:25,941 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:25,942 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:28,885 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:28,886 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:28,937 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:28,938 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:28,939 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:28,939 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:28,939 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:28,940 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:28,940 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:28,940 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:28,940 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:28,941 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:29,038 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:29,948 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:29,948 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:30,561 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:30,563 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:30,565 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:30,565 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:04:30,566 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.639
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    664.669
  eval_steps_per_second   =        2.4
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.10it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:31,254 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:31,254 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:31,255 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:34,135 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:34,136 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:34,194 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:34,195 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:34,197 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:34,197 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:34,197 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:34,197 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:34,197 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:34,197 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:34,197 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:34,198 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:34,308 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:35,215 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:35,216 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:35,913 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:35,915 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:35,918 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:35,918 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:04:35,918 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6472
  eval_runtime            = 0:00:00.63
  eval_samples            =        277
  eval_samples_per_second =    439.561
  eval_steps_per_second   =      1.587
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.88it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:36,554 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:36,555 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:36,555 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:39,594 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:39,595 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:39,641 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:39,642 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:39,644 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:39,644 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:39,644 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:39,644 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:39,644 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:39,644 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:39,644 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:39,645 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:39,776 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:40,751 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:40,751 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:41,310 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:41,312 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:41,315 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:41,315 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:04:41,315 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8235
  eval_combined_score     =     0.8486
  eval_f1                 =     0.8737
  eval_loss               =     0.4188
  eval_runtime            = 0:00:00.58
  eval_samples            =        408
  eval_samples_per_second =    693.928
  eval_steps_per_second   =      1.701
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]
  0% 0/1 [00:00<?, ?it/s]

100% 1/1 [00:00<00:00, 44.21it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:41,893 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:41,893 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:41,894 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:44,974 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:44,975 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:45,037 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:45,038 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:45,040 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:45,040 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:45,040 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:45,040 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:45,040 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:45,040 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:45,041 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:45,042 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:45,163 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:46,105 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:46,105 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:46,680 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:46,682 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:46,684 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:46,684 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:04:46,684 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =      0.419
  eval_runtime            = 0:00:00.52
  eval_samples            =        408
  eval_samples_per_second =    779.645
  eval_steps_per_second   =      1.911
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.27it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:47,945 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:47,945 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:47,946 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:51,131 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:51,132 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:51,191 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:51,192 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:51,194 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:51,194 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:51,194 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:51,194 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:51,194 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:51,194 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:51,194 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:51,195 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:51,313 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:52,302 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:52,303 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:52,842 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:52,844 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:52,846 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:52,846 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:04:52,846 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4795
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    868.304
  eval_steps_per_second     =      0.833
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.41it/s]
[INFO|training_args.py:1902] 2024-11-05 05:04:54,002 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:04:54,003 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:04:54,003 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:04:56,751 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:56,752 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:04:56,807 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:56,808 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:56,810 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:56,810 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:56,810 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:56,810 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:56,810 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:04:56,810 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:04:56,810 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:04:56,811 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:04:56,926 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:04:57,884 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:04:57,884 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:04:58,459 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:04:58,461 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:04:58,463 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:04:58,463 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:04:58,463 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.10
  eval_samples              =       1043
  eval_samples_per_second   =    944.509
  eval_steps_per_second     =      0.906
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.02it/s]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
[INFO|training_args.py:1902] 2024-11-05 05:05:04,326 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:05:04,326 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:05:04,327 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:05:07,164 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:07,165 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:05:07,215 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:07,216 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:07,218 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:07,218 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:07,218 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:07,218 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:07,218 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:07,218 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:05:07,219 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:07,220 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:05:07,358 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:05:08,312 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:05:08,312 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:05:09,033 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:05:09,035 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:05:09,038 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:05:09,038 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:05:09,038 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:05.80
  eval_samples            =       5463
  eval_samples_per_second =    941.362
  eval_steps_per_second   =      0.517
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
[INFO|training_args.py:1902] 2024-11-05 05:05:15,153 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:05:15,154 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:05:15,154 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:05:18,109 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:18,110 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:05:18,163 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:18,164 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:18,168 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:18,168 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:18,168 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:18,168 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:18,168 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:18,168 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:05:18,168 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:18,169 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:05:18,294 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:05:19,235 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:05:19,235 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:05:19,860 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:05:19,862 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:05:19,864 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:05:19,864 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:05:19,864 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.06
  eval_samples            =       5463
  eval_samples_per_second =    900.615
  eval_steps_per_second   =      0.495
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.67it/s]
[INFO|training_args.py:1902] 2024-11-05 05:05:21,720 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:05:21,720 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:05:21,721 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:05:24,763 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:24,764 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:05:24,812 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:24,813 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:24,815 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:24,815 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:24,815 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:24,815 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:24,815 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:24,815 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:05:24,815 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:24,816 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:05:24,940 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:05:25,859 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:05:25,859 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:05:26,555 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:05:26,557 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:05:26,559 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:05:26,559 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:05:26,559 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =      0.634
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.79
  eval_samples            =       1500
  eval_samples_per_second =     835.76
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.557
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.62it/s]
2024-11-05 05:05:28,440 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 05:05:28,442 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models rte: 0.6389891696750902
2024-11-05 05:05:28,443 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models mrpc: 0.8736842105263158
2024-11-05 05:05:28,444 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 05:05:28,445 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 05:05:28,446 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models stsb: 0.8691289308940601
2024-11-05 05:05:28,447 - INFO - [diffusion][Epoch 11998] average DIFF reconstruction auto_encoder_models accuracy: 0.7899136644345903
2024-11-05 05:05:28,448 - INFO - [diffusion][Epoch 11998] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8736842105263158), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8690666898243592), ('stsb', 0.8691289308940601)]
2024-11-05 05:05:28,449 - INFO - [diffusion][Epoch 11998] ---------------------------------
2024-11-05 05:05:28,566 - INFO - [diffusion][Epoch 11998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:28,567 - INFO - [diffusion][Epoch 11999] Epoch 12000/12000
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6341
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.82
  eval_samples            =       1500
  eval_samples_per_second =    821.418
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.548
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 05:05:31,558 - INFO - [diffusion][Epoch 11999] diffusion training Loss: 0.04013267159461975
2024-11-05 05:05:31,560 - INFO - [diffusion][Epoch 11999] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 05:05:43,949 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:05:43,952 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:05:43,954 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:05:46,807 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:46,809 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:05:46,859 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:46,860 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:46,862 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:46,862 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:46,862 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:46,862 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:46,862 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:46,862 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:05:46,863 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:46,863 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:05:46,998 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:05:47,941 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:05:47,941 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:05:48,540 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:05:48,542 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:05:48,545 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:05:48,545 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:05:48,545 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.59it/s]
[INFO|training_args.py:1902] 2024-11-05 05:05:49,621 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:05:49,621 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:05:49,622 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:05:52,798 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:52,801 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:05:52,851 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:52,852 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:52,853 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:52,854 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:52,854 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:52,854 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:52,854 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:52,854 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:05:52,854 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:52,855 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:05:52,980 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:05:53,939 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:05:53,940 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:05:54,518 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:05:54,520 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:05:54,523 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:05:54,523 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 05:05:54,523 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.01
  eval_samples            =        872
  eval_samples_per_second =    859.866
  eval_steps_per_second   =      0.986
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.08it/s]
[INFO|training_args.py:1902] 2024-11-05 05:05:55,737 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:05:55,738 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:05:55,739 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:05:58,400 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:58,401 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:05:58,470 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:58,471 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:58,473 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:58,473 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:58,473 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:58,473 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:58,473 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:05:58,473 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:05:58,474 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:05:58,474 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:05:58,599 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:05:59,546 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:05:59,546 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:00,131 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:00,134 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:00,136 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:00,136 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:06:00,136 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9369
  eval_loss               =     0.1875
  eval_runtime            = 0:00:01.15
  eval_samples            =        872
  eval_samples_per_second =    757.469
  eval_steps_per_second   =      0.869
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.24it/s]
[INFO|training_args.py:1902] 2024-11-05 05:06:00,598 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:00,598 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:00,599 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:03,401 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:03,402 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:03,462 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:03,463 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:03,465 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:03,465 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:03,465 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:03,465 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:03,465 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:03,466 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:03,466 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:03,467 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:03,599 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:04,845 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:04,846 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:05,457 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:05,459 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:05,462 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:05,462 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 05:06:05,462 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6468
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    688.791
  eval_steps_per_second   =      2.487
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.50it/s]
[INFO|training_args.py:1902] 2024-11-05 05:06:06,021 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:06,021 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:06,021 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:08,997 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:08,998 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:09,061 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:09,062 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:09,064 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:09,064 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:09,064 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:09,064 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:09,064 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:09,064 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:09,065 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:09,066 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:09,197 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:10,139 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:10,139 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:10,874 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:10,876 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:10,878 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:10,878 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:06:10,878 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6318
  eval_loss               =     0.6471
  eval_runtime            = 0:00:00.51
  eval_samples            =        277
  eval_samples_per_second =    539.691
  eval_steps_per_second   =      1.948
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.37it/s]
[INFO|training_args.py:1902] 2024-11-05 05:06:11,501 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:11,502 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:11,502 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:15,833 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:15,835 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:15,886 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:15,887 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:15,890 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:15,890 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:15,890 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:15,890 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:15,890 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:15,890 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:15,890 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:15,891 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:16,034 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:16,971 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:16,972 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:17,534 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:17,536 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:17,539 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:17,539 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 05:06:17,539 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.8235
  eval_combined_score     =     0.8486
  eval_f1                 =     0.8737
  eval_loss               =     0.4188
  eval_runtime            = 0:00:00.55
  eval_samples            =        408
  eval_samples_per_second =    733.628
  eval_steps_per_second   =      1.798
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

100% 1/1 [00:00<00:00, 37.09it/s]
[INFO|training_args.py:1902] 2024-11-05 05:06:18,154 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:18,155 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:18,156 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:20,984 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:20,985 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:21,039 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:21,040 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:21,044 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:21,044 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:21,044 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:21,044 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:21,044 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:21,044 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:21,044 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:21,045 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:21,196 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:22,148 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:22,149 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:22,765 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:22,768 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:22,770 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:22,770 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:06:22,770 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8211
  eval_combined_score     =     0.8466
  eval_f1                 =     0.8722
  eval_loss               =     0.4189
  eval_runtime            = 0:00:00.55
  eval_samples            =        408
  eval_samples_per_second =    735.148
  eval_steps_per_second   =      1.802
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.22it/s]
[INFO|training_args.py:1902] 2024-11-05 05:06:24,217 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:24,218 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:24,219 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:27,352 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:27,353 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:27,404 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:27,405 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:27,407 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:27,407 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:27,407 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:27,407 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:27,407 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:27,408 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:27,408 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:27,409 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:27,534 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:28,492 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:28,492 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:29,150 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:29,152 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:29,155 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:29,155 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 05:06:29,155 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4795
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.38
  eval_samples              =       1043
  eval_samples_per_second   =    753.857
  eval_steps_per_second     =      0.723
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.22it/s]
[INFO|training_args.py:1902] 2024-11-05 05:06:30,455 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:30,455 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:30,456 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:38,744 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:38,745 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:38,793 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:38,794 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:38,796 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:38,796 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:38,796 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:38,796 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:38,796 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:38,796 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:38,796 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:38,797 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:38,942 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:39,888 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:39,889 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:40,486 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:40,488 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:40,490 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:40,490 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:06:40,490 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4794
  eval_matthews_correlation =      0.499
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    848.622
  eval_steps_per_second     =      0.814
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.05s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
[INFO|training_args.py:1902] 2024-11-05 05:06:46,700 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:46,701 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:46,702 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:06:49,631 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:49,632 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:06:49,686 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:49,687 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:49,689 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:49,689 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:49,689 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:49,689 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:49,689 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:06:49,689 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:06:49,690 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:06:49,691 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:06:49,813 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:06:50,728 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:06:50,728 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:06:51,278 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:06:51,280 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: question, idx, sentence. If question, idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:06:51,283 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:06:51,283 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 05:06:51,283 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.15
  eval_samples            =       5463
  eval_samples_per_second =    887.147
  eval_steps_per_second   =      0.487
trainable params: 0 || all params: 124,794,626 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.04s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 05:06:57,722 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:06:57,723 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:06:57,724 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:07:00,725 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:07:00,726 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:07:00,779 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:07:00,780 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:00,782 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:00,782 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:00,782 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:00,782 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:00,782 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:00,782 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:07:00,783 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:07:00,783 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:07:00,925 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:07:01,862 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:07:01,862 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:07:02,444 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:07:02,447 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:07:02,449 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:07:02,449 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:07:02,449 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =      0.201
  eval_runtime            = 0:00:06.36
  eval_samples            =       5463
  eval_samples_per_second =    857.717
  eval_steps_per_second   =      0.471
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.17it/s]
[INFO|training_args.py:1902] 2024-11-05 05:07:04,347 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 05:07:04,348 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 05:07:04,349 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 05:07:06,914 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:07:06,915 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 05:07:06,964 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:07:06,965 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:06,967 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:06,967 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:06,967 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:06,967 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:06,967 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 05:07:06,967 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 05:07:06,968 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 05:07:06,969 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 05:07:07,093 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 05:07:08,002 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 05:07:08,002 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 05:07:08,662 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 05:07:08,664 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 05:07:08,667 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 05:07:08,667 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 05:07:08,667 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8724
  eval_loss               =     0.6335
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.83
  eval_samples            =       1500
  eval_samples_per_second =    818.006
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.545
trainable params: 0 || all params: 124,793,857 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.87it/s]
2024-11-05 05:07:10,461 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 05:07:10,463 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models rte: 0.631768953068592
2024-11-05 05:07:10,464 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models mrpc: 0.8736842105263158
2024-11-05 05:07:10,465 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 05:07:10,466 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 05:07:10,467 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models stsb: 0.8691105115823492
2024-11-05 05:07:10,468 - INFO - [diffusion][Epoch 11999] average DIFF reconstruction auto_encoder_models accuracy: 0.7890127204613347
2024-11-05 05:07:10,469 - INFO - [diffusion][Epoch 11999] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.631768953068592), ('mrpc', 0.8736842105263158), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8691105115823492), ('stsb', 0.8691041063667498)]
2024-11-05 05:07:10,470 - INFO - [diffusion][Epoch 11999] ---------------------------------
2024-11-05 05:07:10,585 - INFO - [diffusion][Epoch 11999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:10,587 - INFO - [diffusion][Epoch 11999] Training complete
***** eval metrics *****
  eval_combined_score     =     0.8723
  eval_loss               =     0.6337
  eval_pearson            =     0.8691
  eval_runtime            = 0:00:01.75
  eval_samples            =       1500
  eval_samples_per_second =    856.994
  eval_spearmanr          =     0.8756
  eval_steps_per_second   =      0.571
=====================================
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: | 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: diffusion learning rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    diffusion train_loss ▆▇▅▆▆█▆▆▅▆▅▆▅▆▃▅▅▅▅▃▅▅▃▄█▄▃▄▃▄▂▅▃▂▃▁▅▂▃▁
wandb: 
wandb: Run summary:
wandb: diffusion learning rate 0.001
wandb:    diffusion train_loss 0.04013
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/jin509/para_diff/lora-cond-p-diff/wandb/offline-run-20241105_002407-23x2ynvt
wandb: Find logs at: ./wandb/offline-run-20241105_002407-23x2ynvt/logs











2024-11-05 05:07:10,461 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models sst2: 0.9369266055045872
2024-11-05 05:07:10,463 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models rte: 0.631768953068592
2024-11-05 05:07:10,464 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models mrpc: 0.8736842105263158
2024-11-05 05:07:10,465 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models cola: 0.49895377962487897
2024-11-05 05:07:10,466 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 05:07:10,467 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models stsb: 0.8691105115823492
2024-11-05 05:07:10,468 - INFO - [diffusion][Epoch 11999] average DIFF reconstruction auto_encoder_models accuracy: 0.7890127204613347
2024-11-05 05:07:10,469 - INFO - [diffusion][Epoch 11999] Diffusion reconstruction accuracy:[('sst2', 0.9369266055045872), ('sst2', 0.9369266055045872), ('rte', 0.631768953068592), ('rte', 0.631768953068592), ('mrpc', 0.8736842105263158), ('mrpc', 0.87215411558669), ('cola', 0.49895377962487897), ('cola', 0.49895377962487897), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8691105115823492), ('stsb', 0.8691041063667498)]
2024-11-05 05:07:10,470 - INFO - [diffusion][Epoch 11999] ---------------------------------
2024-11-05 05:07:10,585 - INFO - [diffusion][Epoch 11999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_2/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:10,587 - INFO - [diffusion][Epoch 11999] Training complete