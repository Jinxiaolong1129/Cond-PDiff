stty: 'standard input': Inappropriate ioctl for device
2024-11-04 19:02:45,555 - INFO - CUDA_VISIBLE_DEVICES: 3,2,1,0
2024-11-04 19:02:45,555 - INFO - layer_num: [[11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]
2024-11-04 19:02:45,555 - INFO - datasets_para: [['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']]
2024-11-04 19:02:45,555 - INFO - gpu_id: 0
2024-11-04 19:02:45,555 - INFO - batch_size: 256
2024-11-04 19:02:45,555 - INFO - epochs: 10000
2024-11-04 19:02:45,555 - INFO - ae_test: True
2024-11-04 19:02:45,555 - INFO - load_ae_checkpoint: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_8999.pth
2024-11-04 19:02:45,555 - INFO - lr: 0.001
2024-11-04 19:02:45,555 - INFO - patience: 5000
transformers.__file__: /home/jin509/para_diff/lora-cond-p-diff/src/transformers/__init__.py 
 peft.__file_: /home/jin509/para_diff/lora-cond-p-diff/src/peft/__init__.py
=====================================
layer number: [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
datasets: ['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']
=====================================
wandb: Tracking run with wandb version 0.12.21
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dataset_data = torch.tensor(dataset_matrix, dtype=torch.float32)
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return (torch.tensor(data_row, dtype=torch.float32) - mean) / std
2024-11-04 19:02:53,653 - INFO - Load model: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_8999.pth
2024-11-04 19:02:53,654 - INFO - Start epoch: 8999
2024-11-04 19:02:53,744 - INFO - Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_8999.pth with best metric -inf
2024-11-04 19:02:53,750 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/model
2024-11-04 19:02:53,750 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/log
2024-11-04 19:02:53,751 - INFO - [auto encoder][Epoch 0] Testing started!
2024-11-04 19:02:53,763 - INFO - ---------------------------------
2024-11-04 19:02:53,763 - INFO - Test the input auto_encoder_model
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:02:53,805 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:02:53,809 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
=====================================
Test the auto_encoder_model

config.json:   0% 0.00/474 [00:00<?, ?B/s]
config.json: 100% 474/474 [00:00<00:00, 37.3kB/s]
[INFO|configuration_utils.py:728] 2024-11-04 19:02:57,914 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:02:57,919 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}


tokenizer_config.json:   0% 0.00/52.0 [00:00<?, ?B/s]
tokenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 22.9kB/s]
[INFO|configuration_utils.py:728] 2024-11-04 19:02:58,091 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:02:58,094 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}


vocab.json:   0% 0.00/899k [00:00<?, ?B/s]
vocab.json: 100% 899k/899k [00:00<00:00, 9.49MB/s]

merges.txt:   0% 0.00/456k [00:00<?, ?B/s]
merges.txt: 100% 456k/456k [00:00<00:00, 22.0MB/s]
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:02:58,716 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:02:58,717 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:02:58,717 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:02:58,717 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:02:58,717 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:02:58,717 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:02:58,719 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:02:58,721 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:02:59,176 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:02:59,177 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}


pytorch_model.bin:   0% 0.00/559M [00:00<?, ?B/s]
pytorch_model.bin:   2% 10.5M/559M [00:00<00:05, 95.2MB/s]
pytorch_model.bin:   6% 31.5M/559M [00:00<00:04, 111MB/s] 
pytorch_model.bin:  11% 62.9M/559M [00:00<00:03, 156MB/s]
pytorch_model.bin:  15% 83.9M/559M [00:00<00:04, 109MB/s]
pytorch_model.bin:  19% 105M/559M [00:01<00:04, 95.1MB/s]
pytorch_model.bin:  23% 126M/559M [00:01<00:03, 110MB/s] 
pytorch_model.bin:  26% 147M/559M [00:01<00:03, 112MB/s]
pytorch_model.bin:  30% 168M/559M [00:01<00:03, 115MB/s]
pytorch_model.bin:  34% 189M/559M [00:01<00:02, 131MB/s]
pytorch_model.bin:  38% 210M/559M [00:01<00:02, 147MB/s]
pytorch_model.bin:  41% 231M/559M [00:01<00:02, 161MB/s]
pytorch_model.bin:  45% 252M/559M [00:01<00:01, 173MB/s]
pytorch_model.bin:  49% 273M/559M [00:02<00:01, 180MB/s]
pytorch_model.bin:  53% 294M/559M [00:02<00:01, 187MB/s]
pytorch_model.bin:  56% 315M/559M [00:02<00:01, 192MB/s]
pytorch_model.bin:  60% 336M/559M [00:02<00:01, 195MB/s]
pytorch_model.bin:  64% 357M/559M [00:02<00:01, 199MB/s]
pytorch_model.bin:  69% 388M/559M [00:02<00:00, 205MB/s]
pytorch_model.bin:  73% 409M/559M [00:02<00:00, 203MB/s]
pytorch_model.bin:  79% 440M/559M [00:02<00:00, 210MB/s]
pytorch_model.bin:  84% 472M/559M [00:02<00:00, 209MB/s]
pytorch_model.bin:  88% 493M/559M [00:03<00:00, 205MB/s]
pytorch_model.bin:  92% 514M/559M [00:03<00:00, 204MB/s]
pytorch_model.bin:  96% 535M/559M [00:03<00:00, 205MB/s]
pytorch_model.bin:  99% 556M/559M [00:03<00:00, 203MB/s]
pytorch_model.bin: 100% 559M/559M [00:03<00:00, 164MB/s]
[INFO|modeling_utils.py:3257] 2024-11-04 19:03:05,138 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:07,572 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:07,573 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/2490 [00:00<?, ? examples/s]trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:  40% 1000/2490 [00:00<00:00, 3841.69 examples/s]
Map:  80% 2000/2490 [00:00<00:00, 3995.55 examples/s]
Map: 100% 2490/2490 [00:00<00:00, 3944.22 examples/s]
Map: 100% 2490/2490 [00:00<00:00, 3740.84 examples/s]

Map:   0% 0/277 [00:00<?, ? examples/s]
Map: 100% 277/277 [00:00<00:00, 3447.01 examples/s]

Map:   0% 0/3000 [00:00<?, ? examples/s]
Map:  33% 1000/3000 [00:00<00:00, 4262.29 examples/s]
Map:  67% 2000/3000 [00:00<00:00, 4869.87 examples/s]
Map: 100% 3000/3000 [00:00<00:00, 3823.87 examples/s]
Map: 100% 3000/3000 [00:00<00:00, 3775.81 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:03:10,011 >> Using auto half precision backend
2024-11-04 19:03:10,011 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:03:10,013 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:03:10,015 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:03:10,015 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:03:10,015 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 12.07it/s]
[INFO|training_args.py:1902] 2024-11-04 19:03:13,043 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:03:13,044 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:03:13,045 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:03:13,048 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:03:13,050 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:03:16,365 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:16,368 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:16,430 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:16,433 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:16,441 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:16,441 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:16,441 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:16,441 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:16,441 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:16,441 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:03:16,443 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:16,445 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:16,536 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:16,537 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:03:16,618 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:16,878 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:16,878 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/277 [00:00<?, ? examples/s]***** eval metrics *****
  eval_accuracy           =     0.6101
  eval_loss               =     0.6588
  eval_runtime            = 0:00:02.89
  eval_samples            =        277
  eval_samples_per_second =     95.843
  eval_steps_per_second   =      0.346
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map: 100% 277/277 [00:00<00:00, 3015.34 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:03:17,720 >> Using auto half precision backend
2024-11-04 19:03:17,721 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:03:17,722 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:03:17,724 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:03:17,724 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:03:17,724 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 26.00it/s]
[INFO|training_args.py:1902] 2024-11-04 19:03:18,109 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:03:18,110 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:03:18,111 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:03:18,113 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:03:18,114 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:03:21,387 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:21,389 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:21,449 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:21,451 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:21,456 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:21,456 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:21,456 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:21,456 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:21,456 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:21,457 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:03:21,458 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:21,461 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:21,553 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:21,556 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:03:21,645 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:21,803 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:21,804 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/3668 [00:00<?, ? examples/s]***** eval metrics *****
  eval_accuracy           =     0.6101
  eval_loss               =     0.6588
  eval_runtime            = 0:00:00.30
  eval_samples            =        277
  eval_samples_per_second =    904.904
  eval_steps_per_second   =      3.267
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:  27% 1000/3668 [00:00<00:00, 4779.07 examples/s]
Map:  55% 2000/3668 [00:00<00:00, 4909.48 examples/s]
Map:  82% 3000/3668 [00:00<00:00, 4782.61 examples/s]
Map: 100% 3668/3668 [00:00<00:00, 4808.77 examples/s]
Map: 100% 3668/3668 [00:00<00:00, 4520.72 examples/s]

Map:   0% 0/408 [00:00<?, ? examples/s]
Map: 100% 408/408 [00:00<00:00, 4124.08 examples/s]

Map:   0% 0/1725 [00:00<?, ? examples/s]
Map:  58% 1000/1725 [00:00<00:00, 4511.41 examples/s]
Map: 100% 1725/1725 [00:00<00:00, 4739.02 examples/s]
Map: 100% 1725/1725 [00:00<00:00, 4354.75 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:03:24,126 >> Using auto half precision backend
2024-11-04 19:03:24,126 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:03:24,127 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:03:24,129 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:03:24,129 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:03:24,129 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.86it/s]
[INFO|training_args.py:1902] 2024-11-04 19:03:24,637 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:03:24,638 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:03:24,639 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:03:24,641 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:03:24,642 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:03:28,139 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:28,141 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:28,197 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:28,200 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:28,206 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:28,206 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:28,206 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:28,206 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:28,206 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:28,206 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:03:28,208 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:28,210 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:28,349 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:28,350 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:03:28,436 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:28,609 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:28,609 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** eval metrics *****
  eval_accuracy           =     0.8113
  eval_combined_score     =     0.8403
  eval_f1                 =     0.8693
  eval_loss               =     0.4113
  eval_runtime            = 0:00:00.42
  eval_samples            =        408
  eval_samples_per_second =    951.878
  eval_steps_per_second   =      2.333
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:   0% 0/408 [00:00<?, ? examples/s]
Map: 100% 408/408 [00:00<00:00, 4123.04 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:03:29,433 >> Using auto half precision backend
2024-11-04 19:03:29,433 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:03:29,435 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:03:29,437 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:03:29,437 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:03:29,437 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 20.34it/s]
[INFO|training_args.py:1902] 2024-11-04 19:03:29,931 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:03:29,931 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:03:29,932 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:03:29,934 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:03:29,936 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:03:33,487 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:33,488 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:33,548 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:33,549 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:33,552 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:33,552 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:33,552 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:33,552 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:33,552 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:33,552 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:03:33,553 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:33,554 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:33,628 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:33,631 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:03:33,718 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:33,854 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:33,854 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/8551 [00:00<?, ? examples/s]***** eval metrics *****
  eval_accuracy           =     0.8113
  eval_combined_score     =     0.8403
  eval_f1                 =     0.8693
  eval_loss               =     0.4113
  eval_runtime            = 0:00:00.42
  eval_samples            =        408
  eval_samples_per_second =    956.885
  eval_steps_per_second   =      2.345
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:  12% 1000/8551 [00:00<00:00, 9595.05 examples/s]
Map:  23% 2000/8551 [00:00<00:00, 9361.37 examples/s]
Map:  47% 4000/8551 [00:00<00:00, 8805.84 examples/s]
Map:  58% 5000/8551 [00:00<00:00, 8827.67 examples/s]
Map:  70% 6000/8551 [00:00<00:00, 9035.92 examples/s]
Map:  82% 7000/8551 [00:00<00:00, 9209.65 examples/s]
Map:  94% 8000/8551 [00:00<00:00, 9154.96 examples/s]
Map: 100% 8551/8551 [00:01<00:00, 8341.47 examples/s]

Map:   0% 0/1043 [00:00<?, ? examples/s]
Map:  96% 1000/1043 [00:00<00:00, 7050.25 examples/s]
Map: 100% 1043/1043 [00:00<00:00, 6341.82 examples/s]

Map:   0% 0/1063 [00:00<?, ? examples/s]
Map:  94% 1000/1063 [00:00<00:00, 9365.76 examples/s]
Map: 100% 1063/1063 [00:00<00:00, 7841.57 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:03:36,372 >> Using auto half precision backend
2024-11-04 19:03:36,373 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:03:36,374 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:03:36,377 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:03:36,377 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:03:36,377 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 16.56it/s]
[INFO|training_args.py:1902] 2024-11-04 19:03:37,373 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:03:37,373 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:03:37,374 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:03:37,377 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:03:37,378 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:03:41,485 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:41,487 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:41,537 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:41,539 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:41,543 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:41,543 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:41,543 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:41,543 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:41,543 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:41,543 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:03:41,544 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:41,545 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:41,697 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:41,699 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:03:41,780 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:41,895 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:41,895 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/1043 [00:00<?, ? examples/s]***** eval metrics *****
  eval_loss                 =     0.4728
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.92
  eval_samples              =       1043
  eval_samples_per_second   =   1133.083
  eval_steps_per_second     =      1.086
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:  96% 1000/1043 [00:00<00:00, 7683.37 examples/s]
Map: 100% 1043/1043 [00:00<00:00, 6827.66 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:03:42,750 >> Using auto half precision backend
2024-11-04 19:03:42,751 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:03:42,752 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:03:42,754 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:03:42,754 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:03:42,754 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.95it/s]
[INFO|training_args.py:1902] 2024-11-04 19:03:43,631 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:03:43,631 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:03:43,632 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:03:43,634 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:03:43,635 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:03:47,433 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:47,435 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:47,496 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:47,499 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:47,503 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:47,503 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:47,503 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:47,503 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:47,503 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:03:47,503 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:03:47,504 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:47,507 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:03:47,594 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:03:47,595 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:03:47,726 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:03:47,922 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:03:47,923 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** eval metrics *****
  eval_loss                 =     0.4728
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.81
  eval_samples              =       1043
  eval_samples_per_second   =   1276.878
  eval_steps_per_second     =      1.224
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:   0% 0/104743 [00:00<?, ? examples/s]
Map:   1% 1000/104743 [00:00<00:20, 4950.98 examples/s]
Map:   2% 2000/104743 [00:00<00:21, 4769.46 examples/s]
Map:   3% 3000/104743 [00:00<00:24, 4212.04 examples/s]
Map:   4% 4000/104743 [00:00<00:23, 4362.43 examples/s]
Map:   5% 5000/104743 [00:01<00:22, 4490.06 examples/s]
Map:   6% 6000/104743 [00:01<00:22, 4465.36 examples/s]
Map:   7% 7000/104743 [00:01<00:21, 4550.23 examples/s]
Map:   8% 8000/104743 [00:01<00:21, 4554.09 examples/s]
Map:   9% 9000/104743 [00:01<00:21, 4538.16 examples/s]
Map:  10% 10000/104743 [00:02<00:21, 4483.97 examples/s]
Map:  11% 11000/104743 [00:02<00:20, 4530.35 examples/s]
Map:  11% 12000/104743 [00:02<00:20, 4603.18 examples/s]
Map:  12% 13000/104743 [00:02<00:20, 4549.55 examples/s]
Map:  13% 14000/104743 [00:03<00:20, 4455.22 examples/s]
Map:  14% 15000/104743 [00:03<00:19, 4528.74 examples/s]
Map:  15% 16000/104743 [00:03<00:19, 4600.23 examples/s]
Map:  16% 17000/104743 [00:03<00:19, 4544.55 examples/s]
Map:  17% 18000/104743 [00:03<00:18, 4598.35 examples/s]
Map:  18% 19000/104743 [00:04<00:18, 4582.66 examples/s]
Map:  19% 20000/104743 [00:04<00:18, 4552.25 examples/s]
Map:  20% 21000/104743 [00:04<00:17, 4675.16 examples/s]
Map:  21% 22000/104743 [00:04<00:17, 4609.86 examples/s]
Map:  22% 23000/104743 [00:05<00:25, 3208.59 examples/s]
Map:  23% 24000/104743 [00:05<00:21, 3751.20 examples/s]
Map:  24% 25000/104743 [00:05<00:18, 4258.58 examples/s]
Map:  25% 26000/104743 [00:05<00:16, 4668.62 examples/s]
Map:  26% 27000/104743 [00:06<00:15, 5048.19 examples/s]
Map:  27% 28000/104743 [00:06<00:14, 5372.49 examples/s]
Map:  28% 29000/104743 [00:06<00:13, 5608.09 examples/s]
Map:  29% 30000/104743 [00:06<00:13, 5732.78 examples/s]
Map:  30% 31000/104743 [00:06<00:12, 5934.95 examples/s]
Map:  31% 32000/104743 [00:06<00:12, 5903.85 examples/s]
Map:  32% 33000/104743 [00:06<00:12, 5962.61 examples/s]
Map:  32% 34000/104743 [00:07<00:11, 6056.63 examples/s]
Map:  33% 35000/104743 [00:07<00:11, 6122.69 examples/s]
Map:  34% 36000/104743 [00:07<00:11, 5879.12 examples/s]
Map:  35% 37000/104743 [00:07<00:12, 5312.44 examples/s]
Map:  36% 38000/104743 [00:07<00:12, 5228.51 examples/s]
Map:  37% 39000/104743 [00:08<00:13, 5054.55 examples/s]
Map:  38% 40000/104743 [00:08<00:13, 4900.24 examples/s]
Map:  39% 41000/104743 [00:08<00:13, 4831.13 examples/s]
Map:  40% 42000/104743 [00:08<00:13, 4765.72 examples/s]
Map:  41% 43000/104743 [00:09<00:13, 4622.24 examples/s]
Map:  42% 44000/104743 [00:09<00:13, 4668.71 examples/s]
Map:  43% 45000/104743 [00:09<00:13, 4587.81 examples/s]
Map:  44% 46000/104743 [00:09<00:12, 4730.49 examples/s]
Map:  45% 47000/104743 [00:09<00:11, 5010.43 examples/s]
Map:  46% 48000/104743 [00:09<00:10, 5206.54 examples/s]
Map:  47% 49000/104743 [00:10<00:10, 5399.42 examples/s]
Map:  48% 50000/104743 [00:10<00:09, 5674.32 examples/s]
Map:  49% 51000/104743 [00:10<00:09, 5786.14 examples/s]
Map:  50% 52000/104743 [00:10<00:08, 5955.89 examples/s]
Map:  51% 53000/104743 [00:11<00:12, 4258.46 examples/s]
Map:  52% 54000/104743 [00:11<00:10, 4614.47 examples/s]
Map:  53% 55000/104743 [00:11<00:09, 4988.71 examples/s]
Map:  53% 56000/104743 [00:11<00:09, 5306.97 examples/s]
Map:  54% 57000/104743 [00:11<00:09, 5117.70 examples/s]
Map:  55% 58000/104743 [00:11<00:08, 5267.63 examples/s]
Map:  56% 59000/104743 [00:12<00:08, 5528.90 examples/s]
Map:  57% 60000/104743 [00:12<00:07, 5594.89 examples/s]
Map:  58% 61000/104743 [00:12<00:07, 5764.18 examples/s]
Map:  59% 62000/104743 [00:12<00:07, 5900.45 examples/s]
Map:  60% 63000/104743 [00:12<00:07, 5916.66 examples/s]
Map:  61% 64000/104743 [00:12<00:06, 5970.75 examples/s]
Map:  62% 65000/104743 [00:13<00:06, 5921.44 examples/s]
Map:  63% 66000/104743 [00:13<00:06, 5886.22 examples/s]
Map:  64% 67000/104743 [00:13<00:06, 5981.44 examples/s]
Map:  65% 68000/104743 [00:13<00:06, 6066.64 examples/s]
Map:  66% 69000/104743 [00:13<00:05, 6009.66 examples/s]
Map:  67% 70000/104743 [00:13<00:05, 5797.50 examples/s]
Map:  68% 71000/104743 [00:14<00:05, 5705.43 examples/s]
Map:  69% 72000/104743 [00:14<00:05, 5689.34 examples/s]
Map:  70% 73000/104743 [00:14<00:05, 5842.83 examples/s]
Map:  71% 74000/104743 [00:14<00:05, 5842.78 examples/s]
Map:  72% 75000/104743 [00:14<00:04, 5977.36 examples/s]
Map:  73% 76000/104743 [00:14<00:04, 6006.72 examples/s]
Map:  74% 77000/104743 [00:15<00:04, 5900.15 examples/s]
Map:  74% 78000/104743 [00:15<00:04, 5948.53 examples/s]
Map:  75% 79000/104743 [00:15<00:04, 5983.46 examples/s]
Map:  76% 80000/104743 [00:15<00:04, 5939.31 examples/s]
Map:  77% 81000/104743 [00:16<00:05, 4266.88 examples/s]
Map:  78% 82000/104743 [00:16<00:04, 4630.75 examples/s]
Map:  79% 83000/104743 [00:16<00:04, 4816.20 examples/s]
Map:  80% 84000/104743 [00:16<00:04, 5155.47 examples/s]
Map:  81% 85000/104743 [00:16<00:03, 5353.36 examples/s]
Map:  82% 86000/104743 [00:16<00:03, 5593.75 examples/s]
Map:  83% 87000/104743 [00:17<00:03, 5612.43 examples/s]
Map:  84% 88000/104743 [00:17<00:02, 5736.24 examples/s]
Map:  85% 89000/104743 [00:17<00:02, 5751.97 examples/s]
Map:  86% 90000/104743 [00:17<00:02, 5883.28 examples/s]
Map:  87% 91000/104743 [00:17<00:02, 5903.69 examples/s]
Map:  88% 92000/104743 [00:17<00:02, 5861.31 examples/s]
Map:  89% 93000/104743 [00:18<00:02, 5817.94 examples/s]
Map:  90% 94000/104743 [00:18<00:01, 5895.25 examples/s]
Map:  91% 95000/104743 [00:18<00:01, 5987.34 examples/s]
Map:  92% 96000/104743 [00:18<00:01, 5747.29 examples/s]
Map:  93% 97000/104743 [00:18<00:01, 5847.60 examples/s]
Map:  94% 98000/104743 [00:18<00:01, 5953.66 examples/s]
Map:  95% 99000/104743 [00:19<00:00, 6007.51 examples/s]
Map:  95% 100000/104743 [00:19<00:00, 6005.71 examples/s]
Map:  96% 101000/104743 [00:19<00:00, 5992.88 examples/s]
Map:  97% 102000/104743 [00:19<00:00, 5787.27 examples/s]
Map:  98% 103000/104743 [00:19<00:00, 5432.05 examples/s]
Map:  99% 104000/104743 [00:20<00:00, 5169.06 examples/s]
Map: 100% 104743/104743 [00:20<00:00, 4977.21 examples/s]
Map: 100% 104743/104743 [00:21<00:00, 4972.76 examples/s]

Map:   0% 0/5463 [00:00<?, ? examples/s]
Map:  18% 1000/5463 [00:00<00:00, 4628.38 examples/s]
Map:  37% 2000/5463 [00:00<00:00, 4609.85 examples/s]
Map:  55% 3000/5463 [00:00<00:00, 4589.02 examples/s]
Map:  73% 4000/5463 [00:00<00:00, 4595.47 examples/s]
Map:  92% 5000/5463 [00:01<00:00, 4635.12 examples/s]
Map: 100% 5463/5463 [00:01<00:00, 4356.53 examples/s]

Map:   0% 0/5463 [00:00<?, ? examples/s]
Map:  18% 1000/5463 [00:00<00:01, 3951.49 examples/s]
Map:  37% 2000/5463 [00:00<00:00, 4291.35 examples/s]
Map:  55% 3000/5463 [00:00<00:00, 4445.94 examples/s]
Map:  73% 4000/5463 [00:00<00:00, 4537.19 examples/s]
Map:  92% 5000/5463 [00:01<00:00, 2899.66 examples/s]
Map: 100% 5463/5463 [00:01<00:00, 3091.04 examples/s]
Map: 100% 5463/5463 [00:01<00:00, 3358.06 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:04:13,203 >> Using auto half precision backend
2024-11-04 19:04:13,204 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:13,206 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:13,209 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:13,209 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:04:13,209 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.37it/s]
100% 3/3 [00:02<00:00,  1.30it/s]
100% 3/3 [00:02<00:00,  1.28it/s]
[INFO|training_args.py:1902] 2024-11-04 19:04:17,739 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:17,740 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:17,741 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:17,744 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:17,745 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:04:20,732 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:20,735 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:20,792 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:20,795 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:20,801 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:20,801 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:20,801 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:20,801 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:20,802 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:20,802 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:04:20,803 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:20,806 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:20,899 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:20,900 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:04:20,988 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:04:21,170 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:04:21,170 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** eval metrics *****
  eval_accuracy           =     0.9281
  eval_loss               =     0.1866
  eval_runtime            = 0:00:04.43
  eval_samples            =       5463
  eval_samples_per_second =   1232.469
  eval_steps_per_second   =      0.677
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:   0% 0/5463 [00:00<?, ? examples/s]
Map:  18% 1000/5463 [00:00<00:01, 4411.07 examples/s]
Map:  37% 2000/5463 [00:00<00:00, 4655.27 examples/s]
Map:  55% 3000/5463 [00:00<00:00, 4369.39 examples/s]
Map:  73% 4000/5463 [00:00<00:00, 4466.84 examples/s]
Map:  92% 5000/5463 [00:01<00:00, 3079.08 examples/s]
Map: 100% 5463/5463 [00:01<00:00, 3265.35 examples/s]
Map: 100% 5463/5463 [00:01<00:00, 3470.61 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:04:23,511 >> Using auto half precision backend
2024-11-04 19:04:23,512 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:23,513 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:23,516 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:23,516 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:04:23,516 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.39it/s]
100% 3/3 [00:02<00:00,  1.32it/s]
100% 3/3 [00:02<00:00,  1.27it/s]
[INFO|training_args.py:1902] 2024-11-04 19:04:27,862 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:27,863 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:27,864 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:27,866 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:27,867 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:04:31,594 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:31,595 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:31,652 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:31,653 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:31,656 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:31,656 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:31,656 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:31,656 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:31,656 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:31,656 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:04:31,657 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:31,658 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:31,737 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:31,738 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:04:31,821 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:04:32,010 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:04:32,010 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/5749 [00:00<?, ? examples/s]***** eval metrics *****
  eval_accuracy           =     0.9281
  eval_loss               =     0.1866
  eval_runtime            = 0:00:04.25
  eval_samples            =       5463
  eval_samples_per_second =   1283.029
  eval_steps_per_second   =      0.705
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0

Map:  17% 1000/5749 [00:00<00:00, 8464.48 examples/s]
Map:  35% 2000/5749 [00:00<00:00, 7684.40 examples/s]
Map:  52% 3000/5749 [00:00<00:00, 6590.20 examples/s]
Map:  70% 4000/5749 [00:00<00:00, 5998.92 examples/s]
Map:  87% 5000/5749 [00:00<00:00, 6344.86 examples/s]
Map: 100% 5749/5749 [00:00<00:00, 6522.87 examples/s]
Map: 100% 5749/5749 [00:00<00:00, 6053.92 examples/s]

Map:   0% 0/1500 [00:00<?, ? examples/s]
Map:  67% 1000/1500 [00:00<00:00, 6894.18 examples/s]
Map: 100% 1500/1500 [00:00<00:00, 5742.18 examples/s]

Map:   0% 0/1379 [00:00<?, ? examples/s]
Map:  73% 1000/1379 [00:00<00:00, 7330.33 examples/s]
Map: 100% 1379/1379 [00:00<00:00, 6102.79 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:04:34,197 >> Using auto half precision backend
2024-11-04 19:04:34,198 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:34,199 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:34,202 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:34,202 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:04:34,202 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

100% 1/1 [00:00<00:00, 19.25it/s]
[INFO|training_args.py:1902] 2024-11-04 19:04:35,784 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:35,785 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:35,785 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:35,788 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:35,790 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:04:39,414 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:39,416 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:39,483 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:39,485 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:39,489 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:39,490 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:39,490 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:39,490 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:39,490 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:39,490 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:04:39,492 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:39,494 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:39,625 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:39,626 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:04:39,734 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:04:39,885 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:04:39,885 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/1500 [00:00<?, ? examples/s]***** eval metrics *****
  eval_combined_score     =     0.8775
  eval_loss               =     0.5775
  eval_pearson            =     0.8759
  eval_runtime            = 0:00:01.51
  eval_samples            =       1500
  eval_samples_per_second =    990.118
  eval_spearmanr          =     0.8791
  eval_steps_per_second   =       0.66
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0

Map:  67% 1000/1500 [00:00<00:00, 6593.22 examples/s]
Map: 100% 1500/1500 [00:00<00:00, 4657.56 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:04:40,987 >> Using auto half precision backend
2024-11-04 19:04:40,987 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:40,989 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:40,991 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:40,991 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:04:40,991 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.41it/s]
2024-11-04 19:04:42,221 - INFO - [auto encoder][Epoch 0] input auto_encoder_model accuracy:[('rte', 0.6101083032490975), ('rte', 0.6101083032490975), ('mrpc', 0.8692699490662139), ('mrpc', 0.8692699490662139), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649), ('qnli', 0.9280615046677649), ('stsb', 0.8759083739074341), ('stsb', 0.8759057167083745)]
2024-11-04 19:04:42,225 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model rte: 0.6101083032490975
2024-11-04 19:04:42,227 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model mrpc: 0.8692699490662139
2024-11-04 19:04:42,229 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model cola: 0.5703223353398024
2024-11-04 19:04:42,230 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model qnli: 0.9280615046677649
2024-11-04 19:04:42,232 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model stsb: 0.8759083739074341
2024-11-04 19:04:42,233 - INFO - [auto encoder][Epoch 0] average input auto_encoder_model: 0.7707338275261566
2024-11-04 19:04:42,235 - INFO - [auto encoder][Epoch 0] ---------------------------------
2024-11-04 19:04:42,237 - INFO - [auto encoder][Epoch 0] Test the AE auto_encoder_model
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-04 19:04:42,471 - INFO - [auto encoder][Epoch 0] latent shape:torch.Size([12, 4, 147])
2024-11-04 19:04:42,475 - INFO - [auto encoder][Epoch 0] ae params shape:torch.Size([12, 92160])
[INFO|training_args.py:1902] 2024-11-04 19:04:42,490 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:42,490 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:42,491 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:42,493 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:42,494 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:04:45,410 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:45,412 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:45,479 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:45,481 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:45,485 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:45,485 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:45,486 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:45,486 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:45,486 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:45,486 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:04:45,487 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:45,490 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:45,581 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:45,585 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:04:45,658 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:04:45,819 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:04:45,820 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** eval metrics *****
  eval_combined_score     =     0.8775
  eval_loss               =     0.5775
  eval_pearson            =     0.8759
  eval_runtime            = 0:00:01.17
  eval_samples            =       1500
  eval_samples_per_second =   1274.235
  eval_spearmanr          =     0.8791
  eval_steps_per_second   =      0.849
The tensors are equal.
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:   0% 0/3000 [00:00<?, ? examples/s]
Map:  33% 1000/3000 [00:00<00:00, 4446.86 examples/s]
Map:  67% 2000/3000 [00:00<00:00, 4683.83 examples/s]
Map: 100% 3000/3000 [00:00<00:00, 3655.74 examples/s]
Map: 100% 3000/3000 [00:00<00:00, 3681.08 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:04:47,308 >> Using auto half precision backend
2024-11-04 19:04:47,308 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:47,309 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:47,312 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:47,312 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:04:47,312 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 29.23it/s]
[INFO|training_args.py:1902] 2024-11-04 19:04:47,695 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:47,695 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:47,696 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:47,699 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:47,701 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:04:50,975 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:50,978 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:51,035 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:51,038 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:51,041 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:51,041 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:51,041 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:51,042 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:51,042 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:51,042 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:04:51,043 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:51,045 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:51,132 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:51,133 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:04:51,252 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:04:51,390 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:04:51,390 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:04:52,024 >> Using auto half precision backend
2024-11-04 19:04:52,025 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:52,027 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:52,029 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:52,029 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:04:52,029 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =      0.639
  eval_loss               =     0.6595
  eval_runtime            = 0:00:00.30
  eval_samples            =        277
  eval_samples_per_second =    916.762
  eval_steps_per_second   =       3.31
ae_rec_accs: [('rte', 0.6389891696750902)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 22.03it/s]
[INFO|training_args.py:1902] 2024-11-04 19:04:52,454 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:52,454 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:52,455 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:52,457 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:52,458 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:04:55,700 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:55,702 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:55,763 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:55,766 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:55,769 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:55,770 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:55,770 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:55,770 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:55,770 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:04:55,770 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:04:55,772 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:55,774 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:04:55,918 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:04:55,919 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:04:55,991 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:04:56,121 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:04:56,121 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** eval metrics *****
  eval_accuracy           =     0.6354
  eval_loss               =     0.6595
  eval_runtime            = 0:00:00.34
  eval_samples            =        277
  eval_samples_per_second =    801.093
  eval_steps_per_second   =      2.892
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:   0% 0/1725 [00:00<?, ? examples/s]
Map:  58% 1000/1725 [00:00<00:00, 4608.75 examples/s]
Map: 100% 1725/1725 [00:00<00:00, 4855.29 examples/s]
Map: 100% 1725/1725 [00:00<00:00, 4378.77 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:04:57,150 >> Using auto half precision backend
2024-11-04 19:04:57,150 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:04:57,152 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:04:57,154 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:04:57,154 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:04:57,154 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.84it/s]
[INFO|training_args.py:1902] 2024-11-04 19:04:57,615 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:04:57,616 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:04:57,618 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:04:57,621 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:04:57,623 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:00,739 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:00,742 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:00,797 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:00,800 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:00,804 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:00,804 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:00,804 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:00,804 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:00,804 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:00,804 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:00,806 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:00,808 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:00,893 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:00,894 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:00,992 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:01,171 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:01,171 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:05:01,825 >> Using auto half precision backend
2024-11-04 19:05:01,826 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:01,827 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:01,829 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:01,830 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:05:01,830 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8015
  eval_combined_score     =     0.8327
  eval_f1                 =     0.8639
  eval_loss               =     0.4169
  eval_runtime            = 0:00:00.38
  eval_samples            =        408
  eval_samples_per_second =    1047.12
  eval_steps_per_second   =      2.566
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.84it/s]
[INFO|training_args.py:1902] 2024-11-04 19:05:02,630 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:05:02,630 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:05:02,631 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:05:02,633 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:05:02,635 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:05,977 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:05,979 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:06,050 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:06,052 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:06,055 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:06,056 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:06,056 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:06,056 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:06,056 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:06,056 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:06,057 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:06,060 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:06,135 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:06,136 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:06,202 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:06,375 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:06,376 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/1063 [00:00<?, ? examples/s]***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8366
  eval_f1                 =     0.8668
  eval_loss               =     0.4164
  eval_runtime            = 0:00:00.71
  eval_samples            =        408
  eval_samples_per_second =    573.531
  eval_steps_per_second   =      1.406
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map: 100% 1063/1063 [00:00<00:00, 10402.87 examples/s]
Map: 100% 1063/1063 [00:00<00:00, 9535.57 examples/s] 
[INFO|trainer.py:602] 2024-11-04 19:05:07,092 >> Using auto half precision backend
2024-11-04 19:05:07,092 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:07,094 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:07,096 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:07,096 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:05:07,096 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.03it/s]
[INFO|training_args.py:1902] 2024-11-04 19:05:08,093 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:05:08,093 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:05:08,094 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:05:08,095 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:05:08,097 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:12,471 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:12,473 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:12,536 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:12,539 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:12,544 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:12,545 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:12,545 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:12,545 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:12,545 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:12,545 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:12,547 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:12,549 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:12,639 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:12,642 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:12,714 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:12,876 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:12,876 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:05:13,520 >> Using auto half precision backend
2024-11-04 19:05:13,521 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:13,522 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:13,524 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:13,524 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:05:13,524 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4746
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.93
  eval_samples              =       1043
  eval_samples_per_second   =   1114.461
  eval_steps_per_second     =      1.069
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.97it/s]
[INFO|training_args.py:1902] 2024-11-04 19:05:14,448 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:05:14,449 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:05:14,450 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:05:14,452 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:05:14,453 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:17,950 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:17,953 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:18,016 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:18,018 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:18,022 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:18,022 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:18,023 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:18,023 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:18,023 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:18,023 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:18,024 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:18,027 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:18,113 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:18,114 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:18,185 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:18,339 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:18,339 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** eval metrics *****
  eval_loss                 =     0.4745
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.84
  eval_samples              =       1043
  eval_samples_per_second   =   1233.915
  eval_steps_per_second     =      1.183
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0

Map:   0% 0/5463 [00:00<?, ? examples/s]
Map:  18% 1000/5463 [00:00<00:00, 4521.80 examples/s]
Map:  37% 2000/5463 [00:00<00:00, 4656.42 examples/s]
Map:  55% 3000/5463 [00:00<00:00, 4709.25 examples/s]
Map:  73% 4000/5463 [00:00<00:00, 4823.19 examples/s]
Map:  92% 5000/5463 [00:01<00:00, 4924.32 examples/s]
Map: 100% 5463/5463 [00:01<00:00, 4501.64 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:05:20,684 >> Using auto half precision backend
2024-11-04 19:05:20,685 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:20,686 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:20,688 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:20,688 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:05:20,688 >>   Batch size = 2048
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.37it/s]
100% 3/3 [00:02<00:00,  1.30it/s]
100% 3/3 [00:02<00:00,  1.27it/s]
[INFO|training_args.py:1902] 2024-11-04 19:05:25,692 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:05:25,693 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:05:25,694 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:05:25,697 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:05:25,700 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:28,713 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:28,716 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:28,773 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:28,775 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:28,778 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:28,778 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:28,778 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:28,779 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:28,779 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:28,779 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:28,780 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:28,782 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:28,870 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:28,871 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:28,942 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:29,139 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:29,139 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:05:30,518 >> Using auto half precision backend
2024-11-04 19:05:30,519 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:30,521 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:30,525 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:30,525 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:05:30,525 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9281
  eval_loss               =      0.187
  eval_runtime            = 0:00:04.90
  eval_samples            =       5463
  eval_samples_per_second =   1113.313
  eval_steps_per_second   =      0.611
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649)]
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.38it/s]
100% 3/3 [00:02<00:00,  1.29it/s]
100% 3/3 [00:02<00:00,  1.27it/s]
[INFO|training_args.py:1902] 2024-11-04 19:05:35,348 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:05:35,349 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:05:35,350 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:05:35,352 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:05:35,353 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:38,994 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:38,996 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:39,074 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:39,077 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:39,082 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:39,082 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:39,082 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:39,082 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:39,082 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:39,082 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:39,084 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:39,086 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:39,171 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:39,171 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:39,276 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:39,441 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:39,442 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:   0% 0/1379 [00:00<?, ? examples/s]***** eval metrics *****
  eval_accuracy           =     0.9281
  eval_loss               =      0.187
  eval_runtime            = 0:00:04.75
  eval_samples            =       5463
  eval_samples_per_second =   1148.516
  eval_steps_per_second   =      0.631
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649), ('qnli', 0.9280615046677649)]
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0

Map:  73% 1000/1379 [00:00<00:00, 7105.85 examples/s]
Map: 100% 1379/1379 [00:00<00:00, 6272.49 examples/s]
[INFO|trainer.py:602] 2024-11-04 19:05:40,317 >> Using auto half precision backend
2024-11-04 19:05:40,317 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:40,319 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:40,321 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:40,321 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:05:40,321 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

100% 1/1 [00:00<00:00,  1.41it/s]
[INFO|training_args.py:1902] 2024-11-04 19:05:41,542 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:05:41,542 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:05:41,543 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:05:41,545 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:05:41,546 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.8999_end_epoch.10000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:05:44,514 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:44,517 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:44,575 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:44,577 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:44,583 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:44,583 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:44,583 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:44,583 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:44,584 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:44,584 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:44,585 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:44,587 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:05:44,674 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:05:44,677 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:44,745 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:05:44,907 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:05:44,907 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:05:45,529 >> Using auto half precision backend
2024-11-04 19:05:45,530 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:05:45,531 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:05:45,533 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:05:45,533 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:05:45,533 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8772
  eval_loss               =     0.5757
  eval_pearson            =     0.8757
  eval_runtime            = 0:00:01.15
  eval_samples            =       1500
  eval_samples_per_second =   1297.719
  eval_spearmanr          =     0.8787
  eval_steps_per_second   =      0.865
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649), ('qnli', 0.9280615046677649), ('stsb', 0.8756748520686412)]
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.38it/s]
2024-11-04 19:05:47,012 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model rte: 0.6101083032490975
2024-11-04 19:05:47,014 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model mrpc: 0.8692699490662139
2024-11-04 19:05:47,016 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model cola: 0.5703223353398024
2024-11-04 19:05:47,018 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model qnli: 0.9280615046677649
2024-11-04 19:05:47,019 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model stsb: 0.8759083739074341
2024-11-04 19:05:47,021 - INFO - [auto encoder][Epoch 0] average input auto_encoder_model: 0.7707338275261566
2024-11-04 19:05:47,022 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models rte: 0.6389891696750902
2024-11-04 19:05:47,023 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models mrpc: 0.8667790893760541
2024-11-04 19:05:47,024 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models cola: 0.5703223353398024
2024-11-04 19:05:47,026 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models qnli: 0.9280615046677649
2024-11-04 19:05:47,027 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models stsb: 0.8756748520686412
2024-11-04 19:05:47,029 - INFO - [auto encoder][Epoch 0] ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649), ('qnli', 0.9280615046677649), ('stsb', 0.8756748520686412), ('stsb', 0.875617678736242)]
2024-11-04 19:05:47,030 - INFO - [auto encoder][Epoch 0] average AE reconstruction auto_encoder_models accuracy: 0.775307307746149
2024-11-04 19:05:47,032 - INFO - [auto encoder][Epoch 0] ---------------------------------
2024-11-04 19:05:47,052 - INFO - [auto encoder][Epoch 0] Testing complete!
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-04 19:05:47,115 - INFO - =========== ddpm diffusion model | num_conditions: 6 ===========
[INFO|configuration_utils.py:728] 2024-11-04 19:05:47,277 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/config.json
[INFO|configuration_clip.py:389] 2024-11-04 19:05:47,281 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.
[INFO|configuration_clip.py:393] 2024-11-04 19:05:47,281 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.
[INFO|configuration_utils.py:791] 2024-11-04 19:05:47,284 >> Model config CLIPConfig {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 512,
  "text_config": {
    "bos_token_id": 0,
    "dropout": 0.0,
    "eos_token_id": 2,
    "model_type": "clip_text_model"
  },
  "transformers_version": "4.39.0.dev0",
  "vision_config": {
    "dropout": 0.0,
    "model_type": "clip_vision_model"
  }
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:05:47,288 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/pytorch_model.bin
[INFO|modeling_utils.py:3992] 2024-11-04 19:05:47,685 >> All model checkpoint weights were used when initializing CLIPModel.

[INFO|modeling_utils.py:4000] 2024-11-04 19:05:47,685 >> All the weights of CLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.
If your task is similar to the task the model of the checkpoint was trained on, you can already use CLIPModel for predictions without further training.
[INFO|image_processing_utils.py:374] 2024-11-04 19:05:47,766 >> loading configuration file preprocessor_config.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/preprocessor_config.json
[INFO|image_processing_utils.py:737] 2024-11-04 19:05:47,767 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.
[INFO|image_processing_utils.py:737] 2024-11-04 19:05:47,767 >> crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.
[INFO|image_processing_utils.py:424] 2024-11-04 19:05:47,767 >> Image processor CLIPImageProcessor {
  "_valid_processor_keys": [
    "images",
    "do_resize",
    "size",
    "resample",
    "do_center_crop",
    "crop_size",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_convert_rgb",
    "return_tensors",
    "data_format",
    "input_data_format"
  ],
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:47,834 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:47,834 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:47,834 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:47,834 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:47,835 >> loading file special_tokens_map.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/special_tokens_map.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:05:47,835 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:05:47,837 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268/config.json
[INFO|configuration_clip.py:389] 2024-11-04 19:05:47,840 >> `text_config` is `None`. Initializing the `CLIPTextConfig` with default values.
[INFO|configuration_clip.py:393] 2024-11-04 19:05:47,840 >> `vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.
[INFO|configuration_utils.py:791] 2024-11-04 19:05:47,844 >> Model config CLIPConfig {
  "_name_or_path": "openai/clip-vit-base-patch32",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 512,
  "text_config": {
    "bos_token_id": 0,
    "dropout": 0.0,
    "eos_token_id": 2,
    "model_type": "clip_text_model"
  },
  "transformers_version": "4.39.0.dev0",
  "vision_config": {
    "dropout": 0.0,
    "model_type": "clip_vision_model"
  }
}

[INFO|processing_utils.py:400] 2024-11-04 19:05:48,156 >> Processor CLIPProcessor:
- image_processor: CLIPImageProcessor {
  "_valid_processor_keys": [
    "images",
    "do_resize",
    "size",
    "resample",
    "do_center_crop",
    "crop_size",
    "do_rescale",
    "rescale_factor",
    "do_normalize",
    "image_mean",
    "image_std",
    "do_convert_rgb",
    "return_tensors",
    "data_format",
    "input_data_format"
  ],
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}

- tokenizer: CLIPTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
	49406: AddedToken("<|startoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
	49407: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}

{
  "processor_class": "CLIPProcessor"
}

2024-11-04 19:05:48,344 - INFO - [diffusion][Epoch 0] Load model: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_7999.pth
2024-11-04 19:05:48,574 - INFO - [diffusion][Epoch 0] Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_7999.pth
2024-11-04 19:05:48,576 - INFO - [diffusion][Epoch 0] ========================================
2024-11-04 19:05:48,596 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/clip_pdiff_model
2024-11-04 19:05:48,597 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/log
2024-11-04 19:05:48,597 - INFO - [diffusion][Epoch 0] Testing started!
[INFO|training_args.py:1902] 2024-11-04 19:06:09,934 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:06:09,935 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:06:09,936 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:06:09,940 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:06:09,942 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:06:18,474 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:18,476 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:06:18,533 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:18,536 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:18,542 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:18,542 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:18,542 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:18,542 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:18,542 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:18,542 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:06:18,544 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:18,546 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:06:18,638 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:18,639 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:06:18,709 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:06:18,941 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:06:18,942 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:06:19,576 >> Using auto half precision backend
2024-11-04 19:06:19,577 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:06:19,578 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:06:19,580 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:06:19,580 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:06:19,580 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8772
  eval_loss               =     0.5758
  eval_pearson            =     0.8756
  eval_runtime            = 0:00:01.40
  eval_samples            =       1500
  eval_samples_per_second =   1064.698
  eval_spearmanr          =     0.8787
  eval_steps_per_second   =       0.71
ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649), ('qnli', 0.9280615046677649), ('stsb', 0.8756748520686412), ('stsb', 0.875617678736242)]
training_parameters:
  ddpm_eval_epoch: 10
  ddpm_save_epoch: 1000
  ddpm_start_epoch: 1
  ddpm_end_epoch: 8000
  batch_size: 256
  lr: 0.001
  patience: 5000
  optimizer: Adam
  loss_function: MSE
  metrics:
  - accuracy
  weight_decay: 2.0e-06
  ddpm_test: false
  test_similarity: false
model_mean_type: eps
model_var_type: fixedlarge
loss_type: mse
model_parameters:
  in_channel: 1
  in_dim: 12
  num_conditions: 2
  cond_emb_size: 512
beta_schedule:
  start: 0.0001
  end: 0.02
  schedule: linear
  n_timestep: 1000
glue_test_parameters:
  config_path: config/multiple/glue.json

=====================================
Test ddpm model
The tensors are equal.
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 23.18it/s]
[INFO|training_args.py:1902] 2024-11-04 19:06:19,978 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:06:19,979 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:06:19,980 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:06:19,982 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:06:19,983 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:06:23,016 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:23,019 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:06:23,079 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:23,081 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:23,085 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:23,085 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:23,085 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:23,085 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:23,085 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:23,085 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:06:23,087 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:23,089 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:06:23,172 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:23,176 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:06:23,247 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:06:23,408 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:06:23,408 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:06:24,059 >> Using auto half precision backend
2024-11-04 19:06:24,059 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:06:24,061 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:06:24,063 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:06:24,064 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:06:24,064 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.33
  eval_samples            =        277
  eval_samples_per_second =    828.207
  eval_steps_per_second   =       2.99
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.85it/s]
[INFO|training_args.py:1902] 2024-11-04 19:06:24,427 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:06:24,428 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:06:24,429 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:06:24,431 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:06:24,433 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:06:28,155 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:28,157 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:06:28,209 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:28,212 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:28,215 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:28,215 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:28,216 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:28,216 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:28,216 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:06:28,216 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:06:28,217 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:28,219 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:06:28,304 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:06:28,305 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:06:28,373 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:06:28,509 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:06:28,509 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:06:29,550 >> Using auto half precision backend
2024-11-04 19:06:29,550 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:06:29,552 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:06:29,554 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:06:29,554 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:06:29,554 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.30
  eval_samples            =        277
  eval_samples_per_second =    908.501
  eval_steps_per_second   =       3.28
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.91it/s]
[INFO|training_args.py:1902] 2024-11-04 19:06:29,898 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:06:29,898 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:06:29,899 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:06:29,901 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:06:29,903 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
2024-11-04 19:07:01,283 - WARNING - Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'rte' at /data3/user/jin509/hf_cache/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Mon Nov  4 19:04:46 2024).
2024-11-04 19:07:01,287 - WARNING - Found the latest cached dataset configuration 'rte' at /data3/user/jin509/hf_cache/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Mon Nov  4 19:04:46 2024).
[INFO|configuration_utils.py:728] 2024-11-04 19:07:01,396 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:01,398 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:01,459 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:01,462 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:01,465 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:01,466 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:01,466 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:01,466 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:01,466 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:01,466 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:01,468 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:01,470 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:01,555 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:01,556 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:01,636 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:01,825 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:01,826 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:02,581 >> Using auto half precision backend
2024-11-04 19:07:02,581 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:02,583 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:02,585 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:02,585 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:02,585 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.29
  eval_samples            =        277
  eval_samples_per_second =    942.019
  eval_steps_per_second   =      3.401
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.15it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:02,987 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:02,987 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:02,988 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:02,990 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:02,991 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:06,376 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:06,379 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:06,443 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:06,445 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:06,449 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:06,449 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:06,449 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:06,449 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:06,449 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:06,449 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:06,451 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:06,453 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:06,539 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:06,540 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:06,617 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:06,775 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:06,775 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:07,864 >> Using auto half precision backend
2024-11-04 19:07:07,864 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:07,866 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:07,870 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:07,870 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:07,870 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.34
  eval_samples            =        277
  eval_samples_per_second =    814.507
  eval_steps_per_second   =       2.94
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 16.07it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:08,520 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:08,522 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:08,523 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:08,525 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:08,526 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:12,427 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:12,429 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:12,486 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:12,488 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:12,491 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:12,491 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:12,491 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:12,491 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:12,492 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:12,492 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:12,493 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:12,495 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:12,622 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:12,626 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:12,699 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:12,810 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:12,810 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:13,625 >> Using auto half precision backend
2024-11-04 19:07:13,625 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:13,627 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:13,629 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:13,629 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:13,629 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.33
  eval_samples            =        277
  eval_samples_per_second =    818.239
  eval_steps_per_second   =      2.954
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

100% 1/1 [00:00<00:00, 28.83it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:13,970 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:13,971 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:13,972 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:13,974 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:13,975 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:16,866 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:16,868 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:16,923 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:16,925 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:16,928 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:16,929 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:16,929 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:16,929 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:16,929 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:16,929 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:16,930 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:16,933 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:17,014 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:17,017 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:17,086 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:17,226 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:17,226 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:17,905 >> Using auto half precision backend
2024-11-04 19:07:17,905 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:17,907 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:17,908 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:17,908 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:17,908 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.29
  eval_samples            =        277
  eval_samples_per_second =    927.249
  eval_steps_per_second   =      3.347
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

100% 1/1 [00:00<00:00, 28.90it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:18,467 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:18,468 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:18,468 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:18,470 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:18,471 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:21,646 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:21,649 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:21,731 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:21,735 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:21,742 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:21,742 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:21,742 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:21,742 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:21,742 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:21,743 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:21,744 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:21,747 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:21,883 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:21,884 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:22,006 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:22,226 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:22,226 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:22,915 >> Using auto half precision backend
2024-11-04 19:07:22,915 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:22,917 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:22,919 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:22,919 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:22,919 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.50
  eval_samples            =        277
  eval_samples_per_second =    550.596
  eval_steps_per_second   =      1.988
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

100% 1/1 [00:00<00:00, 25.64it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:23,616 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:23,617 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:23,618 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:23,619 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:23,620 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:27,223 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:27,227 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:27,288 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:27,290 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:27,310 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:27,311 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:27,311 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:27,311 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:27,311 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:27,311 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:27,317 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:27,319 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:27,439 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:27,442 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:27,601 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:27,780 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:27,781 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:28,458 >> Using auto half precision backend
2024-11-04 19:07:28,458 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:28,460 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:28,462 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:28,462 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:28,462 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.64
  eval_samples            =        277
  eval_samples_per_second =    428.849
  eval_steps_per_second   =      1.548
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

100% 1/1 [00:00<00:00, 24.18it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:28,943 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:28,944 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:28,945 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:28,948 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:28,950 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/rte/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:32,128 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:32,130 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:32,192 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:32,194 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:32,198 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:32,198 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:32,198 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:32,198 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:32,198 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:32,198 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:32,200 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:32,202 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:32,289 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:32,289 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:32,355 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:32,508 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:32,508 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:33,267 >> Using auto half precision backend
2024-11-04 19:07:33,267 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:33,269 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:33,271 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:33,271 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-04 19:07:33,271 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    669.272
  eval_steps_per_second   =      2.416
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 277
})]

100% 1/1 [00:00<00:00, 13.42it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:33,795 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:33,796 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:33,798 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:33,801 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:33,803 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:36,693 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:36,696 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:36,754 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:36,756 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:36,759 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:36,760 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:36,760 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:36,760 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:36,760 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:36,760 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:36,761 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:36,764 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:36,850 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:36,851 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:36,936 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:37,085 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:37,085 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:37,776 >> Using auto half precision backend
2024-11-04 19:07:37,777 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:37,779 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:37,781 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:37,781 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:07:37,781 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6282
  eval_loss               =     0.6611
  eval_runtime            = 0:00:00.44
  eval_samples            =        277
  eval_samples_per_second =    622.545
  eval_steps_per_second   =      2.247
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.25it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:38,675 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:38,675 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:38,676 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:38,679 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:38,680 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:41,797 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:41,800 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:41,856 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:41,857 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:41,860 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:41,860 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:41,860 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:41,860 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:41,860 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:41,860 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:41,861 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:41,862 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:41,947 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:41,948 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:42,019 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:42,175 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:42,176 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:42,938 >> Using auto half precision backend
2024-11-04 19:07:42,938 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:42,939 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:42,941 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:42,941 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:07:42,941 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4247
  eval_runtime            = 0:00:00.84
  eval_samples            =        408
  eval_samples_per_second =    480.275
  eval_steps_per_second   =      1.177
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 27.23it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:43,606 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:43,606 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:43,607 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:43,609 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:43,610 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:46,592 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:46,594 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:46,703 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:46,706 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:46,711 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:46,711 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:46,711 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:46,711 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:46,711 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:46,711 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:46,712 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:46,715 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:46,793 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:46,796 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:46,918 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:47,123 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:47,123 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:47,801 >> Using auto half precision backend
2024-11-04 19:07:47,802 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:47,803 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:47,805 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:47,805 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:07:47,805 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4243
  eval_runtime            = 0:00:00.59
  eval_samples            =        408
  eval_samples_per_second =    684.142
  eval_steps_per_second   =      1.677
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 29.57it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:48,239 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:48,239 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:48,240 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:48,243 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:48,246 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:51,311 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:51,313 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:51,362 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:51,364 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:51,367 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:51,367 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:51,367 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:51,367 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:51,367 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:51,367 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:51,368 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:51,370 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:51,440 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:51,442 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:51,530 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:51,653 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:51,653 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:52,240 >> Using auto half precision backend
2024-11-04 19:07:52,240 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:52,241 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:52,243 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:52,243 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:07:52,243 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4242
  eval_runtime            = 0:00:00.37
  eval_samples            =        408
  eval_samples_per_second =   1075.835
  eval_steps_per_second   =      2.637
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.13it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:52,892 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:52,893 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:52,894 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:52,896 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:52,897 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:07:55,923 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:55,925 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:55,989 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:55,992 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:55,995 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:55,996 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:55,996 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:55,996 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:55,996 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:07:55,996 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:07:55,997 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:55,999 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:07:56,093 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:07:56,094 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:07:56,205 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:07:56,378 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:07:56,378 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:07:57,074 >> Using auto half precision backend
2024-11-04 19:07:57,074 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:07:57,075 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:07:57,077 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:07:57,077 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:07:57,077 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4237
  eval_runtime            = 0:00:00.60
  eval_samples            =        408
  eval_samples_per_second =    676.582
  eval_steps_per_second   =      1.658
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.11it/s]
[INFO|training_args.py:1902] 2024-11-04 19:07:57,535 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:07:57,536 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:07:57,537 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:07:57,539 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:07:57,542 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:00,323 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:00,326 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:00,387 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:00,391 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:00,395 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:00,395 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:00,395 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:00,395 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:00,395 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:00,395 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:00,397 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:00,400 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:00,501 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:00,505 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:00,574 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:00,724 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:00,724 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:01,398 >> Using auto half precision backend
2024-11-04 19:08:01,399 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:01,400 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:01,402 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:01,402 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:08:01,402 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4232
  eval_runtime            = 0:00:00.40
  eval_samples            =        408
  eval_samples_per_second =   1009.062
  eval_steps_per_second   =      2.473
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 14.64it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:01,895 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:01,895 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:01,896 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:01,899 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:01,900 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:05,453 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:05,456 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:05,514 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:05,515 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:05,520 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:05,520 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:05,520 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:05,520 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:05,520 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:05,520 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:05,522 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:05,524 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:05,594 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:05,596 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:05,649 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:05,757 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:05,757 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:06,529 >> Using auto half precision backend
2024-11-04 19:08:06,530 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:06,531 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:06,532 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:06,532 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:08:06,533 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4241
  eval_runtime            = 0:00:00.39
  eval_samples            =        408
  eval_samples_per_second =   1023.276
  eval_steps_per_second   =      2.508
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.53it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:06,963 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:06,964 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:06,965 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:06,967 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:06,968 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:10,575 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:10,577 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:10,683 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:10,685 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:10,689 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:10,689 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:10,689 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:10,689 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:10,689 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:10,689 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:10,691 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:10,693 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:10,763 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:10,764 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:10,813 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:10,915 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:10,915 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:12,594 >> Using auto half precision backend
2024-11-04 19:08:12,594 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:12,595 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:12,597 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:12,597 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:08:12,597 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4238
  eval_runtime            = 0:00:00.37
  eval_samples            =        408
  eval_samples_per_second =   1083.597
  eval_steps_per_second   =      2.656
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.52it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:13,035 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:13,036 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:13,037 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:13,040 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:13,041 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:16,231 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:16,234 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:16,293 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:16,295 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:16,298 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:16,298 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:16,298 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:16,299 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:16,299 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:16,299 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:16,300 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:16,302 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:16,383 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:16,386 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:16,453 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:16,644 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:16,645 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:17,429 >> Using auto half precision backend
2024-11-04 19:08:17,430 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:17,431 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:17,434 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:17,434 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:08:17,434 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4237
  eval_runtime            = 0:00:00.37
  eval_samples            =        408
  eval_samples_per_second =   1086.053
  eval_steps_per_second   =      2.662
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 18.50it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:17,910 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:17,911 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:17,913 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:17,916 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:17,918 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:20,714 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:20,716 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:20,772 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:20,774 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:20,778 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:20,778 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:20,778 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:20,778 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:20,778 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:20,778 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:20,780 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:20,782 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:20,867 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:20,870 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:20,939 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:21,090 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:21,090 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:21,902 >> Using auto half precision backend
2024-11-04 19:08:21,902 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:21,904 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:21,907 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:21,907 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-04 19:08:21,907 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4241
  eval_runtime            = 0:00:00.41
  eval_samples            =        408
  eval_samples_per_second =    987.486
  eval_steps_per_second   =       2.42
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 21.92it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:22,617 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:22,618 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:22,619 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:22,621 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:22,628 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:26,562 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:26,564 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:26,621 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:26,623 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:26,628 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:26,628 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:26,628 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:26,629 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:26,629 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:26,629 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:26,630 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:26,632 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:26,731 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:26,733 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:26,807 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:26,930 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:26,931 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:27,650 >> Using auto half precision backend
2024-11-04 19:08:27,650 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:27,651 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:27,653 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:27,653 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:08:27,653 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8064
  eval_combined_score     =     0.8368
  eval_f1                 =     0.8672
  eval_loss               =     0.4239
  eval_runtime            = 0:00:00.65
  eval_samples            =        408
  eval_samples_per_second =    623.018
  eval_steps_per_second   =      1.527
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.21it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:28,536 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:28,537 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:28,538 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:28,540 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:28,542 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:34,457 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:34,460 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:34,824 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:34,826 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:34,847 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:34,847 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:34,847 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:34,847 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:34,847 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:34,847 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:34,849 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:34,851 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:34,926 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:34,926 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:34,979 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:35,093 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:35,093 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:35,737 >> Using auto half precision backend
2024-11-04 19:08:35,737 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:35,739 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:35,740 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:35,741 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:08:35,741 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4754
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.82
  eval_samples              =       1043
  eval_samples_per_second   =   1269.849
  eval_steps_per_second     =      1.217
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.82it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:36,595 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:36,595 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:36,596 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:36,598 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:36,599 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:41,005 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:41,007 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:41,061 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:41,064 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:41,066 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:41,067 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:41,067 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:41,067 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:41,067 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:41,067 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:41,068 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:41,071 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:41,141 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:41,143 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:41,195 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:41,315 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:41,315 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:42,008 >> Using auto half precision backend
2024-11-04 19:08:42,008 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:42,010 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:42,012 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:42,012 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:08:42,012 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4755
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.81
  eval_samples              =       1043
  eval_samples_per_second   =   1285.122
  eval_steps_per_second     =      1.232
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 23.26it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:42,893 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:42,894 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:42,895 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:42,897 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:42,898 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:46,416 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:46,418 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:46,471 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:46,473 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:46,477 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:46,477 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:46,477 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:46,477 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:46,477 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:46,477 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:46,479 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:46,482 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:46,598 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:46,599 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:46,688 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:46,832 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:46,832 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:47,618 >> Using auto half precision backend
2024-11-04 19:08:47,618 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:47,620 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:47,622 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:47,622 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:08:47,622 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4756
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.83
  eval_samples              =       1043
  eval_samples_per_second   =   1246.922
  eval_steps_per_second     =      1.196
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 23.14it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:48,530 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:48,531 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:48,532 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:48,534 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:48,535 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:51,571 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:51,574 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:51,625 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:51,627 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:51,631 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:51,631 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:51,631 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:51,631 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:51,631 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:51,631 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:51,633 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:51,635 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:51,774 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:51,775 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:51,826 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:51,928 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:51,928 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:52,591 >> Using auto half precision backend
2024-11-04 19:08:52,592 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:52,593 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:52,595 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:52,595 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:08:52,595 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_loss                 =     0.4755
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.86
  eval_samples              =       1043
  eval_samples_per_second   =   1212.703
  eval_steps_per_second     =      1.163
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]

100% 1/1 [00:00<00:00, 20.65it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:53,506 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:53,506 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:53,507 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:53,509 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:53,511 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:08:56,573 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:56,577 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:56,641 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:56,644 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:56,647 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:56,647 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:56,647 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:56,648 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:56,648 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:08:56,648 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:08:56,649 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:56,651 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:08:56,743 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:08:56,744 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:08:56,835 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:08:57,010 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:08:57,010 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:08:57,735 >> Using auto half precision backend
2024-11-04 19:08:57,735 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:08:57,737 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:08:57,738 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:08:57,739 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:08:57,739 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4755
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.86
  eval_samples              =       1043
  eval_samples_per_second   =   1210.204
  eval_steps_per_second     =       1.16
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.59it/s]
[INFO|training_args.py:1902] 2024-11-04 19:08:58,810 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:08:58,810 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:08:58,811 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:08:58,813 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:08:58,813 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:09:32,449 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:32,450 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:32,507 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:32,509 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:32,514 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:32,514 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:32,514 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:32,514 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:32,514 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:32,515 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:09:32,516 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:32,518 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:32,590 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:32,590 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:09:32,682 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:09:32,805 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:09:32,805 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:09:33,422 >> Using auto half precision backend
2024-11-04 19:09:33,422 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:09:33,423 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:09:33,425 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:09:33,425 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:09:33,425 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4755
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:01.03
  eval_samples              =       1043
  eval_samples_per_second   =   1007.108
  eval_steps_per_second     =      0.966
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.98it/s]
[INFO|training_args.py:1902] 2024-11-04 19:09:34,280 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:09:34,281 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:09:34,282 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:09:34,284 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:09:34,285 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:09:37,658 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:37,661 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:37,727 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:37,730 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:37,733 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:37,733 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:37,733 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:37,733 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:37,733 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:37,733 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:09:37,735 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:37,737 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:37,874 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:37,875 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:09:37,939 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:09:38,079 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:09:38,079 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:09:38,767 >> Using auto half precision backend
2024-11-04 19:09:38,767 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:09:38,769 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:09:38,770 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:09:38,770 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:09:38,770 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4755
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.81
  eval_samples              =       1043
  eval_samples_per_second   =   1276.629
  eval_steps_per_second     =      1.224
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.87it/s]
[INFO|training_args.py:1902] 2024-11-04 19:09:39,650 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:09:39,651 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:09:39,652 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:09:39,654 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:09:39,656 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:09:42,514 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:42,517 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:42,574 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:42,576 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:42,580 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:42,580 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:42,580 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:42,580 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:42,580 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:42,580 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:09:42,581 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:42,584 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:42,676 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:42,679 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:09:42,749 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:09:42,882 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:09:42,883 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:09:43,684 >> Using auto half precision backend
2024-11-04 19:09:43,684 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:09:43,686 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:09:43,688 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:09:43,688 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:09:43,688 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4756
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.82
  eval_samples              =       1043
  eval_samples_per_second   =   1259.724
  eval_steps_per_second     =      1.208
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.17it/s]
[INFO|training_args.py:1902] 2024-11-04 19:09:44,793 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:09:44,793 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:09:44,794 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:09:44,796 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:09:44,798 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=matthews_correlation,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:09:48,054 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:48,056 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:48,130 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:48,133 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:48,137 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:48,137 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:48,137 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:48,137 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:48,137 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:48,137 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:09:48,139 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:48,141 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:48,278 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:48,279 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:09:48,343 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:09:48,484 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:09:48,484 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:09:49,163 >> Using auto half precision backend
2024-11-04 19:09:49,163 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:09:49,164 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:09:49,166 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:09:49,166 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-04 19:09:49,166 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4756
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:01.06
  eval_samples              =       1043
  eval_samples_per_second   =    979.856
  eval_steps_per_second     =      0.939
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 19.36it/s]
[INFO|training_args.py:1902] 2024-11-04 19:09:50,066 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:09:50,067 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:09:50,068 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:09:50,070 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:09:50,072 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:09:53,244 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:53,246 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:53,298 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:53,300 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:53,303 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:53,304 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:53,304 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:53,304 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:53,304 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:09:53,304 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:09:53,305 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:53,307 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:09:53,464 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:09:53,467 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:09:53,544 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:09:53,680 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:09:53,680 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:09:54,345 >> Using auto half precision backend
2024-11-04 19:09:54,346 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:09:54,347 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:09:54,349 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:09:54,349 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:09:54,350 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4756
  eval_matthews_correlation =     0.5703
  eval_runtime              = 0:00:00.84
  eval_samples              =       1043
  eval_samples_per_second   =   1231.852
  eval_steps_per_second     =      1.181
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.39it/s]
100% 3/3 [00:02<00:00,  1.32it/s]
100% 3/3 [00:02<00:00,  1.30it/s]
[INFO|training_args.py:1902] 2024-11-04 19:09:58,595 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:09:58,596 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:09:58,598 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:09:58,601 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:09:58,603 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:01,986 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:01,988 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:02,047 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:02,049 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:02,053 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:02,053 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:02,053 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:02,054 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:02,054 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:02,054 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:02,056 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:02,058 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:02,139 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:02,140 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:02,245 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:02,388 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:02,388 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:03,049 >> Using auto half precision backend
2024-11-04 19:10:03,050 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:03,051 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:03,053 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:03,053 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:03,053 >>   Batch size = 2048

  0% 0/3 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.18
  eval_samples            =       5463
  eval_samples_per_second =   1305.007
  eval_steps_per_second   =      0.717
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

 67% 2/3 [00:01<00:00,  1.41it/s]
100% 3/3 [00:02<00:00,  1.32it/s]
100% 3/3 [00:02<00:00,  1.28it/s]
[INFO|training_args.py:1902] 2024-11-04 19:10:07,335 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:10:07,336 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:10:07,337 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:10:07,339 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:10:07,340 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:10,488 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:10,491 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:10,549 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:10,552 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:10,555 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:10,555 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:10,555 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:10,555 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:10,555 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:10,556 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:10,557 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:10,559 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:10,651 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:10,652 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:10,719 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:10,921 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:10,921 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:12,500 >> Using auto half precision backend
2024-11-04 19:10:12,500 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:12,502 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:12,504 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:12,504 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:12,504 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.21
  eval_samples            =       5463
  eval_samples_per_second =   1296.701
  eval_steps_per_second   =      0.712
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.39it/s]
100% 3/3 [00:02<00:00,  1.26it/s]
100% 3/3 [00:02<00:00,  1.25it/s]
[INFO|training_args.py:1902] 2024-11-04 19:10:17,076 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:10:17,076 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:10:17,077 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:10:17,080 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:10:17,081 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:20,490 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:20,493 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:20,548 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:20,551 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:20,554 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:20,554 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:20,555 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:20,555 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:20,555 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:20,555 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:20,556 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:20,558 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:20,638 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:20,639 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:20,741 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:20,889 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:20,889 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:21,578 >> Using auto half precision backend
2024-11-04 19:10:21,579 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:21,581 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:21,582 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:21,583 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:21,583 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.52
  eval_samples            =       5463
  eval_samples_per_second =   1207.937
  eval_steps_per_second   =      0.663
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.41it/s]
100% 3/3 [00:02<00:00,  1.18it/s]
100% 3/3 [00:02<00:00,  1.19it/s]
[INFO|training_args.py:1902] 2024-11-04 19:10:26,032 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:10:26,032 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:10:26,033 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:10:26,036 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:10:26,037 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:29,321 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:29,323 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:29,380 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:29,382 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:29,386 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:29,386 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:29,386 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:29,387 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:29,387 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:29,387 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:29,388 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:29,391 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:29,550 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:29,551 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:29,616 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:29,756 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:29,756 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:30,444 >> Using auto half precision backend
2024-11-04 19:10:30,444 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:30,445 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:30,447 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:30,447 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:30,447 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.38
  eval_samples            =       5463
  eval_samples_per_second =   1245.329
  eval_steps_per_second   =      0.684
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.40it/s]
100% 3/3 [00:02<00:00,  1.32it/s]
100% 3/3 [00:02<00:00,  1.30it/s]
[INFO|training_args.py:1902] 2024-11-04 19:10:34,720 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:10:34,720 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:10:34,721 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:10:34,724 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:10:34,727 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:37,888 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:37,890 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:37,951 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:37,953 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:37,961 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:37,961 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:37,961 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:37,961 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:37,961 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:37,961 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:37,964 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:37,966 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:38,048 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:38,048 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:38,117 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:38,251 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:38,251 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:38,979 >> Using auto half precision backend
2024-11-04 19:10:38,979 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:38,981 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:38,983 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:38,983 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:38,983 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.21
  eval_samples            =       5463
  eval_samples_per_second =    1294.73
  eval_steps_per_second   =      0.711
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.40it/s]
100% 3/3 [00:02<00:00,  1.32it/s]
100% 3/3 [00:02<00:00,  1.29it/s]
[INFO|training_args.py:1902] 2024-11-04 19:10:43,513 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:10:43,513 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:10:43,514 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:10:43,517 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:10:43,518 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:46,712 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:46,714 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:46,770 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:46,773 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:46,777 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:46,777 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:46,777 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:46,777 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:46,778 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:46,778 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:46,779 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:46,781 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:46,861 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:46,862 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:46,927 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:47,078 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:47,079 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:47,880 >> Using auto half precision backend
2024-11-04 19:10:47,880 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:47,882 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:47,884 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:47,884 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:47,884 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.47
  eval_samples            =       5463
  eval_samples_per_second =   1220.889
  eval_steps_per_second   =       0.67
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.37it/s]
100% 3/3 [00:02<00:00,  1.31it/s]
100% 3/3 [00:02<00:00,  1.29it/s]
[INFO|training_args.py:1902] 2024-11-04 19:10:52,142 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:10:52,142 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:10:52,143 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:10:52,146 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:10:52,147 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:10:55,194 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:55,196 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:55,259 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:55,262 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:55,266 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:55,266 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:55,266 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:55,266 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:55,266 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:10:55,266 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:10:55,268 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:55,270 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:10:55,356 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:10:55,357 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:10:55,434 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:10:55,587 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:10:55,587 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:10:56,707 >> Using auto half precision backend
2024-11-04 19:10:56,708 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:10:56,710 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:10:56,712 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:10:56,712 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:10:56,712 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.18
  eval_samples            =       5463
  eval_samples_per_second =   1304.296
  eval_steps_per_second   =      0.716
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.40it/s]
100% 3/3 [00:02<00:00,  1.32it/s]
100% 3/3 [00:02<00:00,  1.31it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:00,975 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:00,975 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:00,976 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:00,979 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:00,980 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:04,357 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:04,360 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:04,418 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:04,421 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:04,424 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:04,424 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:04,424 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:04,424 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:04,425 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:04,425 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:04,426 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:04,429 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:04,522 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:04,523 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:04,650 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:04,795 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:04,795 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:05,613 >> Using auto half precision backend
2024-11-04 19:11:05,614 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:05,616 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:05,618 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:05,618 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:11:05,618 >>   Batch size = 2048

  0% 0/3 [00:00<?, ?it/s]***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.21
  eval_samples            =       5463
  eval_samples_per_second =   1296.493
  eval_steps_per_second   =      0.712
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

 67% 2/3 [00:01<00:00,  1.18it/s]
100% 3/3 [00:02<00:00,  1.20it/s]
100% 3/3 [00:02<00:00,  1.16it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:10,156 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:10,157 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:10,157 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:10,160 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:10,162 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/qnli/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:13,817 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:13,819 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:13,873 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:13,875 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:13,879 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:13,879 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:13,879 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:13,879 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:13,879 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:13,879 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:13,881 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:13,883 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:13,971 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:13,972 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:14,046 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:14,203 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:14,203 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:14,921 >> Using auto half precision backend
2024-11-04 19:11:14,921 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:14,923 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:14,925 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:14,925 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-04 19:11:14,925 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.45
  eval_samples            =       5463
  eval_samples_per_second =   1225.451
  eval_steps_per_second   =      0.673
trainable params: 0 || all params: 139,286,018 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.42it/s]
100% 3/3 [00:02<00:00,  1.33it/s]
100% 3/3 [00:02<00:00,  1.31it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:19,140 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:19,141 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:19,142 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:19,145 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:19,147 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:22,198 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:22,201 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:22,258 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:22,261 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:22,265 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:22,265 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:22,265 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:22,265 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:22,265 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:22,265 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:22,267 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:22,269 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:22,415 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:22,418 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:22,506 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:22,641 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:22,641 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:23,298 >> Using auto half precision backend
2024-11-04 19:11:23,298 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:23,299 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:23,302 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:23,302 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:23,302 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9277
  eval_loss               =     0.1871
  eval_runtime            = 0:00:04.15
  eval_samples            =       5463
  eval_samples_per_second =   1314.851
  eval_steps_per_second   =      0.722
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.61it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:24,572 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:24,572 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:24,573 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:24,576 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:24,578 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:27,738 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:27,740 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:27,799 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:27,801 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:27,807 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:27,807 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:27,807 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:27,807 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:27,807 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:27,807 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:27,809 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:27,811 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:27,899 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:27,900 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:27,973 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:28,116 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:28,116 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:28,810 >> Using auto half precision backend
2024-11-04 19:11:28,810 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:28,812 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:28,813 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:28,814 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:28,814 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_combined_score     =     0.8767
  eval_loss               =     0.5728
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.20
  eval_samples            =       1500
  eval_samples_per_second =   1249.109
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.833
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

100% 1/1 [00:00<00:00,  1.50it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:29,987 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:29,987 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:29,988 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:29,989 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:29,990 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:33,924 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:33,928 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:33,996 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:33,999 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:34,010 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:34,010 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:34,010 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:34,010 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:34,010 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:34,010 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:34,013 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:34,016 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:34,121 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:34,121 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:34,189 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:34,367 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:34,367 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:35,202 >> Using auto half precision backend
2024-11-04 19:11:35,202 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:35,203 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:35,205 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:35,205 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:35,205 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8767
  eval_loss               =     0.5733
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.14
  eval_samples            =       1500
  eval_samples_per_second =   1313.761
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.876
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.42it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:36,418 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:36,419 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:36,420 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:36,422 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:36,423 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:39,449 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:39,452 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:39,511 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:39,514 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:39,519 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:39,519 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:39,519 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:39,519 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:39,520 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:39,520 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:39,521 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:39,523 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:39,607 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:39,610 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:39,682 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:39,815 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:39,815 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:40,592 >> Using auto half precision backend
2024-11-04 19:11:40,592 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:40,593 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:40,595 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:40,595 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:40,595 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8767
  eval_loss               =     0.5733
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.15
  eval_samples            =       1500
  eval_samples_per_second =   1294.421
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.863
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.43it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:41,836 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:41,837 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:41,838 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:41,840 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:41,841 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:45,593 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:45,595 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:45,653 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:45,655 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:45,659 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:45,659 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:45,659 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:45,659 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:45,659 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:45,659 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:45,661 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:45,663 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:45,788 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:45,790 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:45,863 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:46,020 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:46,021 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:46,734 >> Using auto half precision backend
2024-11-04 19:11:46,735 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:46,736 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:46,738 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:46,738 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:46,738 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8768
  eval_loss               =     0.5725
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.16
  eval_samples            =       1500
  eval_samples_per_second =   1284.878
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.857
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.40it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:47,939 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:47,939 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:47,940 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:47,942 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:47,943 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:51,072 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:51,075 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:51,138 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:51,141 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:51,145 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:51,146 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:51,146 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:51,146 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:51,146 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:51,146 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:51,148 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:51,150 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:51,233 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:51,234 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:51,300 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:51,433 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:51,433 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:52,177 >> Using auto half precision backend
2024-11-04 19:11:52,177 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:52,179 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:52,181 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:52,181 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:52,181 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8767
  eval_loss               =     0.5727
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.15
  eval_samples            =       1500
  eval_samples_per_second =   1303.267
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.869
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.37it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:53,434 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:53,434 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:53,435 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:53,438 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:53,439 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:11:56,777 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:56,779 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:56,842 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:56,844 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:56,849 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:56,849 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:56,850 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:56,850 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:56,850 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:11:56,850 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:11:56,852 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:56,854 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:11:56,942 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:11:56,945 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:11:57,016 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:11:57,209 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:11:57,209 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:11:57,897 >> Using auto half precision backend
2024-11-04 19:11:57,897 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:11:57,898 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:11:57,900 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:11:57,900 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:11:57,901 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8768
  eval_loss               =     0.5724
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.19
  eval_samples            =       1500
  eval_samples_per_second =   1252.728
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.835
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.37it/s]
[INFO|training_args.py:1902] 2024-11-04 19:11:59,141 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:11:59,142 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:11:59,143 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:11:59,145 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:11:59,146 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:12:02,364 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:02,367 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:12:02,424 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:02,426 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:02,429 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:02,430 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:02,430 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:02,430 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:02,430 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:02,430 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:12:02,432 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:02,434 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:12:02,516 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:02,517 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:12:02,602 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:12:02,742 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:12:02,742 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:12:03,482 >> Using auto half precision backend
2024-11-04 19:12:03,482 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:12:03,483 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:12:03,485 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:12:03,485 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:12:03,485 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8768
  eval_loss               =     0.5719
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.17
  eval_samples            =       1500
  eval_samples_per_second =   1279.254
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.853
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  1.35it/s]
[INFO|training_args.py:1902] 2024-11-04 19:12:04,718 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:12:04,718 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:12:04,720 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:12:04,722 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:12:04,724 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:12:07,641 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:07,643 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:12:07,693 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:07,696 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:07,699 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:07,699 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:07,699 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:07,699 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:07,700 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:07,700 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:12:07,701 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:07,703 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:12:07,788 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:07,791 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:12:07,862 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:12:07,996 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:12:07,997 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:12:08,755 >> Using auto half precision backend
2024-11-04 19:12:08,755 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:12:08,757 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:12:08,759 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:12:08,759 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:12:08,759 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_combined_score     =     0.8767
  eval_loss               =     0.5727
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.17
  eval_samples            =       1500
  eval_samples_per_second =   1276.849
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.851
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

100% 1/1 [00:00<00:00,  1.38it/s]
[INFO|training_args.py:1902] 2024-11-04 19:12:09,985 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-04 19:12:09,986 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-04 19:12:09,987 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
2024-11-04 19:12:09,989 - WARNING - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: True, 16-bits training: True
2024-11-04 19:12:09,991 - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=combined_score,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/data3/user/jin509/lora-pdiff/dataset/deberta-base/lora_r_1/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.7999_end_epoch.8000/evaluate,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=512,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=log/cola/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:728] 2024-11-04 19:12:13,926 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:13,929 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:12:13,994 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:13,996 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:14,004 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:14,004 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:14,004 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:14,004 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:14,004 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-04 19:12:14,004 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-04 19:12:14,007 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:14,009 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-04 19:12:14,125 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/config.json
[INFO|configuration_utils.py:791] 2024-11-04 19:12:14,127 >> Model config DebertaConfig {
  "_name_or_path": "microsoft/deberta-base",
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "c2p",
    "p2c"
  ],
  "position_biased_input": false,
  "relative_attention": true,
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-04 19:12:14,200 >> loading weights file pytorch_model.bin from cache at /data3/user/jin509/hf_cache/hub/models--microsoft--deberta-base/snapshots/0d1b43ccf21b5acd9f4e5f7b077fa698f05cf195/pytorch_model.bin
[INFO|modeling_utils.py:3982] 2024-11-04 19:12:14,348 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-04 19:12:14,348 >> Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-04 19:12:15,063 >> Using auto half precision backend
2024-11-04 19:12:15,063 - INFO - *** Evaluate ***
[INFO|trainer.py:766] 2024-11-04 19:12:15,064 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-04 19:12:15,066 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-04 19:12:15,066 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-04 19:12:15,066 >>   Batch size = 2048

  0% 0/1 [00:00<?, ?it/s]***** eval metrics *****
  eval_combined_score     =     0.8768
  eval_loss               =     0.5722
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.16
  eval_samples            =       1500
  eval_samples_per_second =   1285.099
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.857
trainable params: 0 || all params: 139,285,249 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1500
})]

100% 1/1 [00:00<00:00,  1.42it/s]
2024-11-04 19:12:16,251 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models rte: 0.628158844765343
2024-11-04 19:12:16,253 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models mrpc: 0.8672268907563025
2024-11-04 19:12:16,255 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models cola: 0.5703223353398024
2024-11-04 19:12:16,257 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models qnli: 0.9276954054548783
2024-11-04 19:12:16,260 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models stsb: 0.8752480471896051
2024-11-04 19:12:16,261 - INFO - [diffusion][Epoch 0] average DIFF reconstruction auto_encoder_models accuracy: 0.773720953107701
2024-11-04 19:12:16,269 - INFO - [diffusion][Epoch 0] Diffusion reconstruction accuracy:[('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('stsb', 0.8751957889548678), ('stsb', 0.875172857009006), ('stsb', 0.8751632228967254), ('stsb', 0.8752220024628992), ('stsb', 0.8751913130919735), ('stsb', 0.8752209055614228), ('stsb', 0.8752480471896051), ('stsb', 0.8751828939486093), ('stsb', 0.8752204560933617), ('stsb', 0.8751954050133155)]
2024-11-04 19:12:16,273 - INFO - [diffusion][Epoch 0] ---------------------------------
2024-11-04 19:12:16,298 - INFO - [diffusion][Epoch 0] Testing complete!
***** eval metrics *****
  eval_combined_score     =     0.8767
  eval_loss               =     0.5726
  eval_pearson            =     0.8752
  eval_runtime            = 0:00:01.14
  eval_samples            =       1500
  eval_samples_per_second =   1308.218
  eval_spearmanr          =     0.8783
  eval_steps_per_second   =      0.872
=====================================
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: | 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: average_ae_rec_accs 
wandb:  average_input_accs 
wandb:               epoch 
wandb: 
wandb: Run summary:
wandb: average_ae_rec_accs 0.77531
wandb:  average_input_accs 0.77073
wandb:               epoch 0
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/jin509/para_diff/lora-cond-p-diff/wandb/offline-run-20241104_190245-oerhyomx
wandb: Find logs at: ./wandb/offline-run-20241104_190245-oerhyomx/logs








100% 1/1 [00:00<00:00,  1.38it/s]
2024-11-04 19:05:47,012 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model rte: 0.6101083032490975
2024-11-04 19:05:47,014 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model mrpc: 0.8692699490662139
2024-11-04 19:05:47,016 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model cola: 0.5703223353398024
2024-11-04 19:05:47,018 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model qnli: 0.9280615046677649
2024-11-04 19:05:47,019 - INFO - [auto encoder][Epoch 0] best input auto_encoder_model stsb: 0.8759083739074341
2024-11-04 19:05:47,021 - INFO - [auto encoder][Epoch 0] average input auto_encoder_model: 0.7707338275261566
2024-11-04 19:05:47,022 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models rte: 0.6389891696750902
2024-11-04 19:05:47,023 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models mrpc: 0.8667790893760541
2024-11-04 19:05:47,024 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models cola: 0.5703223353398024
2024-11-04 19:05:47,026 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models qnli: 0.9280615046677649
2024-11-04 19:05:47,027 - INFO - [auto encoder][Epoch 0] AE reconstruction auto_encoder_models stsb: 0.8756748520686412
2024-11-04 19:05:47,029 - INFO - [auto encoder][Epoch 0] ae_rec_accs: [('rte', 0.6389891696750902), ('rte', 0.6353790613718412), ('mrpc', 0.8638655462184874), ('mrpc', 0.8667790893760541), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9280615046677649), ('qnli', 0.9280615046677649), ('stsb', 0.8756748520686412), ('stsb', 0.875617678736242)]
2024-11-04 19:05:47,030 - INFO - [auto encoder][Epoch 0] average AE reconstruction auto_encoder_models accuracy: 0.775307307746149


100% 1/1 [00:00<00:00,  1.42it/s]
2024-11-04 19:12:16,251 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models rte: 0.628158844765343
2024-11-04 19:12:16,253 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models mrpc: 0.8672268907563025
2024-11-04 19:12:16,255 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models cola: 0.5703223353398024
2024-11-04 19:12:16,257 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models qnli: 0.9276954054548783
2024-11-04 19:12:16,260 - INFO - [diffusion][Epoch 0] DIFF reconstruction auto_encoder_models stsb: 0.8752480471896051
2024-11-04 19:12:16,261 - INFO - [diffusion][Epoch 0] average DIFF reconstruction auto_encoder_models accuracy: 0.773720953107701
2024-11-04 19:12:16,269 - INFO - [diffusion][Epoch 0] Diffusion reconstruction accuracy:[('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('rte', 0.628158844765343), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('mrpc', 0.8672268907563025), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('cola', 0.5703223353398024), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('qnli', 0.9276954054548783), ('stsb', 0.8751957889548678), ('stsb', 0.875172857009006), ('stsb', 0.8751632228967254), ('stsb', 0.8752220024628992), ('stsb', 0.8751913130919735), ('stsb', 0.8752209055614228), ('stsb', 0.8752480471896051), ('stsb', 0.8751828939486093), ('stsb', 0.8752204560933617), ('stsb', 0.8751954050133155)]
2024-11-04 19:12:16,273 - INFO - [diffusion][Epoch 0] ---------------------------------
2024-11-04 19:12:16,298 - INFO - [diffusion][Epoch 0] Testing complete!