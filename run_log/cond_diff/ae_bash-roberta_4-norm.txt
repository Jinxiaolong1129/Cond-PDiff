stty: 'standard input': Inappropriate ioctl for device
2024-11-05 00:20:35,068 - INFO - CUDA_VISIBLE_DEVICES: 6,7,4,5,
2024-11-05 00:20:35,068 - INFO - layer_num: [[11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]]
2024-11-05 00:20:35,068 - INFO - datasets_para: [['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']]
2024-11-05 00:20:35,068 - INFO - gpu_id: 0
2024-11-05 00:20:35,069 - INFO - batch_size: 256
2024-11-05 00:20:35,069 - INFO - epochs: 10000
2024-11-05 00:20:35,069 - INFO - ae_test: True
2024-11-05 00:20:35,069 - INFO - load_ae_checkpoint: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_9999.pth
2024-11-05 00:20:35,069 - INFO - lr: 0.001
2024-11-05 00:20:35,069 - INFO - patience: 5000
transformers.__file__: /home/jin509/para_diff/lora-cond-p-diff/src/transformers/__init__.py 
 peft.__file_: /home/jin509/para_diff/lora-cond-p-diff/src/peft/__init__.py
=====================================
layer number: [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
datasets: ['sst2', 'rte', 'mrpc', 'cola', 'qnli', 'stsb']
=====================================
wandb: Tracking run with wandb version 0.12.21
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  dataset_data = torch.tensor(dataset_matrix, dtype=torch.float32)
/home/jin509/para_diff/lora-cond-p-diff/dataset/weights_dataset.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return (torch.tensor(data_row, dtype=torch.float32) - mean) / std
2024-11-05 00:22:04,112 - INFO - Load model: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_9999.pth
2024-11-05 00:22:04,113 - INFO - Start epoch: 9999
2024-11-05 00:22:04,143 - INFO - Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.10000/model/auto_encoder_model_9999.pth with best metric -inf
2024-11-05 00:22:04,158 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.9999_end_epoch.10000/model
2024-11-05 00:22:04,158 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/norm.exp_batch.256_lr.0.001_patience.5000_start_epoch.9999_end_epoch.10000/log
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 00:22:04,515 - INFO - =========== ddpm diffusion model | num_conditions: 6 ===========
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
2024-11-05 00:22:07,284 - INFO - [diffusion][Epoch 0] Load model: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_5999.pth
2024-11-05 00:22:07,315 - INFO - [diffusion][Epoch 0] Loaded checkpoint from /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.0_end_epoch.8000/clip_pdiff_model/clip_cond_diffusion_model_5999.pth
2024-11-05 00:22:07,317 - INFO - [diffusion][Epoch 0] ========================================
2024-11-05 00:22:07,319 - INFO - Model save dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.5999_end_epoch.12000/clip_pdiff_model
2024-11-05 00:22:07,320 - INFO - Log dir: /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/clip_pdiff_exp_batch.256_lr.0.001_patience.5000_start_epoch.5999_end_epoch.12000/log
2024-11-05 00:22:07,320 - INFO - [diffusion][Epoch 0] Training begin
2024-11-05 00:22:07,321 - INFO - [diffusion][Epoch 5999] Epoch 6000/12000
=====================================
Test the auto_encoder_model
training_parameters:
  ddpm_eval_epoch: 10
  ddpm_save_epoch: 1000
  ddpm_start_epoch: 1
  ddpm_end_epoch: 12000
  batch_size: 256
  lr: 0.001
  patience: 5000
  optimizer: Adam
  loss_function: MSE
  metrics:
  - accuracy
  weight_decay: 2.0e-06
  ddpm_test: false
  test_similarity: false
model_mean_type: eps
model_var_type: fixedlarge
loss_type: mse
model_parameters:
  in_channel: 1
  in_dim: 12
  num_conditions: 2
  cond_emb_size: 512
beta_schedule:
  start: 0.0001
  end: 0.02
  schedule: linear
  n_timestep: 1000
glue_test_parameters:
  config_path: config/multiple/glue.json

2024-11-05 00:22:12,132 - INFO - [diffusion][Epoch 5999] diffusion training Loss: 0.11979756504297256
2024-11-05 00:22:12,134 - INFO - [diffusion][Epoch 5999] diffusion learning rate: 0.001
2024-11-05 00:22:12,337 - INFO - [diffusion][Epoch 5999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:12,339 - INFO - [diffusion][Epoch 6000] Epoch 6001/12000
2024-11-05 00:22:16,242 - INFO - [diffusion][Epoch 6000] diffusion training Loss: 0.11513600870966911
2024-11-05 00:22:16,244 - INFO - [diffusion][Epoch 6000] diffusion learning rate: 0.001
2024-11-05 00:22:16,247 - INFO - [diffusion][Epoch 6000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:16,248 - INFO - [diffusion][Epoch 6001] Epoch 6002/12000
2024-11-05 00:22:20,105 - INFO - [diffusion][Epoch 6001] diffusion training Loss: 0.08565155044198036
2024-11-05 00:22:20,107 - INFO - [diffusion][Epoch 6001] diffusion learning rate: 0.001
2024-11-05 00:22:20,109 - INFO - [diffusion][Epoch 6001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:20,110 - INFO - [diffusion][Epoch 6002] Epoch 6003/12000
2024-11-05 00:22:24,022 - INFO - [diffusion][Epoch 6002] diffusion training Loss: 0.08016481250524521
2024-11-05 00:22:24,023 - INFO - [diffusion][Epoch 6002] diffusion learning rate: 0.001
2024-11-05 00:22:24,025 - INFO - [diffusion][Epoch 6002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:24,026 - INFO - [diffusion][Epoch 6003] Epoch 6004/12000
2024-11-05 00:22:27,963 - INFO - [diffusion][Epoch 6003] diffusion training Loss: 0.07366905361413956
2024-11-05 00:22:27,965 - INFO - [diffusion][Epoch 6003] diffusion learning rate: 0.001
2024-11-05 00:22:27,967 - INFO - [diffusion][Epoch 6003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:27,968 - INFO - [diffusion][Epoch 6004] Epoch 6005/12000
2024-11-05 00:22:32,098 - INFO - [diffusion][Epoch 6004] diffusion training Loss: 0.07029771618545055
2024-11-05 00:22:32,100 - INFO - [diffusion][Epoch 6004] diffusion learning rate: 0.001
2024-11-05 00:22:32,179 - INFO - [diffusion][Epoch 6004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:32,180 - INFO - [diffusion][Epoch 6005] Epoch 6006/12000
2024-11-05 00:22:36,434 - INFO - [diffusion][Epoch 6005] diffusion training Loss: 0.07368210516870022
2024-11-05 00:22:36,436 - INFO - [diffusion][Epoch 6005] diffusion learning rate: 0.001
2024-11-05 00:22:36,438 - INFO - [diffusion][Epoch 6005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:36,439 - INFO - [diffusion][Epoch 6006] Epoch 6007/12000
2024-11-05 00:22:40,511 - INFO - [diffusion][Epoch 6006] diffusion training Loss: 0.07337530143558979
2024-11-05 00:22:40,513 - INFO - [diffusion][Epoch 6006] diffusion learning rate: 0.001
2024-11-05 00:22:40,515 - INFO - [diffusion][Epoch 6006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:40,517 - INFO - [diffusion][Epoch 6007] Epoch 6008/12000
2024-11-05 00:22:44,660 - INFO - [diffusion][Epoch 6007] diffusion training Loss: 0.06067569553852081
2024-11-05 00:22:44,662 - INFO - [diffusion][Epoch 6007] diffusion learning rate: 0.001
2024-11-05 00:22:44,664 - INFO - [diffusion][Epoch 6007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:44,665 - INFO - [diffusion][Epoch 6008] Epoch 6009/12000
2024-11-05 00:22:48,579 - INFO - [diffusion][Epoch 6008] diffusion training Loss: 0.059825566597282887
2024-11-05 00:22:48,581 - INFO - [diffusion][Epoch 6008] diffusion learning rate: 0.001
2024-11-05 00:22:48,583 - INFO - [diffusion][Epoch 6008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:48,584 - INFO - [diffusion][Epoch 6009] Epoch 6010/12000
2024-11-05 00:22:52,488 - INFO - [diffusion][Epoch 6009] diffusion training Loss: 0.057392094284296036
2024-11-05 00:22:52,490 - INFO - [diffusion][Epoch 6009] diffusion learning rate: 0.001
2024-11-05 00:22:52,492 - INFO - [diffusion][Epoch 6009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:52,493 - INFO - [diffusion][Epoch 6010] Epoch 6011/12000
2024-11-05 00:22:56,572 - INFO - [diffusion][Epoch 6010] diffusion training Loss: 0.06161410175263882
2024-11-05 00:22:56,574 - INFO - [diffusion][Epoch 6010] diffusion learning rate: 0.001
2024-11-05 00:22:56,576 - INFO - [diffusion][Epoch 6010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:22:56,577 - INFO - [diffusion][Epoch 6011] Epoch 6012/12000
2024-11-05 00:23:00,522 - INFO - [diffusion][Epoch 6011] diffusion training Loss: 0.05968267656862736
2024-11-05 00:23:00,524 - INFO - [diffusion][Epoch 6011] diffusion learning rate: 0.001
2024-11-05 00:23:00,526 - INFO - [diffusion][Epoch 6011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:00,528 - INFO - [diffusion][Epoch 6012] Epoch 6013/12000
2024-11-05 00:23:04,490 - INFO - [diffusion][Epoch 6012] diffusion training Loss: 0.060542937368154526
2024-11-05 00:23:04,494 - INFO - [diffusion][Epoch 6012] diffusion learning rate: 0.001
2024-11-05 00:23:04,497 - INFO - [diffusion][Epoch 6012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:04,499 - INFO - [diffusion][Epoch 6013] Epoch 6014/12000
2024-11-05 00:23:08,381 - INFO - [diffusion][Epoch 6013] diffusion training Loss: 0.060253689996898174
2024-11-05 00:23:08,383 - INFO - [diffusion][Epoch 6013] diffusion learning rate: 0.001
2024-11-05 00:23:08,385 - INFO - [diffusion][Epoch 6013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:08,386 - INFO - [diffusion][Epoch 6014] Epoch 6015/12000
2024-11-05 00:23:12,427 - INFO - [diffusion][Epoch 6014] diffusion training Loss: 0.06277510896325111
2024-11-05 00:23:12,429 - INFO - [diffusion][Epoch 6014] diffusion learning rate: 0.001
2024-11-05 00:23:12,430 - INFO - [diffusion][Epoch 6014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:12,432 - INFO - [diffusion][Epoch 6015] Epoch 6016/12000
2024-11-05 00:23:16,500 - INFO - [diffusion][Epoch 6015] diffusion training Loss: 0.06171365547925234
2024-11-05 00:23:16,502 - INFO - [diffusion][Epoch 6015] diffusion learning rate: 0.001
2024-11-05 00:23:16,504 - INFO - [diffusion][Epoch 6015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:16,505 - INFO - [diffusion][Epoch 6016] Epoch 6017/12000
2024-11-05 00:23:20,578 - INFO - [diffusion][Epoch 6016] diffusion training Loss: 0.06557219941169024
2024-11-05 00:23:20,579 - INFO - [diffusion][Epoch 6016] diffusion learning rate: 0.001
2024-11-05 00:23:20,581 - INFO - [diffusion][Epoch 6016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:20,583 - INFO - [diffusion][Epoch 6017] Epoch 6018/12000
2024-11-05 00:23:24,656 - INFO - [diffusion][Epoch 6017] diffusion training Loss: 0.06243747565895319
2024-11-05 00:23:24,659 - INFO - [diffusion][Epoch 6017] diffusion learning rate: 0.001
2024-11-05 00:23:24,661 - INFO - [diffusion][Epoch 6017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:24,662 - INFO - [diffusion][Epoch 6018] Epoch 6019/12000
2024-11-05 00:23:29,038 - INFO - [diffusion][Epoch 6018] diffusion training Loss: 0.06568629015237093
2024-11-05 00:23:29,040 - INFO - [diffusion][Epoch 6018] diffusion learning rate: 0.001
2024-11-05 00:23:29,041 - INFO - [diffusion][Epoch 6018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:29,043 - INFO - [diffusion][Epoch 6019] Epoch 6020/12000
2024-11-05 00:23:33,081 - INFO - [diffusion][Epoch 6019] diffusion training Loss: 0.06872208788990974
2024-11-05 00:23:33,084 - INFO - [diffusion][Epoch 6019] diffusion learning rate: 0.001
2024-11-05 00:23:33,086 - INFO - [diffusion][Epoch 6019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:33,087 - INFO - [diffusion][Epoch 6020] Epoch 6021/12000
2024-11-05 00:23:37,122 - INFO - [diffusion][Epoch 6020] diffusion training Loss: 0.06298927590250969
2024-11-05 00:23:37,124 - INFO - [diffusion][Epoch 6020] diffusion learning rate: 0.001
2024-11-05 00:23:37,126 - INFO - [diffusion][Epoch 6020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:37,127 - INFO - [diffusion][Epoch 6021] Epoch 6022/12000
2024-11-05 00:23:41,213 - INFO - [diffusion][Epoch 6021] diffusion training Loss: 0.06262059602886438
2024-11-05 00:23:41,215 - INFO - [diffusion][Epoch 6021] diffusion learning rate: 0.001
2024-11-05 00:23:41,217 - INFO - [diffusion][Epoch 6021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:41,218 - INFO - [diffusion][Epoch 6022] Epoch 6023/12000
2024-11-05 00:23:45,166 - INFO - [diffusion][Epoch 6022] diffusion training Loss: 0.05414982698857784
2024-11-05 00:23:45,168 - INFO - [diffusion][Epoch 6022] diffusion learning rate: 0.001
2024-11-05 00:23:45,170 - INFO - [diffusion][Epoch 6022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:45,171 - INFO - [diffusion][Epoch 6023] Epoch 6024/12000
2024-11-05 00:23:49,199 - INFO - [diffusion][Epoch 6023] diffusion training Loss: 0.06038963980972767
2024-11-05 00:23:49,202 - INFO - [diffusion][Epoch 6023] diffusion learning rate: 0.001
2024-11-05 00:23:49,205 - INFO - [diffusion][Epoch 6023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:49,206 - INFO - [diffusion][Epoch 6024] Epoch 6025/12000
2024-11-05 00:23:53,339 - INFO - [diffusion][Epoch 6024] diffusion training Loss: 0.06021841615438461
2024-11-05 00:23:53,341 - INFO - [diffusion][Epoch 6024] diffusion learning rate: 0.001
2024-11-05 00:23:53,342 - INFO - [diffusion][Epoch 6024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:53,344 - INFO - [diffusion][Epoch 6025] Epoch 6026/12000
2024-11-05 00:23:57,412 - INFO - [diffusion][Epoch 6025] diffusion training Loss: 0.059768203645944595
2024-11-05 00:23:57,415 - INFO - [diffusion][Epoch 6025] diffusion learning rate: 0.001
2024-11-05 00:23:57,418 - INFO - [diffusion][Epoch 6025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:23:57,419 - INFO - [diffusion][Epoch 6026] Epoch 6027/12000
2024-11-05 00:24:01,492 - INFO - [diffusion][Epoch 6026] diffusion training Loss: 0.05265533737838268
2024-11-05 00:24:01,494 - INFO - [diffusion][Epoch 6026] diffusion learning rate: 0.001
2024-11-05 00:24:01,496 - INFO - [diffusion][Epoch 6026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:01,497 - INFO - [diffusion][Epoch 6027] Epoch 6028/12000
2024-11-05 00:24:05,492 - INFO - [diffusion][Epoch 6027] diffusion training Loss: 0.058226145803928375
2024-11-05 00:24:05,496 - INFO - [diffusion][Epoch 6027] diffusion learning rate: 0.001
2024-11-05 00:24:05,499 - INFO - [diffusion][Epoch 6027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:05,501 - INFO - [diffusion][Epoch 6028] Epoch 6029/12000
2024-11-05 00:24:09,607 - INFO - [diffusion][Epoch 6028] diffusion training Loss: 0.05944544542580843
2024-11-05 00:24:09,609 - INFO - [diffusion][Epoch 6028] diffusion learning rate: 0.001
2024-11-05 00:24:09,612 - INFO - [diffusion][Epoch 6028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:09,614 - INFO - [diffusion][Epoch 6029] Epoch 6030/12000
2024-11-05 00:24:13,710 - INFO - [diffusion][Epoch 6029] diffusion training Loss: 0.059395051561295986
2024-11-05 00:24:13,712 - INFO - [diffusion][Epoch 6029] diffusion learning rate: 0.001
2024-11-05 00:24:13,719 - INFO - [diffusion][Epoch 6029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:13,721 - INFO - [diffusion][Epoch 6030] Epoch 6031/12000
2024-11-05 00:24:17,966 - INFO - [diffusion][Epoch 6030] diffusion training Loss: 0.066676982678473
2024-11-05 00:24:17,969 - INFO - [diffusion][Epoch 6030] diffusion learning rate: 0.001
2024-11-05 00:24:17,971 - INFO - [diffusion][Epoch 6030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:17,972 - INFO - [diffusion][Epoch 6031] Epoch 6032/12000
2024-11-05 00:24:22,143 - INFO - [diffusion][Epoch 6031] diffusion training Loss: 0.057265184819698334
2024-11-05 00:24:22,146 - INFO - [diffusion][Epoch 6031] diffusion learning rate: 0.001
2024-11-05 00:24:22,149 - INFO - [diffusion][Epoch 6031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:22,150 - INFO - [diffusion][Epoch 6032] Epoch 6033/12000
2024-11-05 00:24:26,371 - INFO - [diffusion][Epoch 6032] diffusion training Loss: 0.059660378843545914
2024-11-05 00:24:26,373 - INFO - [diffusion][Epoch 6032] diffusion learning rate: 0.001
2024-11-05 00:24:26,375 - INFO - [diffusion][Epoch 6032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:26,376 - INFO - [diffusion][Epoch 6033] Epoch 6034/12000
2024-11-05 00:24:30,571 - INFO - [diffusion][Epoch 6033] diffusion training Loss: 0.0627010790631175
2024-11-05 00:24:30,574 - INFO - [diffusion][Epoch 6033] diffusion learning rate: 0.001
2024-11-05 00:24:30,576 - INFO - [diffusion][Epoch 6033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:30,577 - INFO - [diffusion][Epoch 6034] Epoch 6035/12000
2024-11-05 00:24:34,814 - INFO - [diffusion][Epoch 6034] diffusion training Loss: 0.05788481142371893
2024-11-05 00:24:34,816 - INFO - [diffusion][Epoch 6034] diffusion learning rate: 0.001
2024-11-05 00:24:34,818 - INFO - [diffusion][Epoch 6034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:34,819 - INFO - [diffusion][Epoch 6035] Epoch 6036/12000
2024-11-05 00:24:39,130 - INFO - [diffusion][Epoch 6035] diffusion training Loss: 0.05900316033512354
2024-11-05 00:24:39,132 - INFO - [diffusion][Epoch 6035] diffusion learning rate: 0.001
2024-11-05 00:24:39,135 - INFO - [diffusion][Epoch 6035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:39,137 - INFO - [diffusion][Epoch 6036] Epoch 6037/12000
2024-11-05 00:24:43,409 - INFO - [diffusion][Epoch 6036] diffusion training Loss: 0.0587470019236207
2024-11-05 00:24:43,411 - INFO - [diffusion][Epoch 6036] diffusion learning rate: 0.001
2024-11-05 00:24:43,413 - INFO - [diffusion][Epoch 6036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:43,415 - INFO - [diffusion][Epoch 6037] Epoch 6038/12000
2024-11-05 00:24:47,561 - INFO - [diffusion][Epoch 6037] diffusion training Loss: 0.055359771475195885
2024-11-05 00:24:47,563 - INFO - [diffusion][Epoch 6037] diffusion learning rate: 0.001
2024-11-05 00:24:47,565 - INFO - [diffusion][Epoch 6037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:47,566 - INFO - [diffusion][Epoch 6038] Epoch 6039/12000
2024-11-05 00:24:51,680 - INFO - [diffusion][Epoch 6038] diffusion training Loss: 0.06620961334556341
2024-11-05 00:24:51,683 - INFO - [diffusion][Epoch 6038] diffusion learning rate: 0.001
2024-11-05 00:24:51,686 - INFO - [diffusion][Epoch 6038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:51,687 - INFO - [diffusion][Epoch 6039] Epoch 6040/12000
2024-11-05 00:24:56,035 - INFO - [diffusion][Epoch 6039] diffusion training Loss: 0.05636135395616293
2024-11-05 00:24:56,037 - INFO - [diffusion][Epoch 6039] diffusion learning rate: 0.001
2024-11-05 00:24:56,040 - INFO - [diffusion][Epoch 6039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:24:56,043 - INFO - [diffusion][Epoch 6040] Epoch 6041/12000
2024-11-05 00:25:00,261 - INFO - [diffusion][Epoch 6040] diffusion training Loss: 0.056987373158335686
2024-11-05 00:25:00,264 - INFO - [diffusion][Epoch 6040] diffusion learning rate: 0.001
2024-11-05 00:25:00,266 - INFO - [diffusion][Epoch 6040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:00,268 - INFO - [diffusion][Epoch 6041] Epoch 6042/12000
2024-11-05 00:25:04,482 - INFO - [diffusion][Epoch 6041] diffusion training Loss: 0.060081285424530506
2024-11-05 00:25:04,484 - INFO - [diffusion][Epoch 6041] diffusion learning rate: 0.001
2024-11-05 00:25:04,486 - INFO - [diffusion][Epoch 6041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:04,488 - INFO - [diffusion][Epoch 6042] Epoch 6043/12000
2024-11-05 00:25:08,655 - INFO - [diffusion][Epoch 6042] diffusion training Loss: 0.05883866734802723
2024-11-05 00:25:08,663 - INFO - [diffusion][Epoch 6042] diffusion learning rate: 0.001
2024-11-05 00:25:08,665 - INFO - [diffusion][Epoch 6042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:08,666 - INFO - [diffusion][Epoch 6043] Epoch 6044/12000
2024-11-05 00:25:12,755 - INFO - [diffusion][Epoch 6043] diffusion training Loss: 0.06069515645503998
2024-11-05 00:25:12,761 - INFO - [diffusion][Epoch 6043] diffusion learning rate: 0.001
2024-11-05 00:25:12,763 - INFO - [diffusion][Epoch 6043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:12,764 - INFO - [diffusion][Epoch 6044] Epoch 6045/12000
2024-11-05 00:25:16,901 - INFO - [diffusion][Epoch 6044] diffusion training Loss: 0.06442335061728954
2024-11-05 00:25:16,904 - INFO - [diffusion][Epoch 6044] diffusion learning rate: 0.001
2024-11-05 00:25:16,910 - INFO - [diffusion][Epoch 6044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:16,912 - INFO - [diffusion][Epoch 6045] Epoch 6046/12000
2024-11-05 00:25:20,992 - INFO - [diffusion][Epoch 6045] diffusion training Loss: 0.06255439389497042
2024-11-05 00:25:20,994 - INFO - [diffusion][Epoch 6045] diffusion learning rate: 0.001
2024-11-05 00:25:20,996 - INFO - [diffusion][Epoch 6045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:20,997 - INFO - [diffusion][Epoch 6046] Epoch 6047/12000
2024-11-05 00:25:25,084 - INFO - [diffusion][Epoch 6046] diffusion training Loss: 0.06027224753051996
2024-11-05 00:25:25,086 - INFO - [diffusion][Epoch 6046] diffusion learning rate: 0.001
2024-11-05 00:25:25,088 - INFO - [diffusion][Epoch 6046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:25,090 - INFO - [diffusion][Epoch 6047] Epoch 6048/12000
2024-11-05 00:25:29,109 - INFO - [diffusion][Epoch 6047] diffusion training Loss: 0.06111861113458872
2024-11-05 00:25:29,112 - INFO - [diffusion][Epoch 6047] diffusion learning rate: 0.001
2024-11-05 00:25:29,114 - INFO - [diffusion][Epoch 6047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:29,116 - INFO - [diffusion][Epoch 6048] Epoch 6049/12000
2024-11-05 00:25:33,348 - INFO - [diffusion][Epoch 6048] diffusion training Loss: 0.06738673616200686
2024-11-05 00:25:33,350 - INFO - [diffusion][Epoch 6048] diffusion learning rate: 0.001
2024-11-05 00:25:33,352 - INFO - [diffusion][Epoch 6048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:33,353 - INFO - [diffusion][Epoch 6049] Epoch 6050/12000
2024-11-05 00:25:37,501 - INFO - [diffusion][Epoch 6049] diffusion training Loss: 0.058608246967196465
2024-11-05 00:25:37,504 - INFO - [diffusion][Epoch 6049] diffusion learning rate: 0.001
2024-11-05 00:25:37,506 - INFO - [diffusion][Epoch 6049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:37,507 - INFO - [diffusion][Epoch 6050] Epoch 6051/12000
2024-11-05 00:25:41,476 - INFO - [diffusion][Epoch 6050] diffusion training Loss: 0.05495705455541611
2024-11-05 00:25:41,479 - INFO - [diffusion][Epoch 6050] diffusion learning rate: 0.001
2024-11-05 00:25:41,481 - INFO - [diffusion][Epoch 6050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:41,483 - INFO - [diffusion][Epoch 6051] Epoch 6052/12000
2024-11-05 00:25:45,577 - INFO - [diffusion][Epoch 6051] diffusion training Loss: 0.06007705628871918
2024-11-05 00:25:45,579 - INFO - [diffusion][Epoch 6051] diffusion learning rate: 0.001
2024-11-05 00:25:45,580 - INFO - [diffusion][Epoch 6051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:45,582 - INFO - [diffusion][Epoch 6052] Epoch 6053/12000
2024-11-05 00:25:49,693 - INFO - [diffusion][Epoch 6052] diffusion training Loss: 0.06148765981197357
2024-11-05 00:25:49,695 - INFO - [diffusion][Epoch 6052] diffusion learning rate: 0.001
2024-11-05 00:25:49,743 - INFO - [diffusion][Epoch 6052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:49,744 - INFO - [diffusion][Epoch 6053] Epoch 6054/12000
2024-11-05 00:25:53,864 - INFO - [diffusion][Epoch 6053] diffusion training Loss: 0.05466128420084715
2024-11-05 00:25:53,866 - INFO - [diffusion][Epoch 6053] diffusion learning rate: 0.001
2024-11-05 00:25:53,868 - INFO - [diffusion][Epoch 6053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:53,869 - INFO - [diffusion][Epoch 6054] Epoch 6055/12000
2024-11-05 00:25:57,927 - INFO - [diffusion][Epoch 6054] diffusion training Loss: 0.06065255310386419
2024-11-05 00:25:57,929 - INFO - [diffusion][Epoch 6054] diffusion learning rate: 0.001
2024-11-05 00:25:57,931 - INFO - [diffusion][Epoch 6054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:25:57,933 - INFO - [diffusion][Epoch 6055] Epoch 6056/12000
2024-11-05 00:26:02,063 - INFO - [diffusion][Epoch 6055] diffusion training Loss: 0.06495779752731323
2024-11-05 00:26:02,065 - INFO - [diffusion][Epoch 6055] diffusion learning rate: 0.001
2024-11-05 00:26:02,067 - INFO - [diffusion][Epoch 6055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:02,068 - INFO - [diffusion][Epoch 6056] Epoch 6057/12000
2024-11-05 00:26:06,142 - INFO - [diffusion][Epoch 6056] diffusion training Loss: 0.06276887841522694
2024-11-05 00:26:06,143 - INFO - [diffusion][Epoch 6056] diffusion learning rate: 0.001
2024-11-05 00:26:06,145 - INFO - [diffusion][Epoch 6056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:06,146 - INFO - [diffusion][Epoch 6057] Epoch 6058/12000
2024-11-05 00:26:10,201 - INFO - [diffusion][Epoch 6057] diffusion training Loss: 0.06702076736837626
2024-11-05 00:26:10,204 - INFO - [diffusion][Epoch 6057] diffusion learning rate: 0.001
2024-11-05 00:26:10,206 - INFO - [diffusion][Epoch 6057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:10,207 - INFO - [diffusion][Epoch 6058] Epoch 6059/12000
2024-11-05 00:26:14,164 - INFO - [diffusion][Epoch 6058] diffusion training Loss: 0.058972214348614216
2024-11-05 00:26:14,168 - INFO - [diffusion][Epoch 6058] diffusion learning rate: 0.001
2024-11-05 00:26:14,171 - INFO - [diffusion][Epoch 6058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:14,172 - INFO - [diffusion][Epoch 6059] Epoch 6060/12000
2024-11-05 00:26:18,393 - INFO - [diffusion][Epoch 6059] diffusion training Loss: 0.05839763395488262
2024-11-05 00:26:18,395 - INFO - [diffusion][Epoch 6059] diffusion learning rate: 0.001
2024-11-05 00:26:18,396 - INFO - [diffusion][Epoch 6059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:18,398 - INFO - [diffusion][Epoch 6060] Epoch 6061/12000
2024-11-05 00:26:22,472 - INFO - [diffusion][Epoch 6060] diffusion training Loss: 0.06590729486197233
2024-11-05 00:26:22,474 - INFO - [diffusion][Epoch 6060] diffusion learning rate: 0.001
2024-11-05 00:26:22,475 - INFO - [diffusion][Epoch 6060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:22,477 - INFO - [diffusion][Epoch 6061] Epoch 6062/12000
2024-11-05 00:26:26,586 - INFO - [diffusion][Epoch 6061] diffusion training Loss: 0.06299598049372435
2024-11-05 00:26:26,588 - INFO - [diffusion][Epoch 6061] diffusion learning rate: 0.001
2024-11-05 00:26:26,590 - INFO - [diffusion][Epoch 6061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:26,591 - INFO - [diffusion][Epoch 6062] Epoch 6063/12000
2024-11-05 00:26:30,572 - INFO - [diffusion][Epoch 6062] diffusion training Loss: 0.0624956926330924
2024-11-05 00:26:30,574 - INFO - [diffusion][Epoch 6062] diffusion learning rate: 0.001
2024-11-05 00:26:30,575 - INFO - [diffusion][Epoch 6062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:30,576 - INFO - [diffusion][Epoch 6063] Epoch 6064/12000
2024-11-05 00:26:34,677 - INFO - [diffusion][Epoch 6063] diffusion training Loss: 0.059224058873951435
2024-11-05 00:26:34,679 - INFO - [diffusion][Epoch 6063] diffusion learning rate: 0.001
2024-11-05 00:26:34,681 - INFO - [diffusion][Epoch 6063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:34,683 - INFO - [diffusion][Epoch 6064] Epoch 6065/12000
2024-11-05 00:26:38,834 - INFO - [diffusion][Epoch 6064] diffusion training Loss: 0.058233498595654964
2024-11-05 00:26:38,836 - INFO - [diffusion][Epoch 6064] diffusion learning rate: 0.001
2024-11-05 00:26:38,838 - INFO - [diffusion][Epoch 6064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:38,839 - INFO - [diffusion][Epoch 6065] Epoch 6066/12000
2024-11-05 00:26:42,829 - INFO - [diffusion][Epoch 6065] diffusion training Loss: 0.061555687338113785
2024-11-05 00:26:42,831 - INFO - [diffusion][Epoch 6065] diffusion learning rate: 0.001
2024-11-05 00:26:42,832 - INFO - [diffusion][Epoch 6065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:42,834 - INFO - [diffusion][Epoch 6066] Epoch 6067/12000
2024-11-05 00:26:46,917 - INFO - [diffusion][Epoch 6066] diffusion training Loss: 0.06006001029163599
2024-11-05 00:26:46,920 - INFO - [diffusion][Epoch 6066] diffusion learning rate: 0.001
2024-11-05 00:26:46,922 - INFO - [diffusion][Epoch 6066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:46,923 - INFO - [diffusion][Epoch 6067] Epoch 6068/12000
2024-11-05 00:26:51,037 - INFO - [diffusion][Epoch 6067] diffusion training Loss: 0.060431514866650105
2024-11-05 00:26:51,040 - INFO - [diffusion][Epoch 6067] diffusion learning rate: 0.001
2024-11-05 00:26:51,042 - INFO - [diffusion][Epoch 6067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:51,043 - INFO - [diffusion][Epoch 6068] Epoch 6069/12000
2024-11-05 00:26:55,152 - INFO - [diffusion][Epoch 6068] diffusion training Loss: 0.05873190239071846
2024-11-05 00:26:55,155 - INFO - [diffusion][Epoch 6068] diffusion learning rate: 0.001
2024-11-05 00:26:55,157 - INFO - [diffusion][Epoch 6068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:55,158 - INFO - [diffusion][Epoch 6069] Epoch 6070/12000
2024-11-05 00:26:59,220 - INFO - [diffusion][Epoch 6069] diffusion training Loss: 0.05976833309978247
2024-11-05 00:26:59,222 - INFO - [diffusion][Epoch 6069] diffusion learning rate: 0.001
2024-11-05 00:26:59,224 - INFO - [diffusion][Epoch 6069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:26:59,225 - INFO - [diffusion][Epoch 6070] Epoch 6071/12000
2024-11-05 00:27:03,309 - INFO - [diffusion][Epoch 6070] diffusion training Loss: 0.05671519227325916
2024-11-05 00:27:03,311 - INFO - [diffusion][Epoch 6070] diffusion learning rate: 0.001
2024-11-05 00:27:03,313 - INFO - [diffusion][Epoch 6070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:03,315 - INFO - [diffusion][Epoch 6071] Epoch 6072/12000
2024-11-05 00:27:07,440 - INFO - [diffusion][Epoch 6071] diffusion training Loss: 0.06298560928553343
2024-11-05 00:27:07,443 - INFO - [diffusion][Epoch 6071] diffusion learning rate: 0.001
2024-11-05 00:27:07,445 - INFO - [diffusion][Epoch 6071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:07,446 - INFO - [diffusion][Epoch 6072] Epoch 6073/12000
2024-11-05 00:27:11,580 - INFO - [diffusion][Epoch 6072] diffusion training Loss: 0.055027179419994354
2024-11-05 00:27:11,801 - INFO - [diffusion][Epoch 6072] diffusion learning rate: 0.001
2024-11-05 00:27:11,803 - INFO - [diffusion][Epoch 6072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:11,804 - INFO - [diffusion][Epoch 6073] Epoch 6074/12000
2024-11-05 00:27:15,926 - INFO - [diffusion][Epoch 6073] diffusion training Loss: 0.058239580132067204
2024-11-05 00:27:15,928 - INFO - [diffusion][Epoch 6073] diffusion learning rate: 0.001
2024-11-05 00:27:15,930 - INFO - [diffusion][Epoch 6073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:15,931 - INFO - [diffusion][Epoch 6074] Epoch 6075/12000
2024-11-05 00:27:20,019 - INFO - [diffusion][Epoch 6074] diffusion training Loss: 0.056447080336511135
2024-11-05 00:27:20,021 - INFO - [diffusion][Epoch 6074] diffusion learning rate: 0.001
2024-11-05 00:27:20,022 - INFO - [diffusion][Epoch 6074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:20,024 - INFO - [diffusion][Epoch 6075] Epoch 6076/12000
2024-11-05 00:27:24,131 - INFO - [diffusion][Epoch 6075] diffusion training Loss: 0.06540755089372396
2024-11-05 00:27:24,133 - INFO - [diffusion][Epoch 6075] diffusion learning rate: 0.001
2024-11-05 00:27:24,135 - INFO - [diffusion][Epoch 6075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:24,136 - INFO - [diffusion][Epoch 6076] Epoch 6077/12000
2024-11-05 00:27:28,266 - INFO - [diffusion][Epoch 6076] diffusion training Loss: 0.05750751122832298
2024-11-05 00:27:28,269 - INFO - [diffusion][Epoch 6076] diffusion learning rate: 0.001
2024-11-05 00:27:28,308 - INFO - [diffusion][Epoch 6076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:28,310 - INFO - [diffusion][Epoch 6077] Epoch 6078/12000
2024-11-05 00:27:32,426 - INFO - [diffusion][Epoch 6077] diffusion training Loss: 0.05981669668108225
2024-11-05 00:27:32,428 - INFO - [diffusion][Epoch 6077] diffusion learning rate: 0.001
2024-11-05 00:27:32,430 - INFO - [diffusion][Epoch 6077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:32,431 - INFO - [diffusion][Epoch 6078] Epoch 6079/12000
2024-11-05 00:27:36,372 - INFO - [diffusion][Epoch 6078] diffusion training Loss: 0.06505915801972151
2024-11-05 00:27:36,374 - INFO - [diffusion][Epoch 6078] diffusion learning rate: 0.001
2024-11-05 00:27:36,376 - INFO - [diffusion][Epoch 6078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:36,377 - INFO - [diffusion][Epoch 6079] Epoch 6080/12000
2024-11-05 00:27:40,515 - INFO - [diffusion][Epoch 6079] diffusion training Loss: 0.060631491243839264
2024-11-05 00:27:40,517 - INFO - [diffusion][Epoch 6079] diffusion learning rate: 0.001
2024-11-05 00:27:40,518 - INFO - [diffusion][Epoch 6079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:40,520 - INFO - [diffusion][Epoch 6080] Epoch 6081/12000
2024-11-05 00:27:44,609 - INFO - [diffusion][Epoch 6080] diffusion training Loss: 0.05709188990294933
2024-11-05 00:27:44,611 - INFO - [diffusion][Epoch 6080] diffusion learning rate: 0.001
2024-11-05 00:27:44,613 - INFO - [diffusion][Epoch 6080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:44,615 - INFO - [diffusion][Epoch 6081] Epoch 6082/12000
2024-11-05 00:27:48,649 - INFO - [diffusion][Epoch 6081] diffusion training Loss: 0.06183959078043699
2024-11-05 00:27:48,652 - INFO - [diffusion][Epoch 6081] diffusion learning rate: 0.001
2024-11-05 00:27:48,654 - INFO - [diffusion][Epoch 6081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:48,656 - INFO - [diffusion][Epoch 6082] Epoch 6083/12000
2024-11-05 00:27:52,715 - INFO - [diffusion][Epoch 6082] diffusion training Loss: 0.056800455786287785
2024-11-05 00:27:52,718 - INFO - [diffusion][Epoch 6082] diffusion learning rate: 0.001
2024-11-05 00:27:52,719 - INFO - [diffusion][Epoch 6082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:52,721 - INFO - [diffusion][Epoch 6083] Epoch 6084/12000
2024-11-05 00:27:56,640 - INFO - [diffusion][Epoch 6083] diffusion training Loss: 0.06170031428337097
2024-11-05 00:27:56,642 - INFO - [diffusion][Epoch 6083] diffusion learning rate: 0.001
2024-11-05 00:27:56,644 - INFO - [diffusion][Epoch 6083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:27:56,646 - INFO - [diffusion][Epoch 6084] Epoch 6085/12000
2024-11-05 00:28:00,674 - INFO - [diffusion][Epoch 6084] diffusion training Loss: 0.05525794345885515
2024-11-05 00:28:00,679 - INFO - [diffusion][Epoch 6084] diffusion learning rate: 0.001
2024-11-05 00:28:00,682 - INFO - [diffusion][Epoch 6084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:00,683 - INFO - [diffusion][Epoch 6085] Epoch 6086/12000
2024-11-05 00:28:04,633 - INFO - [diffusion][Epoch 6085] diffusion training Loss: 0.061642478220164776
2024-11-05 00:28:04,635 - INFO - [diffusion][Epoch 6085] diffusion learning rate: 0.001
2024-11-05 00:28:04,651 - INFO - [diffusion][Epoch 6085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:04,653 - INFO - [diffusion][Epoch 6086] Epoch 6087/12000
2024-11-05 00:28:08,757 - INFO - [diffusion][Epoch 6086] diffusion training Loss: 0.060122535564005375
2024-11-05 00:28:08,759 - INFO - [diffusion][Epoch 6086] diffusion learning rate: 0.001
2024-11-05 00:28:08,761 - INFO - [diffusion][Epoch 6086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:08,762 - INFO - [diffusion][Epoch 6087] Epoch 6088/12000
2024-11-05 00:28:12,779 - INFO - [diffusion][Epoch 6087] diffusion training Loss: 0.05799755733460188
2024-11-05 00:28:12,782 - INFO - [diffusion][Epoch 6087] diffusion learning rate: 0.001
2024-11-05 00:28:12,784 - INFO - [diffusion][Epoch 6087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:12,785 - INFO - [diffusion][Epoch 6088] Epoch 6089/12000
2024-11-05 00:28:16,787 - INFO - [diffusion][Epoch 6088] diffusion training Loss: 0.059564465656876564
2024-11-05 00:28:16,790 - INFO - [diffusion][Epoch 6088] diffusion learning rate: 0.001
2024-11-05 00:28:16,792 - INFO - [diffusion][Epoch 6088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:16,793 - INFO - [diffusion][Epoch 6089] Epoch 6090/12000
2024-11-05 00:28:20,916 - INFO - [diffusion][Epoch 6089] diffusion training Loss: 0.05130286514759064
2024-11-05 00:28:20,917 - INFO - [diffusion][Epoch 6089] diffusion learning rate: 0.001
2024-11-05 00:28:20,919 - INFO - [diffusion][Epoch 6089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:20,921 - INFO - [diffusion][Epoch 6090] Epoch 6091/12000
2024-11-05 00:28:25,044 - INFO - [diffusion][Epoch 6090] diffusion training Loss: 0.061530408449471
2024-11-05 00:28:25,046 - INFO - [diffusion][Epoch 6090] diffusion learning rate: 0.001
2024-11-05 00:28:25,048 - INFO - [diffusion][Epoch 6090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:25,049 - INFO - [diffusion][Epoch 6091] Epoch 6092/12000
2024-11-05 00:28:29,081 - INFO - [diffusion][Epoch 6091] diffusion training Loss: 0.059509542770683765
2024-11-05 00:28:29,083 - INFO - [diffusion][Epoch 6091] diffusion learning rate: 0.001
2024-11-05 00:28:29,085 - INFO - [diffusion][Epoch 6091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:29,086 - INFO - [diffusion][Epoch 6092] Epoch 6093/12000
2024-11-05 00:28:33,231 - INFO - [diffusion][Epoch 6092] diffusion training Loss: 0.0588319031521678
2024-11-05 00:28:33,233 - INFO - [diffusion][Epoch 6092] diffusion learning rate: 0.001
2024-11-05 00:28:33,235 - INFO - [diffusion][Epoch 6092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:33,236 - INFO - [diffusion][Epoch 6093] Epoch 6094/12000
2024-11-05 00:28:37,318 - INFO - [diffusion][Epoch 6093] diffusion training Loss: 0.06365148536860943
2024-11-05 00:28:37,320 - INFO - [diffusion][Epoch 6093] diffusion learning rate: 0.001
2024-11-05 00:28:37,322 - INFO - [diffusion][Epoch 6093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:37,323 - INFO - [diffusion][Epoch 6094] Epoch 6095/12000
2024-11-05 00:28:41,438 - INFO - [diffusion][Epoch 6094] diffusion training Loss: 0.06329477019608021
2024-11-05 00:28:41,441 - INFO - [diffusion][Epoch 6094] diffusion learning rate: 0.001
2024-11-05 00:28:41,443 - INFO - [diffusion][Epoch 6094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:41,444 - INFO - [diffusion][Epoch 6095] Epoch 6096/12000
2024-11-05 00:28:45,419 - INFO - [diffusion][Epoch 6095] diffusion training Loss: 0.06377199199050665
2024-11-05 00:28:45,422 - INFO - [diffusion][Epoch 6095] diffusion learning rate: 0.001
2024-11-05 00:28:45,424 - INFO - [diffusion][Epoch 6095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:45,426 - INFO - [diffusion][Epoch 6096] Epoch 6097/12000
2024-11-05 00:28:49,493 - INFO - [diffusion][Epoch 6096] diffusion training Loss: 0.05741825792938471
2024-11-05 00:28:49,495 - INFO - [diffusion][Epoch 6096] diffusion learning rate: 0.001
2024-11-05 00:28:49,522 - INFO - [diffusion][Epoch 6096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:49,523 - INFO - [diffusion][Epoch 6097] Epoch 6098/12000
2024-11-05 00:28:53,452 - INFO - [diffusion][Epoch 6097] diffusion training Loss: 0.06144118029624224
2024-11-05 00:28:53,454 - INFO - [diffusion][Epoch 6097] diffusion learning rate: 0.001
2024-11-05 00:28:53,455 - INFO - [diffusion][Epoch 6097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:53,457 - INFO - [diffusion][Epoch 6098] Epoch 6099/12000
2024-11-05 00:28:57,551 - INFO - [diffusion][Epoch 6098] diffusion training Loss: 0.06018638797104359
2024-11-05 00:28:57,553 - INFO - [diffusion][Epoch 6098] diffusion learning rate: 0.001
2024-11-05 00:28:57,555 - INFO - [diffusion][Epoch 6098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:28:57,556 - INFO - [diffusion][Epoch 6099] Epoch 6100/12000
2024-11-05 00:29:01,665 - INFO - [diffusion][Epoch 6099] diffusion training Loss: 0.0595133313909173
2024-11-05 00:29:01,667 - INFO - [diffusion][Epoch 6099] diffusion learning rate: 0.001
2024-11-05 00:29:01,669 - INFO - [diffusion][Epoch 6099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:01,670 - INFO - [diffusion][Epoch 6100] Epoch 6101/12000
2024-11-05 00:29:05,750 - INFO - [diffusion][Epoch 6100] diffusion training Loss: 0.0607958659529686
2024-11-05 00:29:05,752 - INFO - [diffusion][Epoch 6100] diffusion learning rate: 0.001
2024-11-05 00:29:05,754 - INFO - [diffusion][Epoch 6100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:05,755 - INFO - [diffusion][Epoch 6101] Epoch 6102/12000
2024-11-05 00:29:10,513 - INFO - [diffusion][Epoch 6101] diffusion training Loss: 0.06349866650998592
2024-11-05 00:29:10,516 - INFO - [diffusion][Epoch 6101] diffusion learning rate: 0.001
2024-11-05 00:29:10,518 - INFO - [diffusion][Epoch 6101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:10,519 - INFO - [diffusion][Epoch 6102] Epoch 6103/12000
2024-11-05 00:29:14,482 - INFO - [diffusion][Epoch 6102] diffusion training Loss: 0.06198408454656601
2024-11-05 00:29:14,485 - INFO - [diffusion][Epoch 6102] diffusion learning rate: 0.001
2024-11-05 00:29:14,487 - INFO - [diffusion][Epoch 6102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:14,488 - INFO - [diffusion][Epoch 6103] Epoch 6104/12000
2024-11-05 00:29:18,461 - INFO - [diffusion][Epoch 6103] diffusion training Loss: 0.057126881554722786
2024-11-05 00:29:18,463 - INFO - [diffusion][Epoch 6103] diffusion learning rate: 0.001
2024-11-05 00:29:18,465 - INFO - [diffusion][Epoch 6103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:18,466 - INFO - [diffusion][Epoch 6104] Epoch 6105/12000
2024-11-05 00:29:22,369 - INFO - [diffusion][Epoch 6104] diffusion training Loss: 0.05942417215555906
2024-11-05 00:29:22,371 - INFO - [diffusion][Epoch 6104] diffusion learning rate: 0.001
2024-11-05 00:29:22,373 - INFO - [diffusion][Epoch 6104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:22,374 - INFO - [diffusion][Epoch 6105] Epoch 6106/12000
2024-11-05 00:29:26,536 - INFO - [diffusion][Epoch 6105] diffusion training Loss: 0.06338423863053322
2024-11-05 00:29:26,538 - INFO - [diffusion][Epoch 6105] diffusion learning rate: 0.001
2024-11-05 00:29:26,540 - INFO - [diffusion][Epoch 6105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:26,541 - INFO - [diffusion][Epoch 6106] Epoch 6107/12000
2024-11-05 00:29:30,655 - INFO - [diffusion][Epoch 6106] diffusion training Loss: 0.061783647164702415
2024-11-05 00:29:30,657 - INFO - [diffusion][Epoch 6106] diffusion learning rate: 0.001
2024-11-05 00:29:30,658 - INFO - [diffusion][Epoch 6106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:30,660 - INFO - [diffusion][Epoch 6107] Epoch 6108/12000
2024-11-05 00:29:34,730 - INFO - [diffusion][Epoch 6107] diffusion training Loss: 0.06034610979259014
2024-11-05 00:29:34,732 - INFO - [diffusion][Epoch 6107] diffusion learning rate: 0.001
2024-11-05 00:29:34,734 - INFO - [diffusion][Epoch 6107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:34,736 - INFO - [diffusion][Epoch 6108] Epoch 6109/12000
2024-11-05 00:29:38,832 - INFO - [diffusion][Epoch 6108] diffusion training Loss: 0.05696181394159794
2024-11-05 00:29:38,834 - INFO - [diffusion][Epoch 6108] diffusion learning rate: 0.001
2024-11-05 00:29:38,835 - INFO - [diffusion][Epoch 6108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:38,837 - INFO - [diffusion][Epoch 6109] Epoch 6110/12000
2024-11-05 00:29:42,899 - INFO - [diffusion][Epoch 6109] diffusion training Loss: 0.05984230525791645
2024-11-05 00:29:42,901 - INFO - [diffusion][Epoch 6109] diffusion learning rate: 0.001
2024-11-05 00:29:42,903 - INFO - [diffusion][Epoch 6109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:42,904 - INFO - [diffusion][Epoch 6110] Epoch 6111/12000
2024-11-05 00:29:47,055 - INFO - [diffusion][Epoch 6110] diffusion training Loss: 0.05552294664084911
2024-11-05 00:29:47,063 - INFO - [diffusion][Epoch 6110] diffusion learning rate: 0.001
2024-11-05 00:29:47,064 - INFO - [diffusion][Epoch 6110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:47,066 - INFO - [diffusion][Epoch 6111] Epoch 6112/12000
2024-11-05 00:29:51,201 - INFO - [diffusion][Epoch 6111] diffusion training Loss: 0.06602901685982943
2024-11-05 00:29:51,203 - INFO - [diffusion][Epoch 6111] diffusion learning rate: 0.001
2024-11-05 00:29:51,205 - INFO - [diffusion][Epoch 6111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:51,206 - INFO - [diffusion][Epoch 6112] Epoch 6113/12000
2024-11-05 00:29:55,359 - INFO - [diffusion][Epoch 6112] diffusion training Loss: 0.06121439300477505
2024-11-05 00:29:55,361 - INFO - [diffusion][Epoch 6112] diffusion learning rate: 0.001
2024-11-05 00:29:55,362 - INFO - [diffusion][Epoch 6112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:55,364 - INFO - [diffusion][Epoch 6113] Epoch 6114/12000
2024-11-05 00:29:59,403 - INFO - [diffusion][Epoch 6113] diffusion training Loss: 0.052995224483311176
2024-11-05 00:29:59,405 - INFO - [diffusion][Epoch 6113] diffusion learning rate: 0.001
2024-11-05 00:29:59,406 - INFO - [diffusion][Epoch 6113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:29:59,408 - INFO - [diffusion][Epoch 6114] Epoch 6115/12000
2024-11-05 00:30:03,512 - INFO - [diffusion][Epoch 6114] diffusion training Loss: 0.06252759788185358
2024-11-05 00:30:03,513 - INFO - [diffusion][Epoch 6114] diffusion learning rate: 0.001
2024-11-05 00:30:03,515 - INFO - [diffusion][Epoch 6114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:03,516 - INFO - [diffusion][Epoch 6115] Epoch 6116/12000
2024-11-05 00:30:07,470 - INFO - [diffusion][Epoch 6115] diffusion training Loss: 0.05726136453449726
2024-11-05 00:30:07,472 - INFO - [diffusion][Epoch 6115] diffusion learning rate: 0.001
2024-11-05 00:30:07,474 - INFO - [diffusion][Epoch 6115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:07,476 - INFO - [diffusion][Epoch 6116] Epoch 6117/12000
2024-11-05 00:30:11,538 - INFO - [diffusion][Epoch 6116] diffusion training Loss: 0.05381178203970194
2024-11-05 00:30:11,593 - INFO - [diffusion][Epoch 6116] diffusion learning rate: 0.001
2024-11-05 00:30:11,595 - INFO - [diffusion][Epoch 6116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:11,596 - INFO - [diffusion][Epoch 6117] Epoch 6118/12000
2024-11-05 00:30:15,546 - INFO - [diffusion][Epoch 6117] diffusion training Loss: 0.05463795363903046
2024-11-05 00:30:15,549 - INFO - [diffusion][Epoch 6117] diffusion learning rate: 0.001
2024-11-05 00:30:15,551 - INFO - [diffusion][Epoch 6117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:15,552 - INFO - [diffusion][Epoch 6118] Epoch 6119/12000
2024-11-05 00:30:19,639 - INFO - [diffusion][Epoch 6118] diffusion training Loss: 0.057681046426296234
2024-11-05 00:30:19,640 - INFO - [diffusion][Epoch 6118] diffusion learning rate: 0.001
2024-11-05 00:30:19,642 - INFO - [diffusion][Epoch 6118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:19,643 - INFO - [diffusion][Epoch 6119] Epoch 6120/12000
2024-11-05 00:30:23,662 - INFO - [diffusion][Epoch 6119] diffusion training Loss: 0.06850762479007244
2024-11-05 00:30:23,665 - INFO - [diffusion][Epoch 6119] diffusion learning rate: 0.001
2024-11-05 00:30:23,667 - INFO - [diffusion][Epoch 6119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:23,668 - INFO - [diffusion][Epoch 6120] Epoch 6121/12000
2024-11-05 00:30:27,711 - INFO - [diffusion][Epoch 6120] diffusion training Loss: 0.06313332915306091
2024-11-05 00:30:27,713 - INFO - [diffusion][Epoch 6120] diffusion learning rate: 0.001
2024-11-05 00:30:27,715 - INFO - [diffusion][Epoch 6120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:27,716 - INFO - [diffusion][Epoch 6121] Epoch 6122/12000
2024-11-05 00:30:32,110 - INFO - [diffusion][Epoch 6121] diffusion training Loss: 0.05358922481536865
2024-11-05 00:30:32,113 - INFO - [diffusion][Epoch 6121] diffusion learning rate: 0.001
2024-11-05 00:30:32,114 - INFO - [diffusion][Epoch 6121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:32,116 - INFO - [diffusion][Epoch 6122] Epoch 6123/12000
2024-11-05 00:30:36,089 - INFO - [diffusion][Epoch 6122] diffusion training Loss: 0.06116651277989149
2024-11-05 00:30:36,091 - INFO - [diffusion][Epoch 6122] diffusion learning rate: 0.001
2024-11-05 00:30:36,093 - INFO - [diffusion][Epoch 6122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:36,094 - INFO - [diffusion][Epoch 6123] Epoch 6124/12000
2024-11-05 00:30:40,316 - INFO - [diffusion][Epoch 6123] diffusion training Loss: 0.06296834908425808
2024-11-05 00:30:40,318 - INFO - [diffusion][Epoch 6123] diffusion learning rate: 0.001
2024-11-05 00:30:40,320 - INFO - [diffusion][Epoch 6123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:40,321 - INFO - [diffusion][Epoch 6124] Epoch 6125/12000
2024-11-05 00:30:44,445 - INFO - [diffusion][Epoch 6124] diffusion training Loss: 0.060229094699025154
2024-11-05 00:30:44,450 - INFO - [diffusion][Epoch 6124] diffusion learning rate: 0.001
2024-11-05 00:30:44,452 - INFO - [diffusion][Epoch 6124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:44,453 - INFO - [diffusion][Epoch 6125] Epoch 6126/12000
2024-11-05 00:30:48,528 - INFO - [diffusion][Epoch 6125] diffusion training Loss: 0.0626817001029849
2024-11-05 00:30:48,531 - INFO - [diffusion][Epoch 6125] diffusion learning rate: 0.001
2024-11-05 00:30:48,533 - INFO - [diffusion][Epoch 6125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:48,535 - INFO - [diffusion][Epoch 6126] Epoch 6127/12000
2024-11-05 00:30:52,672 - INFO - [diffusion][Epoch 6126] diffusion training Loss: 0.05903417058289051
2024-11-05 00:30:52,674 - INFO - [diffusion][Epoch 6126] diffusion learning rate: 0.001
2024-11-05 00:30:52,675 - INFO - [diffusion][Epoch 6126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:52,677 - INFO - [diffusion][Epoch 6127] Epoch 6128/12000
2024-11-05 00:30:56,580 - INFO - [diffusion][Epoch 6127] diffusion training Loss: 0.060258498415350914
2024-11-05 00:30:56,582 - INFO - [diffusion][Epoch 6127] diffusion learning rate: 0.001
2024-11-05 00:30:56,584 - INFO - [diffusion][Epoch 6127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:30:56,590 - INFO - [diffusion][Epoch 6128] Epoch 6129/12000
2024-11-05 00:31:00,611 - INFO - [diffusion][Epoch 6128] diffusion training Loss: 0.06712473183870316
2024-11-05 00:31:00,613 - INFO - [diffusion][Epoch 6128] diffusion learning rate: 0.001
2024-11-05 00:31:00,615 - INFO - [diffusion][Epoch 6128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:00,616 - INFO - [diffusion][Epoch 6129] Epoch 6130/12000
2024-11-05 00:31:04,600 - INFO - [diffusion][Epoch 6129] diffusion training Loss: 0.05832906253635883
2024-11-05 00:31:04,603 - INFO - [diffusion][Epoch 6129] diffusion learning rate: 0.001
2024-11-05 00:31:04,605 - INFO - [diffusion][Epoch 6129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:04,607 - INFO - [diffusion][Epoch 6130] Epoch 6131/12000
2024-11-05 00:31:08,550 - INFO - [diffusion][Epoch 6130] diffusion training Loss: 0.06113640684634447
2024-11-05 00:31:08,553 - INFO - [diffusion][Epoch 6130] diffusion learning rate: 0.001
2024-11-05 00:31:08,555 - INFO - [diffusion][Epoch 6130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:08,557 - INFO - [diffusion][Epoch 6131] Epoch 6132/12000
2024-11-05 00:31:12,531 - INFO - [diffusion][Epoch 6131] diffusion training Loss: 0.0614740327000618
2024-11-05 00:31:12,534 - INFO - [diffusion][Epoch 6131] diffusion learning rate: 0.001
2024-11-05 00:31:12,535 - INFO - [diffusion][Epoch 6131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:12,537 - INFO - [diffusion][Epoch 6132] Epoch 6133/12000
2024-11-05 00:31:16,479 - INFO - [diffusion][Epoch 6132] diffusion training Loss: 0.057370164431631565
2024-11-05 00:31:16,481 - INFO - [diffusion][Epoch 6132] diffusion learning rate: 0.001
2024-11-05 00:31:16,483 - INFO - [diffusion][Epoch 6132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:16,484 - INFO - [diffusion][Epoch 6133] Epoch 6134/12000
2024-11-05 00:31:20,437 - INFO - [diffusion][Epoch 6133] diffusion training Loss: 0.056937736459076405
2024-11-05 00:31:20,439 - INFO - [diffusion][Epoch 6133] diffusion learning rate: 0.001
2024-11-05 00:31:20,441 - INFO - [diffusion][Epoch 6133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:20,442 - INFO - [diffusion][Epoch 6134] Epoch 6135/12000
2024-11-05 00:31:24,546 - INFO - [diffusion][Epoch 6134] diffusion training Loss: 0.05953973904252052
2024-11-05 00:31:24,549 - INFO - [diffusion][Epoch 6134] diffusion learning rate: 0.001
2024-11-05 00:31:24,551 - INFO - [diffusion][Epoch 6134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:24,552 - INFO - [diffusion][Epoch 6135] Epoch 6136/12000
2024-11-05 00:31:28,597 - INFO - [diffusion][Epoch 6135] diffusion training Loss: 0.055663601495325565
2024-11-05 00:31:28,599 - INFO - [diffusion][Epoch 6135] diffusion learning rate: 0.001
2024-11-05 00:31:28,601 - INFO - [diffusion][Epoch 6135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:28,603 - INFO - [diffusion][Epoch 6136] Epoch 6137/12000
2024-11-05 00:31:32,725 - INFO - [diffusion][Epoch 6136] diffusion training Loss: 0.06516347452998161
2024-11-05 00:31:32,727 - INFO - [diffusion][Epoch 6136] diffusion learning rate: 0.001
2024-11-05 00:31:32,729 - INFO - [diffusion][Epoch 6136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:32,731 - INFO - [diffusion][Epoch 6137] Epoch 6138/12000
2024-11-05 00:31:36,733 - INFO - [diffusion][Epoch 6137] diffusion training Loss: 0.061904313042759895
2024-11-05 00:31:36,735 - INFO - [diffusion][Epoch 6137] diffusion learning rate: 0.001
2024-11-05 00:31:36,737 - INFO - [diffusion][Epoch 6137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:36,738 - INFO - [diffusion][Epoch 6138] Epoch 6139/12000
2024-11-05 00:31:40,628 - INFO - [diffusion][Epoch 6138] diffusion training Loss: 0.060488528572022915
2024-11-05 00:31:40,631 - INFO - [diffusion][Epoch 6138] diffusion learning rate: 0.001
2024-11-05 00:31:40,632 - INFO - [diffusion][Epoch 6138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:40,633 - INFO - [diffusion][Epoch 6139] Epoch 6140/12000
2024-11-05 00:31:44,555 - INFO - [diffusion][Epoch 6139] diffusion training Loss: 0.06311622075736523
2024-11-05 00:31:44,557 - INFO - [diffusion][Epoch 6139] diffusion learning rate: 0.001
2024-11-05 00:31:44,559 - INFO - [diffusion][Epoch 6139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:44,560 - INFO - [diffusion][Epoch 6140] Epoch 6141/12000
2024-11-05 00:31:48,637 - INFO - [diffusion][Epoch 6140] diffusion training Loss: 0.06562357768416405
2024-11-05 00:31:48,639 - INFO - [diffusion][Epoch 6140] diffusion learning rate: 0.001
2024-11-05 00:31:48,640 - INFO - [diffusion][Epoch 6140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:48,642 - INFO - [diffusion][Epoch 6141] Epoch 6142/12000
2024-11-05 00:31:52,679 - INFO - [diffusion][Epoch 6141] diffusion training Loss: 0.06010130885988474
2024-11-05 00:31:52,681 - INFO - [diffusion][Epoch 6141] diffusion learning rate: 0.001
2024-11-05 00:31:52,683 - INFO - [diffusion][Epoch 6141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:52,685 - INFO - [diffusion][Epoch 6142] Epoch 6143/12000
2024-11-05 00:31:57,077 - INFO - [diffusion][Epoch 6142] diffusion training Loss: 0.060514871031045914
2024-11-05 00:31:57,079 - INFO - [diffusion][Epoch 6142] diffusion learning rate: 0.001
2024-11-05 00:31:57,081 - INFO - [diffusion][Epoch 6142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:31:57,082 - INFO - [diffusion][Epoch 6143] Epoch 6144/12000
2024-11-05 00:32:01,117 - INFO - [diffusion][Epoch 6143] diffusion training Loss: 0.060411253944039345
2024-11-05 00:32:01,119 - INFO - [diffusion][Epoch 6143] diffusion learning rate: 0.001
2024-11-05 00:32:01,121 - INFO - [diffusion][Epoch 6143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:01,122 - INFO - [diffusion][Epoch 6144] Epoch 6145/12000
2024-11-05 00:32:05,021 - INFO - [diffusion][Epoch 6144] diffusion training Loss: 0.058539909310638905
2024-11-05 00:32:05,024 - INFO - [diffusion][Epoch 6144] diffusion learning rate: 0.001
2024-11-05 00:32:05,026 - INFO - [diffusion][Epoch 6144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:05,028 - INFO - [diffusion][Epoch 6145] Epoch 6146/12000
2024-11-05 00:32:08,954 - INFO - [diffusion][Epoch 6145] diffusion training Loss: 0.0639274762943387
2024-11-05 00:32:08,956 - INFO - [diffusion][Epoch 6145] diffusion learning rate: 0.001
2024-11-05 00:32:08,957 - INFO - [diffusion][Epoch 6145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:08,958 - INFO - [diffusion][Epoch 6146] Epoch 6147/12000
2024-11-05 00:32:12,857 - INFO - [diffusion][Epoch 6146] diffusion training Loss: 0.05905350297689438
2024-11-05 00:32:12,860 - INFO - [diffusion][Epoch 6146] diffusion learning rate: 0.001
2024-11-05 00:32:12,862 - INFO - [diffusion][Epoch 6146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:12,864 - INFO - [diffusion][Epoch 6147] Epoch 6148/12000
2024-11-05 00:32:17,009 - INFO - [diffusion][Epoch 6147] diffusion training Loss: 0.05994471162557602
2024-11-05 00:32:17,012 - INFO - [diffusion][Epoch 6147] diffusion learning rate: 0.001
2024-11-05 00:32:17,014 - INFO - [diffusion][Epoch 6147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:17,015 - INFO - [diffusion][Epoch 6148] Epoch 6149/12000
2024-11-05 00:32:21,123 - INFO - [diffusion][Epoch 6148] diffusion training Loss: 0.06389281339943409
2024-11-05 00:32:21,125 - INFO - [diffusion][Epoch 6148] diffusion learning rate: 0.001
2024-11-05 00:32:21,127 - INFO - [diffusion][Epoch 6148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:21,128 - INFO - [diffusion][Epoch 6149] Epoch 6150/12000
2024-11-05 00:32:25,178 - INFO - [diffusion][Epoch 6149] diffusion training Loss: 0.06292752549052238
2024-11-05 00:32:25,180 - INFO - [diffusion][Epoch 6149] diffusion learning rate: 0.001
2024-11-05 00:32:25,182 - INFO - [diffusion][Epoch 6149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:25,183 - INFO - [diffusion][Epoch 6150] Epoch 6151/12000
2024-11-05 00:32:29,228 - INFO - [diffusion][Epoch 6150] diffusion training Loss: 0.061179871670901775
2024-11-05 00:32:29,230 - INFO - [diffusion][Epoch 6150] diffusion learning rate: 0.001
2024-11-05 00:32:29,232 - INFO - [diffusion][Epoch 6150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:29,234 - INFO - [diffusion][Epoch 6151] Epoch 6152/12000
2024-11-05 00:32:33,365 - INFO - [diffusion][Epoch 6151] diffusion training Loss: 0.05571227986365557
2024-11-05 00:32:33,368 - INFO - [diffusion][Epoch 6151] diffusion learning rate: 0.001
2024-11-05 00:32:33,370 - INFO - [diffusion][Epoch 6151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:33,371 - INFO - [diffusion][Epoch 6152] Epoch 6153/12000
2024-11-05 00:32:37,281 - INFO - [diffusion][Epoch 6152] diffusion training Loss: 0.06236269325017929
2024-11-05 00:32:37,283 - INFO - [diffusion][Epoch 6152] diffusion learning rate: 0.001
2024-11-05 00:32:37,286 - INFO - [diffusion][Epoch 6152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:37,287 - INFO - [diffusion][Epoch 6153] Epoch 6154/12000
2024-11-05 00:32:41,301 - INFO - [diffusion][Epoch 6153] diffusion training Loss: 0.0567736541852355
2024-11-05 00:32:41,303 - INFO - [diffusion][Epoch 6153] diffusion learning rate: 0.001
2024-11-05 00:32:41,304 - INFO - [diffusion][Epoch 6153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:41,306 - INFO - [diffusion][Epoch 6154] Epoch 6155/12000
2024-11-05 00:32:45,390 - INFO - [diffusion][Epoch 6154] diffusion training Loss: 0.06145699694752693
2024-11-05 00:32:45,392 - INFO - [diffusion][Epoch 6154] diffusion learning rate: 0.001
2024-11-05 00:32:45,394 - INFO - [diffusion][Epoch 6154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:45,395 - INFO - [diffusion][Epoch 6155] Epoch 6156/12000
2024-11-05 00:32:49,461 - INFO - [diffusion][Epoch 6155] diffusion training Loss: 0.06386640295386314
2024-11-05 00:32:49,464 - INFO - [diffusion][Epoch 6155] diffusion learning rate: 0.001
2024-11-05 00:32:49,466 - INFO - [diffusion][Epoch 6155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:49,467 - INFO - [diffusion][Epoch 6156] Epoch 6157/12000
2024-11-05 00:32:53,589 - INFO - [diffusion][Epoch 6156] diffusion training Loss: 0.06128334626555443
2024-11-05 00:32:53,591 - INFO - [diffusion][Epoch 6156] diffusion learning rate: 0.001
2024-11-05 00:32:53,594 - INFO - [diffusion][Epoch 6156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:53,595 - INFO - [diffusion][Epoch 6157] Epoch 6158/12000
2024-11-05 00:32:57,617 - INFO - [diffusion][Epoch 6157] diffusion training Loss: 0.05618991702795029
2024-11-05 00:32:57,619 - INFO - [diffusion][Epoch 6157] diffusion learning rate: 0.001
2024-11-05 00:32:57,621 - INFO - [diffusion][Epoch 6157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:32:57,622 - INFO - [diffusion][Epoch 6158] Epoch 6159/12000
2024-11-05 00:33:01,617 - INFO - [diffusion][Epoch 6158] diffusion training Loss: 0.0579133415594697
2024-11-05 00:33:01,619 - INFO - [diffusion][Epoch 6158] diffusion learning rate: 0.001
2024-11-05 00:33:01,621 - INFO - [diffusion][Epoch 6158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:01,622 - INFO - [diffusion][Epoch 6159] Epoch 6160/12000
2024-11-05 00:33:05,632 - INFO - [diffusion][Epoch 6159] diffusion training Loss: 0.05523860268294811
2024-11-05 00:33:05,634 - INFO - [diffusion][Epoch 6159] diffusion learning rate: 0.001
2024-11-05 00:33:05,732 - INFO - [diffusion][Epoch 6159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:05,733 - INFO - [diffusion][Epoch 6160] Epoch 6161/12000
2024-11-05 00:33:09,818 - INFO - [diffusion][Epoch 6160] diffusion training Loss: 0.060485128313302994
2024-11-05 00:33:09,820 - INFO - [diffusion][Epoch 6160] diffusion learning rate: 0.001
2024-11-05 00:33:09,822 - INFO - [diffusion][Epoch 6160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:09,823 - INFO - [diffusion][Epoch 6161] Epoch 6162/12000
2024-11-05 00:33:13,935 - INFO - [diffusion][Epoch 6161] diffusion training Loss: 0.060174464248120785
2024-11-05 00:33:13,938 - INFO - [diffusion][Epoch 6161] diffusion learning rate: 0.001
2024-11-05 00:33:13,940 - INFO - [diffusion][Epoch 6161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:13,941 - INFO - [diffusion][Epoch 6162] Epoch 6163/12000
2024-11-05 00:33:18,023 - INFO - [diffusion][Epoch 6162] diffusion training Loss: 0.05959036387503147
2024-11-05 00:33:18,026 - INFO - [diffusion][Epoch 6162] diffusion learning rate: 0.001
2024-11-05 00:33:18,028 - INFO - [diffusion][Epoch 6162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:18,029 - INFO - [diffusion][Epoch 6163] Epoch 6164/12000
2024-11-05 00:33:22,225 - INFO - [diffusion][Epoch 6163] diffusion training Loss: 0.05790118779987097
2024-11-05 00:33:22,227 - INFO - [diffusion][Epoch 6163] diffusion learning rate: 0.001
2024-11-05 00:33:22,229 - INFO - [diffusion][Epoch 6163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:22,230 - INFO - [diffusion][Epoch 6164] Epoch 6165/12000
2024-11-05 00:33:26,320 - INFO - [diffusion][Epoch 6164] diffusion training Loss: 0.058090781792998314
2024-11-05 00:33:26,322 - INFO - [diffusion][Epoch 6164] diffusion learning rate: 0.001
2024-11-05 00:33:26,324 - INFO - [diffusion][Epoch 6164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:26,325 - INFO - [diffusion][Epoch 6165] Epoch 6166/12000
2024-11-05 00:33:30,207 - INFO - [diffusion][Epoch 6165] diffusion training Loss: 0.05588624719530344
2024-11-05 00:33:30,209 - INFO - [diffusion][Epoch 6165] diffusion learning rate: 0.001
2024-11-05 00:33:30,211 - INFO - [diffusion][Epoch 6165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:30,212 - INFO - [diffusion][Epoch 6166] Epoch 6167/12000
2024-11-05 00:33:34,261 - INFO - [diffusion][Epoch 6166] diffusion training Loss: 0.058032792061567307
2024-11-05 00:33:34,263 - INFO - [diffusion][Epoch 6166] diffusion learning rate: 0.001
2024-11-05 00:33:34,264 - INFO - [diffusion][Epoch 6166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:34,266 - INFO - [diffusion][Epoch 6167] Epoch 6168/12000
2024-11-05 00:33:38,234 - INFO - [diffusion][Epoch 6167] diffusion training Loss: 0.057693532668054104
2024-11-05 00:33:38,236 - INFO - [diffusion][Epoch 6167] diffusion learning rate: 0.001
2024-11-05 00:33:38,238 - INFO - [diffusion][Epoch 6167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:38,239 - INFO - [diffusion][Epoch 6168] Epoch 6169/12000
2024-11-05 00:33:42,349 - INFO - [diffusion][Epoch 6168] diffusion training Loss: 0.05912997294217348
2024-11-05 00:33:42,351 - INFO - [diffusion][Epoch 6168] diffusion learning rate: 0.001
2024-11-05 00:33:42,352 - INFO - [diffusion][Epoch 6168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:42,354 - INFO - [diffusion][Epoch 6169] Epoch 6170/12000
2024-11-05 00:33:46,422 - INFO - [diffusion][Epoch 6169] diffusion training Loss: 0.061431439593434334
2024-11-05 00:33:46,425 - INFO - [diffusion][Epoch 6169] diffusion learning rate: 0.001
2024-11-05 00:33:46,427 - INFO - [diffusion][Epoch 6169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:46,428 - INFO - [diffusion][Epoch 6170] Epoch 6171/12000
2024-11-05 00:33:50,443 - INFO - [diffusion][Epoch 6170] diffusion training Loss: 0.06487117521464825
2024-11-05 00:33:50,445 - INFO - [diffusion][Epoch 6170] diffusion learning rate: 0.001
2024-11-05 00:33:50,447 - INFO - [diffusion][Epoch 6170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:50,448 - INFO - [diffusion][Epoch 6171] Epoch 6172/12000
2024-11-05 00:33:54,391 - INFO - [diffusion][Epoch 6171] diffusion training Loss: 0.05579224228858948
2024-11-05 00:33:54,393 - INFO - [diffusion][Epoch 6171] diffusion learning rate: 0.001
2024-11-05 00:33:54,395 - INFO - [diffusion][Epoch 6171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:54,396 - INFO - [diffusion][Epoch 6172] Epoch 6173/12000
2024-11-05 00:33:58,469 - INFO - [diffusion][Epoch 6172] diffusion training Loss: 0.06246563978493214
2024-11-05 00:33:58,471 - INFO - [diffusion][Epoch 6172] diffusion learning rate: 0.001
2024-11-05 00:33:58,473 - INFO - [diffusion][Epoch 6172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:33:58,475 - INFO - [diffusion][Epoch 6173] Epoch 6174/12000
2024-11-05 00:34:02,459 - INFO - [diffusion][Epoch 6173] diffusion training Loss: 0.05769826378673315
2024-11-05 00:34:02,461 - INFO - [diffusion][Epoch 6173] diffusion learning rate: 0.001
2024-11-05 00:34:02,462 - INFO - [diffusion][Epoch 6173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:02,464 - INFO - [diffusion][Epoch 6174] Epoch 6175/12000
2024-11-05 00:34:06,441 - INFO - [diffusion][Epoch 6174] diffusion training Loss: 0.05984911322593689
2024-11-05 00:34:06,443 - INFO - [diffusion][Epoch 6174] diffusion learning rate: 0.001
2024-11-05 00:34:06,445 - INFO - [diffusion][Epoch 6174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:06,447 - INFO - [diffusion][Epoch 6175] Epoch 6176/12000
2024-11-05 00:34:10,535 - INFO - [diffusion][Epoch 6175] diffusion training Loss: 0.059303718619048595
2024-11-05 00:34:10,537 - INFO - [diffusion][Epoch 6175] diffusion learning rate: 0.001
2024-11-05 00:34:10,538 - INFO - [diffusion][Epoch 6175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:10,540 - INFO - [diffusion][Epoch 6176] Epoch 6177/12000
2024-11-05 00:34:14,593 - INFO - [diffusion][Epoch 6176] diffusion training Loss: 0.06675109639763832
2024-11-05 00:34:14,596 - INFO - [diffusion][Epoch 6176] diffusion learning rate: 0.001
2024-11-05 00:34:14,598 - INFO - [diffusion][Epoch 6176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:14,599 - INFO - [diffusion][Epoch 6177] Epoch 6178/12000
2024-11-05 00:34:18,606 - INFO - [diffusion][Epoch 6177] diffusion training Loss: 0.06625932548195124
2024-11-05 00:34:18,609 - INFO - [diffusion][Epoch 6177] diffusion learning rate: 0.001
2024-11-05 00:34:18,611 - INFO - [diffusion][Epoch 6177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:18,612 - INFO - [diffusion][Epoch 6178] Epoch 6179/12000
2024-11-05 00:34:22,649 - INFO - [diffusion][Epoch 6178] diffusion training Loss: 0.05625106580555439
2024-11-05 00:34:22,651 - INFO - [diffusion][Epoch 6178] diffusion learning rate: 0.001
2024-11-05 00:34:22,652 - INFO - [diffusion][Epoch 6178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:22,654 - INFO - [diffusion][Epoch 6179] Epoch 6180/12000
2024-11-05 00:34:26,731 - INFO - [diffusion][Epoch 6179] diffusion training Loss: 0.05606738571077585
2024-11-05 00:34:26,732 - INFO - [diffusion][Epoch 6179] diffusion learning rate: 0.001
2024-11-05 00:34:26,734 - INFO - [diffusion][Epoch 6179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:26,735 - INFO - [diffusion][Epoch 6180] Epoch 6181/12000
2024-11-05 00:34:30,715 - INFO - [diffusion][Epoch 6180] diffusion training Loss: 0.059620074927806854
2024-11-05 00:34:30,717 - INFO - [diffusion][Epoch 6180] diffusion learning rate: 0.001
2024-11-05 00:34:30,719 - INFO - [diffusion][Epoch 6180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:30,720 - INFO - [diffusion][Epoch 6181] Epoch 6182/12000
2024-11-05 00:34:34,645 - INFO - [diffusion][Epoch 6181] diffusion training Loss: 0.06262336671352386
2024-11-05 00:34:34,647 - INFO - [diffusion][Epoch 6181] diffusion learning rate: 0.001
2024-11-05 00:34:34,649 - INFO - [diffusion][Epoch 6181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:34,650 - INFO - [diffusion][Epoch 6182] Epoch 6183/12000
2024-11-05 00:34:38,666 - INFO - [diffusion][Epoch 6182] diffusion training Loss: 0.06195214856415987
2024-11-05 00:34:38,668 - INFO - [diffusion][Epoch 6182] diffusion learning rate: 0.001
2024-11-05 00:34:38,669 - INFO - [diffusion][Epoch 6182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:38,671 - INFO - [diffusion][Epoch 6183] Epoch 6184/12000
2024-11-05 00:34:42,651 - INFO - [diffusion][Epoch 6183] diffusion training Loss: 0.055313510820269585
2024-11-05 00:34:42,652 - INFO - [diffusion][Epoch 6183] diffusion learning rate: 0.001
2024-11-05 00:34:42,654 - INFO - [diffusion][Epoch 6183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:42,655 - INFO - [diffusion][Epoch 6184] Epoch 6185/12000
2024-11-05 00:34:46,662 - INFO - [diffusion][Epoch 6184] diffusion training Loss: 0.058184350840747356
2024-11-05 00:34:46,663 - INFO - [diffusion][Epoch 6184] diffusion learning rate: 0.001
2024-11-05 00:34:46,665 - INFO - [diffusion][Epoch 6184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:46,666 - INFO - [diffusion][Epoch 6185] Epoch 6186/12000
2024-11-05 00:34:51,049 - INFO - [diffusion][Epoch 6185] diffusion training Loss: 0.05859610345214605
2024-11-05 00:34:51,051 - INFO - [diffusion][Epoch 6185] diffusion learning rate: 0.001
2024-11-05 00:34:51,053 - INFO - [diffusion][Epoch 6185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:51,054 - INFO - [diffusion][Epoch 6186] Epoch 6187/12000
2024-11-05 00:34:55,000 - INFO - [diffusion][Epoch 6186] diffusion training Loss: 0.060076963156461716
2024-11-05 00:34:55,002 - INFO - [diffusion][Epoch 6186] diffusion learning rate: 0.001
2024-11-05 00:34:55,004 - INFO - [diffusion][Epoch 6186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:55,005 - INFO - [diffusion][Epoch 6187] Epoch 6188/12000
2024-11-05 00:34:58,982 - INFO - [diffusion][Epoch 6187] diffusion training Loss: 0.05594304669648409
2024-11-05 00:34:58,985 - INFO - [diffusion][Epoch 6187] diffusion learning rate: 0.001
2024-11-05 00:34:58,987 - INFO - [diffusion][Epoch 6187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:34:58,988 - INFO - [diffusion][Epoch 6188] Epoch 6189/12000
2024-11-05 00:35:02,944 - INFO - [diffusion][Epoch 6188] diffusion training Loss: 0.05989041365683079
2024-11-05 00:35:02,946 - INFO - [diffusion][Epoch 6188] diffusion learning rate: 0.001
2024-11-05 00:35:02,948 - INFO - [diffusion][Epoch 6188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:02,950 - INFO - [diffusion][Epoch 6189] Epoch 6190/12000
2024-11-05 00:35:07,131 - INFO - [diffusion][Epoch 6189] diffusion training Loss: 0.061267261393368244
2024-11-05 00:35:07,133 - INFO - [diffusion][Epoch 6189] diffusion learning rate: 0.001
2024-11-05 00:35:07,134 - INFO - [diffusion][Epoch 6189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:07,136 - INFO - [diffusion][Epoch 6190] Epoch 6191/12000
2024-11-05 00:35:11,216 - INFO - [diffusion][Epoch 6190] diffusion training Loss: 0.05925655458122492
2024-11-05 00:35:11,217 - INFO - [diffusion][Epoch 6190] diffusion learning rate: 0.001
2024-11-05 00:35:11,219 - INFO - [diffusion][Epoch 6190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:11,220 - INFO - [diffusion][Epoch 6191] Epoch 6192/12000
2024-11-05 00:35:15,427 - INFO - [diffusion][Epoch 6191] diffusion training Loss: 0.06384687405079603
2024-11-05 00:35:15,428 - INFO - [diffusion][Epoch 6191] diffusion learning rate: 0.001
2024-11-05 00:35:15,431 - INFO - [diffusion][Epoch 6191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:15,432 - INFO - [diffusion][Epoch 6192] Epoch 6193/12000
2024-11-05 00:35:19,527 - INFO - [diffusion][Epoch 6192] diffusion training Loss: 0.06317836046218872
2024-11-05 00:35:19,529 - INFO - [diffusion][Epoch 6192] diffusion learning rate: 0.001
2024-11-05 00:35:19,531 - INFO - [diffusion][Epoch 6192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:19,533 - INFO - [diffusion][Epoch 6193] Epoch 6194/12000
2024-11-05 00:35:23,672 - INFO - [diffusion][Epoch 6193] diffusion training Loss: 0.06120190490037203
2024-11-05 00:35:23,674 - INFO - [diffusion][Epoch 6193] diffusion learning rate: 0.001
2024-11-05 00:35:23,676 - INFO - [diffusion][Epoch 6193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:23,677 - INFO - [diffusion][Epoch 6194] Epoch 6195/12000
2024-11-05 00:35:27,780 - INFO - [diffusion][Epoch 6194] diffusion training Loss: 0.05553128942847252
2024-11-05 00:35:27,782 - INFO - [diffusion][Epoch 6194] diffusion learning rate: 0.001
2024-11-05 00:35:27,783 - INFO - [diffusion][Epoch 6194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:27,785 - INFO - [diffusion][Epoch 6195] Epoch 6196/12000
2024-11-05 00:35:31,944 - INFO - [diffusion][Epoch 6195] diffusion training Loss: 0.06383072212338448
2024-11-05 00:35:31,946 - INFO - [diffusion][Epoch 6195] diffusion learning rate: 0.001
2024-11-05 00:35:31,948 - INFO - [diffusion][Epoch 6195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:31,950 - INFO - [diffusion][Epoch 6196] Epoch 6197/12000
2024-11-05 00:35:36,069 - INFO - [diffusion][Epoch 6196] diffusion training Loss: 0.060432400554418564
2024-11-05 00:35:36,070 - INFO - [diffusion][Epoch 6196] diffusion learning rate: 0.001
2024-11-05 00:35:36,072 - INFO - [diffusion][Epoch 6196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:36,074 - INFO - [diffusion][Epoch 6197] Epoch 6198/12000
2024-11-05 00:35:40,181 - INFO - [diffusion][Epoch 6197] diffusion training Loss: 0.058518992736935616
2024-11-05 00:35:40,183 - INFO - [diffusion][Epoch 6197] diffusion learning rate: 0.001
2024-11-05 00:35:40,185 - INFO - [diffusion][Epoch 6197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:40,192 - INFO - [diffusion][Epoch 6198] Epoch 6199/12000
2024-11-05 00:35:44,263 - INFO - [diffusion][Epoch 6198] diffusion training Loss: 0.06221981067210436
2024-11-05 00:35:44,265 - INFO - [diffusion][Epoch 6198] diffusion learning rate: 0.001
2024-11-05 00:35:44,267 - INFO - [diffusion][Epoch 6198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:44,269 - INFO - [diffusion][Epoch 6199] Epoch 6200/12000
2024-11-05 00:35:48,401 - INFO - [diffusion][Epoch 6199] diffusion training Loss: 0.059403770603239536
2024-11-05 00:35:48,403 - INFO - [diffusion][Epoch 6199] diffusion learning rate: 0.001
2024-11-05 00:35:48,406 - INFO - [diffusion][Epoch 6199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:48,407 - INFO - [diffusion][Epoch 6200] Epoch 6201/12000
2024-11-05 00:35:52,589 - INFO - [diffusion][Epoch 6200] diffusion training Loss: 0.05961122177541256
2024-11-05 00:35:52,591 - INFO - [diffusion][Epoch 6200] diffusion learning rate: 0.001
2024-11-05 00:35:52,623 - INFO - [diffusion][Epoch 6200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:52,624 - INFO - [diffusion][Epoch 6201] Epoch 6202/12000
2024-11-05 00:35:56,733 - INFO - [diffusion][Epoch 6201] diffusion training Loss: 0.05954265408217907
2024-11-05 00:35:56,735 - INFO - [diffusion][Epoch 6201] diffusion learning rate: 0.001
2024-11-05 00:35:56,737 - INFO - [diffusion][Epoch 6201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:35:56,738 - INFO - [diffusion][Epoch 6202] Epoch 6203/12000
2024-11-05 00:36:00,917 - INFO - [diffusion][Epoch 6202] diffusion training Loss: 0.05805227532982826
2024-11-05 00:36:00,919 - INFO - [diffusion][Epoch 6202] diffusion learning rate: 0.001
2024-11-05 00:36:00,921 - INFO - [diffusion][Epoch 6202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:00,922 - INFO - [diffusion][Epoch 6203] Epoch 6204/12000
2024-11-05 00:36:04,982 - INFO - [diffusion][Epoch 6203] diffusion training Loss: 0.058330392464995384
2024-11-05 00:36:04,984 - INFO - [diffusion][Epoch 6203] diffusion learning rate: 0.001
2024-11-05 00:36:04,986 - INFO - [diffusion][Epoch 6203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:04,987 - INFO - [diffusion][Epoch 6204] Epoch 6205/12000
2024-11-05 00:36:09,004 - INFO - [diffusion][Epoch 6204] diffusion training Loss: 0.06576604209840298
2024-11-05 00:36:09,006 - INFO - [diffusion][Epoch 6204] diffusion learning rate: 0.001
2024-11-05 00:36:09,008 - INFO - [diffusion][Epoch 6204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:09,009 - INFO - [diffusion][Epoch 6205] Epoch 6206/12000
2024-11-05 00:36:13,390 - INFO - [diffusion][Epoch 6205] diffusion training Loss: 0.056173878721892834
2024-11-05 00:36:13,392 - INFO - [diffusion][Epoch 6205] diffusion learning rate: 0.001
2024-11-05 00:36:13,394 - INFO - [diffusion][Epoch 6205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:13,395 - INFO - [diffusion][Epoch 6206] Epoch 6207/12000
2024-11-05 00:36:17,522 - INFO - [diffusion][Epoch 6206] diffusion training Loss: 0.058690848760306835
2024-11-05 00:36:17,525 - INFO - [diffusion][Epoch 6206] diffusion learning rate: 0.001
2024-11-05 00:36:17,527 - INFO - [diffusion][Epoch 6206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:17,528 - INFO - [diffusion][Epoch 6207] Epoch 6208/12000
2024-11-05 00:36:21,544 - INFO - [diffusion][Epoch 6207] diffusion training Loss: 0.05465531349182129
2024-11-05 00:36:21,548 - INFO - [diffusion][Epoch 6207] diffusion learning rate: 0.001
2024-11-05 00:36:21,551 - INFO - [diffusion][Epoch 6207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:21,552 - INFO - [diffusion][Epoch 6208] Epoch 6209/12000
2024-11-05 00:36:25,683 - INFO - [diffusion][Epoch 6208] diffusion training Loss: 0.05676095187664032
2024-11-05 00:36:25,685 - INFO - [diffusion][Epoch 6208] diffusion learning rate: 0.001
2024-11-05 00:36:25,687 - INFO - [diffusion][Epoch 6208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:25,688 - INFO - [diffusion][Epoch 6209] Epoch 6210/12000
2024-11-05 00:36:29,822 - INFO - [diffusion][Epoch 6209] diffusion training Loss: 0.05822128616273403
2024-11-05 00:36:29,825 - INFO - [diffusion][Epoch 6209] diffusion learning rate: 0.001
2024-11-05 00:36:29,827 - INFO - [diffusion][Epoch 6209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:29,828 - INFO - [diffusion][Epoch 6210] Epoch 6211/12000
2024-11-05 00:36:33,906 - INFO - [diffusion][Epoch 6210] diffusion training Loss: 0.0580051401630044
2024-11-05 00:36:33,908 - INFO - [diffusion][Epoch 6210] diffusion learning rate: 0.001
2024-11-05 00:36:33,921 - INFO - [diffusion][Epoch 6210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:33,923 - INFO - [diffusion][Epoch 6211] Epoch 6212/12000
2024-11-05 00:36:38,077 - INFO - [diffusion][Epoch 6211] diffusion training Loss: 0.05986956786364317
2024-11-05 00:36:38,079 - INFO - [diffusion][Epoch 6211] diffusion learning rate: 0.001
2024-11-05 00:36:38,081 - INFO - [diffusion][Epoch 6211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:38,083 - INFO - [diffusion][Epoch 6212] Epoch 6213/12000
2024-11-05 00:36:42,192 - INFO - [diffusion][Epoch 6212] diffusion training Loss: 0.05878592561930418
2024-11-05 00:36:42,195 - INFO - [diffusion][Epoch 6212] diffusion learning rate: 0.001
2024-11-05 00:36:42,197 - INFO - [diffusion][Epoch 6212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:42,200 - INFO - [diffusion][Epoch 6213] Epoch 6214/12000
2024-11-05 00:36:46,349 - INFO - [diffusion][Epoch 6213] diffusion training Loss: 0.057444848120212555
2024-11-05 00:36:46,351 - INFO - [diffusion][Epoch 6213] diffusion learning rate: 0.001
2024-11-05 00:36:46,354 - INFO - [diffusion][Epoch 6213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:46,355 - INFO - [diffusion][Epoch 6214] Epoch 6215/12000
2024-11-05 00:36:50,359 - INFO - [diffusion][Epoch 6214] diffusion training Loss: 0.05927335284650326
2024-11-05 00:36:50,361 - INFO - [diffusion][Epoch 6214] diffusion learning rate: 0.001
2024-11-05 00:36:50,363 - INFO - [diffusion][Epoch 6214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:50,364 - INFO - [diffusion][Epoch 6215] Epoch 6216/12000
2024-11-05 00:36:54,370 - INFO - [diffusion][Epoch 6215] diffusion training Loss: 0.06411438435316086
2024-11-05 00:36:54,372 - INFO - [diffusion][Epoch 6215] diffusion learning rate: 0.001
2024-11-05 00:36:54,374 - INFO - [diffusion][Epoch 6215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:54,376 - INFO - [diffusion][Epoch 6216] Epoch 6217/12000
2024-11-05 00:36:58,308 - INFO - [diffusion][Epoch 6216] diffusion training Loss: 0.06275471858680248
2024-11-05 00:36:58,310 - INFO - [diffusion][Epoch 6216] diffusion learning rate: 0.001
2024-11-05 00:36:58,312 - INFO - [diffusion][Epoch 6216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:36:58,313 - INFO - [diffusion][Epoch 6217] Epoch 6218/12000
2024-11-05 00:37:02,333 - INFO - [diffusion][Epoch 6217] diffusion training Loss: 0.06821329146623611
2024-11-05 00:37:02,335 - INFO - [diffusion][Epoch 6217] diffusion learning rate: 0.001
2024-11-05 00:37:02,336 - INFO - [diffusion][Epoch 6217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:02,337 - INFO - [diffusion][Epoch 6218] Epoch 6219/12000
2024-11-05 00:37:06,481 - INFO - [diffusion][Epoch 6218] diffusion training Loss: 0.058356172405183315
2024-11-05 00:37:06,484 - INFO - [diffusion][Epoch 6218] diffusion learning rate: 0.001
2024-11-05 00:37:06,486 - INFO - [diffusion][Epoch 6218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:06,487 - INFO - [diffusion][Epoch 6219] Epoch 6220/12000
2024-11-05 00:37:10,577 - INFO - [diffusion][Epoch 6219] diffusion training Loss: 0.0573495551943779
2024-11-05 00:37:10,579 - INFO - [diffusion][Epoch 6219] diffusion learning rate: 0.001
2024-11-05 00:37:10,581 - INFO - [diffusion][Epoch 6219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:10,583 - INFO - [diffusion][Epoch 6220] Epoch 6221/12000
2024-11-05 00:37:14,566 - INFO - [diffusion][Epoch 6220] diffusion training Loss: 0.06808975618332624
2024-11-05 00:37:14,606 - INFO - [diffusion][Epoch 6220] diffusion learning rate: 0.001
2024-11-05 00:37:14,608 - INFO - [diffusion][Epoch 6220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:14,609 - INFO - [diffusion][Epoch 6221] Epoch 6222/12000
2024-11-05 00:37:18,493 - INFO - [diffusion][Epoch 6221] diffusion training Loss: 0.06350739859044552
2024-11-05 00:37:18,496 - INFO - [diffusion][Epoch 6221] diffusion learning rate: 0.001
2024-11-05 00:37:18,498 - INFO - [diffusion][Epoch 6221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:18,499 - INFO - [diffusion][Epoch 6222] Epoch 6223/12000
2024-11-05 00:37:22,641 - INFO - [diffusion][Epoch 6222] diffusion training Loss: 0.06371649634093046
2024-11-05 00:37:22,644 - INFO - [diffusion][Epoch 6222] diffusion learning rate: 0.001
2024-11-05 00:37:22,646 - INFO - [diffusion][Epoch 6222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:22,647 - INFO - [diffusion][Epoch 6223] Epoch 6224/12000
2024-11-05 00:37:26,747 - INFO - [diffusion][Epoch 6223] diffusion training Loss: 0.0636259913444519
2024-11-05 00:37:26,749 - INFO - [diffusion][Epoch 6223] diffusion learning rate: 0.001
2024-11-05 00:37:26,751 - INFO - [diffusion][Epoch 6223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:26,752 - INFO - [diffusion][Epoch 6224] Epoch 6225/12000
2024-11-05 00:37:30,872 - INFO - [diffusion][Epoch 6224] diffusion training Loss: 0.06264386978000402
2024-11-05 00:37:30,875 - INFO - [diffusion][Epoch 6224] diffusion learning rate: 0.001
2024-11-05 00:37:30,877 - INFO - [diffusion][Epoch 6224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:30,878 - INFO - [diffusion][Epoch 6225] Epoch 6226/12000
2024-11-05 00:37:35,309 - INFO - [diffusion][Epoch 6225] diffusion training Loss: 0.06910453364253044
2024-11-05 00:37:35,311 - INFO - [diffusion][Epoch 6225] diffusion learning rate: 0.001
2024-11-05 00:37:35,313 - INFO - [diffusion][Epoch 6225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:35,314 - INFO - [diffusion][Epoch 6226] Epoch 6227/12000
2024-11-05 00:37:39,465 - INFO - [diffusion][Epoch 6226] diffusion training Loss: 0.052192884497344494
2024-11-05 00:37:39,467 - INFO - [diffusion][Epoch 6226] diffusion learning rate: 0.001
2024-11-05 00:37:39,469 - INFO - [diffusion][Epoch 6226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:39,470 - INFO - [diffusion][Epoch 6227] Epoch 6228/12000
2024-11-05 00:37:43,471 - INFO - [diffusion][Epoch 6227] diffusion training Loss: 0.06430365331470966
2024-11-05 00:37:43,473 - INFO - [diffusion][Epoch 6227] diffusion learning rate: 0.001
2024-11-05 00:37:43,475 - INFO - [diffusion][Epoch 6227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:43,476 - INFO - [diffusion][Epoch 6228] Epoch 6229/12000
2024-11-05 00:37:47,516 - INFO - [diffusion][Epoch 6228] diffusion training Loss: 0.0565152196213603
2024-11-05 00:37:47,518 - INFO - [diffusion][Epoch 6228] diffusion learning rate: 0.001
2024-11-05 00:37:47,519 - INFO - [diffusion][Epoch 6228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:47,521 - INFO - [diffusion][Epoch 6229] Epoch 6230/12000
2024-11-05 00:37:51,544 - INFO - [diffusion][Epoch 6229] diffusion training Loss: 0.06238438002765179
2024-11-05 00:37:51,546 - INFO - [diffusion][Epoch 6229] diffusion learning rate: 0.001
2024-11-05 00:37:51,548 - INFO - [diffusion][Epoch 6229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:51,549 - INFO - [diffusion][Epoch 6230] Epoch 6231/12000
2024-11-05 00:37:55,576 - INFO - [diffusion][Epoch 6230] diffusion training Loss: 0.05854468606412411
2024-11-05 00:37:55,579 - INFO - [diffusion][Epoch 6230] diffusion learning rate: 0.001
2024-11-05 00:37:55,580 - INFO - [diffusion][Epoch 6230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:55,582 - INFO - [diffusion][Epoch 6231] Epoch 6232/12000
2024-11-05 00:37:59,684 - INFO - [diffusion][Epoch 6231] diffusion training Loss: 0.06306170672178268
2024-11-05 00:37:59,687 - INFO - [diffusion][Epoch 6231] diffusion learning rate: 0.001
2024-11-05 00:37:59,689 - INFO - [diffusion][Epoch 6231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:37:59,690 - INFO - [diffusion][Epoch 6232] Epoch 6233/12000
2024-11-05 00:38:03,729 - INFO - [diffusion][Epoch 6232] diffusion training Loss: 0.060137354768812656
2024-11-05 00:38:03,731 - INFO - [diffusion][Epoch 6232] diffusion learning rate: 0.001
2024-11-05 00:38:03,732 - INFO - [diffusion][Epoch 6232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:03,734 - INFO - [diffusion][Epoch 6233] Epoch 6234/12000
2024-11-05 00:38:07,642 - INFO - [diffusion][Epoch 6233] diffusion training Loss: 0.056450068950653076
2024-11-05 00:38:07,644 - INFO - [diffusion][Epoch 6233] diffusion learning rate: 0.001
2024-11-05 00:38:07,646 - INFO - [diffusion][Epoch 6233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:07,647 - INFO - [diffusion][Epoch 6234] Epoch 6235/12000
2024-11-05 00:38:11,653 - INFO - [diffusion][Epoch 6234] diffusion training Loss: 0.05487505719065666
2024-11-05 00:38:11,655 - INFO - [diffusion][Epoch 6234] diffusion learning rate: 0.001
2024-11-05 00:38:11,657 - INFO - [diffusion][Epoch 6234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:11,658 - INFO - [diffusion][Epoch 6235] Epoch 6236/12000
2024-11-05 00:38:15,678 - INFO - [diffusion][Epoch 6235] diffusion training Loss: 0.0583800133317709
2024-11-05 00:38:15,680 - INFO - [diffusion][Epoch 6235] diffusion learning rate: 0.001
2024-11-05 00:38:15,682 - INFO - [diffusion][Epoch 6235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:15,683 - INFO - [diffusion][Epoch 6236] Epoch 6237/12000
2024-11-05 00:38:19,807 - INFO - [diffusion][Epoch 6236] diffusion training Loss: 0.06661668606102467
2024-11-05 00:38:19,810 - INFO - [diffusion][Epoch 6236] diffusion learning rate: 0.001
2024-11-05 00:38:19,812 - INFO - [diffusion][Epoch 6236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:19,813 - INFO - [diffusion][Epoch 6237] Epoch 6238/12000
2024-11-05 00:38:23,763 - INFO - [diffusion][Epoch 6237] diffusion training Loss: 0.06038028094917536
2024-11-05 00:38:23,768 - INFO - [diffusion][Epoch 6237] diffusion learning rate: 0.001
2024-11-05 00:38:23,771 - INFO - [diffusion][Epoch 6237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:23,772 - INFO - [diffusion][Epoch 6238] Epoch 6239/12000
2024-11-05 00:38:27,846 - INFO - [diffusion][Epoch 6238] diffusion training Loss: 0.0641955379396677
2024-11-05 00:38:27,847 - INFO - [diffusion][Epoch 6238] diffusion learning rate: 0.001
2024-11-05 00:38:27,849 - INFO - [diffusion][Epoch 6238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:27,850 - INFO - [diffusion][Epoch 6239] Epoch 6240/12000
2024-11-05 00:38:31,840 - INFO - [diffusion][Epoch 6239] diffusion training Loss: 0.05853495933115482
2024-11-05 00:38:31,842 - INFO - [diffusion][Epoch 6239] diffusion learning rate: 0.001
2024-11-05 00:38:31,844 - INFO - [diffusion][Epoch 6239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:31,845 - INFO - [diffusion][Epoch 6240] Epoch 6241/12000
2024-11-05 00:38:35,979 - INFO - [diffusion][Epoch 6240] diffusion training Loss: 0.06197195500135422
2024-11-05 00:38:35,981 - INFO - [diffusion][Epoch 6240] diffusion learning rate: 0.001
2024-11-05 00:38:35,982 - INFO - [diffusion][Epoch 6240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:35,984 - INFO - [diffusion][Epoch 6241] Epoch 6242/12000
2024-11-05 00:38:40,137 - INFO - [diffusion][Epoch 6241] diffusion training Loss: 0.06613889895379543
2024-11-05 00:38:40,138 - INFO - [diffusion][Epoch 6241] diffusion learning rate: 0.001
2024-11-05 00:38:40,141 - INFO - [diffusion][Epoch 6241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:40,142 - INFO - [diffusion][Epoch 6242] Epoch 6243/12000
2024-11-05 00:38:44,261 - INFO - [diffusion][Epoch 6242] diffusion training Loss: 0.059881340712308884
2024-11-05 00:38:44,263 - INFO - [diffusion][Epoch 6242] diffusion learning rate: 0.001
2024-11-05 00:38:44,265 - INFO - [diffusion][Epoch 6242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:44,267 - INFO - [diffusion][Epoch 6243] Epoch 6244/12000
2024-11-05 00:38:48,272 - INFO - [diffusion][Epoch 6243] diffusion training Loss: 0.057992223650217056
2024-11-05 00:38:48,274 - INFO - [diffusion][Epoch 6243] diffusion learning rate: 0.001
2024-11-05 00:38:48,275 - INFO - [diffusion][Epoch 6243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:48,277 - INFO - [diffusion][Epoch 6244] Epoch 6245/12000
2024-11-05 00:38:52,214 - INFO - [diffusion][Epoch 6244] diffusion training Loss: 0.06224483251571655
2024-11-05 00:38:52,217 - INFO - [diffusion][Epoch 6244] diffusion learning rate: 0.001
2024-11-05 00:38:52,219 - INFO - [diffusion][Epoch 6244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:52,220 - INFO - [diffusion][Epoch 6245] Epoch 6246/12000
2024-11-05 00:38:56,298 - INFO - [diffusion][Epoch 6245] diffusion training Loss: 0.0673087639734149
2024-11-05 00:38:56,300 - INFO - [diffusion][Epoch 6245] diffusion learning rate: 0.001
2024-11-05 00:38:56,353 - INFO - [diffusion][Epoch 6245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:38:56,355 - INFO - [diffusion][Epoch 6246] Epoch 6247/12000
2024-11-05 00:39:00,523 - INFO - [diffusion][Epoch 6246] diffusion training Loss: 0.06632442120462656
2024-11-05 00:39:00,525 - INFO - [diffusion][Epoch 6246] diffusion learning rate: 0.001
2024-11-05 00:39:00,527 - INFO - [diffusion][Epoch 6246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:00,528 - INFO - [diffusion][Epoch 6247] Epoch 6248/12000
2024-11-05 00:39:04,725 - INFO - [diffusion][Epoch 6247] diffusion training Loss: 0.06314010452479124
2024-11-05 00:39:04,727 - INFO - [diffusion][Epoch 6247] diffusion learning rate: 0.001
2024-11-05 00:39:04,729 - INFO - [diffusion][Epoch 6247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:04,731 - INFO - [diffusion][Epoch 6248] Epoch 6249/12000
2024-11-05 00:39:08,923 - INFO - [diffusion][Epoch 6248] diffusion training Loss: 0.06748118437826633
2024-11-05 00:39:08,925 - INFO - [diffusion][Epoch 6248] diffusion learning rate: 0.001
2024-11-05 00:39:08,927 - INFO - [diffusion][Epoch 6248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:08,928 - INFO - [diffusion][Epoch 6249] Epoch 6250/12000
2024-11-05 00:39:13,009 - INFO - [diffusion][Epoch 6249] diffusion training Loss: 0.05811570584774017
2024-11-05 00:39:13,011 - INFO - [diffusion][Epoch 6249] diffusion learning rate: 0.001
2024-11-05 00:39:13,013 - INFO - [diffusion][Epoch 6249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:13,015 - INFO - [diffusion][Epoch 6250] Epoch 6251/12000
2024-11-05 00:39:16,991 - INFO - [diffusion][Epoch 6250] diffusion training Loss: 0.06975901406258345
2024-11-05 00:39:16,993 - INFO - [diffusion][Epoch 6250] diffusion learning rate: 0.001
2024-11-05 00:39:16,995 - INFO - [diffusion][Epoch 6250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:16,996 - INFO - [diffusion][Epoch 6251] Epoch 6252/12000
2024-11-05 00:39:21,054 - INFO - [diffusion][Epoch 6251] diffusion training Loss: 0.058639789931476116
2024-11-05 00:39:21,056 - INFO - [diffusion][Epoch 6251] diffusion learning rate: 0.001
2024-11-05 00:39:21,058 - INFO - [diffusion][Epoch 6251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:21,059 - INFO - [diffusion][Epoch 6252] Epoch 6253/12000
2024-11-05 00:39:25,209 - INFO - [diffusion][Epoch 6252] diffusion training Loss: 0.06462550722062588
2024-11-05 00:39:25,213 - INFO - [diffusion][Epoch 6252] diffusion learning rate: 0.001
2024-11-05 00:39:25,215 - INFO - [diffusion][Epoch 6252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:25,217 - INFO - [diffusion][Epoch 6253] Epoch 6254/12000
2024-11-05 00:39:29,149 - INFO - [diffusion][Epoch 6253] diffusion training Loss: 0.061303227208554745
2024-11-05 00:39:29,151 - INFO - [diffusion][Epoch 6253] diffusion learning rate: 0.001
2024-11-05 00:39:29,154 - INFO - [diffusion][Epoch 6253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:29,155 - INFO - [diffusion][Epoch 6254] Epoch 6255/12000
2024-11-05 00:39:33,329 - INFO - [diffusion][Epoch 6254] diffusion training Loss: 0.06108396500349045
2024-11-05 00:39:33,331 - INFO - [diffusion][Epoch 6254] diffusion learning rate: 0.001
2024-11-05 00:39:33,333 - INFO - [diffusion][Epoch 6254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:33,334 - INFO - [diffusion][Epoch 6255] Epoch 6256/12000
2024-11-05 00:39:37,367 - INFO - [diffusion][Epoch 6255] diffusion training Loss: 0.06261478364467621
2024-11-05 00:39:37,369 - INFO - [diffusion][Epoch 6255] diffusion learning rate: 0.001
2024-11-05 00:39:37,371 - INFO - [diffusion][Epoch 6255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:37,372 - INFO - [diffusion][Epoch 6256] Epoch 6257/12000
2024-11-05 00:39:41,500 - INFO - [diffusion][Epoch 6256] diffusion training Loss: 0.0590920215472579
2024-11-05 00:39:41,502 - INFO - [diffusion][Epoch 6256] diffusion learning rate: 0.001
2024-11-05 00:39:41,504 - INFO - [diffusion][Epoch 6256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:41,505 - INFO - [diffusion][Epoch 6257] Epoch 6258/12000
2024-11-05 00:39:45,661 - INFO - [diffusion][Epoch 6257] diffusion training Loss: 0.06150144059211016
2024-11-05 00:39:45,663 - INFO - [diffusion][Epoch 6257] diffusion learning rate: 0.001
2024-11-05 00:39:45,664 - INFO - [diffusion][Epoch 6257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:45,666 - INFO - [diffusion][Epoch 6258] Epoch 6259/12000
2024-11-05 00:39:49,797 - INFO - [diffusion][Epoch 6258] diffusion training Loss: 0.06425215769559145
2024-11-05 00:39:49,799 - INFO - [diffusion][Epoch 6258] diffusion learning rate: 0.001
2024-11-05 00:39:49,801 - INFO - [diffusion][Epoch 6258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:49,802 - INFO - [diffusion][Epoch 6259] Epoch 6260/12000
2024-11-05 00:39:53,968 - INFO - [diffusion][Epoch 6259] diffusion training Loss: 0.05939052812755108
2024-11-05 00:39:53,970 - INFO - [diffusion][Epoch 6259] diffusion learning rate: 0.001
2024-11-05 00:39:53,972 - INFO - [diffusion][Epoch 6259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:53,973 - INFO - [diffusion][Epoch 6260] Epoch 6261/12000
2024-11-05 00:39:58,093 - INFO - [diffusion][Epoch 6260] diffusion training Loss: 0.06697873398661613
2024-11-05 00:39:58,095 - INFO - [diffusion][Epoch 6260] diffusion learning rate: 0.001
2024-11-05 00:39:58,097 - INFO - [diffusion][Epoch 6260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:39:58,099 - INFO - [diffusion][Epoch 6261] Epoch 6262/12000
2024-11-05 00:40:02,288 - INFO - [diffusion][Epoch 6261] diffusion training Loss: 0.061847676523029804
2024-11-05 00:40:02,290 - INFO - [diffusion][Epoch 6261] diffusion learning rate: 0.001
2024-11-05 00:40:02,292 - INFO - [diffusion][Epoch 6261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:02,294 - INFO - [diffusion][Epoch 6262] Epoch 6263/12000
2024-11-05 00:40:06,448 - INFO - [diffusion][Epoch 6262] diffusion training Loss: 0.0581742562353611
2024-11-05 00:40:06,451 - INFO - [diffusion][Epoch 6262] diffusion learning rate: 0.001
2024-11-05 00:40:06,452 - INFO - [diffusion][Epoch 6262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:06,454 - INFO - [diffusion][Epoch 6263] Epoch 6264/12000
2024-11-05 00:40:10,593 - INFO - [diffusion][Epoch 6263] diffusion training Loss: 0.06834376975893974
2024-11-05 00:40:10,595 - INFO - [diffusion][Epoch 6263] diffusion learning rate: 0.001
2024-11-05 00:40:10,597 - INFO - [diffusion][Epoch 6263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:10,598 - INFO - [diffusion][Epoch 6264] Epoch 6265/12000
2024-11-05 00:40:14,709 - INFO - [diffusion][Epoch 6264] diffusion training Loss: 0.058510854840278625
2024-11-05 00:40:14,712 - INFO - [diffusion][Epoch 6264] diffusion learning rate: 0.001
2024-11-05 00:40:14,713 - INFO - [diffusion][Epoch 6264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:14,715 - INFO - [diffusion][Epoch 6265] Epoch 6266/12000
2024-11-05 00:40:18,839 - INFO - [diffusion][Epoch 6265] diffusion training Loss: 0.0526749212294817
2024-11-05 00:40:18,841 - INFO - [diffusion][Epoch 6265] diffusion learning rate: 0.001
2024-11-05 00:40:18,843 - INFO - [diffusion][Epoch 6265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:18,844 - INFO - [diffusion][Epoch 6266] Epoch 6267/12000
2024-11-05 00:40:22,943 - INFO - [diffusion][Epoch 6266] diffusion training Loss: 0.05687081627547741
2024-11-05 00:40:22,945 - INFO - [diffusion][Epoch 6266] diffusion learning rate: 0.001
2024-11-05 00:40:22,947 - INFO - [diffusion][Epoch 6266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:22,948 - INFO - [diffusion][Epoch 6267] Epoch 6268/12000
2024-11-05 00:40:27,780 - INFO - [diffusion][Epoch 6267] diffusion training Loss: 0.05778794642537832
2024-11-05 00:40:27,783 - INFO - [diffusion][Epoch 6267] diffusion learning rate: 0.001
2024-11-05 00:40:27,785 - INFO - [diffusion][Epoch 6267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:27,787 - INFO - [diffusion][Epoch 6268] Epoch 6269/12000
2024-11-05 00:40:31,904 - INFO - [diffusion][Epoch 6268] diffusion training Loss: 0.06538748275488615
2024-11-05 00:40:31,906 - INFO - [diffusion][Epoch 6268] diffusion learning rate: 0.001
2024-11-05 00:40:31,908 - INFO - [diffusion][Epoch 6268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:31,913 - INFO - [diffusion][Epoch 6269] Epoch 6270/12000
2024-11-05 00:40:36,081 - INFO - [diffusion][Epoch 6269] diffusion training Loss: 0.06302792299538851
2024-11-05 00:40:36,083 - INFO - [diffusion][Epoch 6269] diffusion learning rate: 0.001
2024-11-05 00:40:36,085 - INFO - [diffusion][Epoch 6269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:36,087 - INFO - [diffusion][Epoch 6270] Epoch 6271/12000
2024-11-05 00:40:40,283 - INFO - [diffusion][Epoch 6270] diffusion training Loss: 0.05856425128877163
2024-11-05 00:40:40,285 - INFO - [diffusion][Epoch 6270] diffusion learning rate: 0.001
2024-11-05 00:40:40,292 - INFO - [diffusion][Epoch 6270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:40,293 - INFO - [diffusion][Epoch 6271] Epoch 6272/12000
2024-11-05 00:40:44,369 - INFO - [diffusion][Epoch 6271] diffusion training Loss: 0.055871766060590744
2024-11-05 00:40:44,371 - INFO - [diffusion][Epoch 6271] diffusion learning rate: 0.001
2024-11-05 00:40:44,373 - INFO - [diffusion][Epoch 6271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:44,374 - INFO - [diffusion][Epoch 6272] Epoch 6273/12000
2024-11-05 00:40:48,512 - INFO - [diffusion][Epoch 6272] diffusion training Loss: 0.05798196326941252
2024-11-05 00:40:48,514 - INFO - [diffusion][Epoch 6272] diffusion learning rate: 0.001
2024-11-05 00:40:48,516 - INFO - [diffusion][Epoch 6272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:48,517 - INFO - [diffusion][Epoch 6273] Epoch 6274/12000
2024-11-05 00:40:52,554 - INFO - [diffusion][Epoch 6273] diffusion training Loss: 0.060029882937669754
2024-11-05 00:40:52,556 - INFO - [diffusion][Epoch 6273] diffusion learning rate: 0.001
2024-11-05 00:40:52,558 - INFO - [diffusion][Epoch 6273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:52,559 - INFO - [diffusion][Epoch 6274] Epoch 6275/12000
2024-11-05 00:40:56,694 - INFO - [diffusion][Epoch 6274] diffusion training Loss: 0.05784922558814287
2024-11-05 00:40:56,696 - INFO - [diffusion][Epoch 6274] diffusion learning rate: 0.001
2024-11-05 00:40:56,697 - INFO - [diffusion][Epoch 6274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:40:56,699 - INFO - [diffusion][Epoch 6275] Epoch 6276/12000
2024-11-05 00:41:00,776 - INFO - [diffusion][Epoch 6275] diffusion training Loss: 0.06178444065153599
2024-11-05 00:41:00,779 - INFO - [diffusion][Epoch 6275] diffusion learning rate: 0.001
2024-11-05 00:41:00,784 - INFO - [diffusion][Epoch 6275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:00,786 - INFO - [diffusion][Epoch 6276] Epoch 6277/12000
2024-11-05 00:41:04,943 - INFO - [diffusion][Epoch 6276] diffusion training Loss: 0.057793731801211834
2024-11-05 00:41:04,945 - INFO - [diffusion][Epoch 6276] diffusion learning rate: 0.001
2024-11-05 00:41:04,947 - INFO - [diffusion][Epoch 6276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:04,949 - INFO - [diffusion][Epoch 6277] Epoch 6278/12000
2024-11-05 00:41:09,134 - INFO - [diffusion][Epoch 6277] diffusion training Loss: 0.053245882503688335
2024-11-05 00:41:09,136 - INFO - [diffusion][Epoch 6277] diffusion learning rate: 0.001
2024-11-05 00:41:09,138 - INFO - [diffusion][Epoch 6277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:09,139 - INFO - [diffusion][Epoch 6278] Epoch 6279/12000
2024-11-05 00:41:13,243 - INFO - [diffusion][Epoch 6278] diffusion training Loss: 0.06104077212512493
2024-11-05 00:41:13,246 - INFO - [diffusion][Epoch 6278] diffusion learning rate: 0.001
2024-11-05 00:41:13,248 - INFO - [diffusion][Epoch 6278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:13,249 - INFO - [diffusion][Epoch 6279] Epoch 6280/12000
2024-11-05 00:41:17,252 - INFO - [diffusion][Epoch 6279] diffusion training Loss: 0.06174583174288273
2024-11-05 00:41:17,254 - INFO - [diffusion][Epoch 6279] diffusion learning rate: 0.001
2024-11-05 00:41:17,256 - INFO - [diffusion][Epoch 6279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:17,257 - INFO - [diffusion][Epoch 6280] Epoch 6281/12000
2024-11-05 00:41:21,346 - INFO - [diffusion][Epoch 6280] diffusion training Loss: 0.06087327469140291
2024-11-05 00:41:21,348 - INFO - [diffusion][Epoch 6280] diffusion learning rate: 0.001
2024-11-05 00:41:21,350 - INFO - [diffusion][Epoch 6280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:21,351 - INFO - [diffusion][Epoch 6281] Epoch 6282/12000
2024-11-05 00:41:25,468 - INFO - [diffusion][Epoch 6281] diffusion training Loss: 0.06201237253844738
2024-11-05 00:41:25,471 - INFO - [diffusion][Epoch 6281] diffusion learning rate: 0.001
2024-11-05 00:41:25,473 - INFO - [diffusion][Epoch 6281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:25,474 - INFO - [diffusion][Epoch 6282] Epoch 6283/12000
2024-11-05 00:41:29,593 - INFO - [diffusion][Epoch 6282] diffusion training Loss: 0.05837243050336838
2024-11-05 00:41:29,595 - INFO - [diffusion][Epoch 6282] diffusion learning rate: 0.001
2024-11-05 00:41:29,598 - INFO - [diffusion][Epoch 6282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:29,599 - INFO - [diffusion][Epoch 6283] Epoch 6284/12000
2024-11-05 00:41:33,724 - INFO - [diffusion][Epoch 6283] diffusion training Loss: 0.062021246179938316
2024-11-05 00:41:33,726 - INFO - [diffusion][Epoch 6283] diffusion learning rate: 0.001
2024-11-05 00:41:33,727 - INFO - [diffusion][Epoch 6283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:33,729 - INFO - [diffusion][Epoch 6284] Epoch 6285/12000
2024-11-05 00:41:37,798 - INFO - [diffusion][Epoch 6284] diffusion training Loss: 0.062353805638849735
2024-11-05 00:41:37,800 - INFO - [diffusion][Epoch 6284] diffusion learning rate: 0.001
2024-11-05 00:41:37,801 - INFO - [diffusion][Epoch 6284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:37,803 - INFO - [diffusion][Epoch 6285] Epoch 6286/12000
2024-11-05 00:41:41,956 - INFO - [diffusion][Epoch 6285] diffusion training Loss: 0.05925852432847023
2024-11-05 00:41:41,957 - INFO - [diffusion][Epoch 6285] diffusion learning rate: 0.001
2024-11-05 00:41:41,959 - INFO - [diffusion][Epoch 6285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:41,960 - INFO - [diffusion][Epoch 6286] Epoch 6287/12000
2024-11-05 00:41:46,066 - INFO - [diffusion][Epoch 6286] diffusion training Loss: 0.06276857666671276
2024-11-05 00:41:46,068 - INFO - [diffusion][Epoch 6286] diffusion learning rate: 0.001
2024-11-05 00:41:46,070 - INFO - [diffusion][Epoch 6286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:46,071 - INFO - [diffusion][Epoch 6287] Epoch 6288/12000
2024-11-05 00:41:50,445 - INFO - [diffusion][Epoch 6287] diffusion training Loss: 0.060803101398050785
2024-11-05 00:41:50,447 - INFO - [diffusion][Epoch 6287] diffusion learning rate: 0.001
2024-11-05 00:41:50,449 - INFO - [diffusion][Epoch 6287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:50,450 - INFO - [diffusion][Epoch 6288] Epoch 6289/12000
2024-11-05 00:41:54,607 - INFO - [diffusion][Epoch 6288] diffusion training Loss: 0.05638078507035971
2024-11-05 00:41:54,610 - INFO - [diffusion][Epoch 6288] diffusion learning rate: 0.001
2024-11-05 00:41:54,611 - INFO - [diffusion][Epoch 6288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:54,613 - INFO - [diffusion][Epoch 6289] Epoch 6290/12000
2024-11-05 00:41:58,730 - INFO - [diffusion][Epoch 6289] diffusion training Loss: 0.06208804529160261
2024-11-05 00:41:58,732 - INFO - [diffusion][Epoch 6289] diffusion learning rate: 0.001
2024-11-05 00:41:58,734 - INFO - [diffusion][Epoch 6289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:41:58,736 - INFO - [diffusion][Epoch 6290] Epoch 6291/12000
2024-11-05 00:42:02,813 - INFO - [diffusion][Epoch 6290] diffusion training Loss: 0.0558235477656126
2024-11-05 00:42:02,816 - INFO - [diffusion][Epoch 6290] diffusion learning rate: 0.001
2024-11-05 00:42:02,818 - INFO - [diffusion][Epoch 6290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:02,820 - INFO - [diffusion][Epoch 6291] Epoch 6292/12000
2024-11-05 00:42:06,803 - INFO - [diffusion][Epoch 6291] diffusion training Loss: 0.06036531459540129
2024-11-05 00:42:06,805 - INFO - [diffusion][Epoch 6291] diffusion learning rate: 0.001
2024-11-05 00:42:06,807 - INFO - [diffusion][Epoch 6291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:06,808 - INFO - [diffusion][Epoch 6292] Epoch 6293/12000
2024-11-05 00:42:10,791 - INFO - [diffusion][Epoch 6292] diffusion training Loss: 0.0640489524230361
2024-11-05 00:42:10,793 - INFO - [diffusion][Epoch 6292] diffusion learning rate: 0.001
2024-11-05 00:42:10,795 - INFO - [diffusion][Epoch 6292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:10,796 - INFO - [diffusion][Epoch 6293] Epoch 6294/12000
2024-11-05 00:42:14,896 - INFO - [diffusion][Epoch 6293] diffusion training Loss: 0.06158512458205223
2024-11-05 00:42:14,898 - INFO - [diffusion][Epoch 6293] diffusion learning rate: 0.001
2024-11-05 00:42:14,900 - INFO - [diffusion][Epoch 6293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:14,901 - INFO - [diffusion][Epoch 6294] Epoch 6295/12000
2024-11-05 00:42:19,039 - INFO - [diffusion][Epoch 6294] diffusion training Loss: 0.06510362587869167
2024-11-05 00:42:19,041 - INFO - [diffusion][Epoch 6294] diffusion learning rate: 0.001
2024-11-05 00:42:19,044 - INFO - [diffusion][Epoch 6294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:19,045 - INFO - [diffusion][Epoch 6295] Epoch 6296/12000
2024-11-05 00:42:23,195 - INFO - [diffusion][Epoch 6295] diffusion training Loss: 0.05906005762517452
2024-11-05 00:42:23,197 - INFO - [diffusion][Epoch 6295] diffusion learning rate: 0.001
2024-11-05 00:42:23,199 - INFO - [diffusion][Epoch 6295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:23,201 - INFO - [diffusion][Epoch 6296] Epoch 6297/12000
2024-11-05 00:42:27,320 - INFO - [diffusion][Epoch 6296] diffusion training Loss: 0.059505999088287354
2024-11-05 00:42:27,322 - INFO - [diffusion][Epoch 6296] diffusion learning rate: 0.001
2024-11-05 00:42:27,324 - INFO - [diffusion][Epoch 6296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:27,325 - INFO - [diffusion][Epoch 6297] Epoch 6298/12000
2024-11-05 00:42:31,421 - INFO - [diffusion][Epoch 6297] diffusion training Loss: 0.06265797186642885
2024-11-05 00:42:31,423 - INFO - [diffusion][Epoch 6297] diffusion learning rate: 0.001
2024-11-05 00:42:31,425 - INFO - [diffusion][Epoch 6297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:31,426 - INFO - [diffusion][Epoch 6298] Epoch 6299/12000
2024-11-05 00:42:35,570 - INFO - [diffusion][Epoch 6298] diffusion training Loss: 0.05913206655532122
2024-11-05 00:42:35,572 - INFO - [diffusion][Epoch 6298] diffusion learning rate: 0.001
2024-11-05 00:42:35,574 - INFO - [diffusion][Epoch 6298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:35,575 - INFO - [diffusion][Epoch 6299] Epoch 6300/12000
2024-11-05 00:42:39,507 - INFO - [diffusion][Epoch 6299] diffusion training Loss: 0.06315521895885468
2024-11-05 00:42:39,509 - INFO - [diffusion][Epoch 6299] diffusion learning rate: 0.001
2024-11-05 00:42:39,511 - INFO - [diffusion][Epoch 6299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:39,513 - INFO - [diffusion][Epoch 6300] Epoch 6301/12000
2024-11-05 00:42:43,640 - INFO - [diffusion][Epoch 6300] diffusion training Loss: 0.059546733275055885
2024-11-05 00:42:43,642 - INFO - [diffusion][Epoch 6300] diffusion learning rate: 0.001
2024-11-05 00:42:43,644 - INFO - [diffusion][Epoch 6300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:43,645 - INFO - [diffusion][Epoch 6301] Epoch 6302/12000
2024-11-05 00:42:47,795 - INFO - [diffusion][Epoch 6301] diffusion training Loss: 0.0655656149610877
2024-11-05 00:42:47,797 - INFO - [diffusion][Epoch 6301] diffusion learning rate: 0.001
2024-11-05 00:42:47,799 - INFO - [diffusion][Epoch 6301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:47,801 - INFO - [diffusion][Epoch 6302] Epoch 6303/12000
2024-11-05 00:42:51,979 - INFO - [diffusion][Epoch 6302] diffusion training Loss: 0.0646350709721446
2024-11-05 00:42:51,981 - INFO - [diffusion][Epoch 6302] diffusion learning rate: 0.001
2024-11-05 00:42:51,983 - INFO - [diffusion][Epoch 6302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:51,985 - INFO - [diffusion][Epoch 6303] Epoch 6304/12000
2024-11-05 00:42:56,163 - INFO - [diffusion][Epoch 6303] diffusion training Loss: 0.0653669685125351
2024-11-05 00:42:56,166 - INFO - [diffusion][Epoch 6303] diffusion learning rate: 0.001
2024-11-05 00:42:56,167 - INFO - [diffusion][Epoch 6303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:42:56,169 - INFO - [diffusion][Epoch 6304] Epoch 6305/12000
2024-11-05 00:43:00,269 - INFO - [diffusion][Epoch 6304] diffusion training Loss: 0.058628225699067116
2024-11-05 00:43:00,271 - INFO - [diffusion][Epoch 6304] diffusion learning rate: 0.001
2024-11-05 00:43:00,273 - INFO - [diffusion][Epoch 6304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:00,275 - INFO - [diffusion][Epoch 6305] Epoch 6306/12000
2024-11-05 00:43:04,451 - INFO - [diffusion][Epoch 6305] diffusion training Loss: 0.058678336441516876
2024-11-05 00:43:04,453 - INFO - [diffusion][Epoch 6305] diffusion learning rate: 0.001
2024-11-05 00:43:04,455 - INFO - [diffusion][Epoch 6305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:04,456 - INFO - [diffusion][Epoch 6306] Epoch 6307/12000
2024-11-05 00:43:08,424 - INFO - [diffusion][Epoch 6306] diffusion training Loss: 0.0574533436447382
2024-11-05 00:43:08,427 - INFO - [diffusion][Epoch 6306] diffusion learning rate: 0.001
2024-11-05 00:43:08,428 - INFO - [diffusion][Epoch 6306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:08,430 - INFO - [diffusion][Epoch 6307] Epoch 6308/12000
2024-11-05 00:43:12,632 - INFO - [diffusion][Epoch 6307] diffusion training Loss: 0.058716852217912674
2024-11-05 00:43:12,634 - INFO - [diffusion][Epoch 6307] diffusion learning rate: 0.001
2024-11-05 00:43:12,635 - INFO - [diffusion][Epoch 6307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:12,637 - INFO - [diffusion][Epoch 6308] Epoch 6309/12000
2024-11-05 00:43:16,800 - INFO - [diffusion][Epoch 6308] diffusion training Loss: 0.05100428685545921
2024-11-05 00:43:16,802 - INFO - [diffusion][Epoch 6308] diffusion learning rate: 0.001
2024-11-05 00:43:16,804 - INFO - [diffusion][Epoch 6308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:16,805 - INFO - [diffusion][Epoch 6309] Epoch 6310/12000
2024-11-05 00:43:20,756 - INFO - [diffusion][Epoch 6309] diffusion training Loss: 0.06300822645425797
2024-11-05 00:43:20,758 - INFO - [diffusion][Epoch 6309] diffusion learning rate: 0.001
2024-11-05 00:43:20,760 - INFO - [diffusion][Epoch 6309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:20,762 - INFO - [diffusion][Epoch 6310] Epoch 6311/12000
2024-11-05 00:43:24,870 - INFO - [diffusion][Epoch 6310] diffusion training Loss: 0.06011931877583265
2024-11-05 00:43:24,872 - INFO - [diffusion][Epoch 6310] diffusion learning rate: 0.001
2024-11-05 00:43:24,874 - INFO - [diffusion][Epoch 6310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:24,875 - INFO - [diffusion][Epoch 6311] Epoch 6312/12000
2024-11-05 00:43:28,949 - INFO - [diffusion][Epoch 6311] diffusion training Loss: 0.060760824009776115
2024-11-05 00:43:28,951 - INFO - [diffusion][Epoch 6311] diffusion learning rate: 0.001
2024-11-05 00:43:28,954 - INFO - [diffusion][Epoch 6311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:28,955 - INFO - [diffusion][Epoch 6312] Epoch 6313/12000
2024-11-05 00:43:32,952 - INFO - [diffusion][Epoch 6312] diffusion training Loss: 0.060796430334448814
2024-11-05 00:43:32,955 - INFO - [diffusion][Epoch 6312] diffusion learning rate: 0.001
2024-11-05 00:43:32,957 - INFO - [diffusion][Epoch 6312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:32,959 - INFO - [diffusion][Epoch 6313] Epoch 6314/12000
2024-11-05 00:43:36,898 - INFO - [diffusion][Epoch 6313] diffusion training Loss: 0.05935837049037218
2024-11-05 00:43:36,900 - INFO - [diffusion][Epoch 6313] diffusion learning rate: 0.001
2024-11-05 00:43:36,902 - INFO - [diffusion][Epoch 6313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:36,904 - INFO - [diffusion][Epoch 6314] Epoch 6315/12000
2024-11-05 00:43:40,948 - INFO - [diffusion][Epoch 6314] diffusion training Loss: 0.05905197188258171
2024-11-05 00:43:40,950 - INFO - [diffusion][Epoch 6314] diffusion learning rate: 0.001
2024-11-05 00:43:40,952 - INFO - [diffusion][Epoch 6314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:40,953 - INFO - [diffusion][Epoch 6315] Epoch 6316/12000
2024-11-05 00:43:45,130 - INFO - [diffusion][Epoch 6315] diffusion training Loss: 0.059864236041903496
2024-11-05 00:43:45,132 - INFO - [diffusion][Epoch 6315] diffusion learning rate: 0.001
2024-11-05 00:43:45,134 - INFO - [diffusion][Epoch 6315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:45,135 - INFO - [diffusion][Epoch 6316] Epoch 6317/12000
2024-11-05 00:43:49,264 - INFO - [diffusion][Epoch 6316] diffusion training Loss: 0.05841159727424383
2024-11-05 00:43:49,266 - INFO - [diffusion][Epoch 6316] diffusion learning rate: 0.001
2024-11-05 00:43:49,269 - INFO - [diffusion][Epoch 6316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:49,270 - INFO - [diffusion][Epoch 6317] Epoch 6318/12000
2024-11-05 00:43:53,424 - INFO - [diffusion][Epoch 6317] diffusion training Loss: 0.06326762866228819
2024-11-05 00:43:53,427 - INFO - [diffusion][Epoch 6317] diffusion learning rate: 0.001
2024-11-05 00:43:53,429 - INFO - [diffusion][Epoch 6317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:53,430 - INFO - [diffusion][Epoch 6318] Epoch 6319/12000
2024-11-05 00:43:57,594 - INFO - [diffusion][Epoch 6318] diffusion training Loss: 0.06265120953321457
2024-11-05 00:43:57,597 - INFO - [diffusion][Epoch 6318] diffusion learning rate: 0.001
2024-11-05 00:43:57,599 - INFO - [diffusion][Epoch 6318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:43:57,600 - INFO - [diffusion][Epoch 6319] Epoch 6320/12000
2024-11-05 00:44:01,768 - INFO - [diffusion][Epoch 6319] diffusion training Loss: 0.06090984120965004
2024-11-05 00:44:01,771 - INFO - [diffusion][Epoch 6319] diffusion learning rate: 0.001
2024-11-05 00:44:01,773 - INFO - [diffusion][Epoch 6319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:01,774 - INFO - [diffusion][Epoch 6320] Epoch 6321/12000
2024-11-05 00:44:05,930 - INFO - [diffusion][Epoch 6320] diffusion training Loss: 0.05636472627520561
2024-11-05 00:44:05,932 - INFO - [diffusion][Epoch 6320] diffusion learning rate: 0.001
2024-11-05 00:44:05,934 - INFO - [diffusion][Epoch 6320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:05,935 - INFO - [diffusion][Epoch 6321] Epoch 6322/12000
2024-11-05 00:44:10,047 - INFO - [diffusion][Epoch 6321] diffusion training Loss: 0.05924287252128124
2024-11-05 00:44:10,050 - INFO - [diffusion][Epoch 6321] diffusion learning rate: 0.001
2024-11-05 00:44:10,052 - INFO - [diffusion][Epoch 6321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:10,054 - INFO - [diffusion][Epoch 6322] Epoch 6323/12000
2024-11-05 00:44:14,182 - INFO - [diffusion][Epoch 6322] diffusion training Loss: 0.0629858709871769
2024-11-05 00:44:14,184 - INFO - [diffusion][Epoch 6322] diffusion learning rate: 0.001
2024-11-05 00:44:14,187 - INFO - [diffusion][Epoch 6322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:14,188 - INFO - [diffusion][Epoch 6323] Epoch 6324/12000
2024-11-05 00:44:18,282 - INFO - [diffusion][Epoch 6323] diffusion training Loss: 0.06425348855555058
2024-11-05 00:44:18,284 - INFO - [diffusion][Epoch 6323] diffusion learning rate: 0.001
2024-11-05 00:44:18,287 - INFO - [diffusion][Epoch 6323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:18,289 - INFO - [diffusion][Epoch 6324] Epoch 6325/12000
2024-11-05 00:44:22,444 - INFO - [diffusion][Epoch 6324] diffusion training Loss: 0.05853207129985094
2024-11-05 00:44:22,447 - INFO - [diffusion][Epoch 6324] diffusion learning rate: 0.001
2024-11-05 00:44:22,449 - INFO - [diffusion][Epoch 6324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:22,450 - INFO - [diffusion][Epoch 6325] Epoch 6326/12000
2024-11-05 00:44:26,602 - INFO - [diffusion][Epoch 6325] diffusion training Loss: 0.05826318170875311
2024-11-05 00:44:26,604 - INFO - [diffusion][Epoch 6325] diffusion learning rate: 0.001
2024-11-05 00:44:26,606 - INFO - [diffusion][Epoch 6325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:26,608 - INFO - [diffusion][Epoch 6326] Epoch 6327/12000
2024-11-05 00:44:30,612 - INFO - [diffusion][Epoch 6326] diffusion training Loss: 0.05931885540485382
2024-11-05 00:44:30,614 - INFO - [diffusion][Epoch 6326] diffusion learning rate: 0.001
2024-11-05 00:44:30,616 - INFO - [diffusion][Epoch 6326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:30,617 - INFO - [diffusion][Epoch 6327] Epoch 6328/12000
2024-11-05 00:44:34,737 - INFO - [diffusion][Epoch 6327] diffusion training Loss: 0.06016513518989086
2024-11-05 00:44:34,740 - INFO - [diffusion][Epoch 6327] diffusion learning rate: 0.001
2024-11-05 00:44:34,742 - INFO - [diffusion][Epoch 6327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:34,743 - INFO - [diffusion][Epoch 6328] Epoch 6329/12000
2024-11-05 00:44:38,823 - INFO - [diffusion][Epoch 6328] diffusion training Loss: 0.05573981814086437
2024-11-05 00:44:38,825 - INFO - [diffusion][Epoch 6328] diffusion learning rate: 0.001
2024-11-05 00:44:38,828 - INFO - [diffusion][Epoch 6328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:38,834 - INFO - [diffusion][Epoch 6329] Epoch 6330/12000
2024-11-05 00:44:43,630 - INFO - [diffusion][Epoch 6329] diffusion training Loss: 0.0537595059722662
2024-11-05 00:44:43,632 - INFO - [diffusion][Epoch 6329] diffusion learning rate: 0.001
2024-11-05 00:44:43,634 - INFO - [diffusion][Epoch 6329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:43,635 - INFO - [diffusion][Epoch 6330] Epoch 6331/12000
2024-11-05 00:44:47,773 - INFO - [diffusion][Epoch 6330] diffusion training Loss: 0.06186985597014427
2024-11-05 00:44:47,775 - INFO - [diffusion][Epoch 6330] diffusion learning rate: 0.001
2024-11-05 00:44:47,777 - INFO - [diffusion][Epoch 6330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:47,778 - INFO - [diffusion][Epoch 6331] Epoch 6332/12000
2024-11-05 00:44:51,899 - INFO - [diffusion][Epoch 6331] diffusion training Loss: 0.05310336407274008
2024-11-05 00:44:51,901 - INFO - [diffusion][Epoch 6331] diffusion learning rate: 0.001
2024-11-05 00:44:51,903 - INFO - [diffusion][Epoch 6331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:51,904 - INFO - [diffusion][Epoch 6332] Epoch 6333/12000
2024-11-05 00:44:55,948 - INFO - [diffusion][Epoch 6332] diffusion training Loss: 0.05871934536844492
2024-11-05 00:44:55,951 - INFO - [diffusion][Epoch 6332] diffusion learning rate: 0.001
2024-11-05 00:44:55,953 - INFO - [diffusion][Epoch 6332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:44:55,955 - INFO - [diffusion][Epoch 6333] Epoch 6334/12000
2024-11-05 00:45:00,074 - INFO - [diffusion][Epoch 6333] diffusion training Loss: 0.06308814883232117
2024-11-05 00:45:00,077 - INFO - [diffusion][Epoch 6333] diffusion learning rate: 0.001
2024-11-05 00:45:00,078 - INFO - [diffusion][Epoch 6333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:00,080 - INFO - [diffusion][Epoch 6334] Epoch 6335/12000
2024-11-05 00:45:04,244 - INFO - [diffusion][Epoch 6334] diffusion training Loss: 0.05293481145054102
2024-11-05 00:45:04,246 - INFO - [diffusion][Epoch 6334] diffusion learning rate: 0.001
2024-11-05 00:45:04,248 - INFO - [diffusion][Epoch 6334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:04,249 - INFO - [diffusion][Epoch 6335] Epoch 6336/12000
2024-11-05 00:45:08,327 - INFO - [diffusion][Epoch 6335] diffusion training Loss: 0.05669932998716831
2024-11-05 00:45:08,329 - INFO - [diffusion][Epoch 6335] diffusion learning rate: 0.001
2024-11-05 00:45:08,331 - INFO - [diffusion][Epoch 6335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:08,333 - INFO - [diffusion][Epoch 6336] Epoch 6337/12000
2024-11-05 00:45:12,389 - INFO - [diffusion][Epoch 6336] diffusion training Loss: 0.05667173024266958
2024-11-05 00:45:12,391 - INFO - [diffusion][Epoch 6336] diffusion learning rate: 0.001
2024-11-05 00:45:12,393 - INFO - [diffusion][Epoch 6336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:12,394 - INFO - [diffusion][Epoch 6337] Epoch 6338/12000
2024-11-05 00:45:16,422 - INFO - [diffusion][Epoch 6337] diffusion training Loss: 0.060927700251340866
2024-11-05 00:45:16,424 - INFO - [diffusion][Epoch 6337] diffusion learning rate: 0.001
2024-11-05 00:45:16,426 - INFO - [diffusion][Epoch 6337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:16,427 - INFO - [diffusion][Epoch 6338] Epoch 6339/12000
2024-11-05 00:45:20,598 - INFO - [diffusion][Epoch 6338] diffusion training Loss: 0.05871963873505592
2024-11-05 00:45:20,601 - INFO - [diffusion][Epoch 6338] diffusion learning rate: 0.001
2024-11-05 00:45:20,603 - INFO - [diffusion][Epoch 6338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:20,604 - INFO - [diffusion][Epoch 6339] Epoch 6340/12000
2024-11-05 00:45:24,714 - INFO - [diffusion][Epoch 6339] diffusion training Loss: 0.05504793208092451
2024-11-05 00:45:24,716 - INFO - [diffusion][Epoch 6339] diffusion learning rate: 0.001
2024-11-05 00:45:24,718 - INFO - [diffusion][Epoch 6339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:24,719 - INFO - [diffusion][Epoch 6340] Epoch 6341/12000
2024-11-05 00:45:28,821 - INFO - [diffusion][Epoch 6340] diffusion training Loss: 0.06003871187567711
2024-11-05 00:45:28,823 - INFO - [diffusion][Epoch 6340] diffusion learning rate: 0.001
2024-11-05 00:45:28,825 - INFO - [diffusion][Epoch 6340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:28,827 - INFO - [diffusion][Epoch 6341] Epoch 6342/12000
2024-11-05 00:45:32,954 - INFO - [diffusion][Epoch 6341] diffusion training Loss: 0.06013254635035992
2024-11-05 00:45:32,956 - INFO - [diffusion][Epoch 6341] diffusion learning rate: 0.001
2024-11-05 00:45:32,957 - INFO - [diffusion][Epoch 6341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:32,958 - INFO - [diffusion][Epoch 6342] Epoch 6343/12000
2024-11-05 00:45:36,929 - INFO - [diffusion][Epoch 6342] diffusion training Loss: 0.05880190059542656
2024-11-05 00:45:36,932 - INFO - [diffusion][Epoch 6342] diffusion learning rate: 0.001
2024-11-05 00:45:36,934 - INFO - [diffusion][Epoch 6342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:36,935 - INFO - [diffusion][Epoch 6343] Epoch 6344/12000
2024-11-05 00:45:41,073 - INFO - [diffusion][Epoch 6343] diffusion training Loss: 0.06151461973786354
2024-11-05 00:45:41,075 - INFO - [diffusion][Epoch 6343] diffusion learning rate: 0.001
2024-11-05 00:45:41,133 - INFO - [diffusion][Epoch 6343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:41,134 - INFO - [diffusion][Epoch 6344] Epoch 6345/12000
2024-11-05 00:45:45,241 - INFO - [diffusion][Epoch 6344] diffusion training Loss: 0.05637739412486553
2024-11-05 00:45:45,244 - INFO - [diffusion][Epoch 6344] diffusion learning rate: 0.001
2024-11-05 00:45:45,245 - INFO - [diffusion][Epoch 6344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:45,247 - INFO - [diffusion][Epoch 6345] Epoch 6346/12000
2024-11-05 00:45:49,321 - INFO - [diffusion][Epoch 6345] diffusion training Loss: 0.057916599325835705
2024-11-05 00:45:49,323 - INFO - [diffusion][Epoch 6345] diffusion learning rate: 0.001
2024-11-05 00:45:49,324 - INFO - [diffusion][Epoch 6345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:49,326 - INFO - [diffusion][Epoch 6346] Epoch 6347/12000
2024-11-05 00:45:53,478 - INFO - [diffusion][Epoch 6346] diffusion training Loss: 0.05785918701440096
2024-11-05 00:45:53,480 - INFO - [diffusion][Epoch 6346] diffusion learning rate: 0.001
2024-11-05 00:45:53,482 - INFO - [diffusion][Epoch 6346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:53,483 - INFO - [diffusion][Epoch 6347] Epoch 6348/12000
2024-11-05 00:45:57,637 - INFO - [diffusion][Epoch 6347] diffusion training Loss: 0.05567420367151499
2024-11-05 00:45:57,639 - INFO - [diffusion][Epoch 6347] diffusion learning rate: 0.001
2024-11-05 00:45:57,641 - INFO - [diffusion][Epoch 6347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:45:57,642 - INFO - [diffusion][Epoch 6348] Epoch 6349/12000
2024-11-05 00:46:01,730 - INFO - [diffusion][Epoch 6348] diffusion training Loss: 0.06302936561405659
2024-11-05 00:46:01,732 - INFO - [diffusion][Epoch 6348] diffusion learning rate: 0.001
2024-11-05 00:46:01,734 - INFO - [diffusion][Epoch 6348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:01,736 - INFO - [diffusion][Epoch 6349] Epoch 6350/12000
2024-11-05 00:46:06,117 - INFO - [diffusion][Epoch 6349] diffusion training Loss: 0.060515811666846275
2024-11-05 00:46:06,120 - INFO - [diffusion][Epoch 6349] diffusion learning rate: 0.001
2024-11-05 00:46:06,121 - INFO - [diffusion][Epoch 6349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:06,123 - INFO - [diffusion][Epoch 6350] Epoch 6351/12000
2024-11-05 00:46:10,295 - INFO - [diffusion][Epoch 6350] diffusion training Loss: 0.056399039924144745
2024-11-05 00:46:10,297 - INFO - [diffusion][Epoch 6350] diffusion learning rate: 0.001
2024-11-05 00:46:10,299 - INFO - [diffusion][Epoch 6350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:10,300 - INFO - [diffusion][Epoch 6351] Epoch 6352/12000
2024-11-05 00:46:14,480 - INFO - [diffusion][Epoch 6351] diffusion training Loss: 0.06276760529726744
2024-11-05 00:46:14,482 - INFO - [diffusion][Epoch 6351] diffusion learning rate: 0.001
2024-11-05 00:46:14,484 - INFO - [diffusion][Epoch 6351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:14,485 - INFO - [diffusion][Epoch 6352] Epoch 6353/12000
2024-11-05 00:46:18,621 - INFO - [diffusion][Epoch 6352] diffusion training Loss: 0.05674548540264368
2024-11-05 00:46:18,623 - INFO - [diffusion][Epoch 6352] diffusion learning rate: 0.001
2024-11-05 00:46:18,624 - INFO - [diffusion][Epoch 6352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:18,626 - INFO - [diffusion][Epoch 6353] Epoch 6354/12000
2024-11-05 00:46:22,691 - INFO - [diffusion][Epoch 6353] diffusion training Loss: 0.06167822238057852
2024-11-05 00:46:22,693 - INFO - [diffusion][Epoch 6353] diffusion learning rate: 0.001
2024-11-05 00:46:22,695 - INFO - [diffusion][Epoch 6353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:22,696 - INFO - [diffusion][Epoch 6354] Epoch 6355/12000
2024-11-05 00:46:26,788 - INFO - [diffusion][Epoch 6354] diffusion training Loss: 0.06331868376582861
2024-11-05 00:46:26,790 - INFO - [diffusion][Epoch 6354] diffusion learning rate: 0.001
2024-11-05 00:46:26,792 - INFO - [diffusion][Epoch 6354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:26,793 - INFO - [diffusion][Epoch 6355] Epoch 6356/12000
2024-11-05 00:46:30,907 - INFO - [diffusion][Epoch 6355] diffusion training Loss: 0.05683648493140936
2024-11-05 00:46:30,910 - INFO - [diffusion][Epoch 6355] diffusion learning rate: 0.001
2024-11-05 00:46:30,912 - INFO - [diffusion][Epoch 6355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:30,914 - INFO - [diffusion][Epoch 6356] Epoch 6357/12000
2024-11-05 00:46:34,885 - INFO - [diffusion][Epoch 6356] diffusion training Loss: 0.05796380154788494
2024-11-05 00:46:34,887 - INFO - [diffusion][Epoch 6356] diffusion learning rate: 0.001
2024-11-05 00:46:34,888 - INFO - [diffusion][Epoch 6356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:34,890 - INFO - [diffusion][Epoch 6357] Epoch 6358/12000
2024-11-05 00:46:39,003 - INFO - [diffusion][Epoch 6357] diffusion training Loss: 0.06161534506827593
2024-11-05 00:46:39,007 - INFO - [diffusion][Epoch 6357] diffusion learning rate: 0.001
2024-11-05 00:46:39,009 - INFO - [diffusion][Epoch 6357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:39,010 - INFO - [diffusion][Epoch 6358] Epoch 6359/12000
2024-11-05 00:46:43,044 - INFO - [diffusion][Epoch 6358] diffusion training Loss: 0.06184678338468075
2024-11-05 00:46:43,047 - INFO - [diffusion][Epoch 6358] diffusion learning rate: 0.001
2024-11-05 00:46:43,049 - INFO - [diffusion][Epoch 6358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:43,050 - INFO - [diffusion][Epoch 6359] Epoch 6360/12000
2024-11-05 00:46:47,138 - INFO - [diffusion][Epoch 6359] diffusion training Loss: 0.06259602773934603
2024-11-05 00:46:47,218 - INFO - [diffusion][Epoch 6359] diffusion learning rate: 0.001
2024-11-05 00:46:47,219 - INFO - [diffusion][Epoch 6359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:47,221 - INFO - [diffusion][Epoch 6360] Epoch 6361/12000
2024-11-05 00:46:51,353 - INFO - [diffusion][Epoch 6360] diffusion training Loss: 0.05982683040201664
2024-11-05 00:46:51,355 - INFO - [diffusion][Epoch 6360] diffusion learning rate: 0.001
2024-11-05 00:46:51,357 - INFO - [diffusion][Epoch 6360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:51,358 - INFO - [diffusion][Epoch 6361] Epoch 6362/12000
2024-11-05 00:46:55,449 - INFO - [diffusion][Epoch 6361] diffusion training Loss: 0.06149420980364084
2024-11-05 00:46:55,451 - INFO - [diffusion][Epoch 6361] diffusion learning rate: 0.001
2024-11-05 00:46:55,453 - INFO - [diffusion][Epoch 6361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:55,454 - INFO - [diffusion][Epoch 6362] Epoch 6363/12000
2024-11-05 00:46:59,414 - INFO - [diffusion][Epoch 6362] diffusion training Loss: 0.05811609886586666
2024-11-05 00:46:59,416 - INFO - [diffusion][Epoch 6362] diffusion learning rate: 0.001
2024-11-05 00:46:59,418 - INFO - [diffusion][Epoch 6362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:46:59,419 - INFO - [diffusion][Epoch 6363] Epoch 6364/12000
2024-11-05 00:47:03,584 - INFO - [diffusion][Epoch 6363] diffusion training Loss: 0.05776427872478962
2024-11-05 00:47:03,587 - INFO - [diffusion][Epoch 6363] diffusion learning rate: 0.001
2024-11-05 00:47:03,589 - INFO - [diffusion][Epoch 6363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:03,590 - INFO - [diffusion][Epoch 6364] Epoch 6365/12000
2024-11-05 00:47:07,624 - INFO - [diffusion][Epoch 6364] diffusion training Loss: 0.05682600662112236
2024-11-05 00:47:07,627 - INFO - [diffusion][Epoch 6364] diffusion learning rate: 0.001
2024-11-05 00:47:07,697 - INFO - [diffusion][Epoch 6364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:07,699 - INFO - [diffusion][Epoch 6365] Epoch 6366/12000
2024-11-05 00:47:11,790 - INFO - [diffusion][Epoch 6365] diffusion training Loss: 0.05886938516050577
2024-11-05 00:47:11,792 - INFO - [diffusion][Epoch 6365] diffusion learning rate: 0.001
2024-11-05 00:47:11,794 - INFO - [diffusion][Epoch 6365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:11,795 - INFO - [diffusion][Epoch 6366] Epoch 6367/12000
2024-11-05 00:47:15,913 - INFO - [diffusion][Epoch 6366] diffusion training Loss: 0.061385851353406906
2024-11-05 00:47:15,915 - INFO - [diffusion][Epoch 6366] diffusion learning rate: 0.001
2024-11-05 00:47:15,917 - INFO - [diffusion][Epoch 6366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:15,919 - INFO - [diffusion][Epoch 6367] Epoch 6368/12000
2024-11-05 00:47:20,052 - INFO - [diffusion][Epoch 6367] diffusion training Loss: 0.0596743430942297
2024-11-05 00:47:20,054 - INFO - [diffusion][Epoch 6367] diffusion learning rate: 0.001
2024-11-05 00:47:20,057 - INFO - [diffusion][Epoch 6367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:20,058 - INFO - [diffusion][Epoch 6368] Epoch 6369/12000
2024-11-05 00:47:24,155 - INFO - [diffusion][Epoch 6368] diffusion training Loss: 0.060394441708922386
2024-11-05 00:47:24,157 - INFO - [diffusion][Epoch 6368] diffusion learning rate: 0.001
2024-11-05 00:47:24,159 - INFO - [diffusion][Epoch 6368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:24,161 - INFO - [diffusion][Epoch 6369] Epoch 6370/12000
2024-11-05 00:47:28,293 - INFO - [diffusion][Epoch 6369] diffusion training Loss: 0.061378263868391514
2024-11-05 00:47:28,295 - INFO - [diffusion][Epoch 6369] diffusion learning rate: 0.001
2024-11-05 00:47:28,297 - INFO - [diffusion][Epoch 6369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:28,298 - INFO - [diffusion][Epoch 6370] Epoch 6371/12000
2024-11-05 00:47:33,014 - INFO - [diffusion][Epoch 6370] diffusion training Loss: 0.0549630606546998
2024-11-05 00:47:33,016 - INFO - [diffusion][Epoch 6370] diffusion learning rate: 0.001
2024-11-05 00:47:33,018 - INFO - [diffusion][Epoch 6370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:33,019 - INFO - [diffusion][Epoch 6371] Epoch 6372/12000
2024-11-05 00:47:37,141 - INFO - [diffusion][Epoch 6371] diffusion training Loss: 0.06629523541778326
2024-11-05 00:47:37,143 - INFO - [diffusion][Epoch 6371] diffusion learning rate: 0.001
2024-11-05 00:47:37,145 - INFO - [diffusion][Epoch 6371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:37,146 - INFO - [diffusion][Epoch 6372] Epoch 6373/12000
2024-11-05 00:47:41,268 - INFO - [diffusion][Epoch 6372] diffusion training Loss: 0.05669911205768585
2024-11-05 00:47:41,270 - INFO - [diffusion][Epoch 6372] diffusion learning rate: 0.001
2024-11-05 00:47:41,272 - INFO - [diffusion][Epoch 6372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:41,274 - INFO - [diffusion][Epoch 6373] Epoch 6374/12000
2024-11-05 00:47:45,394 - INFO - [diffusion][Epoch 6373] diffusion training Loss: 0.0636065797880292
2024-11-05 00:47:45,396 - INFO - [diffusion][Epoch 6373] diffusion learning rate: 0.001
2024-11-05 00:47:45,398 - INFO - [diffusion][Epoch 6373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:45,399 - INFO - [diffusion][Epoch 6374] Epoch 6375/12000
2024-11-05 00:47:49,513 - INFO - [diffusion][Epoch 6374] diffusion training Loss: 0.06182171870023012
2024-11-05 00:47:49,515 - INFO - [diffusion][Epoch 6374] diffusion learning rate: 0.001
2024-11-05 00:47:49,516 - INFO - [diffusion][Epoch 6374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:49,518 - INFO - [diffusion][Epoch 6375] Epoch 6376/12000
2024-11-05 00:47:53,622 - INFO - [diffusion][Epoch 6375] diffusion training Loss: 0.05597227904945612
2024-11-05 00:47:53,624 - INFO - [diffusion][Epoch 6375] diffusion learning rate: 0.001
2024-11-05 00:47:53,626 - INFO - [diffusion][Epoch 6375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:53,627 - INFO - [diffusion][Epoch 6376] Epoch 6377/12000
2024-11-05 00:47:57,740 - INFO - [diffusion][Epoch 6376] diffusion training Loss: 0.059612443670630455
2024-11-05 00:47:57,742 - INFO - [diffusion][Epoch 6376] diffusion learning rate: 0.001
2024-11-05 00:47:57,744 - INFO - [diffusion][Epoch 6376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:47:57,746 - INFO - [diffusion][Epoch 6377] Epoch 6378/12000
2024-11-05 00:48:01,906 - INFO - [diffusion][Epoch 6377] diffusion training Loss: 0.06068728119134903
2024-11-05 00:48:01,908 - INFO - [diffusion][Epoch 6377] diffusion learning rate: 0.001
2024-11-05 00:48:01,910 - INFO - [diffusion][Epoch 6377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:01,911 - INFO - [diffusion][Epoch 6378] Epoch 6379/12000
2024-11-05 00:48:06,045 - INFO - [diffusion][Epoch 6378] diffusion training Loss: 0.0638057254254818
2024-11-05 00:48:06,047 - INFO - [diffusion][Epoch 6378] diffusion learning rate: 0.001
2024-11-05 00:48:06,049 - INFO - [diffusion][Epoch 6378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:06,050 - INFO - [diffusion][Epoch 6379] Epoch 6380/12000
2024-11-05 00:48:10,161 - INFO - [diffusion][Epoch 6379] diffusion training Loss: 0.05864201858639717
2024-11-05 00:48:10,163 - INFO - [diffusion][Epoch 6379] diffusion learning rate: 0.001
2024-11-05 00:48:10,166 - INFO - [diffusion][Epoch 6379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:10,167 - INFO - [diffusion][Epoch 6380] Epoch 6381/12000
2024-11-05 00:48:14,317 - INFO - [diffusion][Epoch 6380] diffusion training Loss: 0.05521143600344658
2024-11-05 00:48:14,319 - INFO - [diffusion][Epoch 6380] diffusion learning rate: 0.001
2024-11-05 00:48:14,373 - INFO - [diffusion][Epoch 6380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:14,375 - INFO - [diffusion][Epoch 6381] Epoch 6382/12000
2024-11-05 00:48:18,510 - INFO - [diffusion][Epoch 6381] diffusion training Loss: 0.06345189269632101
2024-11-05 00:48:18,512 - INFO - [diffusion][Epoch 6381] diffusion learning rate: 0.001
2024-11-05 00:48:18,514 - INFO - [diffusion][Epoch 6381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:18,516 - INFO - [diffusion][Epoch 6382] Epoch 6383/12000
2024-11-05 00:48:22,630 - INFO - [diffusion][Epoch 6382] diffusion training Loss: 0.058007958345115185
2024-11-05 00:48:22,632 - INFO - [diffusion][Epoch 6382] diffusion learning rate: 0.001
2024-11-05 00:48:22,634 - INFO - [diffusion][Epoch 6382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:22,636 - INFO - [diffusion][Epoch 6383] Epoch 6384/12000
2024-11-05 00:48:26,727 - INFO - [diffusion][Epoch 6383] diffusion training Loss: 0.0587601400911808
2024-11-05 00:48:26,729 - INFO - [diffusion][Epoch 6383] diffusion learning rate: 0.001
2024-11-05 00:48:26,731 - INFO - [diffusion][Epoch 6383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:26,732 - INFO - [diffusion][Epoch 6384] Epoch 6385/12000
2024-11-05 00:48:30,888 - INFO - [diffusion][Epoch 6384] diffusion training Loss: 0.06617809273302555
2024-11-05 00:48:30,890 - INFO - [diffusion][Epoch 6384] diffusion learning rate: 0.001
2024-11-05 00:48:30,892 - INFO - [diffusion][Epoch 6384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:30,893 - INFO - [diffusion][Epoch 6385] Epoch 6386/12000
2024-11-05 00:48:35,057 - INFO - [diffusion][Epoch 6385] diffusion training Loss: 0.05878197029232979
2024-11-05 00:48:35,059 - INFO - [diffusion][Epoch 6385] diffusion learning rate: 0.001
2024-11-05 00:48:35,061 - INFO - [diffusion][Epoch 6385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:35,062 - INFO - [diffusion][Epoch 6386] Epoch 6387/12000
2024-11-05 00:48:39,182 - INFO - [diffusion][Epoch 6386] diffusion training Loss: 0.061572130769491196
2024-11-05 00:48:39,184 - INFO - [diffusion][Epoch 6386] diffusion learning rate: 0.001
2024-11-05 00:48:39,185 - INFO - [diffusion][Epoch 6386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:39,187 - INFO - [diffusion][Epoch 6387] Epoch 6388/12000
2024-11-05 00:48:43,292 - INFO - [diffusion][Epoch 6387] diffusion training Loss: 0.058217388577759266
2024-11-05 00:48:43,294 - INFO - [diffusion][Epoch 6387] diffusion learning rate: 0.001
2024-11-05 00:48:43,296 - INFO - [diffusion][Epoch 6387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:43,297 - INFO - [diffusion][Epoch 6388] Epoch 6389/12000
2024-11-05 00:48:47,384 - INFO - [diffusion][Epoch 6388] diffusion training Loss: 0.06067858822643757
2024-11-05 00:48:47,386 - INFO - [diffusion][Epoch 6388] diffusion learning rate: 0.001
2024-11-05 00:48:47,389 - INFO - [diffusion][Epoch 6388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:47,390 - INFO - [diffusion][Epoch 6389] Epoch 6390/12000
2024-11-05 00:48:51,518 - INFO - [diffusion][Epoch 6389] diffusion training Loss: 0.05882064998149872
2024-11-05 00:48:51,520 - INFO - [diffusion][Epoch 6389] diffusion learning rate: 0.001
2024-11-05 00:48:51,589 - INFO - [diffusion][Epoch 6389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:51,590 - INFO - [diffusion][Epoch 6390] Epoch 6391/12000
2024-11-05 00:48:55,724 - INFO - [diffusion][Epoch 6390] diffusion training Loss: 0.05519748758524656
2024-11-05 00:48:55,726 - INFO - [diffusion][Epoch 6390] diffusion learning rate: 0.001
2024-11-05 00:48:55,728 - INFO - [diffusion][Epoch 6390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:55,730 - INFO - [diffusion][Epoch 6391] Epoch 6392/12000
2024-11-05 00:48:59,936 - INFO - [diffusion][Epoch 6391] diffusion training Loss: 0.05741244461387396
2024-11-05 00:48:59,938 - INFO - [diffusion][Epoch 6391] diffusion learning rate: 0.001
2024-11-05 00:48:59,940 - INFO - [diffusion][Epoch 6391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:48:59,941 - INFO - [diffusion][Epoch 6392] Epoch 6393/12000
2024-11-05 00:49:04,126 - INFO - [diffusion][Epoch 6392] diffusion training Loss: 0.058923338539898396
2024-11-05 00:49:04,128 - INFO - [diffusion][Epoch 6392] diffusion learning rate: 0.001
2024-11-05 00:49:04,130 - INFO - [diffusion][Epoch 6392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:04,132 - INFO - [diffusion][Epoch 6393] Epoch 6394/12000
2024-11-05 00:49:08,267 - INFO - [diffusion][Epoch 6393] diffusion training Loss: 0.0600305600091815
2024-11-05 00:49:08,269 - INFO - [diffusion][Epoch 6393] diffusion learning rate: 0.001
2024-11-05 00:49:08,321 - INFO - [diffusion][Epoch 6393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:08,323 - INFO - [diffusion][Epoch 6394] Epoch 6395/12000
2024-11-05 00:49:12,459 - INFO - [diffusion][Epoch 6394] diffusion training Loss: 0.05962511617690325
2024-11-05 00:49:12,461 - INFO - [diffusion][Epoch 6394] diffusion learning rate: 0.001
2024-11-05 00:49:12,463 - INFO - [diffusion][Epoch 6394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:12,464 - INFO - [diffusion][Epoch 6395] Epoch 6396/12000
2024-11-05 00:49:16,592 - INFO - [diffusion][Epoch 6395] diffusion training Loss: 0.0594608373939991
2024-11-05 00:49:16,594 - INFO - [diffusion][Epoch 6395] diffusion learning rate: 0.001
2024-11-05 00:49:16,596 - INFO - [diffusion][Epoch 6395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:16,597 - INFO - [diffusion][Epoch 6396] Epoch 6397/12000
2024-11-05 00:49:20,720 - INFO - [diffusion][Epoch 6396] diffusion training Loss: 0.059401665814220905
2024-11-05 00:49:20,724 - INFO - [diffusion][Epoch 6396] diffusion learning rate: 0.001
2024-11-05 00:49:20,726 - INFO - [diffusion][Epoch 6396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:20,727 - INFO - [diffusion][Epoch 6397] Epoch 6398/12000
2024-11-05 00:49:24,850 - INFO - [diffusion][Epoch 6397] diffusion training Loss: 0.059994906187057495
2024-11-05 00:49:24,852 - INFO - [diffusion][Epoch 6397] diffusion learning rate: 0.001
2024-11-05 00:49:24,854 - INFO - [diffusion][Epoch 6397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:24,856 - INFO - [diffusion][Epoch 6398] Epoch 6399/12000
2024-11-05 00:49:28,990 - INFO - [diffusion][Epoch 6398] diffusion training Loss: 0.06409617327153683
2024-11-05 00:49:28,992 - INFO - [diffusion][Epoch 6398] diffusion learning rate: 0.001
2024-11-05 00:49:28,994 - INFO - [diffusion][Epoch 6398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:28,996 - INFO - [diffusion][Epoch 6399] Epoch 6400/12000
2024-11-05 00:49:33,049 - INFO - [diffusion][Epoch 6399] diffusion training Loss: 0.057176005095243454
2024-11-05 00:49:33,051 - INFO - [diffusion][Epoch 6399] diffusion learning rate: 0.001
2024-11-05 00:49:33,053 - INFO - [diffusion][Epoch 6399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:33,055 - INFO - [diffusion][Epoch 6400] Epoch 6401/12000
2024-11-05 00:49:37,096 - INFO - [diffusion][Epoch 6400] diffusion training Loss: 0.05973035842180252
2024-11-05 00:49:37,098 - INFO - [diffusion][Epoch 6400] diffusion learning rate: 0.001
2024-11-05 00:49:37,100 - INFO - [diffusion][Epoch 6400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:37,102 - INFO - [diffusion][Epoch 6401] Epoch 6402/12000
2024-11-05 00:49:41,279 - INFO - [diffusion][Epoch 6401] diffusion training Loss: 0.0624306034296751
2024-11-05 00:49:41,281 - INFO - [diffusion][Epoch 6401] diffusion learning rate: 0.001
2024-11-05 00:49:41,283 - INFO - [diffusion][Epoch 6401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:41,285 - INFO - [diffusion][Epoch 6402] Epoch 6403/12000
2024-11-05 00:49:45,436 - INFO - [diffusion][Epoch 6402] diffusion training Loss: 0.05768209230154753
2024-11-05 00:49:45,439 - INFO - [diffusion][Epoch 6402] diffusion learning rate: 0.001
2024-11-05 00:49:45,441 - INFO - [diffusion][Epoch 6402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:45,442 - INFO - [diffusion][Epoch 6403] Epoch 6404/12000
2024-11-05 00:49:49,570 - INFO - [diffusion][Epoch 6403] diffusion training Loss: 0.05935444217175245
2024-11-05 00:49:49,572 - INFO - [diffusion][Epoch 6403] diffusion learning rate: 0.001
2024-11-05 00:49:49,574 - INFO - [diffusion][Epoch 6403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:49,575 - INFO - [diffusion][Epoch 6404] Epoch 6405/12000
2024-11-05 00:49:53,728 - INFO - [diffusion][Epoch 6404] diffusion training Loss: 0.056423467583954334
2024-11-05 00:49:53,730 - INFO - [diffusion][Epoch 6404] diffusion learning rate: 0.001
2024-11-05 00:49:53,732 - INFO - [diffusion][Epoch 6404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:53,734 - INFO - [diffusion][Epoch 6405] Epoch 6406/12000
2024-11-05 00:49:57,871 - INFO - [diffusion][Epoch 6405] diffusion training Loss: 0.05931264907121658
2024-11-05 00:49:57,873 - INFO - [diffusion][Epoch 6405] diffusion learning rate: 0.001
2024-11-05 00:49:57,874 - INFO - [diffusion][Epoch 6405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:49:57,876 - INFO - [diffusion][Epoch 6406] Epoch 6407/12000
2024-11-05 00:50:01,979 - INFO - [diffusion][Epoch 6406] diffusion training Loss: 0.057305991649627686
2024-11-05 00:50:01,982 - INFO - [diffusion][Epoch 6406] diffusion learning rate: 0.001
2024-11-05 00:50:01,984 - INFO - [diffusion][Epoch 6406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:01,985 - INFO - [diffusion][Epoch 6407] Epoch 6408/12000
2024-11-05 00:50:06,099 - INFO - [diffusion][Epoch 6407] diffusion training Loss: 0.05595164280384779
2024-11-05 00:50:06,102 - INFO - [diffusion][Epoch 6407] diffusion learning rate: 0.001
2024-11-05 00:50:06,103 - INFO - [diffusion][Epoch 6407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:06,105 - INFO - [diffusion][Epoch 6408] Epoch 6409/12000
2024-11-05 00:50:10,300 - INFO - [diffusion][Epoch 6408] diffusion training Loss: 0.06484570167958736
2024-11-05 00:50:10,302 - INFO - [diffusion][Epoch 6408] diffusion learning rate: 0.001
2024-11-05 00:50:10,304 - INFO - [diffusion][Epoch 6408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:10,305 - INFO - [diffusion][Epoch 6409] Epoch 6410/12000
2024-11-05 00:50:14,430 - INFO - [diffusion][Epoch 6409] diffusion training Loss: 0.059783692471683025
2024-11-05 00:50:14,432 - INFO - [diffusion][Epoch 6409] diffusion learning rate: 0.001
2024-11-05 00:50:14,434 - INFO - [diffusion][Epoch 6409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:14,436 - INFO - [diffusion][Epoch 6410] Epoch 6411/12000
2024-11-05 00:50:18,521 - INFO - [diffusion][Epoch 6410] diffusion training Loss: 0.06005294155329466
2024-11-05 00:50:18,523 - INFO - [diffusion][Epoch 6410] diffusion learning rate: 0.001
2024-11-05 00:50:18,525 - INFO - [diffusion][Epoch 6410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:18,526 - INFO - [diffusion][Epoch 6411] Epoch 6412/12000
2024-11-05 00:50:22,683 - INFO - [diffusion][Epoch 6411] diffusion training Loss: 0.06704458687454462
2024-11-05 00:50:22,685 - INFO - [diffusion][Epoch 6411] diffusion learning rate: 0.001
2024-11-05 00:50:22,688 - INFO - [diffusion][Epoch 6411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:22,689 - INFO - [diffusion][Epoch 6412] Epoch 6413/12000
2024-11-05 00:50:26,974 - INFO - [diffusion][Epoch 6412] diffusion training Loss: 0.05830797739326954
2024-11-05 00:50:26,976 - INFO - [diffusion][Epoch 6412] diffusion learning rate: 0.001
2024-11-05 00:50:26,977 - INFO - [diffusion][Epoch 6412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:26,979 - INFO - [diffusion][Epoch 6413] Epoch 6414/12000
2024-11-05 00:50:31,167 - INFO - [diffusion][Epoch 6413] diffusion training Loss: 0.05590245593339205
2024-11-05 00:50:31,169 - INFO - [diffusion][Epoch 6413] diffusion learning rate: 0.001
2024-11-05 00:50:31,171 - INFO - [diffusion][Epoch 6413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:31,172 - INFO - [diffusion][Epoch 6414] Epoch 6415/12000
2024-11-05 00:50:35,273 - INFO - [diffusion][Epoch 6414] diffusion training Loss: 0.05865672789514065
2024-11-05 00:50:35,276 - INFO - [diffusion][Epoch 6414] diffusion learning rate: 0.001
2024-11-05 00:50:35,323 - INFO - [diffusion][Epoch 6414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:35,325 - INFO - [diffusion][Epoch 6415] Epoch 6416/12000
2024-11-05 00:50:39,448 - INFO - [diffusion][Epoch 6415] diffusion training Loss: 0.05632976908236742
2024-11-05 00:50:39,451 - INFO - [diffusion][Epoch 6415] diffusion learning rate: 0.001
2024-11-05 00:50:39,452 - INFO - [diffusion][Epoch 6415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:39,454 - INFO - [diffusion][Epoch 6416] Epoch 6417/12000
2024-11-05 00:50:43,600 - INFO - [diffusion][Epoch 6416] diffusion training Loss: 0.05957658123224974
2024-11-05 00:50:43,603 - INFO - [diffusion][Epoch 6416] diffusion learning rate: 0.001
2024-11-05 00:50:43,611 - INFO - [diffusion][Epoch 6416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:43,612 - INFO - [diffusion][Epoch 6417] Epoch 6418/12000
2024-11-05 00:50:47,755 - INFO - [diffusion][Epoch 6417] diffusion training Loss: 0.0593706090003252
2024-11-05 00:50:47,759 - INFO - [diffusion][Epoch 6417] diffusion learning rate: 0.001
2024-11-05 00:50:47,761 - INFO - [diffusion][Epoch 6417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:47,762 - INFO - [diffusion][Epoch 6418] Epoch 6419/12000
2024-11-05 00:50:51,989 - INFO - [diffusion][Epoch 6418] diffusion training Loss: 0.06081755645573139
2024-11-05 00:50:51,991 - INFO - [diffusion][Epoch 6418] diffusion learning rate: 0.001
2024-11-05 00:50:51,993 - INFO - [diffusion][Epoch 6418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:51,994 - INFO - [diffusion][Epoch 6419] Epoch 6420/12000
2024-11-05 00:50:56,168 - INFO - [diffusion][Epoch 6419] diffusion training Loss: 0.06303817871958017
2024-11-05 00:50:56,170 - INFO - [diffusion][Epoch 6419] diffusion learning rate: 0.001
2024-11-05 00:50:56,172 - INFO - [diffusion][Epoch 6419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:50:56,173 - INFO - [diffusion][Epoch 6420] Epoch 6421/12000
2024-11-05 00:51:00,333 - INFO - [diffusion][Epoch 6420] diffusion training Loss: 0.06142898928374052
2024-11-05 00:51:00,335 - INFO - [diffusion][Epoch 6420] diffusion learning rate: 0.001
2024-11-05 00:51:00,337 - INFO - [diffusion][Epoch 6420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:00,338 - INFO - [diffusion][Epoch 6421] Epoch 6422/12000
2024-11-05 00:51:04,492 - INFO - [diffusion][Epoch 6421] diffusion training Loss: 0.05698935780674219
2024-11-05 00:51:04,494 - INFO - [diffusion][Epoch 6421] diffusion learning rate: 0.001
2024-11-05 00:51:04,496 - INFO - [diffusion][Epoch 6421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:04,497 - INFO - [diffusion][Epoch 6422] Epoch 6423/12000
2024-11-05 00:51:08,633 - INFO - [diffusion][Epoch 6422] diffusion training Loss: 0.05882174056023359
2024-11-05 00:51:08,635 - INFO - [diffusion][Epoch 6422] diffusion learning rate: 0.001
2024-11-05 00:51:08,657 - INFO - [diffusion][Epoch 6422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:08,658 - INFO - [diffusion][Epoch 6423] Epoch 6424/12000
2024-11-05 00:51:12,763 - INFO - [diffusion][Epoch 6423] diffusion training Loss: 0.05581415258347988
2024-11-05 00:51:12,765 - INFO - [diffusion][Epoch 6423] diffusion learning rate: 0.001
2024-11-05 00:51:12,767 - INFO - [diffusion][Epoch 6423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:12,768 - INFO - [diffusion][Epoch 6424] Epoch 6425/12000
2024-11-05 00:51:16,907 - INFO - [diffusion][Epoch 6424] diffusion training Loss: 0.05684154573827982
2024-11-05 00:51:16,909 - INFO - [diffusion][Epoch 6424] diffusion learning rate: 0.001
2024-11-05 00:51:16,911 - INFO - [diffusion][Epoch 6424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:16,913 - INFO - [diffusion][Epoch 6425] Epoch 6426/12000
2024-11-05 00:51:21,084 - INFO - [diffusion][Epoch 6425] diffusion training Loss: 0.05633919592946768
2024-11-05 00:51:21,086 - INFO - [diffusion][Epoch 6425] diffusion learning rate: 0.001
2024-11-05 00:51:21,087 - INFO - [diffusion][Epoch 6425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:21,089 - INFO - [diffusion][Epoch 6426] Epoch 6427/12000
2024-11-05 00:51:25,264 - INFO - [diffusion][Epoch 6426] diffusion training Loss: 0.06023528426885605
2024-11-05 00:51:25,267 - INFO - [diffusion][Epoch 6426] diffusion learning rate: 0.001
2024-11-05 00:51:25,289 - INFO - [diffusion][Epoch 6426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:25,290 - INFO - [diffusion][Epoch 6427] Epoch 6428/12000
2024-11-05 00:51:29,471 - INFO - [diffusion][Epoch 6427] diffusion training Loss: 0.05633056629449129
2024-11-05 00:51:29,473 - INFO - [diffusion][Epoch 6427] diffusion learning rate: 0.001
2024-11-05 00:51:29,475 - INFO - [diffusion][Epoch 6427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:29,476 - INFO - [diffusion][Epoch 6428] Epoch 6429/12000
2024-11-05 00:51:33,690 - INFO - [diffusion][Epoch 6428] diffusion training Loss: 0.05784592032432556
2024-11-05 00:51:33,693 - INFO - [diffusion][Epoch 6428] diffusion learning rate: 0.001
2024-11-05 00:51:33,695 - INFO - [diffusion][Epoch 6428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:33,697 - INFO - [diffusion][Epoch 6429] Epoch 6430/12000
2024-11-05 00:51:37,748 - INFO - [diffusion][Epoch 6429] diffusion training Loss: 0.06146004889160395
2024-11-05 00:51:37,750 - INFO - [diffusion][Epoch 6429] diffusion learning rate: 0.001
2024-11-05 00:51:37,752 - INFO - [diffusion][Epoch 6429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:37,753 - INFO - [diffusion][Epoch 6430] Epoch 6431/12000
2024-11-05 00:51:41,918 - INFO - [diffusion][Epoch 6430] diffusion training Loss: 0.05801162403076887
2024-11-05 00:51:41,920 - INFO - [diffusion][Epoch 6430] diffusion learning rate: 0.001
2024-11-05 00:51:41,962 - INFO - [diffusion][Epoch 6430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:41,963 - INFO - [diffusion][Epoch 6431] Epoch 6432/12000
2024-11-05 00:51:45,975 - INFO - [diffusion][Epoch 6431] diffusion training Loss: 0.06403712835162878
2024-11-05 00:51:45,977 - INFO - [diffusion][Epoch 6431] diffusion learning rate: 0.001
2024-11-05 00:51:45,979 - INFO - [diffusion][Epoch 6431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:45,980 - INFO - [diffusion][Epoch 6432] Epoch 6433/12000
2024-11-05 00:51:50,104 - INFO - [diffusion][Epoch 6432] diffusion training Loss: 0.057286716997623444
2024-11-05 00:51:50,106 - INFO - [diffusion][Epoch 6432] diffusion learning rate: 0.001
2024-11-05 00:51:50,108 - INFO - [diffusion][Epoch 6432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:50,110 - INFO - [diffusion][Epoch 6433] Epoch 6434/12000
2024-11-05 00:51:54,912 - INFO - [diffusion][Epoch 6433] diffusion training Loss: 0.058130824007093906
2024-11-05 00:51:54,914 - INFO - [diffusion][Epoch 6433] diffusion learning rate: 0.001
2024-11-05 00:51:54,916 - INFO - [diffusion][Epoch 6433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:54,917 - INFO - [diffusion][Epoch 6434] Epoch 6435/12000
2024-11-05 00:51:59,036 - INFO - [diffusion][Epoch 6434] diffusion training Loss: 0.06376627832651138
2024-11-05 00:51:59,039 - INFO - [diffusion][Epoch 6434] diffusion learning rate: 0.001
2024-11-05 00:51:59,040 - INFO - [diffusion][Epoch 6434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:51:59,042 - INFO - [diffusion][Epoch 6435] Epoch 6436/12000
2024-11-05 00:52:02,964 - INFO - [diffusion][Epoch 6435] diffusion training Loss: 0.06295214593410492
2024-11-05 00:52:02,966 - INFO - [diffusion][Epoch 6435] diffusion learning rate: 0.001
2024-11-05 00:52:02,968 - INFO - [diffusion][Epoch 6435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:02,969 - INFO - [diffusion][Epoch 6436] Epoch 6437/12000
2024-11-05 00:52:07,173 - INFO - [diffusion][Epoch 6436] diffusion training Loss: 0.05912470631301403
2024-11-05 00:52:07,175 - INFO - [diffusion][Epoch 6436] diffusion learning rate: 0.001
2024-11-05 00:52:07,177 - INFO - [diffusion][Epoch 6436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:07,178 - INFO - [diffusion][Epoch 6437] Epoch 6438/12000
2024-11-05 00:52:11,138 - INFO - [diffusion][Epoch 6437] diffusion training Loss: 0.062332313507795334
2024-11-05 00:52:11,312 - INFO - [diffusion][Epoch 6437] diffusion learning rate: 0.001
2024-11-05 00:52:11,314 - INFO - [diffusion][Epoch 6437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:11,316 - INFO - [diffusion][Epoch 6438] Epoch 6439/12000
2024-11-05 00:52:15,387 - INFO - [diffusion][Epoch 6438] diffusion training Loss: 0.06299484614282846
2024-11-05 00:52:15,390 - INFO - [diffusion][Epoch 6438] diffusion learning rate: 0.001
2024-11-05 00:52:15,392 - INFO - [diffusion][Epoch 6438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:15,393 - INFO - [diffusion][Epoch 6439] Epoch 6440/12000
2024-11-05 00:52:19,515 - INFO - [diffusion][Epoch 6439] diffusion training Loss: 0.062415262684226036
2024-11-05 00:52:19,517 - INFO - [diffusion][Epoch 6439] diffusion learning rate: 0.001
2024-11-05 00:52:19,519 - INFO - [diffusion][Epoch 6439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:19,521 - INFO - [diffusion][Epoch 6440] Epoch 6441/12000
2024-11-05 00:52:23,609 - INFO - [diffusion][Epoch 6440] diffusion training Loss: 0.059958988800644875
2024-11-05 00:52:23,611 - INFO - [diffusion][Epoch 6440] diffusion learning rate: 0.001
2024-11-05 00:52:23,613 - INFO - [diffusion][Epoch 6440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:23,614 - INFO - [diffusion][Epoch 6441] Epoch 6442/12000
2024-11-05 00:52:27,780 - INFO - [diffusion][Epoch 6441] diffusion training Loss: 0.06062047090381384
2024-11-05 00:52:27,782 - INFO - [diffusion][Epoch 6441] diffusion learning rate: 0.001
2024-11-05 00:52:27,834 - INFO - [diffusion][Epoch 6441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:27,835 - INFO - [diffusion][Epoch 6442] Epoch 6443/12000
2024-11-05 00:52:31,940 - INFO - [diffusion][Epoch 6442] diffusion training Loss: 0.05853594094514847
2024-11-05 00:52:31,942 - INFO - [diffusion][Epoch 6442] diffusion learning rate: 0.001
2024-11-05 00:52:31,945 - INFO - [diffusion][Epoch 6442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:31,947 - INFO - [diffusion][Epoch 6443] Epoch 6444/12000
2024-11-05 00:52:35,930 - INFO - [diffusion][Epoch 6443] diffusion training Loss: 0.06015203054994345
2024-11-05 00:52:35,932 - INFO - [diffusion][Epoch 6443] diffusion learning rate: 0.001
2024-11-05 00:52:35,934 - INFO - [diffusion][Epoch 6443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:35,935 - INFO - [diffusion][Epoch 6444] Epoch 6445/12000
2024-11-05 00:52:40,038 - INFO - [diffusion][Epoch 6444] diffusion training Loss: 0.056438843719661236
2024-11-05 00:52:40,040 - INFO - [diffusion][Epoch 6444] diffusion learning rate: 0.001
2024-11-05 00:52:40,042 - INFO - [diffusion][Epoch 6444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:40,043 - INFO - [diffusion][Epoch 6445] Epoch 6446/12000
2024-11-05 00:52:44,071 - INFO - [diffusion][Epoch 6445] diffusion training Loss: 0.06615972146391869
2024-11-05 00:52:44,073 - INFO - [diffusion][Epoch 6445] diffusion learning rate: 0.001
2024-11-05 00:52:44,075 - INFO - [diffusion][Epoch 6445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:44,076 - INFO - [diffusion][Epoch 6446] Epoch 6447/12000
2024-11-05 00:52:48,111 - INFO - [diffusion][Epoch 6446] diffusion training Loss: 0.06272499449551105
2024-11-05 00:52:48,113 - INFO - [diffusion][Epoch 6446] diffusion learning rate: 0.001
2024-11-05 00:52:48,115 - INFO - [diffusion][Epoch 6446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:48,116 - INFO - [diffusion][Epoch 6447] Epoch 6448/12000
2024-11-05 00:52:52,249 - INFO - [diffusion][Epoch 6447] diffusion training Loss: 0.055954329669475555
2024-11-05 00:52:52,252 - INFO - [diffusion][Epoch 6447] diffusion learning rate: 0.001
2024-11-05 00:52:52,254 - INFO - [diffusion][Epoch 6447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:52,255 - INFO - [diffusion][Epoch 6448] Epoch 6449/12000
2024-11-05 00:52:56,354 - INFO - [diffusion][Epoch 6448] diffusion training Loss: 0.05931201670318842
2024-11-05 00:52:56,357 - INFO - [diffusion][Epoch 6448] diffusion learning rate: 0.001
2024-11-05 00:52:56,358 - INFO - [diffusion][Epoch 6448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:52:56,360 - INFO - [diffusion][Epoch 6449] Epoch 6450/12000
2024-11-05 00:53:00,442 - INFO - [diffusion][Epoch 6449] diffusion training Loss: 0.05295402277261019
2024-11-05 00:53:00,444 - INFO - [diffusion][Epoch 6449] diffusion learning rate: 0.001
2024-11-05 00:53:00,446 - INFO - [diffusion][Epoch 6449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:00,447 - INFO - [diffusion][Epoch 6450] Epoch 6451/12000
2024-11-05 00:53:04,620 - INFO - [diffusion][Epoch 6450] diffusion training Loss: 0.0577007457613945
2024-11-05 00:53:04,622 - INFO - [diffusion][Epoch 6450] diffusion learning rate: 0.001
2024-11-05 00:53:04,624 - INFO - [diffusion][Epoch 6450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:04,626 - INFO - [diffusion][Epoch 6451] Epoch 6452/12000
2024-11-05 00:53:08,762 - INFO - [diffusion][Epoch 6451] diffusion training Loss: 0.057059936225414276
2024-11-05 00:53:08,764 - INFO - [diffusion][Epoch 6451] diffusion learning rate: 0.001
2024-11-05 00:53:08,766 - INFO - [diffusion][Epoch 6451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:08,768 - INFO - [diffusion][Epoch 6452] Epoch 6453/12000
2024-11-05 00:53:12,870 - INFO - [diffusion][Epoch 6452] diffusion training Loss: 0.05526639148592949
2024-11-05 00:53:12,872 - INFO - [diffusion][Epoch 6452] diffusion learning rate: 0.001
2024-11-05 00:53:12,874 - INFO - [diffusion][Epoch 6452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:12,876 - INFO - [diffusion][Epoch 6453] Epoch 6454/12000
2024-11-05 00:53:17,110 - INFO - [diffusion][Epoch 6453] diffusion training Loss: 0.06572014093399048
2024-11-05 00:53:17,112 - INFO - [diffusion][Epoch 6453] diffusion learning rate: 0.001
2024-11-05 00:53:17,113 - INFO - [diffusion][Epoch 6453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:17,115 - INFO - [diffusion][Epoch 6454] Epoch 6455/12000
2024-11-05 00:53:21,227 - INFO - [diffusion][Epoch 6454] diffusion training Loss: 0.06030621565878391
2024-11-05 00:53:21,230 - INFO - [diffusion][Epoch 6454] diffusion learning rate: 0.001
2024-11-05 00:53:21,232 - INFO - [diffusion][Epoch 6454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:21,233 - INFO - [diffusion][Epoch 6455] Epoch 6456/12000
2024-11-05 00:53:25,294 - INFO - [diffusion][Epoch 6455] diffusion training Loss: 0.061027891933918
2024-11-05 00:53:25,296 - INFO - [diffusion][Epoch 6455] diffusion learning rate: 0.001
2024-11-05 00:53:25,298 - INFO - [diffusion][Epoch 6455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:25,299 - INFO - [diffusion][Epoch 6456] Epoch 6457/12000
2024-11-05 00:53:29,274 - INFO - [diffusion][Epoch 6456] diffusion training Loss: 0.05373658053576946
2024-11-05 00:53:29,276 - INFO - [diffusion][Epoch 6456] diffusion learning rate: 0.001
2024-11-05 00:53:29,278 - INFO - [diffusion][Epoch 6456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:29,279 - INFO - [diffusion][Epoch 6457] Epoch 6458/12000
2024-11-05 00:53:33,347 - INFO - [diffusion][Epoch 6457] diffusion training Loss: 0.06481578480452299
2024-11-05 00:53:33,349 - INFO - [diffusion][Epoch 6457] diffusion learning rate: 0.001
2024-11-05 00:53:33,351 - INFO - [diffusion][Epoch 6457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:33,353 - INFO - [diffusion][Epoch 6458] Epoch 6459/12000
2024-11-05 00:53:37,394 - INFO - [diffusion][Epoch 6458] diffusion training Loss: 0.06395402550697327
2024-11-05 00:53:37,396 - INFO - [diffusion][Epoch 6458] diffusion learning rate: 0.001
2024-11-05 00:53:37,398 - INFO - [diffusion][Epoch 6458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:37,399 - INFO - [diffusion][Epoch 6459] Epoch 6460/12000
2024-11-05 00:53:41,587 - INFO - [diffusion][Epoch 6459] diffusion training Loss: 0.06189853511750698
2024-11-05 00:53:41,589 - INFO - [diffusion][Epoch 6459] diffusion learning rate: 0.001
2024-11-05 00:53:41,591 - INFO - [diffusion][Epoch 6459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:41,593 - INFO - [diffusion][Epoch 6460] Epoch 6461/12000
2024-11-05 00:53:45,677 - INFO - [diffusion][Epoch 6460] diffusion training Loss: 0.05944367125630379
2024-11-05 00:53:45,679 - INFO - [diffusion][Epoch 6460] diffusion learning rate: 0.001
2024-11-05 00:53:45,681 - INFO - [diffusion][Epoch 6460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:45,682 - INFO - [diffusion][Epoch 6461] Epoch 6462/12000
2024-11-05 00:53:49,853 - INFO - [diffusion][Epoch 6461] diffusion training Loss: 0.05727007519453764
2024-11-05 00:53:49,855 - INFO - [diffusion][Epoch 6461] diffusion learning rate: 0.001
2024-11-05 00:53:49,857 - INFO - [diffusion][Epoch 6461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:49,859 - INFO - [diffusion][Epoch 6462] Epoch 6463/12000
2024-11-05 00:53:54,006 - INFO - [diffusion][Epoch 6462] diffusion training Loss: 0.05753005389124155
2024-11-05 00:53:54,009 - INFO - [diffusion][Epoch 6462] diffusion learning rate: 0.001
2024-11-05 00:53:54,011 - INFO - [diffusion][Epoch 6462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:54,013 - INFO - [diffusion][Epoch 6463] Epoch 6464/12000
2024-11-05 00:53:58,134 - INFO - [diffusion][Epoch 6463] diffusion training Loss: 0.056240906938910484
2024-11-05 00:53:58,136 - INFO - [diffusion][Epoch 6463] diffusion learning rate: 0.001
2024-11-05 00:53:58,138 - INFO - [diffusion][Epoch 6463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:53:58,139 - INFO - [diffusion][Epoch 6464] Epoch 6465/12000
2024-11-05 00:54:02,247 - INFO - [diffusion][Epoch 6464] diffusion training Loss: 0.05852237995713949
2024-11-05 00:54:02,249 - INFO - [diffusion][Epoch 6464] diffusion learning rate: 0.001
2024-11-05 00:54:02,251 - INFO - [diffusion][Epoch 6464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:02,253 - INFO - [diffusion][Epoch 6465] Epoch 6466/12000
2024-11-05 00:54:06,249 - INFO - [diffusion][Epoch 6465] diffusion training Loss: 0.05841873586177826
2024-11-05 00:54:06,251 - INFO - [diffusion][Epoch 6465] diffusion learning rate: 0.001
2024-11-05 00:54:06,253 - INFO - [diffusion][Epoch 6465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:06,254 - INFO - [diffusion][Epoch 6466] Epoch 6467/12000
2024-11-05 00:54:10,385 - INFO - [diffusion][Epoch 6466] diffusion training Loss: 0.05787067301571369
2024-11-05 00:54:10,406 - INFO - [diffusion][Epoch 6466] diffusion learning rate: 0.001
2024-11-05 00:54:10,408 - INFO - [diffusion][Epoch 6466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:10,409 - INFO - [diffusion][Epoch 6467] Epoch 6468/12000
2024-11-05 00:54:14,533 - INFO - [diffusion][Epoch 6467] diffusion training Loss: 0.05893201660364866
2024-11-05 00:54:14,535 - INFO - [diffusion][Epoch 6467] diffusion learning rate: 0.001
2024-11-05 00:54:14,537 - INFO - [diffusion][Epoch 6467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:14,538 - INFO - [diffusion][Epoch 6468] Epoch 6469/12000
2024-11-05 00:54:18,557 - INFO - [diffusion][Epoch 6468] diffusion training Loss: 0.06177189573645592
2024-11-05 00:54:18,559 - INFO - [diffusion][Epoch 6468] diffusion learning rate: 0.001
2024-11-05 00:54:18,561 - INFO - [diffusion][Epoch 6468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:18,562 - INFO - [diffusion][Epoch 6469] Epoch 6470/12000
2024-11-05 00:54:22,552 - INFO - [diffusion][Epoch 6469] diffusion training Loss: 0.05705893412232399
2024-11-05 00:54:22,554 - INFO - [diffusion][Epoch 6469] diffusion learning rate: 0.001
2024-11-05 00:54:22,555 - INFO - [diffusion][Epoch 6469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:22,557 - INFO - [diffusion][Epoch 6470] Epoch 6471/12000
2024-11-05 00:54:26,539 - INFO - [diffusion][Epoch 6470] diffusion training Loss: 0.06195382308214903
2024-11-05 00:54:26,540 - INFO - [diffusion][Epoch 6470] diffusion learning rate: 0.001
2024-11-05 00:54:26,542 - INFO - [diffusion][Epoch 6470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:26,543 - INFO - [diffusion][Epoch 6471] Epoch 6472/12000
2024-11-05 00:54:30,660 - INFO - [diffusion][Epoch 6471] diffusion training Loss: 0.0588292246684432
2024-11-05 00:54:30,662 - INFO - [diffusion][Epoch 6471] diffusion learning rate: 0.001
2024-11-05 00:54:30,664 - INFO - [diffusion][Epoch 6471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:30,665 - INFO - [diffusion][Epoch 6472] Epoch 6473/12000
2024-11-05 00:54:34,694 - INFO - [diffusion][Epoch 6472] diffusion training Loss: 0.055068908259272575
2024-11-05 00:54:34,696 - INFO - [diffusion][Epoch 6472] diffusion learning rate: 0.001
2024-11-05 00:54:34,698 - INFO - [diffusion][Epoch 6472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:34,699 - INFO - [diffusion][Epoch 6473] Epoch 6474/12000
2024-11-05 00:54:38,841 - INFO - [diffusion][Epoch 6473] diffusion training Loss: 0.05786116048693657
2024-11-05 00:54:38,843 - INFO - [diffusion][Epoch 6473] diffusion learning rate: 0.001
2024-11-05 00:54:38,844 - INFO - [diffusion][Epoch 6473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:38,846 - INFO - [diffusion][Epoch 6474] Epoch 6475/12000
2024-11-05 00:54:43,565 - INFO - [diffusion][Epoch 6474] diffusion training Loss: 0.05868186242878437
2024-11-05 00:54:43,567 - INFO - [diffusion][Epoch 6474] diffusion learning rate: 0.001
2024-11-05 00:54:43,569 - INFO - [diffusion][Epoch 6474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:43,570 - INFO - [diffusion][Epoch 6475] Epoch 6476/12000
2024-11-05 00:54:47,715 - INFO - [diffusion][Epoch 6475] diffusion training Loss: 0.06517444364726543
2024-11-05 00:54:47,717 - INFO - [diffusion][Epoch 6475] diffusion learning rate: 0.001
2024-11-05 00:54:47,719 - INFO - [diffusion][Epoch 6475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:47,720 - INFO - [diffusion][Epoch 6476] Epoch 6477/12000
2024-11-05 00:54:51,815 - INFO - [diffusion][Epoch 6476] diffusion training Loss: 0.06197759509086609
2024-11-05 00:54:51,817 - INFO - [diffusion][Epoch 6476] diffusion learning rate: 0.001
2024-11-05 00:54:51,819 - INFO - [diffusion][Epoch 6476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:51,820 - INFO - [diffusion][Epoch 6477] Epoch 6478/12000
2024-11-05 00:54:55,970 - INFO - [diffusion][Epoch 6477] diffusion training Loss: 0.0574296023696661
2024-11-05 00:54:55,972 - INFO - [diffusion][Epoch 6477] diffusion learning rate: 0.001
2024-11-05 00:54:55,974 - INFO - [diffusion][Epoch 6477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:54:55,975 - INFO - [diffusion][Epoch 6478] Epoch 6479/12000
2024-11-05 00:55:00,093 - INFO - [diffusion][Epoch 6478] diffusion training Loss: 0.06219165585935116
2024-11-05 00:55:00,095 - INFO - [diffusion][Epoch 6478] diffusion learning rate: 0.001
2024-11-05 00:55:00,097 - INFO - [diffusion][Epoch 6478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:00,099 - INFO - [diffusion][Epoch 6479] Epoch 6480/12000
2024-11-05 00:55:04,257 - INFO - [diffusion][Epoch 6479] diffusion training Loss: 0.06060750596225262
2024-11-05 00:55:04,260 - INFO - [diffusion][Epoch 6479] diffusion learning rate: 0.001
2024-11-05 00:55:04,262 - INFO - [diffusion][Epoch 6479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:04,263 - INFO - [diffusion][Epoch 6480] Epoch 6481/12000
2024-11-05 00:55:08,374 - INFO - [diffusion][Epoch 6480] diffusion training Loss: 0.0544604891911149
2024-11-05 00:55:08,376 - INFO - [diffusion][Epoch 6480] diffusion learning rate: 0.001
2024-11-05 00:55:08,378 - INFO - [diffusion][Epoch 6480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:08,379 - INFO - [diffusion][Epoch 6481] Epoch 6482/12000
2024-11-05 00:55:12,426 - INFO - [diffusion][Epoch 6481] diffusion training Loss: 0.06286525167524815
2024-11-05 00:55:12,428 - INFO - [diffusion][Epoch 6481] diffusion learning rate: 0.001
2024-11-05 00:55:12,430 - INFO - [diffusion][Epoch 6481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:12,431 - INFO - [diffusion][Epoch 6482] Epoch 6483/12000
2024-11-05 00:55:16,467 - INFO - [diffusion][Epoch 6482] diffusion training Loss: 0.061719510704278946
2024-11-05 00:55:16,469 - INFO - [diffusion][Epoch 6482] diffusion learning rate: 0.001
2024-11-05 00:55:16,471 - INFO - [diffusion][Epoch 6482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:16,472 - INFO - [diffusion][Epoch 6483] Epoch 6484/12000
2024-11-05 00:55:20,610 - INFO - [diffusion][Epoch 6483] diffusion training Loss: 0.055202750489115715
2024-11-05 00:55:20,612 - INFO - [diffusion][Epoch 6483] diffusion learning rate: 0.001
2024-11-05 00:55:20,614 - INFO - [diffusion][Epoch 6483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:20,615 - INFO - [diffusion][Epoch 6484] Epoch 6485/12000
2024-11-05 00:55:24,757 - INFO - [diffusion][Epoch 6484] diffusion training Loss: 0.06107937823981047
2024-11-05 00:55:24,759 - INFO - [diffusion][Epoch 6484] diffusion learning rate: 0.001
2024-11-05 00:55:24,760 - INFO - [diffusion][Epoch 6484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:24,764 - INFO - [diffusion][Epoch 6485] Epoch 6486/12000
2024-11-05 00:55:28,833 - INFO - [diffusion][Epoch 6485] diffusion training Loss: 0.056196773424744606
2024-11-05 00:55:28,835 - INFO - [diffusion][Epoch 6485] diffusion learning rate: 0.001
2024-11-05 00:55:28,839 - INFO - [diffusion][Epoch 6485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:28,840 - INFO - [diffusion][Epoch 6486] Epoch 6487/12000
2024-11-05 00:55:32,941 - INFO - [diffusion][Epoch 6486] diffusion training Loss: 0.056242652237415314
2024-11-05 00:55:32,943 - INFO - [diffusion][Epoch 6486] diffusion learning rate: 0.001
2024-11-05 00:55:32,945 - INFO - [diffusion][Epoch 6486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:32,946 - INFO - [diffusion][Epoch 6487] Epoch 6488/12000
2024-11-05 00:55:36,962 - INFO - [diffusion][Epoch 6487] diffusion training Loss: 0.0659445058554411
2024-11-05 00:55:36,964 - INFO - [diffusion][Epoch 6487] diffusion learning rate: 0.001
2024-11-05 00:55:36,966 - INFO - [diffusion][Epoch 6487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:36,967 - INFO - [diffusion][Epoch 6488] Epoch 6489/12000
2024-11-05 00:55:41,116 - INFO - [diffusion][Epoch 6488] diffusion training Loss: 0.05919950921088457
2024-11-05 00:55:41,118 - INFO - [diffusion][Epoch 6488] diffusion learning rate: 0.001
2024-11-05 00:55:41,119 - INFO - [diffusion][Epoch 6488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:41,121 - INFO - [diffusion][Epoch 6489] Epoch 6490/12000
2024-11-05 00:55:45,294 - INFO - [diffusion][Epoch 6489] diffusion training Loss: 0.062055748887360096
2024-11-05 00:55:45,297 - INFO - [diffusion][Epoch 6489] diffusion learning rate: 0.001
2024-11-05 00:55:45,299 - INFO - [diffusion][Epoch 6489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:45,300 - INFO - [diffusion][Epoch 6490] Epoch 6491/12000
2024-11-05 00:55:49,417 - INFO - [diffusion][Epoch 6490] diffusion training Loss: 0.05750620551407337
2024-11-05 00:55:49,419 - INFO - [diffusion][Epoch 6490] diffusion learning rate: 0.001
2024-11-05 00:55:49,421 - INFO - [diffusion][Epoch 6490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:49,422 - INFO - [diffusion][Epoch 6491] Epoch 6492/12000
2024-11-05 00:55:53,535 - INFO - [diffusion][Epoch 6491] diffusion training Loss: 0.058722635731101036
2024-11-05 00:55:53,537 - INFO - [diffusion][Epoch 6491] diffusion learning rate: 0.001
2024-11-05 00:55:53,538 - INFO - [diffusion][Epoch 6491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:53,539 - INFO - [diffusion][Epoch 6492] Epoch 6493/12000
2024-11-05 00:55:57,702 - INFO - [diffusion][Epoch 6492] diffusion training Loss: 0.058308602310717106
2024-11-05 00:55:57,706 - INFO - [diffusion][Epoch 6492] diffusion learning rate: 0.001
2024-11-05 00:55:57,707 - INFO - [diffusion][Epoch 6492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:55:57,709 - INFO - [diffusion][Epoch 6493] Epoch 6494/12000
2024-11-05 00:56:01,776 - INFO - [diffusion][Epoch 6493] diffusion training Loss: 0.06065608840435743
2024-11-05 00:56:01,777 - INFO - [diffusion][Epoch 6493] diffusion learning rate: 0.001
2024-11-05 00:56:01,779 - INFO - [diffusion][Epoch 6493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:01,780 - INFO - [diffusion][Epoch 6494] Epoch 6495/12000
2024-11-05 00:56:06,126 - INFO - [diffusion][Epoch 6494] diffusion training Loss: 0.05942499078810215
2024-11-05 00:56:06,128 - INFO - [diffusion][Epoch 6494] diffusion learning rate: 0.001
2024-11-05 00:56:06,131 - INFO - [diffusion][Epoch 6494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:06,132 - INFO - [diffusion][Epoch 6495] Epoch 6496/12000
2024-11-05 00:56:10,323 - INFO - [diffusion][Epoch 6495] diffusion training Loss: 0.05996992066502571
2024-11-05 00:56:10,325 - INFO - [diffusion][Epoch 6495] diffusion learning rate: 0.001
2024-11-05 00:56:10,327 - INFO - [diffusion][Epoch 6495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:10,329 - INFO - [diffusion][Epoch 6496] Epoch 6497/12000
2024-11-05 00:56:14,503 - INFO - [diffusion][Epoch 6496] diffusion training Loss: 0.06173621490597725
2024-11-05 00:56:14,505 - INFO - [diffusion][Epoch 6496] diffusion learning rate: 0.001
2024-11-05 00:56:14,507 - INFO - [diffusion][Epoch 6496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:14,509 - INFO - [diffusion][Epoch 6497] Epoch 6498/12000
2024-11-05 00:56:18,678 - INFO - [diffusion][Epoch 6497] diffusion training Loss: 0.05214711558073759
2024-11-05 00:56:18,680 - INFO - [diffusion][Epoch 6497] diffusion learning rate: 0.001
2024-11-05 00:56:18,682 - INFO - [diffusion][Epoch 6497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:18,683 - INFO - [diffusion][Epoch 6498] Epoch 6499/12000
2024-11-05 00:56:22,763 - INFO - [diffusion][Epoch 6498] diffusion training Loss: 0.05801647063344717
2024-11-05 00:56:22,765 - INFO - [diffusion][Epoch 6498] diffusion learning rate: 0.001
2024-11-05 00:56:22,767 - INFO - [diffusion][Epoch 6498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:22,768 - INFO - [diffusion][Epoch 6499] Epoch 6500/12000
2024-11-05 00:56:26,906 - INFO - [diffusion][Epoch 6499] diffusion training Loss: 0.060074981302022934
2024-11-05 00:56:26,908 - INFO - [diffusion][Epoch 6499] diffusion learning rate: 0.001
2024-11-05 00:56:26,910 - INFO - [diffusion][Epoch 6499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:26,911 - INFO - [diffusion][Epoch 6500] Epoch 6501/12000
2024-11-05 00:56:31,016 - INFO - [diffusion][Epoch 6500] diffusion training Loss: 0.060706643387675285
2024-11-05 00:56:31,018 - INFO - [diffusion][Epoch 6500] diffusion learning rate: 0.001
2024-11-05 00:56:31,020 - INFO - [diffusion][Epoch 6500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:31,021 - INFO - [diffusion][Epoch 6501] Epoch 6502/12000
2024-11-05 00:56:35,024 - INFO - [diffusion][Epoch 6501] diffusion training Loss: 0.057385231368243694
2024-11-05 00:56:35,026 - INFO - [diffusion][Epoch 6501] diffusion learning rate: 0.001
2024-11-05 00:56:35,046 - INFO - [diffusion][Epoch 6501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:35,048 - INFO - [diffusion][Epoch 6502] Epoch 6503/12000
2024-11-05 00:56:39,187 - INFO - [diffusion][Epoch 6502] diffusion training Loss: 0.0625549191609025
2024-11-05 00:56:39,189 - INFO - [diffusion][Epoch 6502] diffusion learning rate: 0.001
2024-11-05 00:56:39,191 - INFO - [diffusion][Epoch 6502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:39,192 - INFO - [diffusion][Epoch 6503] Epoch 6504/12000
2024-11-05 00:56:43,293 - INFO - [diffusion][Epoch 6503] diffusion training Loss: 0.05482533946633339
2024-11-05 00:56:43,295 - INFO - [diffusion][Epoch 6503] diffusion learning rate: 0.001
2024-11-05 00:56:43,297 - INFO - [diffusion][Epoch 6503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:43,298 - INFO - [diffusion][Epoch 6504] Epoch 6505/12000
2024-11-05 00:56:47,404 - INFO - [diffusion][Epoch 6504] diffusion training Loss: 0.06142732873558998
2024-11-05 00:56:47,406 - INFO - [diffusion][Epoch 6504] diffusion learning rate: 0.001
2024-11-05 00:56:47,408 - INFO - [diffusion][Epoch 6504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:47,409 - INFO - [diffusion][Epoch 6505] Epoch 6506/12000
2024-11-05 00:56:51,463 - INFO - [diffusion][Epoch 6505] diffusion training Loss: 0.06597174610942602
2024-11-05 00:56:51,465 - INFO - [diffusion][Epoch 6505] diffusion learning rate: 0.001
2024-11-05 00:56:51,467 - INFO - [diffusion][Epoch 6505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:51,468 - INFO - [diffusion][Epoch 6506] Epoch 6507/12000
2024-11-05 00:56:55,535 - INFO - [diffusion][Epoch 6506] diffusion training Loss: 0.05666555464267731
2024-11-05 00:56:55,537 - INFO - [diffusion][Epoch 6506] diffusion learning rate: 0.001
2024-11-05 00:56:55,597 - INFO - [diffusion][Epoch 6506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:55,598 - INFO - [diffusion][Epoch 6507] Epoch 6508/12000
2024-11-05 00:56:59,701 - INFO - [diffusion][Epoch 6507] diffusion training Loss: 0.060277966782450676
2024-11-05 00:56:59,704 - INFO - [diffusion][Epoch 6507] diffusion learning rate: 0.001
2024-11-05 00:56:59,706 - INFO - [diffusion][Epoch 6507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:56:59,708 - INFO - [diffusion][Epoch 6508] Epoch 6509/12000
2024-11-05 00:57:03,799 - INFO - [diffusion][Epoch 6508] diffusion training Loss: 0.05699030589312315
2024-11-05 00:57:03,801 - INFO - [diffusion][Epoch 6508] diffusion learning rate: 0.001
2024-11-05 00:57:03,803 - INFO - [diffusion][Epoch 6508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:03,804 - INFO - [diffusion][Epoch 6509] Epoch 6510/12000
2024-11-05 00:57:07,982 - INFO - [diffusion][Epoch 6509] diffusion training Loss: 0.054697698913514614
2024-11-05 00:57:07,984 - INFO - [diffusion][Epoch 6509] diffusion learning rate: 0.001
2024-11-05 00:57:07,985 - INFO - [diffusion][Epoch 6509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:07,987 - INFO - [diffusion][Epoch 6510] Epoch 6511/12000
2024-11-05 00:57:11,993 - INFO - [diffusion][Epoch 6510] diffusion training Loss: 0.06165288295596838
2024-11-05 00:57:11,995 - INFO - [diffusion][Epoch 6510] diffusion learning rate: 0.001
2024-11-05 00:57:11,996 - INFO - [diffusion][Epoch 6510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:11,997 - INFO - [diffusion][Epoch 6511] Epoch 6512/12000
2024-11-05 00:57:15,943 - INFO - [diffusion][Epoch 6511] diffusion training Loss: 0.06036457512527704
2024-11-05 00:57:15,945 - INFO - [diffusion][Epoch 6511] diffusion learning rate: 0.001
2024-11-05 00:57:15,947 - INFO - [diffusion][Epoch 6511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:15,949 - INFO - [diffusion][Epoch 6512] Epoch 6513/12000
2024-11-05 00:57:20,101 - INFO - [diffusion][Epoch 6512] diffusion training Loss: 0.06377325393259525
2024-11-05 00:57:20,103 - INFO - [diffusion][Epoch 6512] diffusion learning rate: 0.001
2024-11-05 00:57:20,105 - INFO - [diffusion][Epoch 6512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:20,113 - INFO - [diffusion][Epoch 6513] Epoch 6514/12000
2024-11-05 00:57:24,260 - INFO - [diffusion][Epoch 6513] diffusion training Loss: 0.05560583621263504
2024-11-05 00:57:24,262 - INFO - [diffusion][Epoch 6513] diffusion learning rate: 0.001
2024-11-05 00:57:24,264 - INFO - [diffusion][Epoch 6513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:24,265 - INFO - [diffusion][Epoch 6514] Epoch 6515/12000
2024-11-05 00:57:28,720 - INFO - [diffusion][Epoch 6514] diffusion training Loss: 0.06326141953468323
2024-11-05 00:57:28,722 - INFO - [diffusion][Epoch 6514] diffusion learning rate: 0.001
2024-11-05 00:57:28,724 - INFO - [diffusion][Epoch 6514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:28,725 - INFO - [diffusion][Epoch 6515] Epoch 6516/12000
2024-11-05 00:57:32,871 - INFO - [diffusion][Epoch 6515] diffusion training Loss: 0.06044635735452175
2024-11-05 00:57:32,873 - INFO - [diffusion][Epoch 6515] diffusion learning rate: 0.001
2024-11-05 00:57:32,875 - INFO - [diffusion][Epoch 6515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:32,876 - INFO - [diffusion][Epoch 6516] Epoch 6517/12000
2024-11-05 00:57:36,966 - INFO - [diffusion][Epoch 6516] diffusion training Loss: 0.05554033163934946
2024-11-05 00:57:36,968 - INFO - [diffusion][Epoch 6516] diffusion learning rate: 0.001
2024-11-05 00:57:36,970 - INFO - [diffusion][Epoch 6516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:36,971 - INFO - [diffusion][Epoch 6517] Epoch 6518/12000
2024-11-05 00:57:41,089 - INFO - [diffusion][Epoch 6517] diffusion training Loss: 0.06126461178064346
2024-11-05 00:57:41,091 - INFO - [diffusion][Epoch 6517] diffusion learning rate: 0.001
2024-11-05 00:57:41,093 - INFO - [diffusion][Epoch 6517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:41,094 - INFO - [diffusion][Epoch 6518] Epoch 6519/12000
2024-11-05 00:57:45,187 - INFO - [diffusion][Epoch 6518] diffusion training Loss: 0.05869920924305916
2024-11-05 00:57:45,206 - INFO - [diffusion][Epoch 6518] diffusion learning rate: 0.001
2024-11-05 00:57:45,208 - INFO - [diffusion][Epoch 6518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:45,209 - INFO - [diffusion][Epoch 6519] Epoch 6520/12000
2024-11-05 00:57:49,341 - INFO - [diffusion][Epoch 6519] diffusion training Loss: 0.057639523409307
2024-11-05 00:57:49,344 - INFO - [diffusion][Epoch 6519] diffusion learning rate: 0.001
2024-11-05 00:57:49,346 - INFO - [diffusion][Epoch 6519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:49,347 - INFO - [diffusion][Epoch 6520] Epoch 6521/12000
2024-11-05 00:57:53,503 - INFO - [diffusion][Epoch 6520] diffusion training Loss: 0.0528792105615139
2024-11-05 00:57:53,505 - INFO - [diffusion][Epoch 6520] diffusion learning rate: 0.001
2024-11-05 00:57:53,507 - INFO - [diffusion][Epoch 6520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:53,508 - INFO - [diffusion][Epoch 6521] Epoch 6522/12000
2024-11-05 00:57:57,710 - INFO - [diffusion][Epoch 6521] diffusion training Loss: 0.0623065447434783
2024-11-05 00:57:57,770 - INFO - [diffusion][Epoch 6521] diffusion learning rate: 0.001
2024-11-05 00:57:57,780 - INFO - [diffusion][Epoch 6521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:57:57,781 - INFO - [diffusion][Epoch 6522] Epoch 6523/12000
2024-11-05 00:58:01,910 - INFO - [diffusion][Epoch 6522] diffusion training Loss: 0.05704185739159584
2024-11-05 00:58:01,913 - INFO - [diffusion][Epoch 6522] diffusion learning rate: 0.001
2024-11-05 00:58:01,914 - INFO - [diffusion][Epoch 6522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:01,916 - INFO - [diffusion][Epoch 6523] Epoch 6524/12000
2024-11-05 00:58:06,029 - INFO - [diffusion][Epoch 6523] diffusion training Loss: 0.059107630513608456
2024-11-05 00:58:06,031 - INFO - [diffusion][Epoch 6523] diffusion learning rate: 0.001
2024-11-05 00:58:06,033 - INFO - [diffusion][Epoch 6523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:06,034 - INFO - [diffusion][Epoch 6524] Epoch 6525/12000
2024-11-05 00:58:10,173 - INFO - [diffusion][Epoch 6524] diffusion training Loss: 0.06221150420606136
2024-11-05 00:58:10,175 - INFO - [diffusion][Epoch 6524] diffusion learning rate: 0.001
2024-11-05 00:58:10,177 - INFO - [diffusion][Epoch 6524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:10,178 - INFO - [diffusion][Epoch 6525] Epoch 6526/12000
2024-11-05 00:58:14,363 - INFO - [diffusion][Epoch 6525] diffusion training Loss: 0.06194212660193443
2024-11-05 00:58:14,365 - INFO - [diffusion][Epoch 6525] diffusion learning rate: 0.001
2024-11-05 00:58:14,367 - INFO - [diffusion][Epoch 6525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:14,368 - INFO - [diffusion][Epoch 6526] Epoch 6527/12000
2024-11-05 00:58:18,465 - INFO - [diffusion][Epoch 6526] diffusion training Loss: 0.06344863027334213
2024-11-05 00:58:18,467 - INFO - [diffusion][Epoch 6526] diffusion learning rate: 0.001
2024-11-05 00:58:18,469 - INFO - [diffusion][Epoch 6526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:18,470 - INFO - [diffusion][Epoch 6527] Epoch 6528/12000
2024-11-05 00:58:22,622 - INFO - [diffusion][Epoch 6527] diffusion training Loss: 0.06005973741412163
2024-11-05 00:58:22,624 - INFO - [diffusion][Epoch 6527] diffusion learning rate: 0.001
2024-11-05 00:58:22,626 - INFO - [diffusion][Epoch 6527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:22,627 - INFO - [diffusion][Epoch 6528] Epoch 6529/12000
2024-11-05 00:58:26,750 - INFO - [diffusion][Epoch 6528] diffusion training Loss: 0.05806736834347248
2024-11-05 00:58:26,752 - INFO - [diffusion][Epoch 6528] diffusion learning rate: 0.001
2024-11-05 00:58:26,753 - INFO - [diffusion][Epoch 6528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:26,755 - INFO - [diffusion][Epoch 6529] Epoch 6530/12000
2024-11-05 00:58:30,870 - INFO - [diffusion][Epoch 6529] diffusion training Loss: 0.05969146452844143
2024-11-05 00:58:30,872 - INFO - [diffusion][Epoch 6529] diffusion learning rate: 0.001
2024-11-05 00:58:30,873 - INFO - [diffusion][Epoch 6529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:30,875 - INFO - [diffusion][Epoch 6530] Epoch 6531/12000
2024-11-05 00:58:34,998 - INFO - [diffusion][Epoch 6530] diffusion training Loss: 0.06107424758374691
2024-11-05 00:58:35,000 - INFO - [diffusion][Epoch 6530] diffusion learning rate: 0.001
2024-11-05 00:58:35,002 - INFO - [diffusion][Epoch 6530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:35,003 - INFO - [diffusion][Epoch 6531] Epoch 6532/12000
2024-11-05 00:58:39,103 - INFO - [diffusion][Epoch 6531] diffusion training Loss: 0.05995060410350561
2024-11-05 00:58:39,105 - INFO - [diffusion][Epoch 6531] diffusion learning rate: 0.001
2024-11-05 00:58:39,107 - INFO - [diffusion][Epoch 6531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:39,108 - INFO - [diffusion][Epoch 6532] Epoch 6533/12000
2024-11-05 00:58:43,220 - INFO - [diffusion][Epoch 6532] diffusion training Loss: 0.06172759737819433
2024-11-05 00:58:43,222 - INFO - [diffusion][Epoch 6532] diffusion learning rate: 0.001
2024-11-05 00:58:43,224 - INFO - [diffusion][Epoch 6532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:43,225 - INFO - [diffusion][Epoch 6533] Epoch 6534/12000
2024-11-05 00:58:47,410 - INFO - [diffusion][Epoch 6533] diffusion training Loss: 0.06206355430185795
2024-11-05 00:58:47,412 - INFO - [diffusion][Epoch 6533] diffusion learning rate: 0.001
2024-11-05 00:58:47,414 - INFO - [diffusion][Epoch 6533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:47,415 - INFO - [diffusion][Epoch 6534] Epoch 6535/12000
2024-11-05 00:58:51,818 - INFO - [diffusion][Epoch 6534] diffusion training Loss: 0.061968255788087845
2024-11-05 00:58:51,819 - INFO - [diffusion][Epoch 6534] diffusion learning rate: 0.001
2024-11-05 00:58:51,821 - INFO - [diffusion][Epoch 6534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:51,822 - INFO - [diffusion][Epoch 6535] Epoch 6536/12000
2024-11-05 00:58:55,923 - INFO - [diffusion][Epoch 6535] diffusion training Loss: 0.06278952676802874
2024-11-05 00:58:55,926 - INFO - [diffusion][Epoch 6535] diffusion learning rate: 0.001
2024-11-05 00:58:55,928 - INFO - [diffusion][Epoch 6535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:58:55,929 - INFO - [diffusion][Epoch 6536] Epoch 6537/12000
2024-11-05 00:59:00,095 - INFO - [diffusion][Epoch 6536] diffusion training Loss: 0.06035424116998911
2024-11-05 00:59:00,097 - INFO - [diffusion][Epoch 6536] diffusion learning rate: 0.001
2024-11-05 00:59:00,099 - INFO - [diffusion][Epoch 6536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:00,101 - INFO - [diffusion][Epoch 6537] Epoch 6538/12000
2024-11-05 00:59:04,248 - INFO - [diffusion][Epoch 6537] diffusion training Loss: 0.060094159096479416
2024-11-05 00:59:04,250 - INFO - [diffusion][Epoch 6537] diffusion learning rate: 0.001
2024-11-05 00:59:04,318 - INFO - [diffusion][Epoch 6537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:04,319 - INFO - [diffusion][Epoch 6538] Epoch 6539/12000
2024-11-05 00:59:08,414 - INFO - [diffusion][Epoch 6538] diffusion training Loss: 0.05686171166598797
2024-11-05 00:59:08,416 - INFO - [diffusion][Epoch 6538] diffusion learning rate: 0.001
2024-11-05 00:59:08,418 - INFO - [diffusion][Epoch 6538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:08,420 - INFO - [diffusion][Epoch 6539] Epoch 6540/12000
2024-11-05 00:59:12,562 - INFO - [diffusion][Epoch 6539] diffusion training Loss: 0.06057840492576361
2024-11-05 00:59:12,564 - INFO - [diffusion][Epoch 6539] diffusion learning rate: 0.001
2024-11-05 00:59:12,566 - INFO - [diffusion][Epoch 6539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:12,567 - INFO - [diffusion][Epoch 6540] Epoch 6541/12000
2024-11-05 00:59:16,672 - INFO - [diffusion][Epoch 6540] diffusion training Loss: 0.06026693619787693
2024-11-05 00:59:16,674 - INFO - [diffusion][Epoch 6540] diffusion learning rate: 0.001
2024-11-05 00:59:16,676 - INFO - [diffusion][Epoch 6540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:16,678 - INFO - [diffusion][Epoch 6541] Epoch 6542/12000
2024-11-05 00:59:20,824 - INFO - [diffusion][Epoch 6541] diffusion training Loss: 0.06115568894892931
2024-11-05 00:59:20,827 - INFO - [diffusion][Epoch 6541] diffusion learning rate: 0.001
2024-11-05 00:59:20,829 - INFO - [diffusion][Epoch 6541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:20,830 - INFO - [diffusion][Epoch 6542] Epoch 6543/12000
2024-11-05 00:59:24,969 - INFO - [diffusion][Epoch 6542] diffusion training Loss: 0.06247822009027004
2024-11-05 00:59:24,971 - INFO - [diffusion][Epoch 6542] diffusion learning rate: 0.001
2024-11-05 00:59:24,973 - INFO - [diffusion][Epoch 6542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:24,974 - INFO - [diffusion][Epoch 6543] Epoch 6544/12000
2024-11-05 00:59:29,088 - INFO - [diffusion][Epoch 6543] diffusion training Loss: 0.06261819414794445
2024-11-05 00:59:29,090 - INFO - [diffusion][Epoch 6543] diffusion learning rate: 0.001
2024-11-05 00:59:29,092 - INFO - [diffusion][Epoch 6543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:29,094 - INFO - [diffusion][Epoch 6544] Epoch 6545/12000
2024-11-05 00:59:33,236 - INFO - [diffusion][Epoch 6544] diffusion training Loss: 0.06115333829075098
2024-11-05 00:59:33,238 - INFO - [diffusion][Epoch 6544] diffusion learning rate: 0.001
2024-11-05 00:59:33,240 - INFO - [diffusion][Epoch 6544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:33,242 - INFO - [diffusion][Epoch 6545] Epoch 6546/12000
2024-11-05 00:59:37,361 - INFO - [diffusion][Epoch 6545] diffusion training Loss: 0.05734315328299999
2024-11-05 00:59:37,364 - INFO - [diffusion][Epoch 6545] diffusion learning rate: 0.001
2024-11-05 00:59:37,366 - INFO - [diffusion][Epoch 6545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:37,367 - INFO - [diffusion][Epoch 6546] Epoch 6547/12000
2024-11-05 00:59:41,485 - INFO - [diffusion][Epoch 6546] diffusion training Loss: 0.06000527832657099
2024-11-05 00:59:41,487 - INFO - [diffusion][Epoch 6546] diffusion learning rate: 0.001
2024-11-05 00:59:41,489 - INFO - [diffusion][Epoch 6546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:41,490 - INFO - [diffusion][Epoch 6547] Epoch 6548/12000
2024-11-05 00:59:45,598 - INFO - [diffusion][Epoch 6547] diffusion training Loss: 0.06129978317767382
2024-11-05 00:59:45,600 - INFO - [diffusion][Epoch 6547] diffusion learning rate: 0.001
2024-11-05 00:59:45,761 - INFO - [diffusion][Epoch 6547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:45,762 - INFO - [diffusion][Epoch 6548] Epoch 6549/12000
2024-11-05 00:59:49,892 - INFO - [diffusion][Epoch 6548] diffusion training Loss: 0.06170444097369909
2024-11-05 00:59:49,894 - INFO - [diffusion][Epoch 6548] diffusion learning rate: 0.001
2024-11-05 00:59:49,896 - INFO - [diffusion][Epoch 6548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:49,897 - INFO - [diffusion][Epoch 6549] Epoch 6550/12000
2024-11-05 00:59:53,951 - INFO - [diffusion][Epoch 6549] diffusion training Loss: 0.05754412431269884
2024-11-05 00:59:53,953 - INFO - [diffusion][Epoch 6549] diffusion learning rate: 0.001
2024-11-05 00:59:53,955 - INFO - [diffusion][Epoch 6549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:53,956 - INFO - [diffusion][Epoch 6550] Epoch 6551/12000
2024-11-05 00:59:58,146 - INFO - [diffusion][Epoch 6550] diffusion training Loss: 0.06046159006655216
2024-11-05 00:59:58,148 - INFO - [diffusion][Epoch 6550] diffusion learning rate: 0.001
2024-11-05 00:59:58,150 - INFO - [diffusion][Epoch 6550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 00:59:58,151 - INFO - [diffusion][Epoch 6551] Epoch 6552/12000
2024-11-05 01:00:02,288 - INFO - [diffusion][Epoch 6551] diffusion training Loss: 0.06367458123713732
2024-11-05 01:00:02,290 - INFO - [diffusion][Epoch 6551] diffusion learning rate: 0.001
2024-11-05 01:00:02,292 - INFO - [diffusion][Epoch 6551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:02,294 - INFO - [diffusion][Epoch 6552] Epoch 6553/12000
2024-11-05 01:00:06,424 - INFO - [diffusion][Epoch 6552] diffusion training Loss: 0.06082609109580517
2024-11-05 01:00:06,426 - INFO - [diffusion][Epoch 6552] diffusion learning rate: 0.001
2024-11-05 01:00:06,428 - INFO - [diffusion][Epoch 6552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:06,430 - INFO - [diffusion][Epoch 6553] Epoch 6554/12000
2024-11-05 01:00:10,569 - INFO - [diffusion][Epoch 6553] diffusion training Loss: 0.05746164545416832
2024-11-05 01:00:10,571 - INFO - [diffusion][Epoch 6553] diffusion learning rate: 0.001
2024-11-05 01:00:10,572 - INFO - [diffusion][Epoch 6553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:10,574 - INFO - [diffusion][Epoch 6554] Epoch 6555/12000
2024-11-05 01:00:14,935 - INFO - [diffusion][Epoch 6554] diffusion training Loss: 0.06417183950543404
2024-11-05 01:00:14,937 - INFO - [diffusion][Epoch 6554] diffusion learning rate: 0.001
2024-11-05 01:00:14,938 - INFO - [diffusion][Epoch 6554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:14,940 - INFO - [diffusion][Epoch 6555] Epoch 6556/12000
2024-11-05 01:00:19,089 - INFO - [diffusion][Epoch 6555] diffusion training Loss: 0.05476819630712271
2024-11-05 01:00:19,091 - INFO - [diffusion][Epoch 6555] diffusion learning rate: 0.001
2024-11-05 01:00:19,093 - INFO - [diffusion][Epoch 6555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:19,094 - INFO - [diffusion][Epoch 6556] Epoch 6557/12000
2024-11-05 01:00:23,208 - INFO - [diffusion][Epoch 6556] diffusion training Loss: 0.05848090071231127
2024-11-05 01:00:23,210 - INFO - [diffusion][Epoch 6556] diffusion learning rate: 0.001
2024-11-05 01:00:23,212 - INFO - [diffusion][Epoch 6556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:23,213 - INFO - [diffusion][Epoch 6557] Epoch 6558/12000
2024-11-05 01:00:27,387 - INFO - [diffusion][Epoch 6557] diffusion training Loss: 0.06499314680695534
2024-11-05 01:00:27,389 - INFO - [diffusion][Epoch 6557] diffusion learning rate: 0.001
2024-11-05 01:00:27,391 - INFO - [diffusion][Epoch 6557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:27,393 - INFO - [diffusion][Epoch 6558] Epoch 6559/12000
2024-11-05 01:00:31,514 - INFO - [diffusion][Epoch 6558] diffusion training Loss: 0.058363777585327625
2024-11-05 01:00:31,516 - INFO - [diffusion][Epoch 6558] diffusion learning rate: 0.001
2024-11-05 01:00:31,518 - INFO - [diffusion][Epoch 6558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:31,519 - INFO - [diffusion][Epoch 6559] Epoch 6560/12000
2024-11-05 01:00:35,601 - INFO - [diffusion][Epoch 6559] diffusion training Loss: 0.057183084078133106
2024-11-05 01:00:35,604 - INFO - [diffusion][Epoch 6559] diffusion learning rate: 0.001
2024-11-05 01:00:35,606 - INFO - [diffusion][Epoch 6559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:35,607 - INFO - [diffusion][Epoch 6560] Epoch 6561/12000
2024-11-05 01:00:39,718 - INFO - [diffusion][Epoch 6560] diffusion training Loss: 0.05731481686234474
2024-11-05 01:00:39,720 - INFO - [diffusion][Epoch 6560] diffusion learning rate: 0.001
2024-11-05 01:00:39,722 - INFO - [diffusion][Epoch 6560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:39,723 - INFO - [diffusion][Epoch 6561] Epoch 6562/12000
2024-11-05 01:00:43,843 - INFO - [diffusion][Epoch 6561] diffusion training Loss: 0.05890818405896425
2024-11-05 01:00:43,845 - INFO - [diffusion][Epoch 6561] diffusion learning rate: 0.001
2024-11-05 01:00:43,847 - INFO - [diffusion][Epoch 6561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:43,848 - INFO - [diffusion][Epoch 6562] Epoch 6563/12000
2024-11-05 01:00:47,950 - INFO - [diffusion][Epoch 6562] diffusion training Loss: 0.056838967837393284
2024-11-05 01:00:47,952 - INFO - [diffusion][Epoch 6562] diffusion learning rate: 0.001
2024-11-05 01:00:47,956 - INFO - [diffusion][Epoch 6562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:47,957 - INFO - [diffusion][Epoch 6563] Epoch 6564/12000
2024-11-05 01:00:52,060 - INFO - [diffusion][Epoch 6563] diffusion training Loss: 0.06384237483143806
2024-11-05 01:00:52,062 - INFO - [diffusion][Epoch 6563] diffusion learning rate: 0.001
2024-11-05 01:00:52,064 - INFO - [diffusion][Epoch 6563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:52,065 - INFO - [diffusion][Epoch 6564] Epoch 6565/12000
2024-11-05 01:00:56,176 - INFO - [diffusion][Epoch 6564] diffusion training Loss: 0.05803956277668476
2024-11-05 01:00:56,178 - INFO - [diffusion][Epoch 6564] diffusion learning rate: 0.001
2024-11-05 01:00:56,180 - INFO - [diffusion][Epoch 6564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:00:56,181 - INFO - [diffusion][Epoch 6565] Epoch 6566/12000
2024-11-05 01:01:00,305 - INFO - [diffusion][Epoch 6565] diffusion training Loss: 0.0634210966527462
2024-11-05 01:01:00,307 - INFO - [diffusion][Epoch 6565] diffusion learning rate: 0.001
2024-11-05 01:01:00,309 - INFO - [diffusion][Epoch 6565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:00,310 - INFO - [diffusion][Epoch 6566] Epoch 6567/12000
2024-11-05 01:01:04,422 - INFO - [diffusion][Epoch 6566] diffusion training Loss: 0.06061284337192774
2024-11-05 01:01:04,424 - INFO - [diffusion][Epoch 6566] diffusion learning rate: 0.001
2024-11-05 01:01:04,426 - INFO - [diffusion][Epoch 6566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:04,427 - INFO - [diffusion][Epoch 6567] Epoch 6568/12000
2024-11-05 01:01:08,419 - INFO - [diffusion][Epoch 6567] diffusion training Loss: 0.06042004469782114
2024-11-05 01:01:08,422 - INFO - [diffusion][Epoch 6567] diffusion learning rate: 0.001
2024-11-05 01:01:08,424 - INFO - [diffusion][Epoch 6567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:08,426 - INFO - [diffusion][Epoch 6568] Epoch 6569/12000
2024-11-05 01:01:12,559 - INFO - [diffusion][Epoch 6568] diffusion training Loss: 0.05955468025058508
2024-11-05 01:01:12,737 - INFO - [diffusion][Epoch 6568] diffusion learning rate: 0.001
2024-11-05 01:01:12,739 - INFO - [diffusion][Epoch 6568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:12,740 - INFO - [diffusion][Epoch 6569] Epoch 6570/12000
2024-11-05 01:01:16,956 - INFO - [diffusion][Epoch 6569] diffusion training Loss: 0.06365831196308136
2024-11-05 01:01:16,959 - INFO - [diffusion][Epoch 6569] diffusion learning rate: 0.001
2024-11-05 01:01:16,960 - INFO - [diffusion][Epoch 6569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:16,962 - INFO - [diffusion][Epoch 6570] Epoch 6571/12000
2024-11-05 01:01:21,052 - INFO - [diffusion][Epoch 6570] diffusion training Loss: 0.05294554028660059
2024-11-05 01:01:21,054 - INFO - [diffusion][Epoch 6570] diffusion learning rate: 0.001
2024-11-05 01:01:21,055 - INFO - [diffusion][Epoch 6570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:21,057 - INFO - [diffusion][Epoch 6571] Epoch 6572/12000
2024-11-05 01:01:25,148 - INFO - [diffusion][Epoch 6571] diffusion training Loss: 0.06218493916094303
2024-11-05 01:01:25,151 - INFO - [diffusion][Epoch 6571] diffusion learning rate: 0.001
2024-11-05 01:01:25,153 - INFO - [diffusion][Epoch 6571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:25,155 - INFO - [diffusion][Epoch 6572] Epoch 6573/12000
2024-11-05 01:01:29,261 - INFO - [diffusion][Epoch 6572] diffusion training Loss: 0.05455989670008421
2024-11-05 01:01:29,263 - INFO - [diffusion][Epoch 6572] diffusion learning rate: 0.001
2024-11-05 01:01:29,265 - INFO - [diffusion][Epoch 6572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:29,266 - INFO - [diffusion][Epoch 6573] Epoch 6574/12000
2024-11-05 01:01:33,410 - INFO - [diffusion][Epoch 6573] diffusion training Loss: 0.05460868403315544
2024-11-05 01:01:33,412 - INFO - [diffusion][Epoch 6573] diffusion learning rate: 0.001
2024-11-05 01:01:33,414 - INFO - [diffusion][Epoch 6573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:33,415 - INFO - [diffusion][Epoch 6574] Epoch 6575/12000
2024-11-05 01:01:37,510 - INFO - [diffusion][Epoch 6574] diffusion training Loss: 0.058487219735980034
2024-11-05 01:01:37,512 - INFO - [diffusion][Epoch 6574] diffusion learning rate: 0.001
2024-11-05 01:01:37,514 - INFO - [diffusion][Epoch 6574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:37,515 - INFO - [diffusion][Epoch 6575] Epoch 6576/12000
2024-11-05 01:01:41,593 - INFO - [diffusion][Epoch 6575] diffusion training Loss: 0.05954580940306187
2024-11-05 01:01:41,595 - INFO - [diffusion][Epoch 6575] diffusion learning rate: 0.001
2024-11-05 01:01:41,597 - INFO - [diffusion][Epoch 6575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:41,599 - INFO - [diffusion][Epoch 6576] Epoch 6577/12000
2024-11-05 01:01:46,352 - INFO - [diffusion][Epoch 6576] diffusion training Loss: 0.06679024361073971
2024-11-05 01:01:46,354 - INFO - [diffusion][Epoch 6576] diffusion learning rate: 0.001
2024-11-05 01:01:46,356 - INFO - [diffusion][Epoch 6576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:46,357 - INFO - [diffusion][Epoch 6577] Epoch 6578/12000
2024-11-05 01:01:50,495 - INFO - [diffusion][Epoch 6577] diffusion training Loss: 0.05786520708352327
2024-11-05 01:01:50,497 - INFO - [diffusion][Epoch 6577] diffusion learning rate: 0.001
2024-11-05 01:01:50,499 - INFO - [diffusion][Epoch 6577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:50,500 - INFO - [diffusion][Epoch 6578] Epoch 6579/12000
2024-11-05 01:01:54,638 - INFO - [diffusion][Epoch 6578] diffusion training Loss: 0.05407645832747221
2024-11-05 01:01:54,640 - INFO - [diffusion][Epoch 6578] diffusion learning rate: 0.001
2024-11-05 01:01:54,642 - INFO - [diffusion][Epoch 6578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:54,643 - INFO - [diffusion][Epoch 6579] Epoch 6580/12000
2024-11-05 01:01:58,626 - INFO - [diffusion][Epoch 6579] diffusion training Loss: 0.058268385007977486
2024-11-05 01:01:58,628 - INFO - [diffusion][Epoch 6579] diffusion learning rate: 0.001
2024-11-05 01:01:58,630 - INFO - [diffusion][Epoch 6579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:01:58,631 - INFO - [diffusion][Epoch 6580] Epoch 6581/12000
2024-11-05 01:02:02,710 - INFO - [diffusion][Epoch 6580] diffusion training Loss: 0.05821151938289404
2024-11-05 01:02:02,712 - INFO - [diffusion][Epoch 6580] diffusion learning rate: 0.001
2024-11-05 01:02:02,775 - INFO - [diffusion][Epoch 6580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:02,776 - INFO - [diffusion][Epoch 6581] Epoch 6582/12000
2024-11-05 01:02:06,865 - INFO - [diffusion][Epoch 6581] diffusion training Loss: 0.06376450508832932
2024-11-05 01:02:06,867 - INFO - [diffusion][Epoch 6581] diffusion learning rate: 0.001
2024-11-05 01:02:06,868 - INFO - [diffusion][Epoch 6581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:06,870 - INFO - [diffusion][Epoch 6582] Epoch 6583/12000
2024-11-05 01:02:10,981 - INFO - [diffusion][Epoch 6582] diffusion training Loss: 0.055766209959983826
2024-11-05 01:02:10,984 - INFO - [diffusion][Epoch 6582] diffusion learning rate: 0.001
2024-11-05 01:02:10,986 - INFO - [diffusion][Epoch 6582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:10,988 - INFO - [diffusion][Epoch 6583] Epoch 6584/12000
2024-11-05 01:02:15,006 - INFO - [diffusion][Epoch 6583] diffusion training Loss: 0.05968333315104246
2024-11-05 01:02:15,008 - INFO - [diffusion][Epoch 6583] diffusion learning rate: 0.001
2024-11-05 01:02:15,010 - INFO - [diffusion][Epoch 6583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:15,011 - INFO - [diffusion][Epoch 6584] Epoch 6585/12000
2024-11-05 01:02:19,062 - INFO - [diffusion][Epoch 6584] diffusion training Loss: 0.05893657077103853
2024-11-05 01:02:19,064 - INFO - [diffusion][Epoch 6584] diffusion learning rate: 0.001
2024-11-05 01:02:19,065 - INFO - [diffusion][Epoch 6584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:19,066 - INFO - [diffusion][Epoch 6585] Epoch 6586/12000
2024-11-05 01:02:22,947 - INFO - [diffusion][Epoch 6585] diffusion training Loss: 0.06081070750951767
2024-11-05 01:02:22,949 - INFO - [diffusion][Epoch 6585] diffusion learning rate: 0.001
2024-11-05 01:02:22,951 - INFO - [diffusion][Epoch 6585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:22,952 - INFO - [diffusion][Epoch 6586] Epoch 6587/12000
2024-11-05 01:02:27,018 - INFO - [diffusion][Epoch 6586] diffusion training Loss: 0.06230524834245443
2024-11-05 01:02:27,020 - INFO - [diffusion][Epoch 6586] diffusion learning rate: 0.001
2024-11-05 01:02:27,021 - INFO - [diffusion][Epoch 6586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:27,023 - INFO - [diffusion][Epoch 6587] Epoch 6588/12000
2024-11-05 01:02:31,136 - INFO - [diffusion][Epoch 6587] diffusion training Loss: 0.05934509076178074
2024-11-05 01:02:31,138 - INFO - [diffusion][Epoch 6587] diffusion learning rate: 0.001
2024-11-05 01:02:31,140 - INFO - [diffusion][Epoch 6587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:31,141 - INFO - [diffusion][Epoch 6588] Epoch 6589/12000
2024-11-05 01:02:35,309 - INFO - [diffusion][Epoch 6588] diffusion training Loss: 0.06624563410878181
2024-11-05 01:02:35,310 - INFO - [diffusion][Epoch 6588] diffusion learning rate: 0.001
2024-11-05 01:02:35,358 - INFO - [diffusion][Epoch 6588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:35,360 - INFO - [diffusion][Epoch 6589] Epoch 6590/12000
2024-11-05 01:02:39,456 - INFO - [diffusion][Epoch 6589] diffusion training Loss: 0.06062059756368399
2024-11-05 01:02:39,459 - INFO - [diffusion][Epoch 6589] diffusion learning rate: 0.001
2024-11-05 01:02:39,461 - INFO - [diffusion][Epoch 6589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:39,462 - INFO - [diffusion][Epoch 6590] Epoch 6591/12000
2024-11-05 01:02:43,464 - INFO - [diffusion][Epoch 6590] diffusion training Loss: 0.06075080391019583
2024-11-05 01:02:43,466 - INFO - [diffusion][Epoch 6590] diffusion learning rate: 0.001
2024-11-05 01:02:43,468 - INFO - [diffusion][Epoch 6590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:43,469 - INFO - [diffusion][Epoch 6591] Epoch 6592/12000
2024-11-05 01:02:47,632 - INFO - [diffusion][Epoch 6591] diffusion training Loss: 0.05616180785000324
2024-11-05 01:02:47,633 - INFO - [diffusion][Epoch 6591] diffusion learning rate: 0.001
2024-11-05 01:02:47,635 - INFO - [diffusion][Epoch 6591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:47,637 - INFO - [diffusion][Epoch 6592] Epoch 6593/12000
2024-11-05 01:02:51,698 - INFO - [diffusion][Epoch 6592] diffusion training Loss: 0.05557187274098396
2024-11-05 01:02:51,700 - INFO - [diffusion][Epoch 6592] diffusion learning rate: 0.001
2024-11-05 01:02:51,735 - INFO - [diffusion][Epoch 6592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:51,737 - INFO - [diffusion][Epoch 6593] Epoch 6594/12000
2024-11-05 01:02:55,862 - INFO - [diffusion][Epoch 6593] diffusion training Loss: 0.061346665024757385
2024-11-05 01:02:55,864 - INFO - [diffusion][Epoch 6593] diffusion learning rate: 0.001
2024-11-05 01:02:55,866 - INFO - [diffusion][Epoch 6593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:02:55,867 - INFO - [diffusion][Epoch 6594] Epoch 6595/12000
2024-11-05 01:03:00,050 - INFO - [diffusion][Epoch 6594] diffusion training Loss: 0.05912232119590044
2024-11-05 01:03:00,052 - INFO - [diffusion][Epoch 6594] diffusion learning rate: 0.001
2024-11-05 01:03:00,054 - INFO - [diffusion][Epoch 6594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:00,055 - INFO - [diffusion][Epoch 6595] Epoch 6596/12000
2024-11-05 01:03:04,088 - INFO - [diffusion][Epoch 6595] diffusion training Loss: 0.06136639788746834
2024-11-05 01:03:04,090 - INFO - [diffusion][Epoch 6595] diffusion learning rate: 0.001
2024-11-05 01:03:04,092 - INFO - [diffusion][Epoch 6595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:04,094 - INFO - [diffusion][Epoch 6596] Epoch 6597/12000
2024-11-05 01:03:08,175 - INFO - [diffusion][Epoch 6596] diffusion training Loss: 0.05747583135962486
2024-11-05 01:03:08,177 - INFO - [diffusion][Epoch 6596] diffusion learning rate: 0.001
2024-11-05 01:03:08,179 - INFO - [diffusion][Epoch 6596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:08,180 - INFO - [diffusion][Epoch 6597] Epoch 6598/12000
2024-11-05 01:03:12,809 - INFO - [diffusion][Epoch 6597] diffusion training Loss: 0.05946554895490408
2024-11-05 01:03:12,811 - INFO - [diffusion][Epoch 6597] diffusion learning rate: 0.001
2024-11-05 01:03:12,813 - INFO - [diffusion][Epoch 6597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:12,814 - INFO - [diffusion][Epoch 6598] Epoch 6599/12000
2024-11-05 01:03:16,982 - INFO - [diffusion][Epoch 6598] diffusion training Loss: 0.05491852853447199
2024-11-05 01:03:16,984 - INFO - [diffusion][Epoch 6598] diffusion learning rate: 0.001
2024-11-05 01:03:16,986 - INFO - [diffusion][Epoch 6598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:16,988 - INFO - [diffusion][Epoch 6599] Epoch 6600/12000
2024-11-05 01:03:21,164 - INFO - [diffusion][Epoch 6599] diffusion training Loss: 0.055705832317471504
2024-11-05 01:03:21,166 - INFO - [diffusion][Epoch 6599] diffusion learning rate: 0.001
2024-11-05 01:03:21,168 - INFO - [diffusion][Epoch 6599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:21,170 - INFO - [diffusion][Epoch 6600] Epoch 6601/12000
2024-11-05 01:03:25,291 - INFO - [diffusion][Epoch 6600] diffusion training Loss: 0.057915071956813335
2024-11-05 01:03:25,294 - INFO - [diffusion][Epoch 6600] diffusion learning rate: 0.001
2024-11-05 01:03:25,296 - INFO - [diffusion][Epoch 6600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:25,297 - INFO - [diffusion][Epoch 6601] Epoch 6602/12000
2024-11-05 01:03:29,439 - INFO - [diffusion][Epoch 6601] diffusion training Loss: 0.06061162147670984
2024-11-05 01:03:29,441 - INFO - [diffusion][Epoch 6601] diffusion learning rate: 0.001
2024-11-05 01:03:29,443 - INFO - [diffusion][Epoch 6601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:29,444 - INFO - [diffusion][Epoch 6602] Epoch 6603/12000
2024-11-05 01:03:33,545 - INFO - [diffusion][Epoch 6602] diffusion training Loss: 0.06080056447535753
2024-11-05 01:03:33,546 - INFO - [diffusion][Epoch 6602] diffusion learning rate: 0.001
2024-11-05 01:03:33,548 - INFO - [diffusion][Epoch 6602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:33,549 - INFO - [diffusion][Epoch 6603] Epoch 6604/12000
2024-11-05 01:03:37,664 - INFO - [diffusion][Epoch 6603] diffusion training Loss: 0.056373381055891514
2024-11-05 01:03:37,668 - INFO - [diffusion][Epoch 6603] diffusion learning rate: 0.001
2024-11-05 01:03:37,669 - INFO - [diffusion][Epoch 6603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:37,671 - INFO - [diffusion][Epoch 6604] Epoch 6605/12000
2024-11-05 01:03:41,769 - INFO - [diffusion][Epoch 6604] diffusion training Loss: 0.06009544711560011
2024-11-05 01:03:41,771 - INFO - [diffusion][Epoch 6604] diffusion learning rate: 0.001
2024-11-05 01:03:41,773 - INFO - [diffusion][Epoch 6604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:41,774 - INFO - [diffusion][Epoch 6605] Epoch 6606/12000
2024-11-05 01:03:45,858 - INFO - [diffusion][Epoch 6605] diffusion training Loss: 0.06031183619052172
2024-11-05 01:03:45,860 - INFO - [diffusion][Epoch 6605] diffusion learning rate: 0.001
2024-11-05 01:03:45,906 - INFO - [diffusion][Epoch 6605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:45,907 - INFO - [diffusion][Epoch 6606] Epoch 6607/12000
2024-11-05 01:03:49,910 - INFO - [diffusion][Epoch 6606] diffusion training Loss: 0.060549475252628326
2024-11-05 01:03:49,912 - INFO - [diffusion][Epoch 6606] diffusion learning rate: 0.001
2024-11-05 01:03:49,914 - INFO - [diffusion][Epoch 6606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:49,916 - INFO - [diffusion][Epoch 6607] Epoch 6608/12000
2024-11-05 01:03:54,091 - INFO - [diffusion][Epoch 6607] diffusion training Loss: 0.061047946102917194
2024-11-05 01:03:54,093 - INFO - [diffusion][Epoch 6607] diffusion learning rate: 0.001
2024-11-05 01:03:54,095 - INFO - [diffusion][Epoch 6607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:54,096 - INFO - [diffusion][Epoch 6608] Epoch 6609/12000
2024-11-05 01:03:58,263 - INFO - [diffusion][Epoch 6608] diffusion training Loss: 0.06216567941009998
2024-11-05 01:03:58,265 - INFO - [diffusion][Epoch 6608] diffusion learning rate: 0.001
2024-11-05 01:03:58,268 - INFO - [diffusion][Epoch 6608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:03:58,269 - INFO - [diffusion][Epoch 6609] Epoch 6610/12000
2024-11-05 01:04:02,316 - INFO - [diffusion][Epoch 6609] diffusion training Loss: 0.05427309311926365
2024-11-05 01:04:02,318 - INFO - [diffusion][Epoch 6609] diffusion learning rate: 0.001
2024-11-05 01:04:02,321 - INFO - [diffusion][Epoch 6609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:02,322 - INFO - [diffusion][Epoch 6610] Epoch 6611/12000
2024-11-05 01:04:06,450 - INFO - [diffusion][Epoch 6610] diffusion training Loss: 0.0602030074223876
2024-11-05 01:04:06,452 - INFO - [diffusion][Epoch 6610] diffusion learning rate: 0.001
2024-11-05 01:04:06,454 - INFO - [diffusion][Epoch 6610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:06,455 - INFO - [diffusion][Epoch 6611] Epoch 6612/12000
2024-11-05 01:04:10,561 - INFO - [diffusion][Epoch 6611] diffusion training Loss: 0.057744940742850304
2024-11-05 01:04:10,563 - INFO - [diffusion][Epoch 6611] diffusion learning rate: 0.001
2024-11-05 01:04:10,565 - INFO - [diffusion][Epoch 6611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:10,566 - INFO - [diffusion][Epoch 6612] Epoch 6613/12000
2024-11-05 01:04:14,678 - INFO - [diffusion][Epoch 6612] diffusion training Loss: 0.06004944536834955
2024-11-05 01:04:14,681 - INFO - [diffusion][Epoch 6612] diffusion learning rate: 0.001
2024-11-05 01:04:14,683 - INFO - [diffusion][Epoch 6612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:14,684 - INFO - [diffusion][Epoch 6613] Epoch 6614/12000
2024-11-05 01:04:18,769 - INFO - [diffusion][Epoch 6613] diffusion training Loss: 0.05999023839831352
2024-11-05 01:04:18,771 - INFO - [diffusion][Epoch 6613] diffusion learning rate: 0.001
2024-11-05 01:04:18,772 - INFO - [diffusion][Epoch 6613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:18,774 - INFO - [diffusion][Epoch 6614] Epoch 6615/12000
2024-11-05 01:04:22,935 - INFO - [diffusion][Epoch 6614] diffusion training Loss: 0.061775967478752136
2024-11-05 01:04:22,937 - INFO - [diffusion][Epoch 6614] diffusion learning rate: 0.001
2024-11-05 01:04:22,938 - INFO - [diffusion][Epoch 6614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:22,940 - INFO - [diffusion][Epoch 6615] Epoch 6616/12000
2024-11-05 01:04:27,070 - INFO - [diffusion][Epoch 6615] diffusion training Loss: 0.05974336061626673
2024-11-05 01:04:27,072 - INFO - [diffusion][Epoch 6615] diffusion learning rate: 0.001
2024-11-05 01:04:27,074 - INFO - [diffusion][Epoch 6615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:27,075 - INFO - [diffusion][Epoch 6616] Epoch 6617/12000
2024-11-05 01:04:31,137 - INFO - [diffusion][Epoch 6616] diffusion training Loss: 0.061208163388073444
2024-11-05 01:04:31,139 - INFO - [diffusion][Epoch 6616] diffusion learning rate: 0.001
2024-11-05 01:04:31,141 - INFO - [diffusion][Epoch 6616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:31,142 - INFO - [diffusion][Epoch 6617] Epoch 6618/12000
2024-11-05 01:04:35,554 - INFO - [diffusion][Epoch 6617] diffusion training Loss: 0.05999047588557005
2024-11-05 01:04:35,556 - INFO - [diffusion][Epoch 6617] diffusion learning rate: 0.001
2024-11-05 01:04:35,558 - INFO - [diffusion][Epoch 6617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:35,560 - INFO - [diffusion][Epoch 6618] Epoch 6619/12000
2024-11-05 01:04:39,631 - INFO - [diffusion][Epoch 6618] diffusion training Loss: 0.05509753618389368
2024-11-05 01:04:39,633 - INFO - [diffusion][Epoch 6618] diffusion learning rate: 0.001
2024-11-05 01:04:39,635 - INFO - [diffusion][Epoch 6618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:39,637 - INFO - [diffusion][Epoch 6619] Epoch 6620/12000
2024-11-05 01:04:43,787 - INFO - [diffusion][Epoch 6619] diffusion training Loss: 0.05599334556609392
2024-11-05 01:04:43,789 - INFO - [diffusion][Epoch 6619] diffusion learning rate: 0.001
2024-11-05 01:04:43,792 - INFO - [diffusion][Epoch 6619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:43,793 - INFO - [diffusion][Epoch 6620] Epoch 6621/12000
2024-11-05 01:04:47,937 - INFO - [diffusion][Epoch 6620] diffusion training Loss: 0.05725515075027943
2024-11-05 01:04:47,939 - INFO - [diffusion][Epoch 6620] diffusion learning rate: 0.001
2024-11-05 01:04:47,941 - INFO - [diffusion][Epoch 6620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:47,942 - INFO - [diffusion][Epoch 6621] Epoch 6622/12000
2024-11-05 01:04:52,017 - INFO - [diffusion][Epoch 6621] diffusion training Loss: 0.05902152322232723
2024-11-05 01:04:52,019 - INFO - [diffusion][Epoch 6621] diffusion learning rate: 0.001
2024-11-05 01:04:52,020 - INFO - [diffusion][Epoch 6621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:52,022 - INFO - [diffusion][Epoch 6622] Epoch 6623/12000
2024-11-05 01:04:56,182 - INFO - [diffusion][Epoch 6622] diffusion training Loss: 0.05751880910247564
2024-11-05 01:04:56,184 - INFO - [diffusion][Epoch 6622] diffusion learning rate: 0.001
2024-11-05 01:04:56,186 - INFO - [diffusion][Epoch 6622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:04:56,188 - INFO - [diffusion][Epoch 6623] Epoch 6624/12000
2024-11-05 01:05:00,308 - INFO - [diffusion][Epoch 6623] diffusion training Loss: 0.06001756899058819
2024-11-05 01:05:00,310 - INFO - [diffusion][Epoch 6623] diffusion learning rate: 0.001
2024-11-05 01:05:00,311 - INFO - [diffusion][Epoch 6623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:00,312 - INFO - [diffusion][Epoch 6624] Epoch 6625/12000
2024-11-05 01:05:04,433 - INFO - [diffusion][Epoch 6624] diffusion training Loss: 0.05750037916004658
2024-11-05 01:05:04,435 - INFO - [diffusion][Epoch 6624] diffusion learning rate: 0.001
2024-11-05 01:05:04,437 - INFO - [diffusion][Epoch 6624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:04,438 - INFO - [diffusion][Epoch 6625] Epoch 6626/12000
2024-11-05 01:05:08,535 - INFO - [diffusion][Epoch 6625] diffusion training Loss: 0.0633954107761383
2024-11-05 01:05:08,537 - INFO - [diffusion][Epoch 6625] diffusion learning rate: 0.001
2024-11-05 01:05:08,539 - INFO - [diffusion][Epoch 6625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:08,540 - INFO - [diffusion][Epoch 6626] Epoch 6627/12000
2024-11-05 01:05:12,662 - INFO - [diffusion][Epoch 6626] diffusion training Loss: 0.0578638082370162
2024-11-05 01:05:12,700 - INFO - [diffusion][Epoch 6626] diffusion learning rate: 0.001
2024-11-05 01:05:12,702 - INFO - [diffusion][Epoch 6626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:12,704 - INFO - [diffusion][Epoch 6627] Epoch 6628/12000
2024-11-05 01:05:16,796 - INFO - [diffusion][Epoch 6627] diffusion training Loss: 0.059382012113928795
2024-11-05 01:05:16,799 - INFO - [diffusion][Epoch 6627] diffusion learning rate: 0.001
2024-11-05 01:05:16,801 - INFO - [diffusion][Epoch 6627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:16,802 - INFO - [diffusion][Epoch 6628] Epoch 6629/12000
2024-11-05 01:05:20,943 - INFO - [diffusion][Epoch 6628] diffusion training Loss: 0.05382650066167116
2024-11-05 01:05:20,945 - INFO - [diffusion][Epoch 6628] diffusion learning rate: 0.001
2024-11-05 01:05:20,947 - INFO - [diffusion][Epoch 6628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:20,948 - INFO - [diffusion][Epoch 6629] Epoch 6630/12000
2024-11-05 01:05:25,115 - INFO - [diffusion][Epoch 6629] diffusion training Loss: 0.062166725285351276
2024-11-05 01:05:25,118 - INFO - [diffusion][Epoch 6629] diffusion learning rate: 0.001
2024-11-05 01:05:25,120 - INFO - [diffusion][Epoch 6629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:25,121 - INFO - [diffusion][Epoch 6630] Epoch 6631/12000
2024-11-05 01:05:29,246 - INFO - [diffusion][Epoch 6630] diffusion training Loss: 0.06436418276280165
2024-11-05 01:05:29,248 - INFO - [diffusion][Epoch 6630] diffusion learning rate: 0.001
2024-11-05 01:05:29,250 - INFO - [diffusion][Epoch 6630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:29,252 - INFO - [diffusion][Epoch 6631] Epoch 6632/12000
2024-11-05 01:05:33,390 - INFO - [diffusion][Epoch 6631] diffusion training Loss: 0.05608776584267616
2024-11-05 01:05:33,392 - INFO - [diffusion][Epoch 6631] diffusion learning rate: 0.001
2024-11-05 01:05:33,393 - INFO - [diffusion][Epoch 6631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:33,395 - INFO - [diffusion][Epoch 6632] Epoch 6633/12000
2024-11-05 01:05:37,367 - INFO - [diffusion][Epoch 6632] diffusion training Loss: 0.0603276751935482
2024-11-05 01:05:37,369 - INFO - [diffusion][Epoch 6632] diffusion learning rate: 0.001
2024-11-05 01:05:37,370 - INFO - [diffusion][Epoch 6632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:37,372 - INFO - [diffusion][Epoch 6633] Epoch 6634/12000
2024-11-05 01:05:41,461 - INFO - [diffusion][Epoch 6633] diffusion training Loss: 0.0541300754994154
2024-11-05 01:05:41,463 - INFO - [diffusion][Epoch 6633] diffusion learning rate: 0.001
2024-11-05 01:05:41,465 - INFO - [diffusion][Epoch 6633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:41,466 - INFO - [diffusion][Epoch 6634] Epoch 6635/12000
2024-11-05 01:05:45,538 - INFO - [diffusion][Epoch 6634] diffusion training Loss: 0.06692585628479719
2024-11-05 01:05:45,540 - INFO - [diffusion][Epoch 6634] diffusion learning rate: 0.001
2024-11-05 01:05:45,542 - INFO - [diffusion][Epoch 6634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:45,543 - INFO - [diffusion][Epoch 6635] Epoch 6636/12000
2024-11-05 01:05:49,634 - INFO - [diffusion][Epoch 6635] diffusion training Loss: 0.05820791609585285
2024-11-05 01:05:49,636 - INFO - [diffusion][Epoch 6635] diffusion learning rate: 0.001
2024-11-05 01:05:49,638 - INFO - [diffusion][Epoch 6635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:49,639 - INFO - [diffusion][Epoch 6636] Epoch 6637/12000
2024-11-05 01:05:53,759 - INFO - [diffusion][Epoch 6636] diffusion training Loss: 0.06066433433443308
2024-11-05 01:05:53,761 - INFO - [diffusion][Epoch 6636] diffusion learning rate: 0.001
2024-11-05 01:05:53,762 - INFO - [diffusion][Epoch 6636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:53,764 - INFO - [diffusion][Epoch 6637] Epoch 6638/12000
2024-11-05 01:05:57,797 - INFO - [diffusion][Epoch 6637] diffusion training Loss: 0.06412057392299175
2024-11-05 01:05:57,799 - INFO - [diffusion][Epoch 6637] diffusion learning rate: 0.001
2024-11-05 01:05:57,866 - INFO - [diffusion][Epoch 6637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:05:57,868 - INFO - [diffusion][Epoch 6638] Epoch 6639/12000
2024-11-05 01:06:02,581 - INFO - [diffusion][Epoch 6638] diffusion training Loss: 0.06389633286744356
2024-11-05 01:06:02,583 - INFO - [diffusion][Epoch 6638] diffusion learning rate: 0.001
2024-11-05 01:06:02,585 - INFO - [diffusion][Epoch 6638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:02,586 - INFO - [diffusion][Epoch 6639] Epoch 6640/12000
2024-11-05 01:06:06,703 - INFO - [diffusion][Epoch 6639] diffusion training Loss: 0.06571664102375507
2024-11-05 01:06:06,705 - INFO - [diffusion][Epoch 6639] diffusion learning rate: 0.001
2024-11-05 01:06:06,707 - INFO - [diffusion][Epoch 6639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:06,708 - INFO - [diffusion][Epoch 6640] Epoch 6641/12000
2024-11-05 01:06:10,832 - INFO - [diffusion][Epoch 6640] diffusion training Loss: 0.05440184008330107
2024-11-05 01:06:10,834 - INFO - [diffusion][Epoch 6640] diffusion learning rate: 0.001
2024-11-05 01:06:10,836 - INFO - [diffusion][Epoch 6640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:10,837 - INFO - [diffusion][Epoch 6641] Epoch 6642/12000
2024-11-05 01:06:14,951 - INFO - [diffusion][Epoch 6641] diffusion training Loss: 0.05999703612178564
2024-11-05 01:06:14,953 - INFO - [diffusion][Epoch 6641] diffusion learning rate: 0.001
2024-11-05 01:06:14,955 - INFO - [diffusion][Epoch 6641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:14,956 - INFO - [diffusion][Epoch 6642] Epoch 6643/12000
2024-11-05 01:06:19,071 - INFO - [diffusion][Epoch 6642] diffusion training Loss: 0.05621774680912495
2024-11-05 01:06:19,073 - INFO - [diffusion][Epoch 6642] diffusion learning rate: 0.001
2024-11-05 01:06:19,075 - INFO - [diffusion][Epoch 6642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:19,076 - INFO - [diffusion][Epoch 6643] Epoch 6644/12000
2024-11-05 01:06:23,150 - INFO - [diffusion][Epoch 6643] diffusion training Loss: 0.05880110990256071
2024-11-05 01:06:23,153 - INFO - [diffusion][Epoch 6643] diffusion learning rate: 0.001
2024-11-05 01:06:23,155 - INFO - [diffusion][Epoch 6643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:23,156 - INFO - [diffusion][Epoch 6644] Epoch 6645/12000
2024-11-05 01:06:27,043 - INFO - [diffusion][Epoch 6644] diffusion training Loss: 0.05997028946876526
2024-11-05 01:06:27,046 - INFO - [diffusion][Epoch 6644] diffusion learning rate: 0.001
2024-11-05 01:06:27,047 - INFO - [diffusion][Epoch 6644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:27,049 - INFO - [diffusion][Epoch 6645] Epoch 6646/12000
2024-11-05 01:06:31,154 - INFO - [diffusion][Epoch 6645] diffusion training Loss: 0.06477232277393341
2024-11-05 01:06:31,156 - INFO - [diffusion][Epoch 6645] diffusion learning rate: 0.001
2024-11-05 01:06:31,158 - INFO - [diffusion][Epoch 6645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:31,159 - INFO - [diffusion][Epoch 6646] Epoch 6647/12000
2024-11-05 01:06:35,125 - INFO - [diffusion][Epoch 6646] diffusion training Loss: 0.05473558884114027
2024-11-05 01:06:35,128 - INFO - [diffusion][Epoch 6646] diffusion learning rate: 0.001
2024-11-05 01:06:35,131 - INFO - [diffusion][Epoch 6646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:35,133 - INFO - [diffusion][Epoch 6647] Epoch 6648/12000
2024-11-05 01:06:39,235 - INFO - [diffusion][Epoch 6647] diffusion training Loss: 0.058598014526069164
2024-11-05 01:06:39,237 - INFO - [diffusion][Epoch 6647] diffusion learning rate: 0.001
2024-11-05 01:06:39,239 - INFO - [diffusion][Epoch 6647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:39,240 - INFO - [diffusion][Epoch 6648] Epoch 6649/12000
2024-11-05 01:06:43,386 - INFO - [diffusion][Epoch 6648] diffusion training Loss: 0.05577090568840504
2024-11-05 01:06:43,388 - INFO - [diffusion][Epoch 6648] diffusion learning rate: 0.001
2024-11-05 01:06:43,390 - INFO - [diffusion][Epoch 6648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:43,392 - INFO - [diffusion][Epoch 6649] Epoch 6650/12000
2024-11-05 01:06:47,471 - INFO - [diffusion][Epoch 6649] diffusion training Loss: 0.057511077262461185
2024-11-05 01:06:47,472 - INFO - [diffusion][Epoch 6649] diffusion learning rate: 0.001
2024-11-05 01:06:47,474 - INFO - [diffusion][Epoch 6649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:47,475 - INFO - [diffusion][Epoch 6650] Epoch 6651/12000
2024-11-05 01:06:51,571 - INFO - [diffusion][Epoch 6650] diffusion training Loss: 0.06065267510712147
2024-11-05 01:06:51,573 - INFO - [diffusion][Epoch 6650] diffusion learning rate: 0.001
2024-11-05 01:06:51,575 - INFO - [diffusion][Epoch 6650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:51,576 - INFO - [diffusion][Epoch 6651] Epoch 6652/12000
2024-11-05 01:06:55,719 - INFO - [diffusion][Epoch 6651] diffusion training Loss: 0.05874264519661665
2024-11-05 01:06:55,721 - INFO - [diffusion][Epoch 6651] diffusion learning rate: 0.001
2024-11-05 01:06:55,723 - INFO - [diffusion][Epoch 6651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:55,726 - INFO - [diffusion][Epoch 6652] Epoch 6653/12000
2024-11-05 01:06:59,858 - INFO - [diffusion][Epoch 6652] diffusion training Loss: 0.06346354261040688
2024-11-05 01:06:59,860 - INFO - [diffusion][Epoch 6652] diffusion learning rate: 0.001
2024-11-05 01:06:59,862 - INFO - [diffusion][Epoch 6652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:06:59,865 - INFO - [diffusion][Epoch 6653] Epoch 6654/12000
2024-11-05 01:07:04,035 - INFO - [diffusion][Epoch 6653] diffusion training Loss: 0.06303412653505802
2024-11-05 01:07:04,037 - INFO - [diffusion][Epoch 6653] diffusion learning rate: 0.001
2024-11-05 01:07:04,038 - INFO - [diffusion][Epoch 6653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:04,040 - INFO - [diffusion][Epoch 6654] Epoch 6655/12000
2024-11-05 01:07:08,152 - INFO - [diffusion][Epoch 6654] diffusion training Loss: 0.057757141068577766
2024-11-05 01:07:08,154 - INFO - [diffusion][Epoch 6654] diffusion learning rate: 0.001
2024-11-05 01:07:08,156 - INFO - [diffusion][Epoch 6654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:08,158 - INFO - [diffusion][Epoch 6655] Epoch 6656/12000
2024-11-05 01:07:12,278 - INFO - [diffusion][Epoch 6655] diffusion training Loss: 0.06263274792581797
2024-11-05 01:07:12,280 - INFO - [diffusion][Epoch 6655] diffusion learning rate: 0.001
2024-11-05 01:07:12,282 - INFO - [diffusion][Epoch 6655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:12,283 - INFO - [diffusion][Epoch 6656] Epoch 6657/12000
2024-11-05 01:07:16,393 - INFO - [diffusion][Epoch 6656] diffusion training Loss: 0.05975453369319439
2024-11-05 01:07:16,395 - INFO - [diffusion][Epoch 6656] diffusion learning rate: 0.001
2024-11-05 01:07:16,396 - INFO - [diffusion][Epoch 6656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:16,397 - INFO - [diffusion][Epoch 6657] Epoch 6658/12000
2024-11-05 01:07:20,556 - INFO - [diffusion][Epoch 6657] diffusion training Loss: 0.05764948111027479
2024-11-05 01:07:20,559 - INFO - [diffusion][Epoch 6657] diffusion learning rate: 0.001
2024-11-05 01:07:20,561 - INFO - [diffusion][Epoch 6657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:20,562 - INFO - [diffusion][Epoch 6658] Epoch 6659/12000
2024-11-05 01:07:24,703 - INFO - [diffusion][Epoch 6658] diffusion training Loss: 0.056083064526319504
2024-11-05 01:07:24,705 - INFO - [diffusion][Epoch 6658] diffusion learning rate: 0.001
2024-11-05 01:07:24,707 - INFO - [diffusion][Epoch 6658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:24,709 - INFO - [diffusion][Epoch 6659] Epoch 6660/12000
2024-11-05 01:07:29,007 - INFO - [diffusion][Epoch 6659] diffusion training Loss: 0.05770317371934652
2024-11-05 01:07:29,009 - INFO - [diffusion][Epoch 6659] diffusion learning rate: 0.001
2024-11-05 01:07:29,011 - INFO - [diffusion][Epoch 6659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:29,012 - INFO - [diffusion][Epoch 6660] Epoch 6661/12000
2024-11-05 01:07:33,121 - INFO - [diffusion][Epoch 6660] diffusion training Loss: 0.05708278622478247
2024-11-05 01:07:33,124 - INFO - [diffusion][Epoch 6660] diffusion learning rate: 0.001
2024-11-05 01:07:33,125 - INFO - [diffusion][Epoch 6660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:33,127 - INFO - [diffusion][Epoch 6661] Epoch 6662/12000
2024-11-05 01:07:37,123 - INFO - [diffusion][Epoch 6661] diffusion training Loss: 0.0550692155957222
2024-11-05 01:07:37,125 - INFO - [diffusion][Epoch 6661] diffusion learning rate: 0.001
2024-11-05 01:07:37,127 - INFO - [diffusion][Epoch 6661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:37,128 - INFO - [diffusion][Epoch 6662] Epoch 6663/12000
2024-11-05 01:07:41,179 - INFO - [diffusion][Epoch 6662] diffusion training Loss: 0.056055583991110325
2024-11-05 01:07:41,181 - INFO - [diffusion][Epoch 6662] diffusion learning rate: 0.001
2024-11-05 01:07:41,183 - INFO - [diffusion][Epoch 6662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:41,185 - INFO - [diffusion][Epoch 6663] Epoch 6664/12000
2024-11-05 01:07:45,254 - INFO - [diffusion][Epoch 6663] diffusion training Loss: 0.05899315979331732
2024-11-05 01:07:45,257 - INFO - [diffusion][Epoch 6663] diffusion learning rate: 0.001
2024-11-05 01:07:45,259 - INFO - [diffusion][Epoch 6663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:45,260 - INFO - [diffusion][Epoch 6664] Epoch 6665/12000
2024-11-05 01:07:49,437 - INFO - [diffusion][Epoch 6664] diffusion training Loss: 0.05919921398162842
2024-11-05 01:07:49,439 - INFO - [diffusion][Epoch 6664] diffusion learning rate: 0.001
2024-11-05 01:07:49,441 - INFO - [diffusion][Epoch 6664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:49,443 - INFO - [diffusion][Epoch 6665] Epoch 6666/12000
2024-11-05 01:07:53,640 - INFO - [diffusion][Epoch 6665] diffusion training Loss: 0.055563898757100105
2024-11-05 01:07:53,643 - INFO - [diffusion][Epoch 6665] diffusion learning rate: 0.001
2024-11-05 01:07:53,683 - INFO - [diffusion][Epoch 6665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:53,685 - INFO - [diffusion][Epoch 6666] Epoch 6667/12000
2024-11-05 01:07:57,776 - INFO - [diffusion][Epoch 6666] diffusion training Loss: 0.05920080374926329
2024-11-05 01:07:57,778 - INFO - [diffusion][Epoch 6666] diffusion learning rate: 0.001
2024-11-05 01:07:57,780 - INFO - [diffusion][Epoch 6666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:07:57,781 - INFO - [diffusion][Epoch 6667] Epoch 6668/12000
2024-11-05 01:08:01,944 - INFO - [diffusion][Epoch 6667] diffusion training Loss: 0.05836518947035074
2024-11-05 01:08:01,946 - INFO - [diffusion][Epoch 6667] diffusion learning rate: 0.001
2024-11-05 01:08:01,948 - INFO - [diffusion][Epoch 6667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:01,949 - INFO - [diffusion][Epoch 6668] Epoch 6669/12000
2024-11-05 01:08:06,086 - INFO - [diffusion][Epoch 6668] diffusion training Loss: 0.06700494699180126
2024-11-05 01:08:06,088 - INFO - [diffusion][Epoch 6668] diffusion learning rate: 0.001
2024-11-05 01:08:06,090 - INFO - [diffusion][Epoch 6668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:06,093 - INFO - [diffusion][Epoch 6669] Epoch 6670/12000
2024-11-05 01:08:10,207 - INFO - [diffusion][Epoch 6669] diffusion training Loss: 0.05867889150977135
2024-11-05 01:08:10,209 - INFO - [diffusion][Epoch 6669] diffusion learning rate: 0.001
2024-11-05 01:08:10,211 - INFO - [diffusion][Epoch 6669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:10,212 - INFO - [diffusion][Epoch 6670] Epoch 6671/12000
2024-11-05 01:08:14,339 - INFO - [diffusion][Epoch 6670] diffusion training Loss: 0.05868884176015854
2024-11-05 01:08:14,341 - INFO - [diffusion][Epoch 6670] diffusion learning rate: 0.001
2024-11-05 01:08:14,390 - INFO - [diffusion][Epoch 6670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:14,392 - INFO - [diffusion][Epoch 6671] Epoch 6672/12000
2024-11-05 01:08:18,528 - INFO - [diffusion][Epoch 6671] diffusion training Loss: 0.057710565626621246
2024-11-05 01:08:18,530 - INFO - [diffusion][Epoch 6671] diffusion learning rate: 0.001
2024-11-05 01:08:18,532 - INFO - [diffusion][Epoch 6671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:18,533 - INFO - [diffusion][Epoch 6672] Epoch 6673/12000
2024-11-05 01:08:22,652 - INFO - [diffusion][Epoch 6672] diffusion training Loss: 0.05539287533611059
2024-11-05 01:08:22,655 - INFO - [diffusion][Epoch 6672] diffusion learning rate: 0.001
2024-11-05 01:08:22,657 - INFO - [diffusion][Epoch 6672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:22,658 - INFO - [diffusion][Epoch 6673] Epoch 6674/12000
2024-11-05 01:08:26,776 - INFO - [diffusion][Epoch 6673] diffusion training Loss: 0.05850662011653185
2024-11-05 01:08:26,778 - INFO - [diffusion][Epoch 6673] diffusion learning rate: 0.001
2024-11-05 01:08:26,780 - INFO - [diffusion][Epoch 6673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:26,781 - INFO - [diffusion][Epoch 6674] Epoch 6675/12000
2024-11-05 01:08:30,882 - INFO - [diffusion][Epoch 6674] diffusion training Loss: 0.0619934955611825
2024-11-05 01:08:30,885 - INFO - [diffusion][Epoch 6674] diffusion learning rate: 0.001
2024-11-05 01:08:30,924 - INFO - [diffusion][Epoch 6674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:30,926 - INFO - [diffusion][Epoch 6675] Epoch 6676/12000
2024-11-05 01:08:34,879 - INFO - [diffusion][Epoch 6675] diffusion training Loss: 0.05981009267270565
2024-11-05 01:08:34,881 - INFO - [diffusion][Epoch 6675] diffusion learning rate: 0.001
2024-11-05 01:08:34,883 - INFO - [diffusion][Epoch 6675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:34,884 - INFO - [diffusion][Epoch 6676] Epoch 6677/12000
2024-11-05 01:08:38,989 - INFO - [diffusion][Epoch 6676] diffusion training Loss: 0.06331144366413355
2024-11-05 01:08:38,991 - INFO - [diffusion][Epoch 6676] diffusion learning rate: 0.001
2024-11-05 01:08:38,993 - INFO - [diffusion][Epoch 6676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:38,994 - INFO - [diffusion][Epoch 6677] Epoch 6678/12000
2024-11-05 01:08:43,127 - INFO - [diffusion][Epoch 6677] diffusion training Loss: 0.06096085533499718
2024-11-05 01:08:43,129 - INFO - [diffusion][Epoch 6677] diffusion learning rate: 0.001
2024-11-05 01:08:43,130 - INFO - [diffusion][Epoch 6677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:43,132 - INFO - [diffusion][Epoch 6678] Epoch 6679/12000
2024-11-05 01:08:47,232 - INFO - [diffusion][Epoch 6678] diffusion training Loss: 0.054035600274801254
2024-11-05 01:08:47,234 - INFO - [diffusion][Epoch 6678] diffusion learning rate: 0.001
2024-11-05 01:08:47,235 - INFO - [diffusion][Epoch 6678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:47,237 - INFO - [diffusion][Epoch 6679] Epoch 6680/12000
2024-11-05 01:08:51,556 - INFO - [diffusion][Epoch 6679] diffusion training Loss: 0.05716611351817846
2024-11-05 01:08:51,558 - INFO - [diffusion][Epoch 6679] diffusion learning rate: 0.001
2024-11-05 01:08:51,559 - INFO - [diffusion][Epoch 6679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:51,561 - INFO - [diffusion][Epoch 6680] Epoch 6681/12000
2024-11-05 01:08:55,611 - INFO - [diffusion][Epoch 6680] diffusion training Loss: 0.05440455675125122
2024-11-05 01:08:55,613 - INFO - [diffusion][Epoch 6680] diffusion learning rate: 0.001
2024-11-05 01:08:55,644 - INFO - [diffusion][Epoch 6680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:55,645 - INFO - [diffusion][Epoch 6681] Epoch 6682/12000
2024-11-05 01:08:59,699 - INFO - [diffusion][Epoch 6681] diffusion training Loss: 0.05996575299650431
2024-11-05 01:08:59,701 - INFO - [diffusion][Epoch 6681] diffusion learning rate: 0.001
2024-11-05 01:08:59,702 - INFO - [diffusion][Epoch 6681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:08:59,703 - INFO - [diffusion][Epoch 6682] Epoch 6683/12000
2024-11-05 01:09:03,824 - INFO - [diffusion][Epoch 6682] diffusion training Loss: 0.05937367305159569
2024-11-05 01:09:03,826 - INFO - [diffusion][Epoch 6682] diffusion learning rate: 0.001
2024-11-05 01:09:03,829 - INFO - [diffusion][Epoch 6682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:03,830 - INFO - [diffusion][Epoch 6683] Epoch 6684/12000
2024-11-05 01:09:08,010 - INFO - [diffusion][Epoch 6683] diffusion training Loss: 0.06618425901979208
2024-11-05 01:09:08,012 - INFO - [diffusion][Epoch 6683] diffusion learning rate: 0.001
2024-11-05 01:09:08,014 - INFO - [diffusion][Epoch 6683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:08,015 - INFO - [diffusion][Epoch 6684] Epoch 6685/12000
2024-11-05 01:09:12,084 - INFO - [diffusion][Epoch 6684] diffusion training Loss: 0.05535736680030823
2024-11-05 01:09:12,086 - INFO - [diffusion][Epoch 6684] diffusion learning rate: 0.001
2024-11-05 01:09:12,158 - INFO - [diffusion][Epoch 6684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:12,160 - INFO - [diffusion][Epoch 6685] Epoch 6686/12000
2024-11-05 01:09:16,268 - INFO - [diffusion][Epoch 6685] diffusion training Loss: 0.05913787707686424
2024-11-05 01:09:16,270 - INFO - [diffusion][Epoch 6685] diffusion learning rate: 0.001
2024-11-05 01:09:16,272 - INFO - [diffusion][Epoch 6685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:16,273 - INFO - [diffusion][Epoch 6686] Epoch 6687/12000
2024-11-05 01:09:20,413 - INFO - [diffusion][Epoch 6686] diffusion training Loss: 0.05813561845570803
2024-11-05 01:09:20,415 - INFO - [diffusion][Epoch 6686] diffusion learning rate: 0.001
2024-11-05 01:09:20,417 - INFO - [diffusion][Epoch 6686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:20,419 - INFO - [diffusion][Epoch 6687] Epoch 6688/12000
2024-11-05 01:09:24,542 - INFO - [diffusion][Epoch 6687] diffusion training Loss: 0.06573080085217953
2024-11-05 01:09:24,545 - INFO - [diffusion][Epoch 6687] diffusion learning rate: 0.001
2024-11-05 01:09:24,546 - INFO - [diffusion][Epoch 6687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:24,548 - INFO - [diffusion][Epoch 6688] Epoch 6689/12000
2024-11-05 01:09:28,722 - INFO - [diffusion][Epoch 6688] diffusion training Loss: 0.0645823497325182
2024-11-05 01:09:28,724 - INFO - [diffusion][Epoch 6688] diffusion learning rate: 0.001
2024-11-05 01:09:28,726 - INFO - [diffusion][Epoch 6688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:28,727 - INFO - [diffusion][Epoch 6689] Epoch 6690/12000
2024-11-05 01:09:32,912 - INFO - [diffusion][Epoch 6689] diffusion training Loss: 0.0557628246024251
2024-11-05 01:09:32,914 - INFO - [diffusion][Epoch 6689] diffusion learning rate: 0.001
2024-11-05 01:09:32,916 - INFO - [diffusion][Epoch 6689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:32,917 - INFO - [diffusion][Epoch 6690] Epoch 6691/12000
2024-11-05 01:09:37,077 - INFO - [diffusion][Epoch 6690] diffusion training Loss: 0.05916539952158928
2024-11-05 01:09:37,079 - INFO - [diffusion][Epoch 6690] diffusion learning rate: 0.001
2024-11-05 01:09:37,081 - INFO - [diffusion][Epoch 6690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:37,082 - INFO - [diffusion][Epoch 6691] Epoch 6692/12000
2024-11-05 01:09:41,190 - INFO - [diffusion][Epoch 6691] diffusion training Loss: 0.06611405871808529
2024-11-05 01:09:41,192 - INFO - [diffusion][Epoch 6691] diffusion learning rate: 0.001
2024-11-05 01:09:41,194 - INFO - [diffusion][Epoch 6691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:41,195 - INFO - [diffusion][Epoch 6692] Epoch 6693/12000
2024-11-05 01:09:45,337 - INFO - [diffusion][Epoch 6692] diffusion training Loss: 0.059555417858064175
2024-11-05 01:09:45,342 - INFO - [diffusion][Epoch 6692] diffusion learning rate: 0.001
2024-11-05 01:09:45,345 - INFO - [diffusion][Epoch 6692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:45,347 - INFO - [diffusion][Epoch 6693] Epoch 6694/12000
2024-11-05 01:09:49,278 - INFO - [diffusion][Epoch 6693] diffusion training Loss: 0.060311419889330864
2024-11-05 01:09:49,280 - INFO - [diffusion][Epoch 6693] diffusion learning rate: 0.001
2024-11-05 01:09:49,281 - INFO - [diffusion][Epoch 6693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:49,283 - INFO - [diffusion][Epoch 6694] Epoch 6695/12000
2024-11-05 01:09:53,453 - INFO - [diffusion][Epoch 6694] diffusion training Loss: 0.059409831650555134
2024-11-05 01:09:53,455 - INFO - [diffusion][Epoch 6694] diffusion learning rate: 0.001
2024-11-05 01:09:53,457 - INFO - [diffusion][Epoch 6694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:53,459 - INFO - [diffusion][Epoch 6695] Epoch 6696/12000
2024-11-05 01:09:57,600 - INFO - [diffusion][Epoch 6695] diffusion training Loss: 0.060160886496305466
2024-11-05 01:09:57,603 - INFO - [diffusion][Epoch 6695] diffusion learning rate: 0.001
2024-11-05 01:09:57,604 - INFO - [diffusion][Epoch 6695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:09:57,606 - INFO - [diffusion][Epoch 6696] Epoch 6697/12000
2024-11-05 01:10:01,720 - INFO - [diffusion][Epoch 6696] diffusion training Loss: 0.06167343445122242
2024-11-05 01:10:01,722 - INFO - [diffusion][Epoch 6696] diffusion learning rate: 0.001
2024-11-05 01:10:01,723 - INFO - [diffusion][Epoch 6696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:01,725 - INFO - [diffusion][Epoch 6697] Epoch 6698/12000
2024-11-05 01:10:05,786 - INFO - [diffusion][Epoch 6697] diffusion training Loss: 0.058879878371953964
2024-11-05 01:10:05,788 - INFO - [diffusion][Epoch 6697] diffusion learning rate: 0.001
2024-11-05 01:10:05,791 - INFO - [diffusion][Epoch 6697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:05,792 - INFO - [diffusion][Epoch 6698] Epoch 6699/12000
2024-11-05 01:10:09,900 - INFO - [diffusion][Epoch 6698] diffusion training Loss: 0.055877890437841415
2024-11-05 01:10:09,902 - INFO - [diffusion][Epoch 6698] diffusion learning rate: 0.001
2024-11-05 01:10:09,904 - INFO - [diffusion][Epoch 6698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:09,905 - INFO - [diffusion][Epoch 6699] Epoch 6700/12000
2024-11-05 01:10:13,924 - INFO - [diffusion][Epoch 6699] diffusion training Loss: 0.0612836554646492
2024-11-05 01:10:13,926 - INFO - [diffusion][Epoch 6699] diffusion learning rate: 0.001
2024-11-05 01:10:13,927 - INFO - [diffusion][Epoch 6699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:13,929 - INFO - [diffusion][Epoch 6700] Epoch 6701/12000
2024-11-05 01:10:18,013 - INFO - [diffusion][Epoch 6700] diffusion training Loss: 0.06657376885414124
2024-11-05 01:10:18,015 - INFO - [diffusion][Epoch 6700] diffusion learning rate: 0.001
2024-11-05 01:10:18,017 - INFO - [diffusion][Epoch 6700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:18,018 - INFO - [diffusion][Epoch 6701] Epoch 6702/12000
2024-11-05 01:10:21,929 - INFO - [diffusion][Epoch 6701] diffusion training Loss: 0.06019067484885454
2024-11-05 01:10:21,931 - INFO - [diffusion][Epoch 6701] diffusion learning rate: 0.001
2024-11-05 01:10:21,933 - INFO - [diffusion][Epoch 6701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:21,934 - INFO - [diffusion][Epoch 6702] Epoch 6703/12000
2024-11-05 01:10:26,045 - INFO - [diffusion][Epoch 6702] diffusion training Loss: 0.06097187753766775
2024-11-05 01:10:26,048 - INFO - [diffusion][Epoch 6702] diffusion learning rate: 0.001
2024-11-05 01:10:26,050 - INFO - [diffusion][Epoch 6702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:26,051 - INFO - [diffusion][Epoch 6703] Epoch 6704/12000
2024-11-05 01:10:30,106 - INFO - [diffusion][Epoch 6703] diffusion training Loss: 0.05701849889010191
2024-11-05 01:10:30,108 - INFO - [diffusion][Epoch 6703] diffusion learning rate: 0.001
2024-11-05 01:10:30,110 - INFO - [diffusion][Epoch 6703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:30,111 - INFO - [diffusion][Epoch 6704] Epoch 6705/12000
2024-11-05 01:10:34,181 - INFO - [diffusion][Epoch 6704] diffusion training Loss: 0.06132012140005827
2024-11-05 01:10:34,183 - INFO - [diffusion][Epoch 6704] diffusion learning rate: 0.001
2024-11-05 01:10:34,185 - INFO - [diffusion][Epoch 6704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:34,186 - INFO - [diffusion][Epoch 6705] Epoch 6706/12000
2024-11-05 01:10:38,171 - INFO - [diffusion][Epoch 6705] diffusion training Loss: 0.05849220510572195
2024-11-05 01:10:38,173 - INFO - [diffusion][Epoch 6705] diffusion learning rate: 0.001
2024-11-05 01:10:38,175 - INFO - [diffusion][Epoch 6705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:38,176 - INFO - [diffusion][Epoch 6706] Epoch 6707/12000
2024-11-05 01:10:42,215 - INFO - [diffusion][Epoch 6706] diffusion training Loss: 0.057519903406500816
2024-11-05 01:10:42,217 - INFO - [diffusion][Epoch 6706] diffusion learning rate: 0.001
2024-11-05 01:10:42,219 - INFO - [diffusion][Epoch 6706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:42,220 - INFO - [diffusion][Epoch 6707] Epoch 6708/12000
2024-11-05 01:10:46,191 - INFO - [diffusion][Epoch 6707] diffusion training Loss: 0.05559972859919071
2024-11-05 01:10:46,193 - INFO - [diffusion][Epoch 6707] diffusion learning rate: 0.001
2024-11-05 01:10:46,195 - INFO - [diffusion][Epoch 6707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:46,196 - INFO - [diffusion][Epoch 6708] Epoch 6709/12000
2024-11-05 01:10:50,298 - INFO - [diffusion][Epoch 6708] diffusion training Loss: 0.0585564523935318
2024-11-05 01:10:50,300 - INFO - [diffusion][Epoch 6708] diffusion learning rate: 0.001
2024-11-05 01:10:50,302 - INFO - [diffusion][Epoch 6708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:50,303 - INFO - [diffusion][Epoch 6709] Epoch 6710/12000
2024-11-05 01:10:54,408 - INFO - [diffusion][Epoch 6709] diffusion training Loss: 0.060240888968110085
2024-11-05 01:10:54,411 - INFO - [diffusion][Epoch 6709] diffusion learning rate: 0.001
2024-11-05 01:10:54,414 - INFO - [diffusion][Epoch 6709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:54,415 - INFO - [diffusion][Epoch 6710] Epoch 6711/12000
2024-11-05 01:10:58,475 - INFO - [diffusion][Epoch 6710] diffusion training Loss: 0.05820358172059059
2024-11-05 01:10:58,477 - INFO - [diffusion][Epoch 6710] diffusion learning rate: 0.001
2024-11-05 01:10:58,479 - INFO - [diffusion][Epoch 6710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:10:58,481 - INFO - [diffusion][Epoch 6711] Epoch 6712/12000
2024-11-05 01:11:02,610 - INFO - [diffusion][Epoch 6711] diffusion training Loss: 0.05843653529882431
2024-11-05 01:11:02,612 - INFO - [diffusion][Epoch 6711] diffusion learning rate: 0.001
2024-11-05 01:11:02,683 - INFO - [diffusion][Epoch 6711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:02,685 - INFO - [diffusion][Epoch 6712] Epoch 6713/12000
2024-11-05 01:11:06,834 - INFO - [diffusion][Epoch 6712] diffusion training Loss: 0.06118094176054001
2024-11-05 01:11:06,836 - INFO - [diffusion][Epoch 6712] diffusion learning rate: 0.001
2024-11-05 01:11:06,838 - INFO - [diffusion][Epoch 6712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:06,839 - INFO - [diffusion][Epoch 6713] Epoch 6714/12000
2024-11-05 01:11:10,976 - INFO - [diffusion][Epoch 6713] diffusion training Loss: 0.05833026394248009
2024-11-05 01:11:10,979 - INFO - [diffusion][Epoch 6713] diffusion learning rate: 0.001
2024-11-05 01:11:10,981 - INFO - [diffusion][Epoch 6713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:10,983 - INFO - [diffusion][Epoch 6714] Epoch 6715/12000
2024-11-05 01:11:15,123 - INFO - [diffusion][Epoch 6714] diffusion training Loss: 0.06140861101448536
2024-11-05 01:11:15,125 - INFO - [diffusion][Epoch 6714] diffusion learning rate: 0.001
2024-11-05 01:11:15,126 - INFO - [diffusion][Epoch 6714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:15,128 - INFO - [diffusion][Epoch 6715] Epoch 6716/12000
2024-11-05 01:11:19,282 - INFO - [diffusion][Epoch 6715] diffusion training Loss: 0.05607455875724554
2024-11-05 01:11:19,301 - INFO - [diffusion][Epoch 6715] diffusion learning rate: 0.001
2024-11-05 01:11:19,303 - INFO - [diffusion][Epoch 6715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:19,304 - INFO - [diffusion][Epoch 6716] Epoch 6717/12000
2024-11-05 01:11:23,289 - INFO - [diffusion][Epoch 6716] diffusion training Loss: 0.06306367926299572
2024-11-05 01:11:23,292 - INFO - [diffusion][Epoch 6716] diffusion learning rate: 0.001
2024-11-05 01:11:23,294 - INFO - [diffusion][Epoch 6716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:23,295 - INFO - [diffusion][Epoch 6717] Epoch 6718/12000
2024-11-05 01:11:27,288 - INFO - [diffusion][Epoch 6717] diffusion training Loss: 0.058655581437051296
2024-11-05 01:11:27,291 - INFO - [diffusion][Epoch 6717] diffusion learning rate: 0.001
2024-11-05 01:11:27,293 - INFO - [diffusion][Epoch 6717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:27,294 - INFO - [diffusion][Epoch 6718] Epoch 6719/12000
2024-11-05 01:11:31,287 - INFO - [diffusion][Epoch 6718] diffusion training Loss: 0.05661054793745279
2024-11-05 01:11:31,289 - INFO - [diffusion][Epoch 6718] diffusion learning rate: 0.001
2024-11-05 01:11:31,291 - INFO - [diffusion][Epoch 6718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:31,292 - INFO - [diffusion][Epoch 6719] Epoch 6720/12000
2024-11-05 01:11:35,198 - INFO - [diffusion][Epoch 6719] diffusion training Loss: 0.055052733048796654
2024-11-05 01:11:35,200 - INFO - [diffusion][Epoch 6719] diffusion learning rate: 0.001
2024-11-05 01:11:35,202 - INFO - [diffusion][Epoch 6719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:35,203 - INFO - [diffusion][Epoch 6720] Epoch 6721/12000
2024-11-05 01:11:39,289 - INFO - [diffusion][Epoch 6720] diffusion training Loss: 0.05837804824113846
2024-11-05 01:11:39,292 - INFO - [diffusion][Epoch 6720] diffusion learning rate: 0.001
2024-11-05 01:11:39,294 - INFO - [diffusion][Epoch 6720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:39,295 - INFO - [diffusion][Epoch 6721] Epoch 6722/12000
2024-11-05 01:11:43,375 - INFO - [diffusion][Epoch 6721] diffusion training Loss: 0.05871258769184351
2024-11-05 01:11:43,377 - INFO - [diffusion][Epoch 6721] diffusion learning rate: 0.001
2024-11-05 01:11:43,378 - INFO - [diffusion][Epoch 6721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:43,380 - INFO - [diffusion][Epoch 6722] Epoch 6723/12000
2024-11-05 01:11:48,120 - INFO - [diffusion][Epoch 6722] diffusion training Loss: 0.05693880561739206
2024-11-05 01:11:48,122 - INFO - [diffusion][Epoch 6722] diffusion learning rate: 0.001
2024-11-05 01:11:48,123 - INFO - [diffusion][Epoch 6722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:48,125 - INFO - [diffusion][Epoch 6723] Epoch 6724/12000
2024-11-05 01:11:52,188 - INFO - [diffusion][Epoch 6723] diffusion training Loss: 0.06039446499198675
2024-11-05 01:11:52,190 - INFO - [diffusion][Epoch 6723] diffusion learning rate: 0.001
2024-11-05 01:11:52,192 - INFO - [diffusion][Epoch 6723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:52,193 - INFO - [diffusion][Epoch 6724] Epoch 6725/12000
2024-11-05 01:11:56,267 - INFO - [diffusion][Epoch 6724] diffusion training Loss: 0.054535805247724056
2024-11-05 01:11:56,269 - INFO - [diffusion][Epoch 6724] diffusion learning rate: 0.001
2024-11-05 01:11:56,271 - INFO - [diffusion][Epoch 6724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:11:56,272 - INFO - [diffusion][Epoch 6725] Epoch 6726/12000
2024-11-05 01:12:00,346 - INFO - [diffusion][Epoch 6725] diffusion training Loss: 0.05940511729568243
2024-11-05 01:12:00,348 - INFO - [diffusion][Epoch 6725] diffusion learning rate: 0.001
2024-11-05 01:12:00,350 - INFO - [diffusion][Epoch 6725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:00,351 - INFO - [diffusion][Epoch 6726] Epoch 6727/12000
2024-11-05 01:12:04,471 - INFO - [diffusion][Epoch 6726] diffusion training Loss: 0.06015733536332846
2024-11-05 01:12:04,473 - INFO - [diffusion][Epoch 6726] diffusion learning rate: 0.001
2024-11-05 01:12:04,508 - INFO - [diffusion][Epoch 6726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:04,509 - INFO - [diffusion][Epoch 6727] Epoch 6728/12000
2024-11-05 01:12:08,665 - INFO - [diffusion][Epoch 6727] diffusion training Loss: 0.05962100625038147
2024-11-05 01:12:08,667 - INFO - [diffusion][Epoch 6727] diffusion learning rate: 0.001
2024-11-05 01:12:08,669 - INFO - [diffusion][Epoch 6727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:08,670 - INFO - [diffusion][Epoch 6728] Epoch 6729/12000
2024-11-05 01:12:12,785 - INFO - [diffusion][Epoch 6728] diffusion training Loss: 0.06100056879222393
2024-11-05 01:12:12,787 - INFO - [diffusion][Epoch 6728] diffusion learning rate: 0.001
2024-11-05 01:12:12,788 - INFO - [diffusion][Epoch 6728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:12,789 - INFO - [diffusion][Epoch 6729] Epoch 6730/12000
2024-11-05 01:12:16,799 - INFO - [diffusion][Epoch 6729] diffusion training Loss: 0.058311705477535725
2024-11-05 01:12:16,801 - INFO - [diffusion][Epoch 6729] diffusion learning rate: 0.001
2024-11-05 01:12:16,803 - INFO - [diffusion][Epoch 6729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:16,804 - INFO - [diffusion][Epoch 6730] Epoch 6731/12000
2024-11-05 01:12:20,823 - INFO - [diffusion][Epoch 6730] diffusion training Loss: 0.057073818519711494
2024-11-05 01:12:20,825 - INFO - [diffusion][Epoch 6730] diffusion learning rate: 0.001
2024-11-05 01:12:20,827 - INFO - [diffusion][Epoch 6730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:20,829 - INFO - [diffusion][Epoch 6731] Epoch 6732/12000
2024-11-05 01:12:24,703 - INFO - [diffusion][Epoch 6731] diffusion training Loss: 0.05908027198165655
2024-11-05 01:12:24,705 - INFO - [diffusion][Epoch 6731] diffusion learning rate: 0.001
2024-11-05 01:12:24,707 - INFO - [diffusion][Epoch 6731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:24,709 - INFO - [diffusion][Epoch 6732] Epoch 6733/12000
2024-11-05 01:12:28,676 - INFO - [diffusion][Epoch 6732] diffusion training Loss: 0.06092030927538872
2024-11-05 01:12:28,678 - INFO - [diffusion][Epoch 6732] diffusion learning rate: 0.001
2024-11-05 01:12:28,680 - INFO - [diffusion][Epoch 6732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:28,681 - INFO - [diffusion][Epoch 6733] Epoch 6734/12000
2024-11-05 01:12:32,782 - INFO - [diffusion][Epoch 6733] diffusion training Loss: 0.05571248009800911
2024-11-05 01:12:32,785 - INFO - [diffusion][Epoch 6733] diffusion learning rate: 0.001
2024-11-05 01:12:32,787 - INFO - [diffusion][Epoch 6733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:32,788 - INFO - [diffusion][Epoch 6734] Epoch 6735/12000
2024-11-05 01:12:36,880 - INFO - [diffusion][Epoch 6734] diffusion training Loss: 0.0584022356197238
2024-11-05 01:12:36,883 - INFO - [diffusion][Epoch 6734] diffusion learning rate: 0.001
2024-11-05 01:12:36,884 - INFO - [diffusion][Epoch 6734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:36,886 - INFO - [diffusion][Epoch 6735] Epoch 6736/12000
2024-11-05 01:12:40,981 - INFO - [diffusion][Epoch 6735] diffusion training Loss: 0.05478218197822571
2024-11-05 01:12:40,998 - INFO - [diffusion][Epoch 6735] diffusion learning rate: 0.001
2024-11-05 01:12:41,000 - INFO - [diffusion][Epoch 6735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:41,002 - INFO - [diffusion][Epoch 6736] Epoch 6737/12000
2024-11-05 01:12:44,974 - INFO - [diffusion][Epoch 6736] diffusion training Loss: 0.055371745489537716
2024-11-05 01:12:44,977 - INFO - [diffusion][Epoch 6736] diffusion learning rate: 0.001
2024-11-05 01:12:44,979 - INFO - [diffusion][Epoch 6736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:44,981 - INFO - [diffusion][Epoch 6737] Epoch 6738/12000
2024-11-05 01:12:48,858 - INFO - [diffusion][Epoch 6737] diffusion training Loss: 0.052724157460033894
2024-11-05 01:12:48,860 - INFO - [diffusion][Epoch 6737] diffusion learning rate: 0.001
2024-11-05 01:12:48,861 - INFO - [diffusion][Epoch 6737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:48,863 - INFO - [diffusion][Epoch 6738] Epoch 6739/12000
2024-11-05 01:12:52,926 - INFO - [diffusion][Epoch 6738] diffusion training Loss: 0.06064346805214882
2024-11-05 01:12:52,928 - INFO - [diffusion][Epoch 6738] diffusion learning rate: 0.001
2024-11-05 01:12:52,930 - INFO - [diffusion][Epoch 6738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:52,931 - INFO - [diffusion][Epoch 6739] Epoch 6740/12000
2024-11-05 01:12:57,039 - INFO - [diffusion][Epoch 6739] diffusion training Loss: 0.05556565057486296
2024-11-05 01:12:57,041 - INFO - [diffusion][Epoch 6739] diffusion learning rate: 0.001
2024-11-05 01:12:57,043 - INFO - [diffusion][Epoch 6739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:12:57,044 - INFO - [diffusion][Epoch 6740] Epoch 6741/12000
2024-11-05 01:13:01,112 - INFO - [diffusion][Epoch 6740] diffusion training Loss: 0.057236358523368835
2024-11-05 01:13:01,114 - INFO - [diffusion][Epoch 6740] diffusion learning rate: 0.001
2024-11-05 01:13:01,116 - INFO - [diffusion][Epoch 6740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:01,117 - INFO - [diffusion][Epoch 6741] Epoch 6742/12000
2024-11-05 01:13:05,280 - INFO - [diffusion][Epoch 6741] diffusion training Loss: 0.056303082033991814
2024-11-05 01:13:05,282 - INFO - [diffusion][Epoch 6741] diffusion learning rate: 0.001
2024-11-05 01:13:05,284 - INFO - [diffusion][Epoch 6741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:05,285 - INFO - [diffusion][Epoch 6742] Epoch 6743/12000
2024-11-05 01:13:09,658 - INFO - [diffusion][Epoch 6742] diffusion training Loss: 0.05867540091276169
2024-11-05 01:13:09,660 - INFO - [diffusion][Epoch 6742] diffusion learning rate: 0.001
2024-11-05 01:13:09,662 - INFO - [diffusion][Epoch 6742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:09,663 - INFO - [diffusion][Epoch 6743] Epoch 6744/12000
2024-11-05 01:13:13,669 - INFO - [diffusion][Epoch 6743] diffusion training Loss: 0.05943349841982126
2024-11-05 01:13:13,671 - INFO - [diffusion][Epoch 6743] diffusion learning rate: 0.001
2024-11-05 01:13:13,673 - INFO - [diffusion][Epoch 6743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:13,675 - INFO - [diffusion][Epoch 6744] Epoch 6745/12000
2024-11-05 01:13:17,726 - INFO - [diffusion][Epoch 6744] diffusion training Loss: 0.06019946373999119
2024-11-05 01:13:17,729 - INFO - [diffusion][Epoch 6744] diffusion learning rate: 0.001
2024-11-05 01:13:17,731 - INFO - [diffusion][Epoch 6744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:17,732 - INFO - [diffusion][Epoch 6745] Epoch 6746/12000
2024-11-05 01:13:21,776 - INFO - [diffusion][Epoch 6745] diffusion training Loss: 0.06258251424878836
2024-11-05 01:13:21,778 - INFO - [diffusion][Epoch 6745] diffusion learning rate: 0.001
2024-11-05 01:13:21,780 - INFO - [diffusion][Epoch 6745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:21,781 - INFO - [diffusion][Epoch 6746] Epoch 6747/12000
2024-11-05 01:13:25,772 - INFO - [diffusion][Epoch 6746] diffusion training Loss: 0.06152775976806879
2024-11-05 01:13:25,774 - INFO - [diffusion][Epoch 6746] diffusion learning rate: 0.001
2024-11-05 01:13:25,776 - INFO - [diffusion][Epoch 6746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:25,778 - INFO - [diffusion][Epoch 6747] Epoch 6748/12000
2024-11-05 01:13:29,831 - INFO - [diffusion][Epoch 6747] diffusion training Loss: 0.0582913001999259
2024-11-05 01:13:29,834 - INFO - [diffusion][Epoch 6747] diffusion learning rate: 0.001
2024-11-05 01:13:29,836 - INFO - [diffusion][Epoch 6747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:29,838 - INFO - [diffusion][Epoch 6748] Epoch 6749/12000
2024-11-05 01:13:33,808 - INFO - [diffusion][Epoch 6748] diffusion training Loss: 0.05952270142734051
2024-11-05 01:13:33,810 - INFO - [diffusion][Epoch 6748] diffusion learning rate: 0.001
2024-11-05 01:13:33,812 - INFO - [diffusion][Epoch 6748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:33,813 - INFO - [diffusion][Epoch 6749] Epoch 6750/12000
2024-11-05 01:13:37,969 - INFO - [diffusion][Epoch 6749] diffusion training Loss: 0.054040865041315556
2024-11-05 01:13:37,971 - INFO - [diffusion][Epoch 6749] diffusion learning rate: 0.001
2024-11-05 01:13:37,973 - INFO - [diffusion][Epoch 6749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:37,974 - INFO - [diffusion][Epoch 6750] Epoch 6751/12000
2024-11-05 01:13:42,115 - INFO - [diffusion][Epoch 6750] diffusion training Loss: 0.056379334069788456
2024-11-05 01:13:42,118 - INFO - [diffusion][Epoch 6750] diffusion learning rate: 0.001
2024-11-05 01:13:42,120 - INFO - [diffusion][Epoch 6750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:42,121 - INFO - [diffusion][Epoch 6751] Epoch 6752/12000
2024-11-05 01:13:46,253 - INFO - [diffusion][Epoch 6751] diffusion training Loss: 0.059215230867266655
2024-11-05 01:13:46,255 - INFO - [diffusion][Epoch 6751] diffusion learning rate: 0.001
2024-11-05 01:13:46,257 - INFO - [diffusion][Epoch 6751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:46,259 - INFO - [diffusion][Epoch 6752] Epoch 6753/12000
2024-11-05 01:13:50,355 - INFO - [diffusion][Epoch 6752] diffusion training Loss: 0.06218758784234524
2024-11-05 01:13:50,357 - INFO - [diffusion][Epoch 6752] diffusion learning rate: 0.001
2024-11-05 01:13:50,359 - INFO - [diffusion][Epoch 6752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:50,360 - INFO - [diffusion][Epoch 6753] Epoch 6754/12000
2024-11-05 01:13:54,525 - INFO - [diffusion][Epoch 6753] diffusion training Loss: 0.05478569120168686
2024-11-05 01:13:54,527 - INFO - [diffusion][Epoch 6753] diffusion learning rate: 0.001
2024-11-05 01:13:54,529 - INFO - [diffusion][Epoch 6753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:54,530 - INFO - [diffusion][Epoch 6754] Epoch 6755/12000
2024-11-05 01:13:58,687 - INFO - [diffusion][Epoch 6754] diffusion training Loss: 0.05742207262665033
2024-11-05 01:13:58,688 - INFO - [diffusion][Epoch 6754] diffusion learning rate: 0.001
2024-11-05 01:13:58,690 - INFO - [diffusion][Epoch 6754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:13:58,691 - INFO - [diffusion][Epoch 6755] Epoch 6756/12000
2024-11-05 01:14:02,765 - INFO - [diffusion][Epoch 6755] diffusion training Loss: 0.05382668413221836
2024-11-05 01:14:02,767 - INFO - [diffusion][Epoch 6755] diffusion learning rate: 0.001
2024-11-05 01:14:02,769 - INFO - [diffusion][Epoch 6755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:02,770 - INFO - [diffusion][Epoch 6756] Epoch 6757/12000
2024-11-05 01:14:06,868 - INFO - [diffusion][Epoch 6756] diffusion training Loss: 0.05858696158975363
2024-11-05 01:14:06,871 - INFO - [diffusion][Epoch 6756] diffusion learning rate: 0.001
2024-11-05 01:14:06,872 - INFO - [diffusion][Epoch 6756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:06,874 - INFO - [diffusion][Epoch 6757] Epoch 6758/12000
2024-11-05 01:14:11,009 - INFO - [diffusion][Epoch 6757] diffusion training Loss: 0.06171839218586683
2024-11-05 01:14:11,011 - INFO - [diffusion][Epoch 6757] diffusion learning rate: 0.001
2024-11-05 01:14:11,012 - INFO - [diffusion][Epoch 6757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:11,014 - INFO - [diffusion][Epoch 6758] Epoch 6759/12000
2024-11-05 01:14:15,136 - INFO - [diffusion][Epoch 6758] diffusion training Loss: 0.05540491361171007
2024-11-05 01:14:15,573 - INFO - [diffusion][Epoch 6758] diffusion learning rate: 0.001
2024-11-05 01:14:15,575 - INFO - [diffusion][Epoch 6758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:15,576 - INFO - [diffusion][Epoch 6759] Epoch 6760/12000
2024-11-05 01:14:19,727 - INFO - [diffusion][Epoch 6759] diffusion training Loss: 0.06193395145237446
2024-11-05 01:14:19,729 - INFO - [diffusion][Epoch 6759] diffusion learning rate: 0.001
2024-11-05 01:14:19,731 - INFO - [diffusion][Epoch 6759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:19,732 - INFO - [diffusion][Epoch 6760] Epoch 6761/12000
2024-11-05 01:14:23,833 - INFO - [diffusion][Epoch 6760] diffusion training Loss: 0.060801420360803604
2024-11-05 01:14:23,835 - INFO - [diffusion][Epoch 6760] diffusion learning rate: 0.001
2024-11-05 01:14:23,837 - INFO - [diffusion][Epoch 6760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:23,839 - INFO - [diffusion][Epoch 6761] Epoch 6762/12000
2024-11-05 01:14:27,964 - INFO - [diffusion][Epoch 6761] diffusion training Loss: 0.058369952253997326
2024-11-05 01:14:27,966 - INFO - [diffusion][Epoch 6761] diffusion learning rate: 0.001
2024-11-05 01:14:27,968 - INFO - [diffusion][Epoch 6761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:27,969 - INFO - [diffusion][Epoch 6762] Epoch 6763/12000
2024-11-05 01:14:31,974 - INFO - [diffusion][Epoch 6762] diffusion training Loss: 0.055297840386629105
2024-11-05 01:14:31,976 - INFO - [diffusion][Epoch 6762] diffusion learning rate: 0.001
2024-11-05 01:14:32,006 - INFO - [diffusion][Epoch 6762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:32,007 - INFO - [diffusion][Epoch 6763] Epoch 6764/12000
2024-11-05 01:14:36,102 - INFO - [diffusion][Epoch 6763] diffusion training Loss: 0.06274704355746508
2024-11-05 01:14:36,105 - INFO - [diffusion][Epoch 6763] diffusion learning rate: 0.001
2024-11-05 01:14:36,107 - INFO - [diffusion][Epoch 6763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:36,108 - INFO - [diffusion][Epoch 6764] Epoch 6765/12000
2024-11-05 01:14:40,352 - INFO - [diffusion][Epoch 6764] diffusion training Loss: 0.0577846672385931
2024-11-05 01:14:40,354 - INFO - [diffusion][Epoch 6764] diffusion learning rate: 0.001
2024-11-05 01:14:40,356 - INFO - [diffusion][Epoch 6764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:40,357 - INFO - [diffusion][Epoch 6765] Epoch 6766/12000
2024-11-05 01:14:44,471 - INFO - [diffusion][Epoch 6765] diffusion training Loss: 0.05949308816343546
2024-11-05 01:14:44,473 - INFO - [diffusion][Epoch 6765] diffusion learning rate: 0.001
2024-11-05 01:14:44,475 - INFO - [diffusion][Epoch 6765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:44,476 - INFO - [diffusion][Epoch 6766] Epoch 6767/12000
2024-11-05 01:14:48,666 - INFO - [diffusion][Epoch 6766] diffusion training Loss: 0.05410202592611313
2024-11-05 01:14:48,668 - INFO - [diffusion][Epoch 6766] diffusion learning rate: 0.001
2024-11-05 01:14:48,734 - INFO - [diffusion][Epoch 6766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:48,736 - INFO - [diffusion][Epoch 6767] Epoch 6768/12000
2024-11-05 01:14:52,837 - INFO - [diffusion][Epoch 6767] diffusion training Loss: 0.06260369997471571
2024-11-05 01:14:52,839 - INFO - [diffusion][Epoch 6767] diffusion learning rate: 0.001
2024-11-05 01:14:52,840 - INFO - [diffusion][Epoch 6767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:52,842 - INFO - [diffusion][Epoch 6768] Epoch 6769/12000
2024-11-05 01:14:56,932 - INFO - [diffusion][Epoch 6768] diffusion training Loss: 0.0573619045317173
2024-11-05 01:14:56,934 - INFO - [diffusion][Epoch 6768] diffusion learning rate: 0.001
2024-11-05 01:14:56,936 - INFO - [diffusion][Epoch 6768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:14:56,937 - INFO - [diffusion][Epoch 6769] Epoch 6770/12000
2024-11-05 01:15:01,128 - INFO - [diffusion][Epoch 6769] diffusion training Loss: 0.059467983432114124
2024-11-05 01:15:01,130 - INFO - [diffusion][Epoch 6769] diffusion learning rate: 0.001
2024-11-05 01:15:01,132 - INFO - [diffusion][Epoch 6769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:01,133 - INFO - [diffusion][Epoch 6770] Epoch 6771/12000
2024-11-05 01:15:05,246 - INFO - [diffusion][Epoch 6770] diffusion training Loss: 0.06062920019030571
2024-11-05 01:15:05,248 - INFO - [diffusion][Epoch 6770] diffusion learning rate: 0.001
2024-11-05 01:15:05,297 - INFO - [diffusion][Epoch 6770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:05,298 - INFO - [diffusion][Epoch 6771] Epoch 6772/12000
2024-11-05 01:15:09,436 - INFO - [diffusion][Epoch 6771] diffusion training Loss: 0.0632508136332035
2024-11-05 01:15:09,438 - INFO - [diffusion][Epoch 6771] diffusion learning rate: 0.001
2024-11-05 01:15:09,440 - INFO - [diffusion][Epoch 6771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:09,441 - INFO - [diffusion][Epoch 6772] Epoch 6773/12000
2024-11-05 01:15:13,433 - INFO - [diffusion][Epoch 6772] diffusion training Loss: 0.05783374886959791
2024-11-05 01:15:13,435 - INFO - [diffusion][Epoch 6772] diffusion learning rate: 0.001
2024-11-05 01:15:13,437 - INFO - [diffusion][Epoch 6772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:13,438 - INFO - [diffusion][Epoch 6773] Epoch 6774/12000
2024-11-05 01:15:17,554 - INFO - [diffusion][Epoch 6773] diffusion training Loss: 0.058595238253474236
2024-11-05 01:15:17,556 - INFO - [diffusion][Epoch 6773] diffusion learning rate: 0.001
2024-11-05 01:15:17,558 - INFO - [diffusion][Epoch 6773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:17,559 - INFO - [diffusion][Epoch 6774] Epoch 6775/12000
2024-11-05 01:15:21,690 - INFO - [diffusion][Epoch 6774] diffusion training Loss: 0.06471229158341885
2024-11-05 01:15:21,693 - INFO - [diffusion][Epoch 6774] diffusion learning rate: 0.001
2024-11-05 01:15:21,694 - INFO - [diffusion][Epoch 6774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:21,696 - INFO - [diffusion][Epoch 6775] Epoch 6776/12000
2024-11-05 01:15:25,835 - INFO - [diffusion][Epoch 6775] diffusion training Loss: 0.056988355703651905
2024-11-05 01:15:25,838 - INFO - [diffusion][Epoch 6775] diffusion learning rate: 0.001
2024-11-05 01:15:25,839 - INFO - [diffusion][Epoch 6775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:25,841 - INFO - [diffusion][Epoch 6776] Epoch 6777/12000
2024-11-05 01:15:29,975 - INFO - [diffusion][Epoch 6776] diffusion training Loss: 0.05453081615269184
2024-11-05 01:15:29,977 - INFO - [diffusion][Epoch 6776] diffusion learning rate: 0.001
2024-11-05 01:15:29,979 - INFO - [diffusion][Epoch 6776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:29,980 - INFO - [diffusion][Epoch 6777] Epoch 6778/12000
2024-11-05 01:15:34,070 - INFO - [diffusion][Epoch 6777] diffusion training Loss: 0.06343316473066807
2024-11-05 01:15:34,072 - INFO - [diffusion][Epoch 6777] diffusion learning rate: 0.001
2024-11-05 01:15:34,075 - INFO - [diffusion][Epoch 6777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:34,076 - INFO - [diffusion][Epoch 6778] Epoch 6779/12000
2024-11-05 01:15:38,034 - INFO - [diffusion][Epoch 6778] diffusion training Loss: 0.06161901634186506
2024-11-05 01:15:38,036 - INFO - [diffusion][Epoch 6778] diffusion learning rate: 0.001
2024-11-05 01:15:38,038 - INFO - [diffusion][Epoch 6778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:38,039 - INFO - [diffusion][Epoch 6779] Epoch 6780/12000
2024-11-05 01:15:42,141 - INFO - [diffusion][Epoch 6779] diffusion training Loss: 0.05749482475221157
2024-11-05 01:15:42,143 - INFO - [diffusion][Epoch 6779] diffusion learning rate: 0.001
2024-11-05 01:15:42,145 - INFO - [diffusion][Epoch 6779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:42,146 - INFO - [diffusion][Epoch 6780] Epoch 6781/12000
2024-11-05 01:15:46,117 - INFO - [diffusion][Epoch 6780] diffusion training Loss: 0.0544176921248436
2024-11-05 01:15:46,119 - INFO - [diffusion][Epoch 6780] diffusion learning rate: 0.001
2024-11-05 01:15:46,121 - INFO - [diffusion][Epoch 6780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:46,122 - INFO - [diffusion][Epoch 6781] Epoch 6782/12000
2024-11-05 01:15:49,986 - INFO - [diffusion][Epoch 6781] diffusion training Loss: 0.05672070290893316
2024-11-05 01:15:49,988 - INFO - [diffusion][Epoch 6781] diffusion learning rate: 0.001
2024-11-05 01:15:49,990 - INFO - [diffusion][Epoch 6781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:49,991 - INFO - [diffusion][Epoch 6782] Epoch 6783/12000
2024-11-05 01:15:54,119 - INFO - [diffusion][Epoch 6782] diffusion training Loss: 0.0613075140863657
2024-11-05 01:15:54,121 - INFO - [diffusion][Epoch 6782] diffusion learning rate: 0.001
2024-11-05 01:15:54,123 - INFO - [diffusion][Epoch 6782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:54,124 - INFO - [diffusion][Epoch 6783] Epoch 6784/12000
2024-11-05 01:15:58,085 - INFO - [diffusion][Epoch 6783] diffusion training Loss: 0.06277379207313061
2024-11-05 01:15:58,087 - INFO - [diffusion][Epoch 6783] diffusion learning rate: 0.001
2024-11-05 01:15:58,089 - INFO - [diffusion][Epoch 6783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:15:58,090 - INFO - [diffusion][Epoch 6784] Epoch 6785/12000
2024-11-05 01:16:02,065 - INFO - [diffusion][Epoch 6784] diffusion training Loss: 0.0640235934406519
2024-11-05 01:16:02,067 - INFO - [diffusion][Epoch 6784] diffusion learning rate: 0.001
2024-11-05 01:16:02,069 - INFO - [diffusion][Epoch 6784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:02,070 - INFO - [diffusion][Epoch 6785] Epoch 6786/12000
2024-11-05 01:16:06,267 - INFO - [diffusion][Epoch 6785] diffusion training Loss: 0.05292590521275997
2024-11-05 01:16:06,269 - INFO - [diffusion][Epoch 6785] diffusion learning rate: 0.001
2024-11-05 01:16:06,271 - INFO - [diffusion][Epoch 6785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:06,273 - INFO - [diffusion][Epoch 6786] Epoch 6787/12000
2024-11-05 01:16:10,387 - INFO - [diffusion][Epoch 6786] diffusion training Loss: 0.05998029466718435
2024-11-05 01:16:10,389 - INFO - [diffusion][Epoch 6786] diffusion learning rate: 0.001
2024-11-05 01:16:10,391 - INFO - [diffusion][Epoch 6786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:10,392 - INFO - [diffusion][Epoch 6787] Epoch 6788/12000
2024-11-05 01:16:14,360 - INFO - [diffusion][Epoch 6787] diffusion training Loss: 0.06300716754049063
2024-11-05 01:16:14,362 - INFO - [diffusion][Epoch 6787] diffusion learning rate: 0.001
2024-11-05 01:16:14,364 - INFO - [diffusion][Epoch 6787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:14,365 - INFO - [diffusion][Epoch 6788] Epoch 6789/12000
2024-11-05 01:16:18,501 - INFO - [diffusion][Epoch 6788] diffusion training Loss: 0.06013865862041712
2024-11-05 01:16:18,503 - INFO - [diffusion][Epoch 6788] diffusion learning rate: 0.001
2024-11-05 01:16:18,505 - INFO - [diffusion][Epoch 6788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:18,507 - INFO - [diffusion][Epoch 6789] Epoch 6790/12000
2024-11-05 01:16:22,643 - INFO - [diffusion][Epoch 6789] diffusion training Loss: 0.05684636253863573
2024-11-05 01:16:22,645 - INFO - [diffusion][Epoch 6789] diffusion learning rate: 0.001
2024-11-05 01:16:22,647 - INFO - [diffusion][Epoch 6789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:22,649 - INFO - [diffusion][Epoch 6790] Epoch 6791/12000
2024-11-05 01:16:26,585 - INFO - [diffusion][Epoch 6790] diffusion training Loss: 0.058726053684949875
2024-11-05 01:16:26,589 - INFO - [diffusion][Epoch 6790] diffusion learning rate: 0.001
2024-11-05 01:16:26,592 - INFO - [diffusion][Epoch 6790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:26,593 - INFO - [diffusion][Epoch 6791] Epoch 6792/12000
2024-11-05 01:16:30,626 - INFO - [diffusion][Epoch 6791] diffusion training Loss: 0.054831475019454956
2024-11-05 01:16:30,629 - INFO - [diffusion][Epoch 6791] diffusion learning rate: 0.001
2024-11-05 01:16:30,630 - INFO - [diffusion][Epoch 6791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:30,632 - INFO - [diffusion][Epoch 6792] Epoch 6793/12000
2024-11-05 01:16:34,649 - INFO - [diffusion][Epoch 6792] diffusion training Loss: 0.05826593283563852
2024-11-05 01:16:34,652 - INFO - [diffusion][Epoch 6792] diffusion learning rate: 0.001
2024-11-05 01:16:34,654 - INFO - [diffusion][Epoch 6792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:34,655 - INFO - [diffusion][Epoch 6793] Epoch 6794/12000
2024-11-05 01:16:38,732 - INFO - [diffusion][Epoch 6793] diffusion training Loss: 0.05842698831111193
2024-11-05 01:16:38,734 - INFO - [diffusion][Epoch 6793] diffusion learning rate: 0.001
2024-11-05 01:16:38,736 - INFO - [diffusion][Epoch 6793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:38,737 - INFO - [diffusion][Epoch 6794] Epoch 6795/12000
2024-11-05 01:16:42,828 - INFO - [diffusion][Epoch 6794] diffusion training Loss: 0.05689573846757412
2024-11-05 01:16:42,830 - INFO - [diffusion][Epoch 6794] diffusion learning rate: 0.001
2024-11-05 01:16:42,832 - INFO - [diffusion][Epoch 6794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:42,833 - INFO - [diffusion][Epoch 6795] Epoch 6796/12000
2024-11-05 01:16:46,980 - INFO - [diffusion][Epoch 6795] diffusion training Loss: 0.06095673609524965
2024-11-05 01:16:46,982 - INFO - [diffusion][Epoch 6795] diffusion learning rate: 0.001
2024-11-05 01:16:46,984 - INFO - [diffusion][Epoch 6795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:46,985 - INFO - [diffusion][Epoch 6796] Epoch 6797/12000
2024-11-05 01:16:50,963 - INFO - [diffusion][Epoch 6796] diffusion training Loss: 0.05614581238478422
2024-11-05 01:16:50,965 - INFO - [diffusion][Epoch 6796] diffusion learning rate: 0.001
2024-11-05 01:16:50,967 - INFO - [diffusion][Epoch 6796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:50,968 - INFO - [diffusion][Epoch 6797] Epoch 6798/12000
2024-11-05 01:16:55,103 - INFO - [diffusion][Epoch 6797] diffusion training Loss: 0.05696483142673969
2024-11-05 01:16:55,105 - INFO - [diffusion][Epoch 6797] diffusion learning rate: 0.001
2024-11-05 01:16:55,107 - INFO - [diffusion][Epoch 6797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:55,108 - INFO - [diffusion][Epoch 6798] Epoch 6799/12000
2024-11-05 01:16:59,266 - INFO - [diffusion][Epoch 6798] diffusion training Loss: 0.05413416214287281
2024-11-05 01:16:59,268 - INFO - [diffusion][Epoch 6798] diffusion learning rate: 0.001
2024-11-05 01:16:59,311 - INFO - [diffusion][Epoch 6798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:16:59,312 - INFO - [diffusion][Epoch 6799] Epoch 6800/12000
2024-11-05 01:17:03,315 - INFO - [diffusion][Epoch 6799] diffusion training Loss: 0.06380360946059227
2024-11-05 01:17:03,317 - INFO - [diffusion][Epoch 6799] diffusion learning rate: 0.001
2024-11-05 01:17:03,318 - INFO - [diffusion][Epoch 6799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:03,320 - INFO - [diffusion][Epoch 6800] Epoch 6801/12000
2024-11-05 01:17:07,283 - INFO - [diffusion][Epoch 6800] diffusion training Loss: 0.06606402434408665
2024-11-05 01:17:07,285 - INFO - [diffusion][Epoch 6800] diffusion learning rate: 0.001
2024-11-05 01:17:07,287 - INFO - [diffusion][Epoch 6800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:07,288 - INFO - [diffusion][Epoch 6801] Epoch 6802/12000
2024-11-05 01:17:11,303 - INFO - [diffusion][Epoch 6801] diffusion training Loss: 0.059267256408929825
2024-11-05 01:17:11,305 - INFO - [diffusion][Epoch 6801] diffusion learning rate: 0.001
2024-11-05 01:17:11,307 - INFO - [diffusion][Epoch 6801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:11,308 - INFO - [diffusion][Epoch 6802] Epoch 6803/12000
2024-11-05 01:17:15,403 - INFO - [diffusion][Epoch 6802] diffusion training Loss: 0.05431453511118889
2024-11-05 01:17:15,405 - INFO - [diffusion][Epoch 6802] diffusion learning rate: 0.001
2024-11-05 01:17:15,432 - INFO - [diffusion][Epoch 6802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:15,433 - INFO - [diffusion][Epoch 6803] Epoch 6804/12000
2024-11-05 01:17:19,610 - INFO - [diffusion][Epoch 6803] diffusion training Loss: 0.05886033922433853
2024-11-05 01:17:19,612 - INFO - [diffusion][Epoch 6803] diffusion learning rate: 0.001
2024-11-05 01:17:19,614 - INFO - [diffusion][Epoch 6803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:19,616 - INFO - [diffusion][Epoch 6804] Epoch 6805/12000
2024-11-05 01:17:23,776 - INFO - [diffusion][Epoch 6804] diffusion training Loss: 0.05659250635653734
2024-11-05 01:17:23,778 - INFO - [diffusion][Epoch 6804] diffusion learning rate: 0.001
2024-11-05 01:17:23,780 - INFO - [diffusion][Epoch 6804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:23,782 - INFO - [diffusion][Epoch 6805] Epoch 6806/12000
2024-11-05 01:17:27,874 - INFO - [diffusion][Epoch 6805] diffusion training Loss: 0.05820298194885254
2024-11-05 01:17:27,876 - INFO - [diffusion][Epoch 6805] diffusion learning rate: 0.001
2024-11-05 01:17:27,878 - INFO - [diffusion][Epoch 6805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:27,880 - INFO - [diffusion][Epoch 6806] Epoch 6807/12000
2024-11-05 01:17:32,603 - INFO - [diffusion][Epoch 6806] diffusion training Loss: 0.05622702278196812
2024-11-05 01:17:32,605 - INFO - [diffusion][Epoch 6806] diffusion learning rate: 0.001
2024-11-05 01:17:32,607 - INFO - [diffusion][Epoch 6806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:32,608 - INFO - [diffusion][Epoch 6807] Epoch 6808/12000
2024-11-05 01:17:36,773 - INFO - [diffusion][Epoch 6807] diffusion training Loss: 0.05709409713745117
2024-11-05 01:17:36,781 - INFO - [diffusion][Epoch 6807] diffusion learning rate: 0.001
2024-11-05 01:17:36,824 - INFO - [diffusion][Epoch 6807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:36,826 - INFO - [diffusion][Epoch 6808] Epoch 6809/12000
2024-11-05 01:17:40,992 - INFO - [diffusion][Epoch 6808] diffusion training Loss: 0.0549768740311265
2024-11-05 01:17:40,994 - INFO - [diffusion][Epoch 6808] diffusion learning rate: 0.001
2024-11-05 01:17:40,996 - INFO - [diffusion][Epoch 6808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:40,997 - INFO - [diffusion][Epoch 6809] Epoch 6810/12000
2024-11-05 01:17:45,077 - INFO - [diffusion][Epoch 6809] diffusion training Loss: 0.0547465393319726
2024-11-05 01:17:45,079 - INFO - [diffusion][Epoch 6809] diffusion learning rate: 0.001
2024-11-05 01:17:45,080 - INFO - [diffusion][Epoch 6809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:45,082 - INFO - [diffusion][Epoch 6810] Epoch 6811/12000
2024-11-05 01:17:49,225 - INFO - [diffusion][Epoch 6810] diffusion training Loss: 0.05964099708944559
2024-11-05 01:17:49,227 - INFO - [diffusion][Epoch 6810] diffusion learning rate: 0.001
2024-11-05 01:17:49,229 - INFO - [diffusion][Epoch 6810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:49,235 - INFO - [diffusion][Epoch 6811] Epoch 6812/12000
2024-11-05 01:17:53,259 - INFO - [diffusion][Epoch 6811] diffusion training Loss: 0.055967909283936024
2024-11-05 01:17:53,261 - INFO - [diffusion][Epoch 6811] diffusion learning rate: 0.001
2024-11-05 01:17:53,263 - INFO - [diffusion][Epoch 6811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:53,264 - INFO - [diffusion][Epoch 6812] Epoch 6813/12000
2024-11-05 01:17:57,396 - INFO - [diffusion][Epoch 6812] diffusion training Loss: 0.05985941458493471
2024-11-05 01:17:57,398 - INFO - [diffusion][Epoch 6812] diffusion learning rate: 0.001
2024-11-05 01:17:57,400 - INFO - [diffusion][Epoch 6812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:17:57,401 - INFO - [diffusion][Epoch 6813] Epoch 6814/12000
2024-11-05 01:18:01,558 - INFO - [diffusion][Epoch 6813] diffusion training Loss: 0.056204941123723984
2024-11-05 01:18:01,560 - INFO - [diffusion][Epoch 6813] diffusion learning rate: 0.001
2024-11-05 01:18:01,562 - INFO - [diffusion][Epoch 6813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:01,563 - INFO - [diffusion][Epoch 6814] Epoch 6815/12000
2024-11-05 01:18:05,708 - INFO - [diffusion][Epoch 6814] diffusion training Loss: 0.06144282501190901
2024-11-05 01:18:05,711 - INFO - [diffusion][Epoch 6814] diffusion learning rate: 0.001
2024-11-05 01:18:05,712 - INFO - [diffusion][Epoch 6814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:05,714 - INFO - [diffusion][Epoch 6815] Epoch 6816/12000
2024-11-05 01:18:09,778 - INFO - [diffusion][Epoch 6815] diffusion training Loss: 0.051604947075247765
2024-11-05 01:18:09,780 - INFO - [diffusion][Epoch 6815] diffusion learning rate: 0.001
2024-11-05 01:18:09,781 - INFO - [diffusion][Epoch 6815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:09,783 - INFO - [diffusion][Epoch 6816] Epoch 6817/12000
2024-11-05 01:18:13,970 - INFO - [diffusion][Epoch 6816] diffusion training Loss: 0.06690643168985844
2024-11-05 01:18:13,971 - INFO - [diffusion][Epoch 6816] diffusion learning rate: 0.001
2024-11-05 01:18:13,974 - INFO - [diffusion][Epoch 6816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:13,976 - INFO - [diffusion][Epoch 6817] Epoch 6818/12000
2024-11-05 01:18:18,063 - INFO - [diffusion][Epoch 6817] diffusion training Loss: 0.06284092832356691
2024-11-05 01:18:18,065 - INFO - [diffusion][Epoch 6817] diffusion learning rate: 0.001
2024-11-05 01:18:18,067 - INFO - [diffusion][Epoch 6817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:18,068 - INFO - [diffusion][Epoch 6818] Epoch 6819/12000
2024-11-05 01:18:22,213 - INFO - [diffusion][Epoch 6818] diffusion training Loss: 0.059935358352959156
2024-11-05 01:18:22,215 - INFO - [diffusion][Epoch 6818] diffusion learning rate: 0.001
2024-11-05 01:18:22,217 - INFO - [diffusion][Epoch 6818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:22,218 - INFO - [diffusion][Epoch 6819] Epoch 6820/12000
2024-11-05 01:18:26,372 - INFO - [diffusion][Epoch 6819] diffusion training Loss: 0.06323164608329535
2024-11-05 01:18:26,375 - INFO - [diffusion][Epoch 6819] diffusion learning rate: 0.001
2024-11-05 01:18:26,376 - INFO - [diffusion][Epoch 6819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:26,378 - INFO - [diffusion][Epoch 6820] Epoch 6821/12000
2024-11-05 01:18:30,487 - INFO - [diffusion][Epoch 6820] diffusion training Loss: 0.06118637230247259
2024-11-05 01:18:30,489 - INFO - [diffusion][Epoch 6820] diffusion learning rate: 0.001
2024-11-05 01:18:30,490 - INFO - [diffusion][Epoch 6820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:30,492 - INFO - [diffusion][Epoch 6821] Epoch 6822/12000
2024-11-05 01:18:34,644 - INFO - [diffusion][Epoch 6821] diffusion training Loss: 0.058422692120075226
2024-11-05 01:18:34,646 - INFO - [diffusion][Epoch 6821] diffusion learning rate: 0.001
2024-11-05 01:18:34,698 - INFO - [diffusion][Epoch 6821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:34,699 - INFO - [diffusion][Epoch 6822] Epoch 6823/12000
2024-11-05 01:18:38,800 - INFO - [diffusion][Epoch 6822] diffusion training Loss: 0.05995981767773628
2024-11-05 01:18:38,803 - INFO - [diffusion][Epoch 6822] diffusion learning rate: 0.001
2024-11-05 01:18:38,804 - INFO - [diffusion][Epoch 6822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:38,806 - INFO - [diffusion][Epoch 6823] Epoch 6824/12000
2024-11-05 01:18:42,924 - INFO - [diffusion][Epoch 6823] diffusion training Loss: 0.0661178519949317
2024-11-05 01:18:42,926 - INFO - [diffusion][Epoch 6823] diffusion learning rate: 0.001
2024-11-05 01:18:42,928 - INFO - [diffusion][Epoch 6823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:42,929 - INFO - [diffusion][Epoch 6824] Epoch 6825/12000
2024-11-05 01:18:47,055 - INFO - [diffusion][Epoch 6824] diffusion training Loss: 0.061785316094756126
2024-11-05 01:18:47,057 - INFO - [diffusion][Epoch 6824] diffusion learning rate: 0.001
2024-11-05 01:18:47,058 - INFO - [diffusion][Epoch 6824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:47,060 - INFO - [diffusion][Epoch 6825] Epoch 6826/12000
2024-11-05 01:18:51,094 - INFO - [diffusion][Epoch 6825] diffusion training Loss: 0.05824444629251957
2024-11-05 01:18:51,096 - INFO - [diffusion][Epoch 6825] diffusion learning rate: 0.001
2024-11-05 01:18:51,098 - INFO - [diffusion][Epoch 6825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:51,099 - INFO - [diffusion][Epoch 6826] Epoch 6827/12000
2024-11-05 01:18:55,180 - INFO - [diffusion][Epoch 6826] diffusion training Loss: 0.05910835228860378
2024-11-05 01:18:55,182 - INFO - [diffusion][Epoch 6826] diffusion learning rate: 0.001
2024-11-05 01:18:55,184 - INFO - [diffusion][Epoch 6826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:55,185 - INFO - [diffusion][Epoch 6827] Epoch 6828/12000
2024-11-05 01:18:59,347 - INFO - [diffusion][Epoch 6827] diffusion training Loss: 0.056232839822769165
2024-11-05 01:18:59,349 - INFO - [diffusion][Epoch 6827] diffusion learning rate: 0.001
2024-11-05 01:18:59,351 - INFO - [diffusion][Epoch 6827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:18:59,352 - INFO - [diffusion][Epoch 6828] Epoch 6829/12000
2024-11-05 01:19:03,469 - INFO - [diffusion][Epoch 6828] diffusion training Loss: 0.057737707160413265
2024-11-05 01:19:03,471 - INFO - [diffusion][Epoch 6828] diffusion learning rate: 0.001
2024-11-05 01:19:03,473 - INFO - [diffusion][Epoch 6828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:03,474 - INFO - [diffusion][Epoch 6829] Epoch 6830/12000
2024-11-05 01:19:07,612 - INFO - [diffusion][Epoch 6829] diffusion training Loss: 0.05533221736550331
2024-11-05 01:19:07,614 - INFO - [diffusion][Epoch 6829] diffusion learning rate: 0.001
2024-11-05 01:19:07,615 - INFO - [diffusion][Epoch 6829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:07,617 - INFO - [diffusion][Epoch 6830] Epoch 6831/12000
2024-11-05 01:19:11,763 - INFO - [diffusion][Epoch 6830] diffusion training Loss: 0.056700839661061764
2024-11-05 01:19:11,765 - INFO - [diffusion][Epoch 6830] diffusion learning rate: 0.001
2024-11-05 01:19:11,768 - INFO - [diffusion][Epoch 6830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:11,769 - INFO - [diffusion][Epoch 6831] Epoch 6832/12000
2024-11-05 01:19:15,881 - INFO - [diffusion][Epoch 6831] diffusion training Loss: 0.057034384459257126
2024-11-05 01:19:15,883 - INFO - [diffusion][Epoch 6831] diffusion learning rate: 0.001
2024-11-05 01:19:15,885 - INFO - [diffusion][Epoch 6831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:15,886 - INFO - [diffusion][Epoch 6832] Epoch 6833/12000
2024-11-05 01:19:20,015 - INFO - [diffusion][Epoch 6832] diffusion training Loss: 0.05765929073095322
2024-11-05 01:19:20,017 - INFO - [diffusion][Epoch 6832] diffusion learning rate: 0.001
2024-11-05 01:19:20,019 - INFO - [diffusion][Epoch 6832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:20,021 - INFO - [diffusion][Epoch 6833] Epoch 6834/12000
2024-11-05 01:19:24,112 - INFO - [diffusion][Epoch 6833] diffusion training Loss: 0.054458944126963615
2024-11-05 01:19:24,114 - INFO - [diffusion][Epoch 6833] diffusion learning rate: 0.001
2024-11-05 01:19:24,116 - INFO - [diffusion][Epoch 6833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:24,117 - INFO - [diffusion][Epoch 6834] Epoch 6835/12000
2024-11-05 01:19:28,261 - INFO - [diffusion][Epoch 6834] diffusion training Loss: 0.06390543561428785
2024-11-05 01:19:28,264 - INFO - [diffusion][Epoch 6834] diffusion learning rate: 0.001
2024-11-05 01:19:28,266 - INFO - [diffusion][Epoch 6834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:28,267 - INFO - [diffusion][Epoch 6835] Epoch 6836/12000
2024-11-05 01:19:32,388 - INFO - [diffusion][Epoch 6835] diffusion training Loss: 0.05206168722361326
2024-11-05 01:19:32,390 - INFO - [diffusion][Epoch 6835] diffusion learning rate: 0.001
2024-11-05 01:19:32,391 - INFO - [diffusion][Epoch 6835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:32,393 - INFO - [diffusion][Epoch 6836] Epoch 6837/12000
2024-11-05 01:19:36,496 - INFO - [diffusion][Epoch 6836] diffusion training Loss: 0.05996934324502945
2024-11-05 01:19:36,498 - INFO - [diffusion][Epoch 6836] diffusion learning rate: 0.001
2024-11-05 01:19:36,500 - INFO - [diffusion][Epoch 6836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:36,501 - INFO - [diffusion][Epoch 6837] Epoch 6838/12000
2024-11-05 01:19:40,560 - INFO - [diffusion][Epoch 6837] diffusion training Loss: 0.0545189194381237
2024-11-05 01:19:40,563 - INFO - [diffusion][Epoch 6837] diffusion learning rate: 0.001
2024-11-05 01:19:40,565 - INFO - [diffusion][Epoch 6837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:40,566 - INFO - [diffusion][Epoch 6838] Epoch 6839/12000
2024-11-05 01:19:44,493 - INFO - [diffusion][Epoch 6838] diffusion training Loss: 0.06666521355509758
2024-11-05 01:19:44,495 - INFO - [diffusion][Epoch 6838] diffusion learning rate: 0.001
2024-11-05 01:19:44,497 - INFO - [diffusion][Epoch 6838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:44,498 - INFO - [diffusion][Epoch 6839] Epoch 6840/12000
2024-11-05 01:19:48,624 - INFO - [diffusion][Epoch 6839] diffusion training Loss: 0.05677491519600153
2024-11-05 01:19:48,626 - INFO - [diffusion][Epoch 6839] diffusion learning rate: 0.001
2024-11-05 01:19:48,628 - INFO - [diffusion][Epoch 6839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:48,629 - INFO - [diffusion][Epoch 6840] Epoch 6841/12000
2024-11-05 01:19:52,723 - INFO - [diffusion][Epoch 6840] diffusion training Loss: 0.057867580093443394
2024-11-05 01:19:52,725 - INFO - [diffusion][Epoch 6840] diffusion learning rate: 0.001
2024-11-05 01:19:52,726 - INFO - [diffusion][Epoch 6840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:52,728 - INFO - [diffusion][Epoch 6841] Epoch 6842/12000
2024-11-05 01:19:56,831 - INFO - [diffusion][Epoch 6841] diffusion training Loss: 0.06321122869849205
2024-11-05 01:19:56,832 - INFO - [diffusion][Epoch 6841] diffusion learning rate: 0.001
2024-11-05 01:19:56,834 - INFO - [diffusion][Epoch 6841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:19:56,835 - INFO - [diffusion][Epoch 6842] Epoch 6843/12000
2024-11-05 01:20:00,816 - INFO - [diffusion][Epoch 6842] diffusion training Loss: 0.05880448315292597
2024-11-05 01:20:00,819 - INFO - [diffusion][Epoch 6842] diffusion learning rate: 0.001
2024-11-05 01:20:00,820 - INFO - [diffusion][Epoch 6842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:00,822 - INFO - [diffusion][Epoch 6843] Epoch 6844/12000
2024-11-05 01:20:04,931 - INFO - [diffusion][Epoch 6843] diffusion training Loss: 0.05607138480991125
2024-11-05 01:20:04,933 - INFO - [diffusion][Epoch 6843] diffusion learning rate: 0.001
2024-11-05 01:20:04,972 - INFO - [diffusion][Epoch 6843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:04,973 - INFO - [diffusion][Epoch 6844] Epoch 6845/12000
2024-11-05 01:20:09,048 - INFO - [diffusion][Epoch 6844] diffusion training Loss: 0.055868200957775116
2024-11-05 01:20:09,051 - INFO - [diffusion][Epoch 6844] diffusion learning rate: 0.001
2024-11-05 01:20:09,053 - INFO - [diffusion][Epoch 6844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:09,055 - INFO - [diffusion][Epoch 6845] Epoch 6846/12000
2024-11-05 01:20:12,899 - INFO - [diffusion][Epoch 6845] diffusion training Loss: 0.05666928365826607
2024-11-05 01:20:12,901 - INFO - [diffusion][Epoch 6845] diffusion learning rate: 0.001
2024-11-05 01:20:12,903 - INFO - [diffusion][Epoch 6845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:12,904 - INFO - [diffusion][Epoch 6846] Epoch 6847/12000
2024-11-05 01:20:17,331 - INFO - [diffusion][Epoch 6846] diffusion training Loss: 0.0581259997561574
2024-11-05 01:20:17,333 - INFO - [diffusion][Epoch 6846] diffusion learning rate: 0.001
2024-11-05 01:20:17,335 - INFO - [diffusion][Epoch 6846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:17,337 - INFO - [diffusion][Epoch 6847] Epoch 6848/12000
2024-11-05 01:20:21,411 - INFO - [diffusion][Epoch 6847] diffusion training Loss: 0.057366784662008286
2024-11-05 01:20:21,413 - INFO - [diffusion][Epoch 6847] diffusion learning rate: 0.001
2024-11-05 01:20:21,415 - INFO - [diffusion][Epoch 6847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:21,417 - INFO - [diffusion][Epoch 6848] Epoch 6849/12000
2024-11-05 01:20:25,454 - INFO - [diffusion][Epoch 6848] diffusion training Loss: 0.05844083521515131
2024-11-05 01:20:25,456 - INFO - [diffusion][Epoch 6848] diffusion learning rate: 0.001
2024-11-05 01:20:25,459 - INFO - [diffusion][Epoch 6848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:25,460 - INFO - [diffusion][Epoch 6849] Epoch 6850/12000
2024-11-05 01:20:29,442 - INFO - [diffusion][Epoch 6849] diffusion training Loss: 0.06074138078838587
2024-11-05 01:20:29,443 - INFO - [diffusion][Epoch 6849] diffusion learning rate: 0.001
2024-11-05 01:20:29,445 - INFO - [diffusion][Epoch 6849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:29,446 - INFO - [diffusion][Epoch 6850] Epoch 6851/12000
2024-11-05 01:20:33,567 - INFO - [diffusion][Epoch 6850] diffusion training Loss: 0.057117337360978127
2024-11-05 01:20:33,569 - INFO - [diffusion][Epoch 6850] diffusion learning rate: 0.001
2024-11-05 01:20:33,571 - INFO - [diffusion][Epoch 6850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:33,572 - INFO - [diffusion][Epoch 6851] Epoch 6852/12000
2024-11-05 01:20:37,751 - INFO - [diffusion][Epoch 6851] diffusion training Loss: 0.055027659982442856
2024-11-05 01:20:37,753 - INFO - [diffusion][Epoch 6851] diffusion learning rate: 0.001
2024-11-05 01:20:37,755 - INFO - [diffusion][Epoch 6851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:37,756 - INFO - [diffusion][Epoch 6852] Epoch 6853/12000
2024-11-05 01:20:41,769 - INFO - [diffusion][Epoch 6852] diffusion training Loss: 0.05838705971837044
2024-11-05 01:20:41,772 - INFO - [diffusion][Epoch 6852] diffusion learning rate: 0.001
2024-11-05 01:20:41,774 - INFO - [diffusion][Epoch 6852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:41,775 - INFO - [diffusion][Epoch 6853] Epoch 6854/12000
2024-11-05 01:20:45,801 - INFO - [diffusion][Epoch 6853] diffusion training Loss: 0.0594619894400239
2024-11-05 01:20:45,803 - INFO - [diffusion][Epoch 6853] diffusion learning rate: 0.001
2024-11-05 01:20:45,804 - INFO - [diffusion][Epoch 6853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:45,806 - INFO - [diffusion][Epoch 6854] Epoch 6855/12000
2024-11-05 01:20:49,674 - INFO - [diffusion][Epoch 6854] diffusion training Loss: 0.05733270011842251
2024-11-05 01:20:49,676 - INFO - [diffusion][Epoch 6854] diffusion learning rate: 0.001
2024-11-05 01:20:49,678 - INFO - [diffusion][Epoch 6854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:49,679 - INFO - [diffusion][Epoch 6855] Epoch 6856/12000
2024-11-05 01:20:53,702 - INFO - [diffusion][Epoch 6855] diffusion training Loss: 0.06178548466414213
2024-11-05 01:20:53,704 - INFO - [diffusion][Epoch 6855] diffusion learning rate: 0.001
2024-11-05 01:20:53,706 - INFO - [diffusion][Epoch 6855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:53,707 - INFO - [diffusion][Epoch 6856] Epoch 6857/12000
2024-11-05 01:20:57,772 - INFO - [diffusion][Epoch 6856] diffusion training Loss: 0.05731133744120598
2024-11-05 01:20:57,774 - INFO - [diffusion][Epoch 6856] diffusion learning rate: 0.001
2024-11-05 01:20:57,821 - INFO - [diffusion][Epoch 6856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:20:57,822 - INFO - [diffusion][Epoch 6857] Epoch 6858/12000
2024-11-05 01:21:01,936 - INFO - [diffusion][Epoch 6857] diffusion training Loss: 0.05850996170192957
2024-11-05 01:21:01,937 - INFO - [diffusion][Epoch 6857] diffusion learning rate: 0.001
2024-11-05 01:21:01,939 - INFO - [diffusion][Epoch 6857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:01,941 - INFO - [diffusion][Epoch 6858] Epoch 6859/12000
2024-11-05 01:21:05,981 - INFO - [diffusion][Epoch 6858] diffusion training Loss: 0.06077235285192728
2024-11-05 01:21:05,983 - INFO - [diffusion][Epoch 6858] diffusion learning rate: 0.001
2024-11-05 01:21:05,985 - INFO - [diffusion][Epoch 6858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:05,986 - INFO - [diffusion][Epoch 6859] Epoch 6860/12000
2024-11-05 01:21:10,064 - INFO - [diffusion][Epoch 6859] diffusion training Loss: 0.06605130434036255
2024-11-05 01:21:10,066 - INFO - [diffusion][Epoch 6859] diffusion learning rate: 0.001
2024-11-05 01:21:10,067 - INFO - [diffusion][Epoch 6859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:10,069 - INFO - [diffusion][Epoch 6860] Epoch 6861/12000
2024-11-05 01:21:14,094 - INFO - [diffusion][Epoch 6860] diffusion training Loss: 0.06081932503730059
2024-11-05 01:21:14,096 - INFO - [diffusion][Epoch 6860] diffusion learning rate: 0.001
2024-11-05 01:21:14,098 - INFO - [diffusion][Epoch 6860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:14,100 - INFO - [diffusion][Epoch 6861] Epoch 6862/12000
2024-11-05 01:21:18,147 - INFO - [diffusion][Epoch 6861] diffusion training Loss: 0.054746491834521294
2024-11-05 01:21:18,149 - INFO - [diffusion][Epoch 6861] diffusion learning rate: 0.001
2024-11-05 01:21:18,151 - INFO - [diffusion][Epoch 6861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:18,152 - INFO - [diffusion][Epoch 6862] Epoch 6863/12000
2024-11-05 01:21:22,205 - INFO - [diffusion][Epoch 6862] diffusion training Loss: 0.05986825376749039
2024-11-05 01:21:22,207 - INFO - [diffusion][Epoch 6862] diffusion learning rate: 0.001
2024-11-05 01:21:22,209 - INFO - [diffusion][Epoch 6862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:22,210 - INFO - [diffusion][Epoch 6863] Epoch 6864/12000
2024-11-05 01:21:26,247 - INFO - [diffusion][Epoch 6863] diffusion training Loss: 0.06060337647795677
2024-11-05 01:21:26,249 - INFO - [diffusion][Epoch 6863] diffusion learning rate: 0.001
2024-11-05 01:21:26,251 - INFO - [diffusion][Epoch 6863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:26,252 - INFO - [diffusion][Epoch 6864] Epoch 6865/12000
2024-11-05 01:21:30,213 - INFO - [diffusion][Epoch 6864] diffusion training Loss: 0.053780196234583855
2024-11-05 01:21:30,215 - INFO - [diffusion][Epoch 6864] diffusion learning rate: 0.001
2024-11-05 01:21:30,217 - INFO - [diffusion][Epoch 6864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:30,219 - INFO - [diffusion][Epoch 6865] Epoch 6866/12000
2024-11-05 01:21:34,346 - INFO - [diffusion][Epoch 6865] diffusion training Loss: 0.05945288669317961
2024-11-05 01:21:34,348 - INFO - [diffusion][Epoch 6865] diffusion learning rate: 0.001
2024-11-05 01:21:34,349 - INFO - [diffusion][Epoch 6865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:34,351 - INFO - [diffusion][Epoch 6866] Epoch 6867/12000
2024-11-05 01:21:38,670 - INFO - [diffusion][Epoch 6866] diffusion training Loss: 0.054850028827786446
2024-11-05 01:21:38,673 - INFO - [diffusion][Epoch 6866] diffusion learning rate: 0.001
2024-11-05 01:21:38,675 - INFO - [diffusion][Epoch 6866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:38,676 - INFO - [diffusion][Epoch 6867] Epoch 6868/12000
2024-11-05 01:21:42,822 - INFO - [diffusion][Epoch 6867] diffusion training Loss: 0.06197554524987936
2024-11-05 01:21:42,825 - INFO - [diffusion][Epoch 6867] diffusion learning rate: 0.001
2024-11-05 01:21:42,827 - INFO - [diffusion][Epoch 6867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:42,828 - INFO - [diffusion][Epoch 6868] Epoch 6869/12000
2024-11-05 01:21:46,779 - INFO - [diffusion][Epoch 6868] diffusion training Loss: 0.06119328364729881
2024-11-05 01:21:46,781 - INFO - [diffusion][Epoch 6868] diffusion learning rate: 0.001
2024-11-05 01:21:46,782 - INFO - [diffusion][Epoch 6868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:46,784 - INFO - [diffusion][Epoch 6869] Epoch 6870/12000
2024-11-05 01:21:50,775 - INFO - [diffusion][Epoch 6869] diffusion training Loss: 0.05956308264285326
2024-11-05 01:21:50,778 - INFO - [diffusion][Epoch 6869] diffusion learning rate: 0.001
2024-11-05 01:21:50,809 - INFO - [diffusion][Epoch 6869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:50,810 - INFO - [diffusion][Epoch 6870] Epoch 6871/12000
2024-11-05 01:21:54,794 - INFO - [diffusion][Epoch 6870] diffusion training Loss: 0.05804777704179287
2024-11-05 01:21:54,796 - INFO - [diffusion][Epoch 6870] diffusion learning rate: 0.001
2024-11-05 01:21:54,798 - INFO - [diffusion][Epoch 6870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:54,799 - INFO - [diffusion][Epoch 6871] Epoch 6872/12000
2024-11-05 01:21:58,765 - INFO - [diffusion][Epoch 6871] diffusion training Loss: 0.05772959627211094
2024-11-05 01:21:58,767 - INFO - [diffusion][Epoch 6871] diffusion learning rate: 0.001
2024-11-05 01:21:58,769 - INFO - [diffusion][Epoch 6871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:21:58,771 - INFO - [diffusion][Epoch 6872] Epoch 6873/12000
2024-11-05 01:22:02,849 - INFO - [diffusion][Epoch 6872] diffusion training Loss: 0.061356015503406525
2024-11-05 01:22:02,852 - INFO - [diffusion][Epoch 6872] diffusion learning rate: 0.001
2024-11-05 01:22:02,854 - INFO - [diffusion][Epoch 6872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:02,855 - INFO - [diffusion][Epoch 6873] Epoch 6874/12000
2024-11-05 01:22:06,992 - INFO - [diffusion][Epoch 6873] diffusion training Loss: 0.0579840000718832
2024-11-05 01:22:06,994 - INFO - [diffusion][Epoch 6873] diffusion learning rate: 0.001
2024-11-05 01:22:06,996 - INFO - [diffusion][Epoch 6873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:06,997 - INFO - [diffusion][Epoch 6874] Epoch 6875/12000
2024-11-05 01:22:11,055 - INFO - [diffusion][Epoch 6874] diffusion training Loss: 0.05934872291982174
2024-11-05 01:22:11,057 - INFO - [diffusion][Epoch 6874] diffusion learning rate: 0.001
2024-11-05 01:22:11,059 - INFO - [diffusion][Epoch 6874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:11,060 - INFO - [diffusion][Epoch 6875] Epoch 6876/12000
2024-11-05 01:22:15,177 - INFO - [diffusion][Epoch 6875] diffusion training Loss: 0.054201492108404636
2024-11-05 01:22:15,179 - INFO - [diffusion][Epoch 6875] diffusion learning rate: 0.001
2024-11-05 01:22:15,180 - INFO - [diffusion][Epoch 6875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:15,182 - INFO - [diffusion][Epoch 6876] Epoch 6877/12000
2024-11-05 01:22:19,066 - INFO - [diffusion][Epoch 6876] diffusion training Loss: 0.05720664467662573
2024-11-05 01:22:19,068 - INFO - [diffusion][Epoch 6876] diffusion learning rate: 0.001
2024-11-05 01:22:19,069 - INFO - [diffusion][Epoch 6876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:19,071 - INFO - [diffusion][Epoch 6877] Epoch 6878/12000
2024-11-05 01:22:23,056 - INFO - [diffusion][Epoch 6877] diffusion training Loss: 0.05663086101412773
2024-11-05 01:22:23,058 - INFO - [diffusion][Epoch 6877] diffusion learning rate: 0.001
2024-11-05 01:22:23,060 - INFO - [diffusion][Epoch 6877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:23,061 - INFO - [diffusion][Epoch 6878] Epoch 6879/12000
2024-11-05 01:22:27,192 - INFO - [diffusion][Epoch 6878] diffusion training Loss: 0.053804779425263405
2024-11-05 01:22:27,193 - INFO - [diffusion][Epoch 6878] diffusion learning rate: 0.001
2024-11-05 01:22:27,195 - INFO - [diffusion][Epoch 6878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:27,197 - INFO - [diffusion][Epoch 6879] Epoch 6880/12000
2024-11-05 01:22:31,352 - INFO - [diffusion][Epoch 6879] diffusion training Loss: 0.06090802885591984
2024-11-05 01:22:31,354 - INFO - [diffusion][Epoch 6879] diffusion learning rate: 0.001
2024-11-05 01:22:31,356 - INFO - [diffusion][Epoch 6879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:31,358 - INFO - [diffusion][Epoch 6880] Epoch 6881/12000
2024-11-05 01:22:35,559 - INFO - [diffusion][Epoch 6880] diffusion training Loss: 0.05805313494056463
2024-11-05 01:22:35,561 - INFO - [diffusion][Epoch 6880] diffusion learning rate: 0.001
2024-11-05 01:22:35,563 - INFO - [diffusion][Epoch 6880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:35,564 - INFO - [diffusion][Epoch 6881] Epoch 6882/12000
2024-11-05 01:22:39,702 - INFO - [diffusion][Epoch 6881] diffusion training Loss: 0.05812271870672703
2024-11-05 01:22:39,704 - INFO - [diffusion][Epoch 6881] diffusion learning rate: 0.001
2024-11-05 01:22:39,706 - INFO - [diffusion][Epoch 6881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:39,707 - INFO - [diffusion][Epoch 6882] Epoch 6883/12000
2024-11-05 01:22:43,822 - INFO - [diffusion][Epoch 6882] diffusion training Loss: 0.06128141190856695
2024-11-05 01:22:43,825 - INFO - [diffusion][Epoch 6882] diffusion learning rate: 0.001
2024-11-05 01:22:43,828 - INFO - [diffusion][Epoch 6882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:43,829 - INFO - [diffusion][Epoch 6883] Epoch 6884/12000
2024-11-05 01:22:48,011 - INFO - [diffusion][Epoch 6883] diffusion training Loss: 0.056594258174300194
2024-11-05 01:22:48,013 - INFO - [diffusion][Epoch 6883] diffusion learning rate: 0.001
2024-11-05 01:22:48,015 - INFO - [diffusion][Epoch 6883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:48,016 - INFO - [diffusion][Epoch 6884] Epoch 6885/12000
2024-11-05 01:22:52,136 - INFO - [diffusion][Epoch 6884] diffusion training Loss: 0.05912473425269127
2024-11-05 01:22:52,139 - INFO - [diffusion][Epoch 6884] diffusion learning rate: 0.001
2024-11-05 01:22:52,140 - INFO - [diffusion][Epoch 6884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:52,142 - INFO - [diffusion][Epoch 6885] Epoch 6886/12000
2024-11-05 01:22:56,152 - INFO - [diffusion][Epoch 6885] diffusion training Loss: 0.06215598154813051
2024-11-05 01:22:56,154 - INFO - [diffusion][Epoch 6885] diffusion learning rate: 0.001
2024-11-05 01:22:56,156 - INFO - [diffusion][Epoch 6885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:22:56,157 - INFO - [diffusion][Epoch 6886] Epoch 6887/12000
2024-11-05 01:23:00,252 - INFO - [diffusion][Epoch 6886] diffusion training Loss: 0.0561421187594533
2024-11-05 01:23:00,254 - INFO - [diffusion][Epoch 6886] diffusion learning rate: 0.001
2024-11-05 01:23:00,256 - INFO - [diffusion][Epoch 6886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:00,257 - INFO - [diffusion][Epoch 6887] Epoch 6888/12000
2024-11-05 01:23:05,006 - INFO - [diffusion][Epoch 6887] diffusion training Loss: 0.056369016878306866
2024-11-05 01:23:05,008 - INFO - [diffusion][Epoch 6887] diffusion learning rate: 0.001
2024-11-05 01:23:05,010 - INFO - [diffusion][Epoch 6887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:05,011 - INFO - [diffusion][Epoch 6888] Epoch 6889/12000
2024-11-05 01:23:09,113 - INFO - [diffusion][Epoch 6888] diffusion training Loss: 0.059186154045164585
2024-11-05 01:23:09,116 - INFO - [diffusion][Epoch 6888] diffusion learning rate: 0.001
2024-11-05 01:23:09,118 - INFO - [diffusion][Epoch 6888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:09,120 - INFO - [diffusion][Epoch 6889] Epoch 6890/12000
2024-11-05 01:23:13,147 - INFO - [diffusion][Epoch 6889] diffusion training Loss: 0.05545685440301895
2024-11-05 01:23:13,149 - INFO - [diffusion][Epoch 6889] diffusion learning rate: 0.001
2024-11-05 01:23:13,151 - INFO - [diffusion][Epoch 6889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:13,152 - INFO - [diffusion][Epoch 6890] Epoch 6891/12000
2024-11-05 01:23:17,291 - INFO - [diffusion][Epoch 6890] diffusion training Loss: 0.060293255373835564
2024-11-05 01:23:17,293 - INFO - [diffusion][Epoch 6890] diffusion learning rate: 0.001
2024-11-05 01:23:17,295 - INFO - [diffusion][Epoch 6890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:17,296 - INFO - [diffusion][Epoch 6891] Epoch 6892/12000
2024-11-05 01:23:21,184 - INFO - [diffusion][Epoch 6891] diffusion training Loss: 0.06295746937394142
2024-11-05 01:23:21,186 - INFO - [diffusion][Epoch 6891] diffusion learning rate: 0.001
2024-11-05 01:23:21,188 - INFO - [diffusion][Epoch 6891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:21,189 - INFO - [diffusion][Epoch 6892] Epoch 6893/12000
2024-11-05 01:23:25,189 - INFO - [diffusion][Epoch 6892] diffusion training Loss: 0.05752990394830704
2024-11-05 01:23:25,191 - INFO - [diffusion][Epoch 6892] diffusion learning rate: 0.001
2024-11-05 01:23:25,193 - INFO - [diffusion][Epoch 6892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:25,194 - INFO - [diffusion][Epoch 6893] Epoch 6894/12000
2024-11-05 01:23:29,229 - INFO - [diffusion][Epoch 6893] diffusion training Loss: 0.0567317558452487
2024-11-05 01:23:29,231 - INFO - [diffusion][Epoch 6893] diffusion learning rate: 0.001
2024-11-05 01:23:29,232 - INFO - [diffusion][Epoch 6893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:29,234 - INFO - [diffusion][Epoch 6894] Epoch 6895/12000
2024-11-05 01:23:33,397 - INFO - [diffusion][Epoch 6894] diffusion training Loss: 0.06327213440090418
2024-11-05 01:23:33,399 - INFO - [diffusion][Epoch 6894] diffusion learning rate: 0.001
2024-11-05 01:23:33,402 - INFO - [diffusion][Epoch 6894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:33,404 - INFO - [diffusion][Epoch 6895] Epoch 6896/12000
2024-11-05 01:23:37,549 - INFO - [diffusion][Epoch 6895] diffusion training Loss: 0.05506543442606926
2024-11-05 01:23:37,551 - INFO - [diffusion][Epoch 6895] diffusion learning rate: 0.001
2024-11-05 01:23:37,553 - INFO - [diffusion][Epoch 6895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:37,554 - INFO - [diffusion][Epoch 6896] Epoch 6897/12000
2024-11-05 01:23:41,567 - INFO - [diffusion][Epoch 6896] diffusion training Loss: 0.05608789995312691
2024-11-05 01:23:41,569 - INFO - [diffusion][Epoch 6896] diffusion learning rate: 0.001
2024-11-05 01:23:41,571 - INFO - [diffusion][Epoch 6896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:41,572 - INFO - [diffusion][Epoch 6897] Epoch 6898/12000
2024-11-05 01:23:45,695 - INFO - [diffusion][Epoch 6897] diffusion training Loss: 0.05592269077897072
2024-11-05 01:23:45,697 - INFO - [diffusion][Epoch 6897] diffusion learning rate: 0.001
2024-11-05 01:23:45,699 - INFO - [diffusion][Epoch 6897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:45,700 - INFO - [diffusion][Epoch 6898] Epoch 6899/12000
2024-11-05 01:23:49,869 - INFO - [diffusion][Epoch 6898] diffusion training Loss: 0.056317029520869255
2024-11-05 01:23:49,871 - INFO - [diffusion][Epoch 6898] diffusion learning rate: 0.001
2024-11-05 01:23:49,873 - INFO - [diffusion][Epoch 6898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:49,874 - INFO - [diffusion][Epoch 6899] Epoch 6900/12000
2024-11-05 01:23:54,005 - INFO - [diffusion][Epoch 6899] diffusion training Loss: 0.05876512546092272
2024-11-05 01:23:54,008 - INFO - [diffusion][Epoch 6899] diffusion learning rate: 0.001
2024-11-05 01:23:54,011 - INFO - [diffusion][Epoch 6899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:54,013 - INFO - [diffusion][Epoch 6900] Epoch 6901/12000
2024-11-05 01:23:58,160 - INFO - [diffusion][Epoch 6900] diffusion training Loss: 0.05775941535830498
2024-11-05 01:23:58,162 - INFO - [diffusion][Epoch 6900] diffusion learning rate: 0.001
2024-11-05 01:23:58,164 - INFO - [diffusion][Epoch 6900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:23:58,165 - INFO - [diffusion][Epoch 6901] Epoch 6902/12000
2024-11-05 01:24:02,326 - INFO - [diffusion][Epoch 6901] diffusion training Loss: 0.05711495038121939
2024-11-05 01:24:02,328 - INFO - [diffusion][Epoch 6901] diffusion learning rate: 0.001
2024-11-05 01:24:02,330 - INFO - [diffusion][Epoch 6901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:02,331 - INFO - [diffusion][Epoch 6902] Epoch 6903/12000
2024-11-05 01:24:06,460 - INFO - [diffusion][Epoch 6902] diffusion training Loss: 0.056400224566459656
2024-11-05 01:24:06,462 - INFO - [diffusion][Epoch 6902] diffusion learning rate: 0.001
2024-11-05 01:24:06,464 - INFO - [diffusion][Epoch 6902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:06,465 - INFO - [diffusion][Epoch 6903] Epoch 6904/12000
2024-11-05 01:24:10,633 - INFO - [diffusion][Epoch 6903] diffusion training Loss: 0.06269743200391531
2024-11-05 01:24:10,635 - INFO - [diffusion][Epoch 6903] diffusion learning rate: 0.001
2024-11-05 01:24:10,636 - INFO - [diffusion][Epoch 6903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:10,638 - INFO - [diffusion][Epoch 6904] Epoch 6905/12000
2024-11-05 01:24:14,593 - INFO - [diffusion][Epoch 6904] diffusion training Loss: 0.060358891263604164
2024-11-05 01:24:14,595 - INFO - [diffusion][Epoch 6904] diffusion learning rate: 0.001
2024-11-05 01:24:14,597 - INFO - [diffusion][Epoch 6904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:14,599 - INFO - [diffusion][Epoch 6905] Epoch 6906/12000
2024-11-05 01:24:18,728 - INFO - [diffusion][Epoch 6905] diffusion training Loss: 0.05738593265414238
2024-11-05 01:24:18,730 - INFO - [diffusion][Epoch 6905] diffusion learning rate: 0.001
2024-11-05 01:24:18,732 - INFO - [diffusion][Epoch 6905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:18,733 - INFO - [diffusion][Epoch 6906] Epoch 6907/12000
2024-11-05 01:24:22,848 - INFO - [diffusion][Epoch 6906] diffusion training Loss: 0.057460070587694645
2024-11-05 01:24:22,850 - INFO - [diffusion][Epoch 6906] diffusion learning rate: 0.001
2024-11-05 01:24:22,852 - INFO - [diffusion][Epoch 6906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:22,853 - INFO - [diffusion][Epoch 6907] Epoch 6908/12000
2024-11-05 01:24:27,727 - INFO - [diffusion][Epoch 6907] diffusion training Loss: 0.05835944972932339
2024-11-05 01:24:27,729 - INFO - [diffusion][Epoch 6907] diffusion learning rate: 0.001
2024-11-05 01:24:27,731 - INFO - [diffusion][Epoch 6907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:27,732 - INFO - [diffusion][Epoch 6908] Epoch 6909/12000
2024-11-05 01:24:31,831 - INFO - [diffusion][Epoch 6908] diffusion training Loss: 0.053953749127686024
2024-11-05 01:24:31,833 - INFO - [diffusion][Epoch 6908] diffusion learning rate: 0.001
2024-11-05 01:24:31,870 - INFO - [diffusion][Epoch 6908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:31,871 - INFO - [diffusion][Epoch 6909] Epoch 6910/12000
2024-11-05 01:24:35,888 - INFO - [diffusion][Epoch 6909] diffusion training Loss: 0.0601177578791976
2024-11-05 01:24:35,890 - INFO - [diffusion][Epoch 6909] diffusion learning rate: 0.001
2024-11-05 01:24:35,892 - INFO - [diffusion][Epoch 6909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:35,893 - INFO - [diffusion][Epoch 6910] Epoch 6911/12000
2024-11-05 01:24:40,143 - INFO - [diffusion][Epoch 6910] diffusion training Loss: 0.04898881074041128
2024-11-05 01:24:40,145 - INFO - [diffusion][Epoch 6910] diffusion learning rate: 0.001
2024-11-05 01:24:40,147 - INFO - [diffusion][Epoch 6910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:40,149 - INFO - [diffusion][Epoch 6911] Epoch 6912/12000
2024-11-05 01:24:44,226 - INFO - [diffusion][Epoch 6911] diffusion training Loss: 0.052709135226905346
2024-11-05 01:24:44,228 - INFO - [diffusion][Epoch 6911] diffusion learning rate: 0.001
2024-11-05 01:24:44,229 - INFO - [diffusion][Epoch 6911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:44,231 - INFO - [diffusion][Epoch 6912] Epoch 6913/12000
2024-11-05 01:24:48,053 - INFO - [diffusion][Epoch 6912] diffusion training Loss: 0.05387166701257229
2024-11-05 01:24:48,056 - INFO - [diffusion][Epoch 6912] diffusion learning rate: 0.001
2024-11-05 01:24:48,109 - INFO - [diffusion][Epoch 6912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:48,110 - INFO - [diffusion][Epoch 6913] Epoch 6914/12000
2024-11-05 01:24:52,260 - INFO - [diffusion][Epoch 6913] diffusion training Loss: 0.05915256589651108
2024-11-05 01:24:52,262 - INFO - [diffusion][Epoch 6913] diffusion learning rate: 0.001
2024-11-05 01:24:52,264 - INFO - [diffusion][Epoch 6913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:52,265 - INFO - [diffusion][Epoch 6914] Epoch 6915/12000
2024-11-05 01:24:56,427 - INFO - [diffusion][Epoch 6914] diffusion training Loss: 0.05500150006264448
2024-11-05 01:24:56,429 - INFO - [diffusion][Epoch 6914] diffusion learning rate: 0.001
2024-11-05 01:24:56,431 - INFO - [diffusion][Epoch 6914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:24:56,432 - INFO - [diffusion][Epoch 6915] Epoch 6916/12000
2024-11-05 01:25:00,556 - INFO - [diffusion][Epoch 6915] diffusion training Loss: 0.05954857263714075
2024-11-05 01:25:00,558 - INFO - [diffusion][Epoch 6915] diffusion learning rate: 0.001
2024-11-05 01:25:00,560 - INFO - [diffusion][Epoch 6915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:00,561 - INFO - [diffusion][Epoch 6916] Epoch 6917/12000
2024-11-05 01:25:04,548 - INFO - [diffusion][Epoch 6916] diffusion training Loss: 0.055741469375789165
2024-11-05 01:25:04,550 - INFO - [diffusion][Epoch 6916] diffusion learning rate: 0.001
2024-11-05 01:25:04,601 - INFO - [diffusion][Epoch 6916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:04,603 - INFO - [diffusion][Epoch 6917] Epoch 6918/12000
2024-11-05 01:25:08,561 - INFO - [diffusion][Epoch 6917] diffusion training Loss: 0.06342106126248837
2024-11-05 01:25:08,563 - INFO - [diffusion][Epoch 6917] diffusion learning rate: 0.001
2024-11-05 01:25:08,564 - INFO - [diffusion][Epoch 6917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:08,566 - INFO - [diffusion][Epoch 6918] Epoch 6919/12000
2024-11-05 01:25:12,563 - INFO - [diffusion][Epoch 6918] diffusion training Loss: 0.05332250893115997
2024-11-05 01:25:12,565 - INFO - [diffusion][Epoch 6918] diffusion learning rate: 0.001
2024-11-05 01:25:12,567 - INFO - [diffusion][Epoch 6918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:12,568 - INFO - [diffusion][Epoch 6919] Epoch 6920/12000
2024-11-05 01:25:16,464 - INFO - [diffusion][Epoch 6919] diffusion training Loss: 0.053810346871614456
2024-11-05 01:25:16,466 - INFO - [diffusion][Epoch 6919] diffusion learning rate: 0.001
2024-11-05 01:25:16,468 - INFO - [diffusion][Epoch 6919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:16,469 - INFO - [diffusion][Epoch 6920] Epoch 6921/12000
2024-11-05 01:25:20,432 - INFO - [diffusion][Epoch 6920] diffusion training Loss: 0.05798970069736242
2024-11-05 01:25:20,434 - INFO - [diffusion][Epoch 6920] diffusion learning rate: 0.001
2024-11-05 01:25:20,436 - INFO - [diffusion][Epoch 6920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:20,438 - INFO - [diffusion][Epoch 6921] Epoch 6922/12000
2024-11-05 01:25:24,533 - INFO - [diffusion][Epoch 6921] diffusion training Loss: 0.06227975431829691
2024-11-05 01:25:24,536 - INFO - [diffusion][Epoch 6921] diffusion learning rate: 0.001
2024-11-05 01:25:24,538 - INFO - [diffusion][Epoch 6921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:24,539 - INFO - [diffusion][Epoch 6922] Epoch 6923/12000
2024-11-05 01:25:28,448 - INFO - [diffusion][Epoch 6922] diffusion training Loss: 0.059476932510733604
2024-11-05 01:25:28,451 - INFO - [diffusion][Epoch 6922] diffusion learning rate: 0.001
2024-11-05 01:25:28,454 - INFO - [diffusion][Epoch 6922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:28,455 - INFO - [diffusion][Epoch 6923] Epoch 6924/12000
2024-11-05 01:25:32,599 - INFO - [diffusion][Epoch 6923] diffusion training Loss: 0.05829556751996279
2024-11-05 01:25:32,600 - INFO - [diffusion][Epoch 6923] diffusion learning rate: 0.001
2024-11-05 01:25:32,602 - INFO - [diffusion][Epoch 6923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:32,603 - INFO - [diffusion][Epoch 6924] Epoch 6925/12000
2024-11-05 01:25:36,667 - INFO - [diffusion][Epoch 6924] diffusion training Loss: 0.06323982682079077
2024-11-05 01:25:36,669 - INFO - [diffusion][Epoch 6924] diffusion learning rate: 0.001
2024-11-05 01:25:36,671 - INFO - [diffusion][Epoch 6924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:36,672 - INFO - [diffusion][Epoch 6925] Epoch 6926/12000
2024-11-05 01:25:40,707 - INFO - [diffusion][Epoch 6925] diffusion training Loss: 0.05753900296986103
2024-11-05 01:25:40,708 - INFO - [diffusion][Epoch 6925] diffusion learning rate: 0.001
2024-11-05 01:25:40,710 - INFO - [diffusion][Epoch 6925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:40,711 - INFO - [diffusion][Epoch 6926] Epoch 6927/12000
2024-11-05 01:25:44,774 - INFO - [diffusion][Epoch 6926] diffusion training Loss: 0.058966231532394886
2024-11-05 01:25:44,776 - INFO - [diffusion][Epoch 6926] diffusion learning rate: 0.001
2024-11-05 01:25:44,778 - INFO - [diffusion][Epoch 6926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:44,779 - INFO - [diffusion][Epoch 6927] Epoch 6928/12000
2024-11-05 01:25:49,147 - INFO - [diffusion][Epoch 6927] diffusion training Loss: 0.05842205137014389
2024-11-05 01:25:49,150 - INFO - [diffusion][Epoch 6927] diffusion learning rate: 0.001
2024-11-05 01:25:49,152 - INFO - [diffusion][Epoch 6927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:49,153 - INFO - [diffusion][Epoch 6928] Epoch 6929/12000
2024-11-05 01:25:53,221 - INFO - [diffusion][Epoch 6928] diffusion training Loss: 0.055401548743247986
2024-11-05 01:25:53,223 - INFO - [diffusion][Epoch 6928] diffusion learning rate: 0.001
2024-11-05 01:25:53,225 - INFO - [diffusion][Epoch 6928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:53,226 - INFO - [diffusion][Epoch 6929] Epoch 6930/12000
2024-11-05 01:25:57,191 - INFO - [diffusion][Epoch 6929] diffusion training Loss: 0.06127746403217316
2024-11-05 01:25:57,193 - INFO - [diffusion][Epoch 6929] diffusion learning rate: 0.001
2024-11-05 01:25:57,195 - INFO - [diffusion][Epoch 6929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:25:57,196 - INFO - [diffusion][Epoch 6930] Epoch 6931/12000
2024-11-05 01:26:01,053 - INFO - [diffusion][Epoch 6930] diffusion training Loss: 0.056109859608113766
2024-11-05 01:26:01,055 - INFO - [diffusion][Epoch 6930] diffusion learning rate: 0.001
2024-11-05 01:26:01,057 - INFO - [diffusion][Epoch 6930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:01,060 - INFO - [diffusion][Epoch 6931] Epoch 6932/12000
2024-11-05 01:26:05,163 - INFO - [diffusion][Epoch 6931] diffusion training Loss: 0.055697694420814514
2024-11-05 01:26:05,165 - INFO - [diffusion][Epoch 6931] diffusion learning rate: 0.001
2024-11-05 01:26:05,167 - INFO - [diffusion][Epoch 6931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:05,168 - INFO - [diffusion][Epoch 6932] Epoch 6933/12000
2024-11-05 01:26:09,165 - INFO - [diffusion][Epoch 6932] diffusion training Loss: 0.05528600141406059
2024-11-05 01:26:09,168 - INFO - [diffusion][Epoch 6932] diffusion learning rate: 0.001
2024-11-05 01:26:09,170 - INFO - [diffusion][Epoch 6932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:09,171 - INFO - [diffusion][Epoch 6933] Epoch 6934/12000
2024-11-05 01:26:13,180 - INFO - [diffusion][Epoch 6933] diffusion training Loss: 0.061408272944390774
2024-11-05 01:26:13,183 - INFO - [diffusion][Epoch 6933] diffusion learning rate: 0.001
2024-11-05 01:26:13,184 - INFO - [diffusion][Epoch 6933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:13,186 - INFO - [diffusion][Epoch 6934] Epoch 6935/12000
2024-11-05 01:26:17,102 - INFO - [diffusion][Epoch 6934] diffusion training Loss: 0.05609961226582527
2024-11-05 01:26:17,105 - INFO - [diffusion][Epoch 6934] diffusion learning rate: 0.001
2024-11-05 01:26:17,107 - INFO - [diffusion][Epoch 6934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:17,108 - INFO - [diffusion][Epoch 6935] Epoch 6936/12000
2024-11-05 01:26:21,228 - INFO - [diffusion][Epoch 6935] diffusion training Loss: 0.056493623182177544
2024-11-05 01:26:21,231 - INFO - [diffusion][Epoch 6935] diffusion learning rate: 0.001
2024-11-05 01:26:21,233 - INFO - [diffusion][Epoch 6935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:21,234 - INFO - [diffusion][Epoch 6936] Epoch 6937/12000
2024-11-05 01:26:25,387 - INFO - [diffusion][Epoch 6936] diffusion training Loss: 0.055251904763281345
2024-11-05 01:26:25,389 - INFO - [diffusion][Epoch 6936] diffusion learning rate: 0.001
2024-11-05 01:26:25,391 - INFO - [diffusion][Epoch 6936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:25,393 - INFO - [diffusion][Epoch 6937] Epoch 6938/12000
2024-11-05 01:26:29,539 - INFO - [diffusion][Epoch 6937] diffusion training Loss: 0.05526008456945419
2024-11-05 01:26:29,541 - INFO - [diffusion][Epoch 6937] diffusion learning rate: 0.001
2024-11-05 01:26:29,542 - INFO - [diffusion][Epoch 6937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:29,544 - INFO - [diffusion][Epoch 6938] Epoch 6939/12000
2024-11-05 01:26:33,703 - INFO - [diffusion][Epoch 6938] diffusion training Loss: 0.06017926800996065
2024-11-05 01:26:33,739 - INFO - [diffusion][Epoch 6938] diffusion learning rate: 0.001
2024-11-05 01:26:33,741 - INFO - [diffusion][Epoch 6938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:33,743 - INFO - [diffusion][Epoch 6939] Epoch 6940/12000
2024-11-05 01:26:37,851 - INFO - [diffusion][Epoch 6939] diffusion training Loss: 0.05677807051688433
2024-11-05 01:26:37,853 - INFO - [diffusion][Epoch 6939] diffusion learning rate: 0.001
2024-11-05 01:26:37,854 - INFO - [diffusion][Epoch 6939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:37,856 - INFO - [diffusion][Epoch 6940] Epoch 6941/12000
2024-11-05 01:26:41,913 - INFO - [diffusion][Epoch 6940] diffusion training Loss: 0.05504900775849819
2024-11-05 01:26:41,915 - INFO - [diffusion][Epoch 6940] diffusion learning rate: 0.001
2024-11-05 01:26:41,917 - INFO - [diffusion][Epoch 6940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:41,919 - INFO - [diffusion][Epoch 6941] Epoch 6942/12000
2024-11-05 01:26:45,916 - INFO - [diffusion][Epoch 6941] diffusion training Loss: 0.05491668917238712
2024-11-05 01:26:45,918 - INFO - [diffusion][Epoch 6941] diffusion learning rate: 0.001
2024-11-05 01:26:45,920 - INFO - [diffusion][Epoch 6941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:45,922 - INFO - [diffusion][Epoch 6942] Epoch 6943/12000
2024-11-05 01:26:49,818 - INFO - [diffusion][Epoch 6942] diffusion training Loss: 0.059872474521398544
2024-11-05 01:26:49,853 - INFO - [diffusion][Epoch 6942] diffusion learning rate: 0.001
2024-11-05 01:26:49,855 - INFO - [diffusion][Epoch 6942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:49,856 - INFO - [diffusion][Epoch 6943] Epoch 6944/12000
2024-11-05 01:26:53,965 - INFO - [diffusion][Epoch 6943] diffusion training Loss: 0.058848717249929905
2024-11-05 01:26:53,967 - INFO - [diffusion][Epoch 6943] diffusion learning rate: 0.001
2024-11-05 01:26:53,969 - INFO - [diffusion][Epoch 6943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:53,970 - INFO - [diffusion][Epoch 6944] Epoch 6945/12000
2024-11-05 01:26:58,117 - INFO - [diffusion][Epoch 6944] diffusion training Loss: 0.05209116265177727
2024-11-05 01:26:58,119 - INFO - [diffusion][Epoch 6944] diffusion learning rate: 0.001
2024-11-05 01:26:58,121 - INFO - [diffusion][Epoch 6944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:26:58,122 - INFO - [diffusion][Epoch 6945] Epoch 6946/12000
2024-11-05 01:27:02,244 - INFO - [diffusion][Epoch 6945] diffusion training Loss: 0.060455490835011005
2024-11-05 01:27:02,246 - INFO - [diffusion][Epoch 6945] diffusion learning rate: 0.001
2024-11-05 01:27:02,248 - INFO - [diffusion][Epoch 6945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:02,249 - INFO - [diffusion][Epoch 6946] Epoch 6947/12000
2024-11-05 01:27:06,382 - INFO - [diffusion][Epoch 6946] diffusion training Loss: 0.059730853885412216
2024-11-05 01:27:06,384 - INFO - [diffusion][Epoch 6946] diffusion learning rate: 0.001
2024-11-05 01:27:06,386 - INFO - [diffusion][Epoch 6946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:06,387 - INFO - [diffusion][Epoch 6947] Epoch 6948/12000
2024-11-05 01:27:10,529 - INFO - [diffusion][Epoch 6947] diffusion training Loss: 0.05458898562937975
2024-11-05 01:27:10,531 - INFO - [diffusion][Epoch 6947] diffusion learning rate: 0.001
2024-11-05 01:27:10,532 - INFO - [diffusion][Epoch 6947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:10,534 - INFO - [diffusion][Epoch 6948] Epoch 6949/12000
2024-11-05 01:27:14,899 - INFO - [diffusion][Epoch 6948] diffusion training Loss: 0.05738591402769089
2024-11-05 01:27:14,901 - INFO - [diffusion][Epoch 6948] diffusion learning rate: 0.001
2024-11-05 01:27:14,903 - INFO - [diffusion][Epoch 6948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:14,904 - INFO - [diffusion][Epoch 6949] Epoch 6950/12000
2024-11-05 01:27:18,973 - INFO - [diffusion][Epoch 6949] diffusion training Loss: 0.0572435911744833
2024-11-05 01:27:18,975 - INFO - [diffusion][Epoch 6949] diffusion learning rate: 0.001
2024-11-05 01:27:18,977 - INFO - [diffusion][Epoch 6949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:18,978 - INFO - [diffusion][Epoch 6950] Epoch 6951/12000
2024-11-05 01:27:22,951 - INFO - [diffusion][Epoch 6950] diffusion training Loss: 0.05725524201989174
2024-11-05 01:27:22,953 - INFO - [diffusion][Epoch 6950] diffusion learning rate: 0.001
2024-11-05 01:27:22,955 - INFO - [diffusion][Epoch 6950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:22,957 - INFO - [diffusion][Epoch 6951] Epoch 6952/12000
2024-11-05 01:27:27,021 - INFO - [diffusion][Epoch 6951] diffusion training Loss: 0.056254493072628975
2024-11-05 01:27:27,023 - INFO - [diffusion][Epoch 6951] diffusion learning rate: 0.001
2024-11-05 01:27:27,025 - INFO - [diffusion][Epoch 6951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:27,026 - INFO - [diffusion][Epoch 6952] Epoch 6953/12000
2024-11-05 01:27:31,117 - INFO - [diffusion][Epoch 6952] diffusion training Loss: 0.057576436549425125
2024-11-05 01:27:31,119 - INFO - [diffusion][Epoch 6952] diffusion learning rate: 0.001
2024-11-05 01:27:31,121 - INFO - [diffusion][Epoch 6952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:31,122 - INFO - [diffusion][Epoch 6953] Epoch 6954/12000
2024-11-05 01:27:35,250 - INFO - [diffusion][Epoch 6953] diffusion training Loss: 0.06242016237229109
2024-11-05 01:27:35,253 - INFO - [diffusion][Epoch 6953] diffusion learning rate: 0.001
2024-11-05 01:27:35,256 - INFO - [diffusion][Epoch 6953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:35,258 - INFO - [diffusion][Epoch 6954] Epoch 6955/12000
2024-11-05 01:27:39,399 - INFO - [diffusion][Epoch 6954] diffusion training Loss: 0.05845094192773104
2024-11-05 01:27:39,401 - INFO - [diffusion][Epoch 6954] diffusion learning rate: 0.001
2024-11-05 01:27:39,403 - INFO - [diffusion][Epoch 6954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:39,404 - INFO - [diffusion][Epoch 6955] Epoch 6956/12000
2024-11-05 01:27:43,442 - INFO - [diffusion][Epoch 6955] diffusion training Loss: 0.06424151360988617
2024-11-05 01:27:43,445 - INFO - [diffusion][Epoch 6955] diffusion learning rate: 0.001
2024-11-05 01:27:43,446 - INFO - [diffusion][Epoch 6955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:43,448 - INFO - [diffusion][Epoch 6956] Epoch 6957/12000
2024-11-05 01:27:47,476 - INFO - [diffusion][Epoch 6956] diffusion training Loss: 0.058379877358675
2024-11-05 01:27:47,478 - INFO - [diffusion][Epoch 6956] diffusion learning rate: 0.001
2024-11-05 01:27:47,480 - INFO - [diffusion][Epoch 6956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:47,481 - INFO - [diffusion][Epoch 6957] Epoch 6958/12000
2024-11-05 01:27:51,368 - INFO - [diffusion][Epoch 6957] diffusion training Loss: 0.06416498590260744
2024-11-05 01:27:51,371 - INFO - [diffusion][Epoch 6957] diffusion learning rate: 0.001
2024-11-05 01:27:51,373 - INFO - [diffusion][Epoch 6957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:51,375 - INFO - [diffusion][Epoch 6958] Epoch 6959/12000
2024-11-05 01:27:55,361 - INFO - [diffusion][Epoch 6958] diffusion training Loss: 0.05546846520155668
2024-11-05 01:27:55,363 - INFO - [diffusion][Epoch 6958] diffusion learning rate: 0.001
2024-11-05 01:27:55,365 - INFO - [diffusion][Epoch 6958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:55,367 - INFO - [diffusion][Epoch 6959] Epoch 6960/12000
2024-11-05 01:27:59,531 - INFO - [diffusion][Epoch 6959] diffusion training Loss: 0.06200776994228363
2024-11-05 01:27:59,533 - INFO - [diffusion][Epoch 6959] diffusion learning rate: 0.001
2024-11-05 01:27:59,535 - INFO - [diffusion][Epoch 6959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:27:59,536 - INFO - [diffusion][Epoch 6960] Epoch 6961/12000
2024-11-05 01:28:03,528 - INFO - [diffusion][Epoch 6960] diffusion training Loss: 0.0580397117882967
2024-11-05 01:28:03,530 - INFO - [diffusion][Epoch 6960] diffusion learning rate: 0.001
2024-11-05 01:28:03,532 - INFO - [diffusion][Epoch 6960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:03,533 - INFO - [diffusion][Epoch 6961] Epoch 6962/12000
2024-11-05 01:28:07,588 - INFO - [diffusion][Epoch 6961] diffusion training Loss: 0.06091994047164917
2024-11-05 01:28:07,591 - INFO - [diffusion][Epoch 6961] diffusion learning rate: 0.001
2024-11-05 01:28:07,593 - INFO - [diffusion][Epoch 6961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:07,594 - INFO - [diffusion][Epoch 6962] Epoch 6963/12000
2024-11-05 01:28:11,531 - INFO - [diffusion][Epoch 6962] diffusion training Loss: 0.05892822798341513
2024-11-05 01:28:11,533 - INFO - [diffusion][Epoch 6962] diffusion learning rate: 0.001
2024-11-05 01:28:11,534 - INFO - [diffusion][Epoch 6962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:11,536 - INFO - [diffusion][Epoch 6963] Epoch 6964/12000
2024-11-05 01:28:15,564 - INFO - [diffusion][Epoch 6963] diffusion training Loss: 0.0549775455147028
2024-11-05 01:28:15,566 - INFO - [diffusion][Epoch 6963] diffusion learning rate: 0.001
2024-11-05 01:28:15,568 - INFO - [diffusion][Epoch 6963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:15,570 - INFO - [diffusion][Epoch 6964] Epoch 6965/12000
2024-11-05 01:28:19,689 - INFO - [diffusion][Epoch 6964] diffusion training Loss: 0.05849778093397617
2024-11-05 01:28:19,692 - INFO - [diffusion][Epoch 6964] diffusion learning rate: 0.001
2024-11-05 01:28:19,735 - INFO - [diffusion][Epoch 6964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:19,737 - INFO - [diffusion][Epoch 6965] Epoch 6966/12000
2024-11-05 01:28:23,840 - INFO - [diffusion][Epoch 6965] diffusion training Loss: 0.06029015593230724
2024-11-05 01:28:23,842 - INFO - [diffusion][Epoch 6965] diffusion learning rate: 0.001
2024-11-05 01:28:23,844 - INFO - [diffusion][Epoch 6965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:23,845 - INFO - [diffusion][Epoch 6966] Epoch 6967/12000
2024-11-05 01:28:27,949 - INFO - [diffusion][Epoch 6966] diffusion training Loss: 0.05486761312931776
2024-11-05 01:28:27,951 - INFO - [diffusion][Epoch 6966] diffusion learning rate: 0.001
2024-11-05 01:28:27,953 - INFO - [diffusion][Epoch 6966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:27,954 - INFO - [diffusion][Epoch 6967] Epoch 6968/12000
2024-11-05 01:28:31,895 - INFO - [diffusion][Epoch 6967] diffusion training Loss: 0.06102981977164745
2024-11-05 01:28:31,897 - INFO - [diffusion][Epoch 6967] diffusion learning rate: 0.001
2024-11-05 01:28:31,898 - INFO - [diffusion][Epoch 6967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:31,900 - INFO - [diffusion][Epoch 6968] Epoch 6969/12000
2024-11-05 01:28:36,056 - INFO - [diffusion][Epoch 6968] diffusion training Loss: 0.05628747306764126
2024-11-05 01:28:36,058 - INFO - [diffusion][Epoch 6968] diffusion learning rate: 0.001
2024-11-05 01:28:36,060 - INFO - [diffusion][Epoch 6968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:36,061 - INFO - [diffusion][Epoch 6969] Epoch 6970/12000
2024-11-05 01:28:40,054 - INFO - [diffusion][Epoch 6969] diffusion training Loss: 0.059219587594270706
2024-11-05 01:28:40,056 - INFO - [diffusion][Epoch 6969] diffusion learning rate: 0.001
2024-11-05 01:28:40,058 - INFO - [diffusion][Epoch 6969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:40,059 - INFO - [diffusion][Epoch 6970] Epoch 6971/12000
2024-11-05 01:28:44,128 - INFO - [diffusion][Epoch 6970] diffusion training Loss: 0.05381359905004501
2024-11-05 01:28:44,130 - INFO - [diffusion][Epoch 6970] diffusion learning rate: 0.001
2024-11-05 01:28:44,132 - INFO - [diffusion][Epoch 6970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:44,133 - INFO - [diffusion][Epoch 6971] Epoch 6972/12000
2024-11-05 01:28:48,060 - INFO - [diffusion][Epoch 6971] diffusion training Loss: 0.05436811875551939
2024-11-05 01:28:48,062 - INFO - [diffusion][Epoch 6971] diffusion learning rate: 0.001
2024-11-05 01:28:48,064 - INFO - [diffusion][Epoch 6971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:48,065 - INFO - [diffusion][Epoch 6972] Epoch 6973/12000
2024-11-05 01:28:52,084 - INFO - [diffusion][Epoch 6972] diffusion training Loss: 0.05833458807319403
2024-11-05 01:28:52,087 - INFO - [diffusion][Epoch 6972] diffusion learning rate: 0.001
2024-11-05 01:28:52,089 - INFO - [diffusion][Epoch 6972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:52,090 - INFO - [diffusion][Epoch 6973] Epoch 6974/12000
2024-11-05 01:28:56,150 - INFO - [diffusion][Epoch 6973] diffusion training Loss: 0.0586685324087739
2024-11-05 01:28:56,152 - INFO - [diffusion][Epoch 6973] diffusion learning rate: 0.001
2024-11-05 01:28:56,154 - INFO - [diffusion][Epoch 6973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:28:56,155 - INFO - [diffusion][Epoch 6974] Epoch 6975/12000
2024-11-05 01:29:00,288 - INFO - [diffusion][Epoch 6974] diffusion training Loss: 0.0573920588940382
2024-11-05 01:29:00,290 - INFO - [diffusion][Epoch 6974] diffusion learning rate: 0.001
2024-11-05 01:29:00,292 - INFO - [diffusion][Epoch 6974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:00,293 - INFO - [diffusion][Epoch 6975] Epoch 6976/12000
2024-11-05 01:29:04,383 - INFO - [diffusion][Epoch 6975] diffusion training Loss: 0.06298990547657013
2024-11-05 01:29:04,385 - INFO - [diffusion][Epoch 6975] diffusion learning rate: 0.001
2024-11-05 01:29:04,387 - INFO - [diffusion][Epoch 6975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:04,389 - INFO - [diffusion][Epoch 6976] Epoch 6977/12000
2024-11-05 01:29:08,541 - INFO - [diffusion][Epoch 6976] diffusion training Loss: 0.05599515791982412
2024-11-05 01:29:08,543 - INFO - [diffusion][Epoch 6976] diffusion learning rate: 0.001
2024-11-05 01:29:08,545 - INFO - [diffusion][Epoch 6976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:08,546 - INFO - [diffusion][Epoch 6977] Epoch 6978/12000
2024-11-05 01:29:12,709 - INFO - [diffusion][Epoch 6977] diffusion training Loss: 0.05837446916848421
2024-11-05 01:29:12,893 - INFO - [diffusion][Epoch 6977] diffusion learning rate: 0.001
2024-11-05 01:29:12,894 - INFO - [diffusion][Epoch 6977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:12,896 - INFO - [diffusion][Epoch 6978] Epoch 6979/12000
2024-11-05 01:29:16,893 - INFO - [diffusion][Epoch 6978] diffusion training Loss: 0.05824964493513107
2024-11-05 01:29:16,896 - INFO - [diffusion][Epoch 6978] diffusion learning rate: 0.001
2024-11-05 01:29:16,897 - INFO - [diffusion][Epoch 6978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:16,899 - INFO - [diffusion][Epoch 6979] Epoch 6980/12000
2024-11-05 01:29:21,043 - INFO - [diffusion][Epoch 6979] diffusion training Loss: 0.057850493118166924
2024-11-05 01:29:21,046 - INFO - [diffusion][Epoch 6979] diffusion learning rate: 0.001
2024-11-05 01:29:21,048 - INFO - [diffusion][Epoch 6979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:21,049 - INFO - [diffusion][Epoch 6980] Epoch 6981/12000
2024-11-05 01:29:25,142 - INFO - [diffusion][Epoch 6980] diffusion training Loss: 0.05820538941770792
2024-11-05 01:29:25,144 - INFO - [diffusion][Epoch 6980] diffusion learning rate: 0.001
2024-11-05 01:29:25,146 - INFO - [diffusion][Epoch 6980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:25,148 - INFO - [diffusion][Epoch 6981] Epoch 6982/12000
2024-11-05 01:29:29,327 - INFO - [diffusion][Epoch 6981] diffusion training Loss: 0.05917810741811991
2024-11-05 01:29:29,329 - INFO - [diffusion][Epoch 6981] diffusion learning rate: 0.001
2024-11-05 01:29:29,331 - INFO - [diffusion][Epoch 6981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:29,332 - INFO - [diffusion][Epoch 6982] Epoch 6983/12000
2024-11-05 01:29:33,415 - INFO - [diffusion][Epoch 6982] diffusion training Loss: 0.056802092120051384
2024-11-05 01:29:33,417 - INFO - [diffusion][Epoch 6982] diffusion learning rate: 0.001
2024-11-05 01:29:33,419 - INFO - [diffusion][Epoch 6982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:33,420 - INFO - [diffusion][Epoch 6983] Epoch 6984/12000
2024-11-05 01:29:37,348 - INFO - [diffusion][Epoch 6983] diffusion training Loss: 0.05882594268769026
2024-11-05 01:29:37,350 - INFO - [diffusion][Epoch 6983] diffusion learning rate: 0.001
2024-11-05 01:29:37,351 - INFO - [diffusion][Epoch 6983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:37,353 - INFO - [diffusion][Epoch 6984] Epoch 6985/12000
2024-11-05 01:29:41,476 - INFO - [diffusion][Epoch 6984] diffusion training Loss: 0.060186984948813915
2024-11-05 01:29:41,478 - INFO - [diffusion][Epoch 6984] diffusion learning rate: 0.001
2024-11-05 01:29:41,480 - INFO - [diffusion][Epoch 6984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:41,481 - INFO - [diffusion][Epoch 6985] Epoch 6986/12000
2024-11-05 01:29:45,614 - INFO - [diffusion][Epoch 6985] diffusion training Loss: 0.0550982803106308
2024-11-05 01:29:45,617 - INFO - [diffusion][Epoch 6985] diffusion learning rate: 0.001
2024-11-05 01:29:45,681 - INFO - [diffusion][Epoch 6985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:45,683 - INFO - [diffusion][Epoch 6986] Epoch 6987/12000
2024-11-05 01:29:49,771 - INFO - [diffusion][Epoch 6986] diffusion training Loss: 0.05761336348950863
2024-11-05 01:29:49,773 - INFO - [diffusion][Epoch 6986] diffusion learning rate: 0.001
2024-11-05 01:29:49,775 - INFO - [diffusion][Epoch 6986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:49,777 - INFO - [diffusion][Epoch 6987] Epoch 6988/12000
2024-11-05 01:29:53,784 - INFO - [diffusion][Epoch 6987] diffusion training Loss: 0.05395723693072796
2024-11-05 01:29:53,787 - INFO - [diffusion][Epoch 6987] diffusion learning rate: 0.001
2024-11-05 01:29:53,789 - INFO - [diffusion][Epoch 6987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:53,790 - INFO - [diffusion][Epoch 6988] Epoch 6989/12000
2024-11-05 01:29:57,886 - INFO - [diffusion][Epoch 6988] diffusion training Loss: 0.057743921875953674
2024-11-05 01:29:57,888 - INFO - [diffusion][Epoch 6988] diffusion learning rate: 0.001
2024-11-05 01:29:57,890 - INFO - [diffusion][Epoch 6988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:29:57,891 - INFO - [diffusion][Epoch 6989] Epoch 6990/12000
2024-11-05 01:30:02,490 - INFO - [diffusion][Epoch 6989] diffusion training Loss: 0.05728685110807419
2024-11-05 01:30:02,491 - INFO - [diffusion][Epoch 6989] diffusion learning rate: 0.001
2024-11-05 01:30:02,493 - INFO - [diffusion][Epoch 6989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:02,494 - INFO - [diffusion][Epoch 6990] Epoch 6991/12000
2024-11-05 01:30:06,549 - INFO - [diffusion][Epoch 6990] diffusion training Loss: 0.059857284650206566
2024-11-05 01:30:06,551 - INFO - [diffusion][Epoch 6990] diffusion learning rate: 0.001
2024-11-05 01:30:06,553 - INFO - [diffusion][Epoch 6990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:06,555 - INFO - [diffusion][Epoch 6991] Epoch 6992/12000
2024-11-05 01:30:10,663 - INFO - [diffusion][Epoch 6991] diffusion training Loss: 0.05813177488744259
2024-11-05 01:30:10,665 - INFO - [diffusion][Epoch 6991] diffusion learning rate: 0.001
2024-11-05 01:30:10,667 - INFO - [diffusion][Epoch 6991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:10,668 - INFO - [diffusion][Epoch 6992] Epoch 6993/12000
2024-11-05 01:30:14,841 - INFO - [diffusion][Epoch 6992] diffusion training Loss: 0.057277471758425236
2024-11-05 01:30:14,843 - INFO - [diffusion][Epoch 6992] diffusion learning rate: 0.001
2024-11-05 01:30:14,845 - INFO - [diffusion][Epoch 6992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:14,846 - INFO - [diffusion][Epoch 6993] Epoch 6994/12000
2024-11-05 01:30:18,972 - INFO - [diffusion][Epoch 6993] diffusion training Loss: 0.05863723251968622
2024-11-05 01:30:18,975 - INFO - [diffusion][Epoch 6993] diffusion learning rate: 0.001
2024-11-05 01:30:18,977 - INFO - [diffusion][Epoch 6993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:18,978 - INFO - [diffusion][Epoch 6994] Epoch 6995/12000
2024-11-05 01:30:23,045 - INFO - [diffusion][Epoch 6994] diffusion training Loss: 0.05110716447234154
2024-11-05 01:30:23,048 - INFO - [diffusion][Epoch 6994] diffusion learning rate: 0.001
2024-11-05 01:30:23,050 - INFO - [diffusion][Epoch 6994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:23,051 - INFO - [diffusion][Epoch 6995] Epoch 6996/12000
2024-11-05 01:30:27,159 - INFO - [diffusion][Epoch 6995] diffusion training Loss: 0.05727669782936573
2024-11-05 01:30:27,161 - INFO - [diffusion][Epoch 6995] diffusion learning rate: 0.001
2024-11-05 01:30:27,163 - INFO - [diffusion][Epoch 6995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:27,164 - INFO - [diffusion][Epoch 6996] Epoch 6997/12000
2024-11-05 01:30:31,325 - INFO - [diffusion][Epoch 6996] diffusion training Loss: 0.058212922886013985
2024-11-05 01:30:31,327 - INFO - [diffusion][Epoch 6996] diffusion learning rate: 0.001
2024-11-05 01:30:31,329 - INFO - [diffusion][Epoch 6996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:31,330 - INFO - [diffusion][Epoch 6997] Epoch 6998/12000
2024-11-05 01:30:35,320 - INFO - [diffusion][Epoch 6997] diffusion training Loss: 0.05459444038569927
2024-11-05 01:30:35,323 - INFO - [diffusion][Epoch 6997] diffusion learning rate: 0.001
2024-11-05 01:30:35,325 - INFO - [diffusion][Epoch 6997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:35,327 - INFO - [diffusion][Epoch 6998] Epoch 6999/12000
2024-11-05 01:30:39,424 - INFO - [diffusion][Epoch 6998] diffusion training Loss: 0.06024136580526829
2024-11-05 01:30:39,426 - INFO - [diffusion][Epoch 6998] diffusion learning rate: 0.001
2024-11-05 01:30:39,428 - INFO - [diffusion][Epoch 6998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:39,429 - INFO - [diffusion][Epoch 6999] Epoch 7000/12000
2024-11-05 01:30:43,520 - INFO - [diffusion][Epoch 6999] diffusion training Loss: 0.05693707801401615
2024-11-05 01:30:43,522 - INFO - [diffusion][Epoch 6999] diffusion learning rate: 0.001
2024-11-05 01:30:43,712 - INFO - [diffusion][Epoch 6999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:43,714 - INFO - [diffusion][Epoch 7000] Epoch 7001/12000
2024-11-05 01:30:47,834 - INFO - [diffusion][Epoch 7000] diffusion training Loss: 0.05587842781096697
2024-11-05 01:30:47,836 - INFO - [diffusion][Epoch 7000] diffusion learning rate: 0.001
2024-11-05 01:30:47,838 - INFO - [diffusion][Epoch 7000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:47,839 - INFO - [diffusion][Epoch 7001] Epoch 7002/12000
2024-11-05 01:30:51,920 - INFO - [diffusion][Epoch 7001] diffusion training Loss: 0.0522434888407588
2024-11-05 01:30:51,922 - INFO - [diffusion][Epoch 7001] diffusion learning rate: 0.001
2024-11-05 01:30:52,001 - INFO - [diffusion][Epoch 7001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:52,003 - INFO - [diffusion][Epoch 7002] Epoch 7003/12000
2024-11-05 01:30:56,109 - INFO - [diffusion][Epoch 7002] diffusion training Loss: 0.06078563071787357
2024-11-05 01:30:56,111 - INFO - [diffusion][Epoch 7002] diffusion learning rate: 0.001
2024-11-05 01:30:56,113 - INFO - [diffusion][Epoch 7002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:30:56,114 - INFO - [diffusion][Epoch 7003] Epoch 7004/12000
2024-11-05 01:31:00,241 - INFO - [diffusion][Epoch 7003] diffusion training Loss: 0.0570563031360507
2024-11-05 01:31:00,243 - INFO - [diffusion][Epoch 7003] diffusion learning rate: 0.001
2024-11-05 01:31:00,245 - INFO - [diffusion][Epoch 7003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:00,247 - INFO - [diffusion][Epoch 7004] Epoch 7005/12000
2024-11-05 01:31:04,387 - INFO - [diffusion][Epoch 7004] diffusion training Loss: 0.05187187064439058
2024-11-05 01:31:04,389 - INFO - [diffusion][Epoch 7004] diffusion learning rate: 0.001
2024-11-05 01:31:04,391 - INFO - [diffusion][Epoch 7004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:04,393 - INFO - [diffusion][Epoch 7005] Epoch 7006/12000
2024-11-05 01:31:08,533 - INFO - [diffusion][Epoch 7005] diffusion training Loss: 0.059133014641702175
2024-11-05 01:31:08,535 - INFO - [diffusion][Epoch 7005] diffusion learning rate: 0.001
2024-11-05 01:31:08,537 - INFO - [diffusion][Epoch 7005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:08,538 - INFO - [diffusion][Epoch 7006] Epoch 7007/12000
2024-11-05 01:31:12,649 - INFO - [diffusion][Epoch 7006] diffusion training Loss: 0.05524786002933979
2024-11-05 01:31:12,651 - INFO - [diffusion][Epoch 7006] diffusion learning rate: 0.001
2024-11-05 01:31:12,653 - INFO - [diffusion][Epoch 7006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:12,654 - INFO - [diffusion][Epoch 7007] Epoch 7008/12000
2024-11-05 01:31:16,825 - INFO - [diffusion][Epoch 7007] diffusion training Loss: 0.0603368179872632
2024-11-05 01:31:16,827 - INFO - [diffusion][Epoch 7007] diffusion learning rate: 0.001
2024-11-05 01:31:16,829 - INFO - [diffusion][Epoch 7007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:16,830 - INFO - [diffusion][Epoch 7008] Epoch 7009/12000
2024-11-05 01:31:21,024 - INFO - [diffusion][Epoch 7008] diffusion training Loss: 0.059509120881557465
2024-11-05 01:31:21,026 - INFO - [diffusion][Epoch 7008] diffusion learning rate: 0.001
2024-11-05 01:31:21,028 - INFO - [diffusion][Epoch 7008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:21,029 - INFO - [diffusion][Epoch 7009] Epoch 7010/12000
2024-11-05 01:31:25,075 - INFO - [diffusion][Epoch 7009] diffusion training Loss: 0.0574756832793355
2024-11-05 01:31:25,077 - INFO - [diffusion][Epoch 7009] diffusion learning rate: 0.001
2024-11-05 01:31:25,080 - INFO - [diffusion][Epoch 7009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:25,081 - INFO - [diffusion][Epoch 7010] Epoch 7011/12000
2024-11-05 01:31:29,089 - INFO - [diffusion][Epoch 7010] diffusion training Loss: 0.06354779750108719
2024-11-05 01:31:29,091 - INFO - [diffusion][Epoch 7010] diffusion learning rate: 0.001
2024-11-05 01:31:29,093 - INFO - [diffusion][Epoch 7010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:29,094 - INFO - [diffusion][Epoch 7011] Epoch 7012/12000
2024-11-05 01:31:33,126 - INFO - [diffusion][Epoch 7011] diffusion training Loss: 0.055639542639255524
2024-11-05 01:31:33,128 - INFO - [diffusion][Epoch 7011] diffusion learning rate: 0.001
2024-11-05 01:31:33,130 - INFO - [diffusion][Epoch 7011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:33,131 - INFO - [diffusion][Epoch 7012] Epoch 7013/12000
2024-11-05 01:31:37,052 - INFO - [diffusion][Epoch 7012] diffusion training Loss: 0.06058367807418108
2024-11-05 01:31:37,111 - INFO - [diffusion][Epoch 7012] diffusion learning rate: 0.001
2024-11-05 01:31:37,113 - INFO - [diffusion][Epoch 7012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:37,114 - INFO - [diffusion][Epoch 7013] Epoch 7014/12000
2024-11-05 01:31:41,298 - INFO - [diffusion][Epoch 7013] diffusion training Loss: 0.056503161787986755
2024-11-05 01:31:41,299 - INFO - [diffusion][Epoch 7013] diffusion learning rate: 0.001
2024-11-05 01:31:41,301 - INFO - [diffusion][Epoch 7013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:41,303 - INFO - [diffusion][Epoch 7014] Epoch 7015/12000
2024-11-05 01:31:45,439 - INFO - [diffusion][Epoch 7014] diffusion training Loss: 0.06018529739230871
2024-11-05 01:31:45,441 - INFO - [diffusion][Epoch 7014] diffusion learning rate: 0.001
2024-11-05 01:31:45,443 - INFO - [diffusion][Epoch 7014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:45,445 - INFO - [diffusion][Epoch 7015] Epoch 7016/12000
2024-11-05 01:31:49,392 - INFO - [diffusion][Epoch 7015] diffusion training Loss: 0.056065925396978855
2024-11-05 01:31:49,394 - INFO - [diffusion][Epoch 7015] diffusion learning rate: 0.001
2024-11-05 01:31:49,396 - INFO - [diffusion][Epoch 7015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:49,397 - INFO - [diffusion][Epoch 7016] Epoch 7017/12000
2024-11-05 01:31:53,426 - INFO - [diffusion][Epoch 7016] diffusion training Loss: 0.05093423929065466
2024-11-05 01:31:53,428 - INFO - [diffusion][Epoch 7016] diffusion learning rate: 0.001
2024-11-05 01:31:53,430 - INFO - [diffusion][Epoch 7016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:53,431 - INFO - [diffusion][Epoch 7017] Epoch 7018/12000
2024-11-05 01:31:57,595 - INFO - [diffusion][Epoch 7017] diffusion training Loss: 0.054923190735280514
2024-11-05 01:31:57,598 - INFO - [diffusion][Epoch 7017] diffusion learning rate: 0.001
2024-11-05 01:31:57,599 - INFO - [diffusion][Epoch 7017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:31:57,601 - INFO - [diffusion][Epoch 7018] Epoch 7019/12000
2024-11-05 01:32:01,466 - INFO - [diffusion][Epoch 7018] diffusion training Loss: 0.0561637245118618
2024-11-05 01:32:01,468 - INFO - [diffusion][Epoch 7018] diffusion learning rate: 0.001
2024-11-05 01:32:01,470 - INFO - [diffusion][Epoch 7018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:01,471 - INFO - [diffusion][Epoch 7019] Epoch 7020/12000
2024-11-05 01:32:05,387 - INFO - [diffusion][Epoch 7019] diffusion training Loss: 0.05602709297090769
2024-11-05 01:32:05,389 - INFO - [diffusion][Epoch 7019] diffusion learning rate: 0.001
2024-11-05 01:32:05,449 - INFO - [diffusion][Epoch 7019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:05,451 - INFO - [diffusion][Epoch 7020] Epoch 7021/12000
2024-11-05 01:32:09,438 - INFO - [diffusion][Epoch 7020] diffusion training Loss: 0.05402381997555494
2024-11-05 01:32:09,441 - INFO - [diffusion][Epoch 7020] diffusion learning rate: 0.001
2024-11-05 01:32:09,444 - INFO - [diffusion][Epoch 7020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:09,445 - INFO - [diffusion][Epoch 7021] Epoch 7022/12000
2024-11-05 01:32:13,508 - INFO - [diffusion][Epoch 7021] diffusion training Loss: 0.06019709259271622
2024-11-05 01:32:13,510 - INFO - [diffusion][Epoch 7021] diffusion learning rate: 0.001
2024-11-05 01:32:13,512 - INFO - [diffusion][Epoch 7021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:13,513 - INFO - [diffusion][Epoch 7022] Epoch 7023/12000
2024-11-05 01:32:17,593 - INFO - [diffusion][Epoch 7022] diffusion training Loss: 0.05945194140076637
2024-11-05 01:32:17,595 - INFO - [diffusion][Epoch 7022] diffusion learning rate: 0.001
2024-11-05 01:32:17,596 - INFO - [diffusion][Epoch 7022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:17,597 - INFO - [diffusion][Epoch 7023] Epoch 7024/12000
2024-11-05 01:32:21,683 - INFO - [diffusion][Epoch 7023] diffusion training Loss: 0.05971251521259546
2024-11-05 01:32:21,685 - INFO - [diffusion][Epoch 7023] diffusion learning rate: 0.001
2024-11-05 01:32:21,687 - INFO - [diffusion][Epoch 7023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:21,688 - INFO - [diffusion][Epoch 7024] Epoch 7025/12000
2024-11-05 01:32:25,778 - INFO - [diffusion][Epoch 7024] diffusion training Loss: 0.056443654000759125
2024-11-05 01:32:25,781 - INFO - [diffusion][Epoch 7024] diffusion learning rate: 0.001
2024-11-05 01:32:25,783 - INFO - [diffusion][Epoch 7024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:25,784 - INFO - [diffusion][Epoch 7025] Epoch 7026/12000
2024-11-05 01:32:29,888 - INFO - [diffusion][Epoch 7025] diffusion training Loss: 0.048874568194150925
2024-11-05 01:32:29,891 - INFO - [diffusion][Epoch 7025] diffusion learning rate: 0.001
2024-11-05 01:32:29,893 - INFO - [diffusion][Epoch 7025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:29,894 - INFO - [diffusion][Epoch 7026] Epoch 7027/12000
2024-11-05 01:32:34,055 - INFO - [diffusion][Epoch 7026] diffusion training Loss: 0.05295547749847174
2024-11-05 01:32:34,058 - INFO - [diffusion][Epoch 7026] diffusion learning rate: 0.001
2024-11-05 01:32:34,084 - INFO - [diffusion][Epoch 7026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:34,086 - INFO - [diffusion][Epoch 7027] Epoch 7028/12000
2024-11-05 01:32:38,242 - INFO - [diffusion][Epoch 7027] diffusion training Loss: 0.05563159193843603
2024-11-05 01:32:38,244 - INFO - [diffusion][Epoch 7027] diffusion learning rate: 0.001
2024-11-05 01:32:38,246 - INFO - [diffusion][Epoch 7027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:38,247 - INFO - [diffusion][Epoch 7028] Epoch 7029/12000
2024-11-05 01:32:42,364 - INFO - [diffusion][Epoch 7028] diffusion training Loss: 0.057118725031614304
2024-11-05 01:32:42,366 - INFO - [diffusion][Epoch 7028] diffusion learning rate: 0.001
2024-11-05 01:32:42,368 - INFO - [diffusion][Epoch 7028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:42,369 - INFO - [diffusion][Epoch 7029] Epoch 7030/12000
2024-11-05 01:32:46,363 - INFO - [diffusion][Epoch 7029] diffusion training Loss: 0.056890745647251606
2024-11-05 01:32:46,366 - INFO - [diffusion][Epoch 7029] diffusion learning rate: 0.001
2024-11-05 01:32:46,368 - INFO - [diffusion][Epoch 7029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:46,369 - INFO - [diffusion][Epoch 7030] Epoch 7031/12000
2024-11-05 01:32:50,483 - INFO - [diffusion][Epoch 7030] diffusion training Loss: 0.0618958929553628
2024-11-05 01:32:50,485 - INFO - [diffusion][Epoch 7030] diffusion learning rate: 0.001
2024-11-05 01:32:50,512 - INFO - [diffusion][Epoch 7030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:50,514 - INFO - [diffusion][Epoch 7031] Epoch 7032/12000
2024-11-05 01:32:54,605 - INFO - [diffusion][Epoch 7031] diffusion training Loss: 0.06303867604583502
2024-11-05 01:32:54,608 - INFO - [diffusion][Epoch 7031] diffusion learning rate: 0.001
2024-11-05 01:32:54,609 - INFO - [diffusion][Epoch 7031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:54,611 - INFO - [diffusion][Epoch 7032] Epoch 7033/12000
2024-11-05 01:32:58,697 - INFO - [diffusion][Epoch 7032] diffusion training Loss: 0.05740884970873594
2024-11-05 01:32:58,700 - INFO - [diffusion][Epoch 7032] diffusion learning rate: 0.001
2024-11-05 01:32:58,702 - INFO - [diffusion][Epoch 7032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:32:58,703 - INFO - [diffusion][Epoch 7033] Epoch 7034/12000
2024-11-05 01:33:02,765 - INFO - [diffusion][Epoch 7033] diffusion training Loss: 0.052643926814198494
2024-11-05 01:33:02,768 - INFO - [diffusion][Epoch 7033] diffusion learning rate: 0.001
2024-11-05 01:33:02,769 - INFO - [diffusion][Epoch 7033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:02,771 - INFO - [diffusion][Epoch 7034] Epoch 7035/12000
2024-11-05 01:33:06,926 - INFO - [diffusion][Epoch 7034] diffusion training Loss: 0.0553358132019639
2024-11-05 01:33:06,959 - INFO - [diffusion][Epoch 7034] diffusion learning rate: 0.001
2024-11-05 01:33:06,961 - INFO - [diffusion][Epoch 7034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:06,962 - INFO - [diffusion][Epoch 7035] Epoch 7036/12000
2024-11-05 01:33:11,116 - INFO - [diffusion][Epoch 7035] diffusion training Loss: 0.06090753432363272
2024-11-05 01:33:11,118 - INFO - [diffusion][Epoch 7035] diffusion learning rate: 0.001
2024-11-05 01:33:11,120 - INFO - [diffusion][Epoch 7035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:11,122 - INFO - [diffusion][Epoch 7036] Epoch 7037/12000
2024-11-05 01:33:15,216 - INFO - [diffusion][Epoch 7036] diffusion training Loss: 0.05848949309438467
2024-11-05 01:33:15,218 - INFO - [diffusion][Epoch 7036] diffusion learning rate: 0.001
2024-11-05 01:33:15,220 - INFO - [diffusion][Epoch 7036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:15,221 - INFO - [diffusion][Epoch 7037] Epoch 7038/12000
2024-11-05 01:33:19,381 - INFO - [diffusion][Epoch 7037] diffusion training Loss: 0.05589609779417515
2024-11-05 01:33:19,383 - INFO - [diffusion][Epoch 7037] diffusion learning rate: 0.001
2024-11-05 01:33:19,385 - INFO - [diffusion][Epoch 7037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:19,386 - INFO - [diffusion][Epoch 7038] Epoch 7039/12000
2024-11-05 01:33:23,516 - INFO - [diffusion][Epoch 7038] diffusion training Loss: 0.053455232642591
2024-11-05 01:33:23,517 - INFO - [diffusion][Epoch 7038] diffusion learning rate: 0.001
2024-11-05 01:33:23,519 - INFO - [diffusion][Epoch 7038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:23,520 - INFO - [diffusion][Epoch 7039] Epoch 7040/12000
2024-11-05 01:33:27,644 - INFO - [diffusion][Epoch 7039] diffusion training Loss: 0.05962581653147936
2024-11-05 01:33:27,647 - INFO - [diffusion][Epoch 7039] diffusion learning rate: 0.001
2024-11-05 01:33:27,648 - INFO - [diffusion][Epoch 7039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:27,650 - INFO - [diffusion][Epoch 7040] Epoch 7041/12000
2024-11-05 01:33:31,705 - INFO - [diffusion][Epoch 7040] diffusion training Loss: 0.05675748363137245
2024-11-05 01:33:31,707 - INFO - [diffusion][Epoch 7040] diffusion learning rate: 0.001
2024-11-05 01:33:31,708 - INFO - [diffusion][Epoch 7040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:31,710 - INFO - [diffusion][Epoch 7041] Epoch 7042/12000
2024-11-05 01:33:35,861 - INFO - [diffusion][Epoch 7041] diffusion training Loss: 0.05888315010815859
2024-11-05 01:33:35,862 - INFO - [diffusion][Epoch 7041] diffusion learning rate: 0.001
2024-11-05 01:33:35,864 - INFO - [diffusion][Epoch 7041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:35,866 - INFO - [diffusion][Epoch 7042] Epoch 7043/12000
2024-11-05 01:33:39,966 - INFO - [diffusion][Epoch 7042] diffusion training Loss: 0.055882795713841915
2024-11-05 01:33:39,968 - INFO - [diffusion][Epoch 7042] diffusion learning rate: 0.001
2024-11-05 01:33:39,969 - INFO - [diffusion][Epoch 7042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:39,971 - INFO - [diffusion][Epoch 7043] Epoch 7044/12000
2024-11-05 01:33:44,087 - INFO - [diffusion][Epoch 7043] diffusion training Loss: 0.0563862519338727
2024-11-05 01:33:44,089 - INFO - [diffusion][Epoch 7043] diffusion learning rate: 0.001
2024-11-05 01:33:44,090 - INFO - [diffusion][Epoch 7043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:44,092 - INFO - [diffusion][Epoch 7044] Epoch 7045/12000
2024-11-05 01:33:48,207 - INFO - [diffusion][Epoch 7044] diffusion training Loss: 0.06304171402007341
2024-11-05 01:33:48,209 - INFO - [diffusion][Epoch 7044] diffusion learning rate: 0.001
2024-11-05 01:33:48,211 - INFO - [diffusion][Epoch 7044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:48,212 - INFO - [diffusion][Epoch 7045] Epoch 7046/12000
2024-11-05 01:33:52,371 - INFO - [diffusion][Epoch 7045] diffusion training Loss: 0.054835766553878784
2024-11-05 01:33:52,373 - INFO - [diffusion][Epoch 7045] diffusion learning rate: 0.001
2024-11-05 01:33:52,375 - INFO - [diffusion][Epoch 7045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:52,376 - INFO - [diffusion][Epoch 7046] Epoch 7047/12000
2024-11-05 01:33:56,540 - INFO - [diffusion][Epoch 7046] diffusion training Loss: 0.06569262221455574
2024-11-05 01:33:56,542 - INFO - [diffusion][Epoch 7046] diffusion learning rate: 0.001
2024-11-05 01:33:56,544 - INFO - [diffusion][Epoch 7046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:33:56,545 - INFO - [diffusion][Epoch 7047] Epoch 7048/12000
2024-11-05 01:34:00,651 - INFO - [diffusion][Epoch 7047] diffusion training Loss: 0.053860872983932495
2024-11-05 01:34:00,653 - INFO - [diffusion][Epoch 7047] diffusion learning rate: 0.001
2024-11-05 01:34:00,655 - INFO - [diffusion][Epoch 7047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:00,656 - INFO - [diffusion][Epoch 7048] Epoch 7049/12000
2024-11-05 01:34:04,780 - INFO - [diffusion][Epoch 7048] diffusion training Loss: 0.057906498201191425
2024-11-05 01:34:04,782 - INFO - [diffusion][Epoch 7048] diffusion learning rate: 0.001
2024-11-05 01:34:04,784 - INFO - [diffusion][Epoch 7048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:04,786 - INFO - [diffusion][Epoch 7049] Epoch 7050/12000
2024-11-05 01:34:08,946 - INFO - [diffusion][Epoch 7049] diffusion training Loss: 0.055724985897541046
2024-11-05 01:34:08,949 - INFO - [diffusion][Epoch 7049] diffusion learning rate: 0.001
2024-11-05 01:34:08,951 - INFO - [diffusion][Epoch 7049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:08,953 - INFO - [diffusion][Epoch 7050] Epoch 7051/12000
2024-11-05 01:34:13,114 - INFO - [diffusion][Epoch 7050] diffusion training Loss: 0.05837501585483551
2024-11-05 01:34:13,116 - INFO - [diffusion][Epoch 7050] diffusion learning rate: 0.001
2024-11-05 01:34:13,118 - INFO - [diffusion][Epoch 7050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:13,119 - INFO - [diffusion][Epoch 7051] Epoch 7052/12000
2024-11-05 01:34:17,303 - INFO - [diffusion][Epoch 7051] diffusion training Loss: 0.058928949758410454
2024-11-05 01:34:17,305 - INFO - [diffusion][Epoch 7051] diffusion learning rate: 0.001
2024-11-05 01:34:17,307 - INFO - [diffusion][Epoch 7051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:17,308 - INFO - [diffusion][Epoch 7052] Epoch 7053/12000
2024-11-05 01:34:21,623 - INFO - [diffusion][Epoch 7052] diffusion training Loss: 0.05535968113690615
2024-11-05 01:34:21,624 - INFO - [diffusion][Epoch 7052] diffusion learning rate: 0.001
2024-11-05 01:34:21,626 - INFO - [diffusion][Epoch 7052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:21,627 - INFO - [diffusion][Epoch 7053] Epoch 7054/12000
2024-11-05 01:34:25,752 - INFO - [diffusion][Epoch 7053] diffusion training Loss: 0.059837089851498604
2024-11-05 01:34:25,754 - INFO - [diffusion][Epoch 7053] diffusion learning rate: 0.001
2024-11-05 01:34:25,755 - INFO - [diffusion][Epoch 7053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:25,757 - INFO - [diffusion][Epoch 7054] Epoch 7055/12000
2024-11-05 01:34:29,886 - INFO - [diffusion][Epoch 7054] diffusion training Loss: 0.05653366260230541
2024-11-05 01:34:29,889 - INFO - [diffusion][Epoch 7054] diffusion learning rate: 0.001
2024-11-05 01:34:29,891 - INFO - [diffusion][Epoch 7054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:29,892 - INFO - [diffusion][Epoch 7055] Epoch 7056/12000
2024-11-05 01:34:34,030 - INFO - [diffusion][Epoch 7055] diffusion training Loss: 0.05534894671291113
2024-11-05 01:34:34,032 - INFO - [diffusion][Epoch 7055] diffusion learning rate: 0.001
2024-11-05 01:34:34,034 - INFO - [diffusion][Epoch 7055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:34,036 - INFO - [diffusion][Epoch 7056] Epoch 7057/12000
2024-11-05 01:34:38,180 - INFO - [diffusion][Epoch 7056] diffusion training Loss: 0.05575820989906788
2024-11-05 01:34:38,182 - INFO - [diffusion][Epoch 7056] diffusion learning rate: 0.001
2024-11-05 01:34:38,184 - INFO - [diffusion][Epoch 7056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:38,185 - INFO - [diffusion][Epoch 7057] Epoch 7058/12000
2024-11-05 01:34:42,329 - INFO - [diffusion][Epoch 7057] diffusion training Loss: 0.05531628895550966
2024-11-05 01:34:42,331 - INFO - [diffusion][Epoch 7057] diffusion learning rate: 0.001
2024-11-05 01:34:42,333 - INFO - [diffusion][Epoch 7057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:42,334 - INFO - [diffusion][Epoch 7058] Epoch 7059/12000
2024-11-05 01:34:46,436 - INFO - [diffusion][Epoch 7058] diffusion training Loss: 0.05817009508609772
2024-11-05 01:34:46,441 - INFO - [diffusion][Epoch 7058] diffusion learning rate: 0.001
2024-11-05 01:34:46,443 - INFO - [diffusion][Epoch 7058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:46,444 - INFO - [diffusion][Epoch 7059] Epoch 7060/12000
2024-11-05 01:34:50,561 - INFO - [diffusion][Epoch 7059] diffusion training Loss: 0.05879743304103613
2024-11-05 01:34:50,563 - INFO - [diffusion][Epoch 7059] diffusion learning rate: 0.001
2024-11-05 01:34:50,565 - INFO - [diffusion][Epoch 7059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:50,567 - INFO - [diffusion][Epoch 7060] Epoch 7061/12000
2024-11-05 01:34:54,725 - INFO - [diffusion][Epoch 7060] diffusion training Loss: 0.05510574672371149
2024-11-05 01:34:54,727 - INFO - [diffusion][Epoch 7060] diffusion learning rate: 0.001
2024-11-05 01:34:54,729 - INFO - [diffusion][Epoch 7060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:54,731 - INFO - [diffusion][Epoch 7061] Epoch 7062/12000
2024-11-05 01:34:58,739 - INFO - [diffusion][Epoch 7061] diffusion training Loss: 0.05695357732474804
2024-11-05 01:34:58,742 - INFO - [diffusion][Epoch 7061] diffusion learning rate: 0.001
2024-11-05 01:34:58,743 - INFO - [diffusion][Epoch 7061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:34:58,745 - INFO - [diffusion][Epoch 7062] Epoch 7063/12000
2024-11-05 01:35:02,839 - INFO - [diffusion][Epoch 7062] diffusion training Loss: 0.059191240929067135
2024-11-05 01:35:02,842 - INFO - [diffusion][Epoch 7062] diffusion learning rate: 0.001
2024-11-05 01:35:02,844 - INFO - [diffusion][Epoch 7062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:02,845 - INFO - [diffusion][Epoch 7063] Epoch 7064/12000
2024-11-05 01:35:06,983 - INFO - [diffusion][Epoch 7063] diffusion training Loss: 0.056083567440509796
2024-11-05 01:35:06,985 - INFO - [diffusion][Epoch 7063] diffusion learning rate: 0.001
2024-11-05 01:35:06,987 - INFO - [diffusion][Epoch 7063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:06,988 - INFO - [diffusion][Epoch 7064] Epoch 7065/12000
2024-11-05 01:35:11,124 - INFO - [diffusion][Epoch 7064] diffusion training Loss: 0.05415687523782253
2024-11-05 01:35:11,126 - INFO - [diffusion][Epoch 7064] diffusion learning rate: 0.001
2024-11-05 01:35:11,128 - INFO - [diffusion][Epoch 7064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:11,129 - INFO - [diffusion][Epoch 7065] Epoch 7066/12000
2024-11-05 01:35:15,216 - INFO - [diffusion][Epoch 7065] diffusion training Loss: 0.0546718779951334
2024-11-05 01:35:15,218 - INFO - [diffusion][Epoch 7065] diffusion learning rate: 0.001
2024-11-05 01:35:15,220 - INFO - [diffusion][Epoch 7065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:15,221 - INFO - [diffusion][Epoch 7066] Epoch 7067/12000
2024-11-05 01:35:19,356 - INFO - [diffusion][Epoch 7066] diffusion training Loss: 0.056932845152914524
2024-11-05 01:35:19,358 - INFO - [diffusion][Epoch 7066] diffusion learning rate: 0.001
2024-11-05 01:35:19,360 - INFO - [diffusion][Epoch 7066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:19,362 - INFO - [diffusion][Epoch 7067] Epoch 7068/12000
2024-11-05 01:35:23,472 - INFO - [diffusion][Epoch 7067] diffusion training Loss: 0.056316920556128025
2024-11-05 01:35:23,474 - INFO - [diffusion][Epoch 7067] diffusion learning rate: 0.001
2024-11-05 01:35:23,476 - INFO - [diffusion][Epoch 7067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:23,477 - INFO - [diffusion][Epoch 7068] Epoch 7069/12000
2024-11-05 01:35:27,614 - INFO - [diffusion][Epoch 7068] diffusion training Loss: 0.05209737457334995
2024-11-05 01:35:27,616 - INFO - [diffusion][Epoch 7068] diffusion learning rate: 0.001
2024-11-05 01:35:27,618 - INFO - [diffusion][Epoch 7068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:27,620 - INFO - [diffusion][Epoch 7069] Epoch 7070/12000
2024-11-05 01:35:31,718 - INFO - [diffusion][Epoch 7069] diffusion training Loss: 0.0568901551887393
2024-11-05 01:35:31,720 - INFO - [diffusion][Epoch 7069] diffusion learning rate: 0.001
2024-11-05 01:35:31,722 - INFO - [diffusion][Epoch 7069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:31,723 - INFO - [diffusion][Epoch 7070] Epoch 7071/12000
2024-11-05 01:35:35,910 - INFO - [diffusion][Epoch 7070] diffusion training Loss: 0.05933758802711964
2024-11-05 01:35:35,912 - INFO - [diffusion][Epoch 7070] diffusion learning rate: 0.001
2024-11-05 01:35:35,913 - INFO - [diffusion][Epoch 7070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:35,915 - INFO - [diffusion][Epoch 7071] Epoch 7072/12000
2024-11-05 01:35:40,039 - INFO - [diffusion][Epoch 7071] diffusion training Loss: 0.05779499467462301
2024-11-05 01:35:40,041 - INFO - [diffusion][Epoch 7071] diffusion learning rate: 0.001
2024-11-05 01:35:40,042 - INFO - [diffusion][Epoch 7071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:40,044 - INFO - [diffusion][Epoch 7072] Epoch 7073/12000
2024-11-05 01:35:44,202 - INFO - [diffusion][Epoch 7072] diffusion training Loss: 0.0591665031388402
2024-11-05 01:35:44,204 - INFO - [diffusion][Epoch 7072] diffusion learning rate: 0.001
2024-11-05 01:35:44,206 - INFO - [diffusion][Epoch 7072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:44,207 - INFO - [diffusion][Epoch 7073] Epoch 7074/12000
2024-11-05 01:35:49,049 - INFO - [diffusion][Epoch 7073] diffusion training Loss: 0.057630253955721855
2024-11-05 01:35:49,051 - INFO - [diffusion][Epoch 7073] diffusion learning rate: 0.001
2024-11-05 01:35:49,053 - INFO - [diffusion][Epoch 7073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:49,055 - INFO - [diffusion][Epoch 7074] Epoch 7075/12000
2024-11-05 01:35:53,188 - INFO - [diffusion][Epoch 7074] diffusion training Loss: 0.06346405670046806
2024-11-05 01:35:53,191 - INFO - [diffusion][Epoch 7074] diffusion learning rate: 0.001
2024-11-05 01:35:53,193 - INFO - [diffusion][Epoch 7074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:53,194 - INFO - [diffusion][Epoch 7075] Epoch 7076/12000
2024-11-05 01:35:57,338 - INFO - [diffusion][Epoch 7075] diffusion training Loss: 0.05698168929666281
2024-11-05 01:35:57,340 - INFO - [diffusion][Epoch 7075] diffusion learning rate: 0.001
2024-11-05 01:35:57,342 - INFO - [diffusion][Epoch 7075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:35:57,343 - INFO - [diffusion][Epoch 7076] Epoch 7077/12000
2024-11-05 01:36:01,468 - INFO - [diffusion][Epoch 7076] diffusion training Loss: 0.05687323957681656
2024-11-05 01:36:01,470 - INFO - [diffusion][Epoch 7076] diffusion learning rate: 0.001
2024-11-05 01:36:01,472 - INFO - [diffusion][Epoch 7076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:01,473 - INFO - [diffusion][Epoch 7077] Epoch 7078/12000
2024-11-05 01:36:05,596 - INFO - [diffusion][Epoch 7077] diffusion training Loss: 0.0588300796225667
2024-11-05 01:36:05,599 - INFO - [diffusion][Epoch 7077] diffusion learning rate: 0.001
2024-11-05 01:36:05,650 - INFO - [diffusion][Epoch 7077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:05,651 - INFO - [diffusion][Epoch 7078] Epoch 7079/12000
2024-11-05 01:36:09,811 - INFO - [diffusion][Epoch 7078] diffusion training Loss: 0.05838465038686991
2024-11-05 01:36:09,813 - INFO - [diffusion][Epoch 7078] diffusion learning rate: 0.001
2024-11-05 01:36:09,814 - INFO - [diffusion][Epoch 7078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:09,816 - INFO - [diffusion][Epoch 7079] Epoch 7080/12000
2024-11-05 01:36:13,939 - INFO - [diffusion][Epoch 7079] diffusion training Loss: 0.05822585429996252
2024-11-05 01:36:13,941 - INFO - [diffusion][Epoch 7079] diffusion learning rate: 0.001
2024-11-05 01:36:13,943 - INFO - [diffusion][Epoch 7079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:13,944 - INFO - [diffusion][Epoch 7080] Epoch 7081/12000
2024-11-05 01:36:18,054 - INFO - [diffusion][Epoch 7080] diffusion training Loss: 0.05091388616710901
2024-11-05 01:36:18,056 - INFO - [diffusion][Epoch 7080] diffusion learning rate: 0.001
2024-11-05 01:36:18,058 - INFO - [diffusion][Epoch 7080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:18,060 - INFO - [diffusion][Epoch 7081] Epoch 7082/12000
2024-11-05 01:36:22,027 - INFO - [diffusion][Epoch 7081] diffusion training Loss: 0.057908160611987114
2024-11-05 01:36:22,029 - INFO - [diffusion][Epoch 7081] diffusion learning rate: 0.001
2024-11-05 01:36:22,031 - INFO - [diffusion][Epoch 7081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:22,032 - INFO - [diffusion][Epoch 7082] Epoch 7083/12000
2024-11-05 01:36:26,114 - INFO - [diffusion][Epoch 7082] diffusion training Loss: 0.05920093320310116
2024-11-05 01:36:26,116 - INFO - [diffusion][Epoch 7082] diffusion learning rate: 0.001
2024-11-05 01:36:26,233 - INFO - [diffusion][Epoch 7082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:26,236 - INFO - [diffusion][Epoch 7083] Epoch 7084/12000
2024-11-05 01:36:30,177 - INFO - [diffusion][Epoch 7083] diffusion training Loss: 0.060022203251719475
2024-11-05 01:36:30,179 - INFO - [diffusion][Epoch 7083] diffusion learning rate: 0.001
2024-11-05 01:36:30,180 - INFO - [diffusion][Epoch 7083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:30,181 - INFO - [diffusion][Epoch 7084] Epoch 7085/12000
2024-11-05 01:36:34,238 - INFO - [diffusion][Epoch 7084] diffusion training Loss: 0.05783664155751467
2024-11-05 01:36:34,240 - INFO - [diffusion][Epoch 7084] diffusion learning rate: 0.001
2024-11-05 01:36:34,242 - INFO - [diffusion][Epoch 7084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:34,244 - INFO - [diffusion][Epoch 7085] Epoch 7086/12000
2024-11-05 01:36:38,274 - INFO - [diffusion][Epoch 7085] diffusion training Loss: 0.05011232942342758
2024-11-05 01:36:38,276 - INFO - [diffusion][Epoch 7085] diffusion learning rate: 0.001
2024-11-05 01:36:38,278 - INFO - [diffusion][Epoch 7085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:38,279 - INFO - [diffusion][Epoch 7086] Epoch 7087/12000
2024-11-05 01:36:42,391 - INFO - [diffusion][Epoch 7086] diffusion training Loss: 0.06000959128141403
2024-11-05 01:36:42,393 - INFO - [diffusion][Epoch 7086] diffusion learning rate: 0.001
2024-11-05 01:36:42,395 - INFO - [diffusion][Epoch 7086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:42,397 - INFO - [diffusion][Epoch 7087] Epoch 7088/12000
2024-11-05 01:36:46,457 - INFO - [diffusion][Epoch 7087] diffusion training Loss: 0.060212256386876106
2024-11-05 01:36:46,459 - INFO - [diffusion][Epoch 7087] diffusion learning rate: 0.001
2024-11-05 01:36:46,461 - INFO - [diffusion][Epoch 7087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:46,463 - INFO - [diffusion][Epoch 7088] Epoch 7089/12000
2024-11-05 01:36:50,620 - INFO - [diffusion][Epoch 7088] diffusion training Loss: 0.05285397917032242
2024-11-05 01:36:50,622 - INFO - [diffusion][Epoch 7088] diffusion learning rate: 0.001
2024-11-05 01:36:50,624 - INFO - [diffusion][Epoch 7088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:50,626 - INFO - [diffusion][Epoch 7089] Epoch 7090/12000
2024-11-05 01:36:54,504 - INFO - [diffusion][Epoch 7089] diffusion training Loss: 0.055432770401239395
2024-11-05 01:36:54,506 - INFO - [diffusion][Epoch 7089] diffusion learning rate: 0.001
2024-11-05 01:36:54,508 - INFO - [diffusion][Epoch 7089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:54,510 - INFO - [diffusion][Epoch 7090] Epoch 7091/12000
2024-11-05 01:36:58,430 - INFO - [diffusion][Epoch 7090] diffusion training Loss: 0.054910941049456596
2024-11-05 01:36:58,432 - INFO - [diffusion][Epoch 7090] diffusion learning rate: 0.001
2024-11-05 01:36:58,434 - INFO - [diffusion][Epoch 7090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:36:58,435 - INFO - [diffusion][Epoch 7091] Epoch 7092/12000
2024-11-05 01:37:02,444 - INFO - [diffusion][Epoch 7091] diffusion training Loss: 0.05428279750049114
2024-11-05 01:37:02,446 - INFO - [diffusion][Epoch 7091] diffusion learning rate: 0.001
2024-11-05 01:37:02,448 - INFO - [diffusion][Epoch 7091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:02,450 - INFO - [diffusion][Epoch 7092] Epoch 7093/12000
2024-11-05 01:37:06,630 - INFO - [diffusion][Epoch 7092] diffusion training Loss: 0.057072377763688564
2024-11-05 01:37:06,632 - INFO - [diffusion][Epoch 7092] diffusion learning rate: 0.001
2024-11-05 01:37:06,634 - INFO - [diffusion][Epoch 7092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:06,635 - INFO - [diffusion][Epoch 7093] Epoch 7094/12000
2024-11-05 01:37:10,643 - INFO - [diffusion][Epoch 7093] diffusion training Loss: 0.05393626447767019
2024-11-05 01:37:10,645 - INFO - [diffusion][Epoch 7093] diffusion learning rate: 0.001
2024-11-05 01:37:10,647 - INFO - [diffusion][Epoch 7093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:10,648 - INFO - [diffusion][Epoch 7094] Epoch 7095/12000
2024-11-05 01:37:14,772 - INFO - [diffusion][Epoch 7094] diffusion training Loss: 0.053114164620637894
2024-11-05 01:37:14,774 - INFO - [diffusion][Epoch 7094] diffusion learning rate: 0.001
2024-11-05 01:37:14,776 - INFO - [diffusion][Epoch 7094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:14,777 - INFO - [diffusion][Epoch 7095] Epoch 7096/12000
2024-11-05 01:37:18,665 - INFO - [diffusion][Epoch 7095] diffusion training Loss: 0.059755842201411724
2024-11-05 01:37:18,667 - INFO - [diffusion][Epoch 7095] diffusion learning rate: 0.001
2024-11-05 01:37:18,669 - INFO - [diffusion][Epoch 7095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:18,670 - INFO - [diffusion][Epoch 7096] Epoch 7097/12000
2024-11-05 01:37:22,625 - INFO - [diffusion][Epoch 7096] diffusion training Loss: 0.05930590350180864
2024-11-05 01:37:22,627 - INFO - [diffusion][Epoch 7096] diffusion learning rate: 0.001
2024-11-05 01:37:22,629 - INFO - [diffusion][Epoch 7096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:22,630 - INFO - [diffusion][Epoch 7097] Epoch 7098/12000
2024-11-05 01:37:26,794 - INFO - [diffusion][Epoch 7097] diffusion training Loss: 0.05507176648825407
2024-11-05 01:37:26,796 - INFO - [diffusion][Epoch 7097] diffusion learning rate: 0.001
2024-11-05 01:37:26,798 - INFO - [diffusion][Epoch 7097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:26,800 - INFO - [diffusion][Epoch 7098] Epoch 7099/12000
2024-11-05 01:37:30,932 - INFO - [diffusion][Epoch 7098] diffusion training Loss: 0.056374684907495975
2024-11-05 01:37:30,934 - INFO - [diffusion][Epoch 7098] diffusion learning rate: 0.001
2024-11-05 01:37:30,936 - INFO - [diffusion][Epoch 7098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:30,937 - INFO - [diffusion][Epoch 7099] Epoch 7100/12000
2024-11-05 01:37:35,101 - INFO - [diffusion][Epoch 7099] diffusion training Loss: 0.05689909867942333
2024-11-05 01:37:35,103 - INFO - [diffusion][Epoch 7099] diffusion learning rate: 0.001
2024-11-05 01:37:35,105 - INFO - [diffusion][Epoch 7099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:35,107 - INFO - [diffusion][Epoch 7100] Epoch 7101/12000
2024-11-05 01:37:39,235 - INFO - [diffusion][Epoch 7100] diffusion training Loss: 0.05670039355754852
2024-11-05 01:37:39,237 - INFO - [diffusion][Epoch 7100] diffusion learning rate: 0.001
2024-11-05 01:37:39,238 - INFO - [diffusion][Epoch 7100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:39,240 - INFO - [diffusion][Epoch 7101] Epoch 7102/12000
2024-11-05 01:37:43,386 - INFO - [diffusion][Epoch 7101] diffusion training Loss: 0.059121512807905674
2024-11-05 01:37:43,388 - INFO - [diffusion][Epoch 7101] diffusion learning rate: 0.001
2024-11-05 01:37:43,390 - INFO - [diffusion][Epoch 7101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:43,391 - INFO - [diffusion][Epoch 7102] Epoch 7103/12000
2024-11-05 01:37:47,582 - INFO - [diffusion][Epoch 7102] diffusion training Loss: 0.05650103744119406
2024-11-05 01:37:47,584 - INFO - [diffusion][Epoch 7102] diffusion learning rate: 0.001
2024-11-05 01:37:47,586 - INFO - [diffusion][Epoch 7102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:47,587 - INFO - [diffusion][Epoch 7103] Epoch 7104/12000
2024-11-05 01:37:51,719 - INFO - [diffusion][Epoch 7103] diffusion training Loss: 0.05703509598970413
2024-11-05 01:37:51,721 - INFO - [diffusion][Epoch 7103] diffusion learning rate: 0.001
2024-11-05 01:37:51,723 - INFO - [diffusion][Epoch 7103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:51,724 - INFO - [diffusion][Epoch 7104] Epoch 7105/12000
2024-11-05 01:37:55,684 - INFO - [diffusion][Epoch 7104] diffusion training Loss: 0.05536088906228542
2024-11-05 01:37:55,686 - INFO - [diffusion][Epoch 7104] diffusion learning rate: 0.001
2024-11-05 01:37:55,688 - INFO - [diffusion][Epoch 7104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:55,689 - INFO - [diffusion][Epoch 7105] Epoch 7106/12000
2024-11-05 01:37:59,766 - INFO - [diffusion][Epoch 7105] diffusion training Loss: 0.051995277404785156
2024-11-05 01:37:59,768 - INFO - [diffusion][Epoch 7105] diffusion learning rate: 0.001
2024-11-05 01:37:59,769 - INFO - [diffusion][Epoch 7105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:37:59,771 - INFO - [diffusion][Epoch 7106] Epoch 7107/12000
2024-11-05 01:38:03,922 - INFO - [diffusion][Epoch 7106] diffusion training Loss: 0.054487451910972595
2024-11-05 01:38:03,924 - INFO - [diffusion][Epoch 7106] diffusion learning rate: 0.001
2024-11-05 01:38:03,926 - INFO - [diffusion][Epoch 7106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:03,927 - INFO - [diffusion][Epoch 7107] Epoch 7108/12000
2024-11-05 01:38:08,032 - INFO - [diffusion][Epoch 7107] diffusion training Loss: 0.05125696677714586
2024-11-05 01:38:08,034 - INFO - [diffusion][Epoch 7107] diffusion learning rate: 0.001
2024-11-05 01:38:08,036 - INFO - [diffusion][Epoch 7107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:08,037 - INFO - [diffusion][Epoch 7108] Epoch 7109/12000
2024-11-05 01:38:12,011 - INFO - [diffusion][Epoch 7108] diffusion training Loss: 0.05311061441898346
2024-11-05 01:38:12,013 - INFO - [diffusion][Epoch 7108] diffusion learning rate: 0.001
2024-11-05 01:38:12,015 - INFO - [diffusion][Epoch 7108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:12,016 - INFO - [diffusion][Epoch 7109] Epoch 7110/12000
2024-11-05 01:38:15,951 - INFO - [diffusion][Epoch 7109] diffusion training Loss: 0.057925524190068245
2024-11-05 01:38:15,953 - INFO - [diffusion][Epoch 7109] diffusion learning rate: 0.001
2024-11-05 01:38:15,955 - INFO - [diffusion][Epoch 7109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:15,958 - INFO - [diffusion][Epoch 7110] Epoch 7111/12000
2024-11-05 01:38:19,983 - INFO - [diffusion][Epoch 7110] diffusion training Loss: 0.05852476879954338
2024-11-05 01:38:19,985 - INFO - [diffusion][Epoch 7110] diffusion learning rate: 0.001
2024-11-05 01:38:19,987 - INFO - [diffusion][Epoch 7110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:19,988 - INFO - [diffusion][Epoch 7111] Epoch 7112/12000
2024-11-05 01:38:23,978 - INFO - [diffusion][Epoch 7111] diffusion training Loss: 0.05814967304468155
2024-11-05 01:38:23,979 - INFO - [diffusion][Epoch 7111] diffusion learning rate: 0.001
2024-11-05 01:38:23,981 - INFO - [diffusion][Epoch 7111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:23,982 - INFO - [diffusion][Epoch 7112] Epoch 7113/12000
2024-11-05 01:38:28,025 - INFO - [diffusion][Epoch 7112] diffusion training Loss: 0.05787551775574684
2024-11-05 01:38:28,027 - INFO - [diffusion][Epoch 7112] diffusion learning rate: 0.001
2024-11-05 01:38:28,029 - INFO - [diffusion][Epoch 7112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:28,030 - INFO - [diffusion][Epoch 7113] Epoch 7114/12000
2024-11-05 01:38:32,040 - INFO - [diffusion][Epoch 7113] diffusion training Loss: 0.056867556646466255
2024-11-05 01:38:32,042 - INFO - [diffusion][Epoch 7113] diffusion learning rate: 0.001
2024-11-05 01:38:32,044 - INFO - [diffusion][Epoch 7113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:32,045 - INFO - [diffusion][Epoch 7114] Epoch 7115/12000
2024-11-05 01:38:36,130 - INFO - [diffusion][Epoch 7114] diffusion training Loss: 0.05846005864441395
2024-11-05 01:38:36,132 - INFO - [diffusion][Epoch 7114] diffusion learning rate: 0.001
2024-11-05 01:38:36,134 - INFO - [diffusion][Epoch 7114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:36,135 - INFO - [diffusion][Epoch 7115] Epoch 7116/12000
2024-11-05 01:38:40,468 - INFO - [diffusion][Epoch 7115] diffusion training Loss: 0.05932042468339205
2024-11-05 01:38:40,470 - INFO - [diffusion][Epoch 7115] diffusion learning rate: 0.001
2024-11-05 01:38:40,473 - INFO - [diffusion][Epoch 7115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:40,474 - INFO - [diffusion][Epoch 7116] Epoch 7117/12000
2024-11-05 01:38:44,511 - INFO - [diffusion][Epoch 7116] diffusion training Loss: 0.05357539560645819
2024-11-05 01:38:44,513 - INFO - [diffusion][Epoch 7116] diffusion learning rate: 0.001
2024-11-05 01:38:44,515 - INFO - [diffusion][Epoch 7116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:44,516 - INFO - [diffusion][Epoch 7117] Epoch 7118/12000
2024-11-05 01:38:48,648 - INFO - [diffusion][Epoch 7117] diffusion training Loss: 0.05549104604870081
2024-11-05 01:38:48,650 - INFO - [diffusion][Epoch 7117] diffusion learning rate: 0.001
2024-11-05 01:38:48,652 - INFO - [diffusion][Epoch 7117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:48,653 - INFO - [diffusion][Epoch 7118] Epoch 7119/12000
2024-11-05 01:38:52,816 - INFO - [diffusion][Epoch 7118] diffusion training Loss: 0.05746620148420334
2024-11-05 01:38:52,819 - INFO - [diffusion][Epoch 7118] diffusion learning rate: 0.001
2024-11-05 01:38:52,821 - INFO - [diffusion][Epoch 7118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:52,822 - INFO - [diffusion][Epoch 7119] Epoch 7120/12000
2024-11-05 01:38:56,917 - INFO - [diffusion][Epoch 7119] diffusion training Loss: 0.05702274106442928
2024-11-05 01:38:56,919 - INFO - [diffusion][Epoch 7119] diffusion learning rate: 0.001
2024-11-05 01:38:56,921 - INFO - [diffusion][Epoch 7119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:38:56,922 - INFO - [diffusion][Epoch 7120] Epoch 7121/12000
2024-11-05 01:39:01,040 - INFO - [diffusion][Epoch 7120] diffusion training Loss: 0.0591565640643239
2024-11-05 01:39:01,042 - INFO - [diffusion][Epoch 7120] diffusion learning rate: 0.001
2024-11-05 01:39:01,044 - INFO - [diffusion][Epoch 7120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:01,045 - INFO - [diffusion][Epoch 7121] Epoch 7122/12000
2024-11-05 01:39:05,142 - INFO - [diffusion][Epoch 7121] diffusion training Loss: 0.05560065992176533
2024-11-05 01:39:05,144 - INFO - [diffusion][Epoch 7121] diffusion learning rate: 0.001
2024-11-05 01:39:05,146 - INFO - [diffusion][Epoch 7121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:05,147 - INFO - [diffusion][Epoch 7122] Epoch 7123/12000
2024-11-05 01:39:09,223 - INFO - [diffusion][Epoch 7122] diffusion training Loss: 0.06066261976957321
2024-11-05 01:39:09,225 - INFO - [diffusion][Epoch 7122] diffusion learning rate: 0.001
2024-11-05 01:39:09,257 - INFO - [diffusion][Epoch 7122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:09,259 - INFO - [diffusion][Epoch 7123] Epoch 7124/12000
2024-11-05 01:39:13,374 - INFO - [diffusion][Epoch 7123] diffusion training Loss: 0.05332529544830322
2024-11-05 01:39:13,376 - INFO - [diffusion][Epoch 7123] diffusion learning rate: 0.001
2024-11-05 01:39:13,378 - INFO - [diffusion][Epoch 7123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:13,380 - INFO - [diffusion][Epoch 7124] Epoch 7125/12000
2024-11-05 01:39:17,432 - INFO - [diffusion][Epoch 7124] diffusion training Loss: 0.05917174834758043
2024-11-05 01:39:17,434 - INFO - [diffusion][Epoch 7124] diffusion learning rate: 0.001
2024-11-05 01:39:17,436 - INFO - [diffusion][Epoch 7124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:17,438 - INFO - [diffusion][Epoch 7125] Epoch 7126/12000
2024-11-05 01:39:21,542 - INFO - [diffusion][Epoch 7125] diffusion training Loss: 0.05906480643898249
2024-11-05 01:39:21,544 - INFO - [diffusion][Epoch 7125] diffusion learning rate: 0.001
2024-11-05 01:39:21,545 - INFO - [diffusion][Epoch 7125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:21,547 - INFO - [diffusion][Epoch 7126] Epoch 7127/12000
2024-11-05 01:39:25,709 - INFO - [diffusion][Epoch 7126] diffusion training Loss: 0.05585802160203457
2024-11-05 01:39:25,711 - INFO - [diffusion][Epoch 7126] diffusion learning rate: 0.001
2024-11-05 01:39:25,832 - INFO - [diffusion][Epoch 7126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:25,834 - INFO - [diffusion][Epoch 7127] Epoch 7128/12000
2024-11-05 01:39:29,942 - INFO - [diffusion][Epoch 7127] diffusion training Loss: 0.056654006242752075
2024-11-05 01:39:29,944 - INFO - [diffusion][Epoch 7127] diffusion learning rate: 0.001
2024-11-05 01:39:29,946 - INFO - [diffusion][Epoch 7127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:29,947 - INFO - [diffusion][Epoch 7128] Epoch 7129/12000
2024-11-05 01:39:34,012 - INFO - [diffusion][Epoch 7128] diffusion training Loss: 0.056268684566020966
2024-11-05 01:39:34,014 - INFO - [diffusion][Epoch 7128] diffusion learning rate: 0.001
2024-11-05 01:39:34,015 - INFO - [diffusion][Epoch 7128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:34,017 - INFO - [diffusion][Epoch 7129] Epoch 7130/12000
2024-11-05 01:39:37,960 - INFO - [diffusion][Epoch 7129] diffusion training Loss: 0.06059344485402107
2024-11-05 01:39:37,962 - INFO - [diffusion][Epoch 7129] diffusion learning rate: 0.001
2024-11-05 01:39:37,964 - INFO - [diffusion][Epoch 7129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:37,966 - INFO - [diffusion][Epoch 7130] Epoch 7131/12000
2024-11-05 01:39:42,078 - INFO - [diffusion][Epoch 7130] diffusion training Loss: 0.060316262766718864
2024-11-05 01:39:42,080 - INFO - [diffusion][Epoch 7130] diffusion learning rate: 0.001
2024-11-05 01:39:42,082 - INFO - [diffusion][Epoch 7130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:42,083 - INFO - [diffusion][Epoch 7131] Epoch 7132/12000
2024-11-05 01:39:46,280 - INFO - [diffusion][Epoch 7131] diffusion training Loss: 0.06056729890406132
2024-11-05 01:39:46,283 - INFO - [diffusion][Epoch 7131] diffusion learning rate: 0.001
2024-11-05 01:39:46,285 - INFO - [diffusion][Epoch 7131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:46,287 - INFO - [diffusion][Epoch 7132] Epoch 7133/12000
2024-11-05 01:39:50,373 - INFO - [diffusion][Epoch 7132] diffusion training Loss: 0.05616486072540283
2024-11-05 01:39:50,375 - INFO - [diffusion][Epoch 7132] diffusion learning rate: 0.001
2024-11-05 01:39:50,377 - INFO - [diffusion][Epoch 7132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:50,378 - INFO - [diffusion][Epoch 7133] Epoch 7134/12000
2024-11-05 01:39:54,509 - INFO - [diffusion][Epoch 7133] diffusion training Loss: 0.05370762478560209
2024-11-05 01:39:54,511 - INFO - [diffusion][Epoch 7133] diffusion learning rate: 0.001
2024-11-05 01:39:54,513 - INFO - [diffusion][Epoch 7133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:54,514 - INFO - [diffusion][Epoch 7134] Epoch 7135/12000
2024-11-05 01:39:58,741 - INFO - [diffusion][Epoch 7134] diffusion training Loss: 0.05448328144848347
2024-11-05 01:39:58,743 - INFO - [diffusion][Epoch 7134] diffusion learning rate: 0.001
2024-11-05 01:39:58,745 - INFO - [diffusion][Epoch 7134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:39:58,746 - INFO - [diffusion][Epoch 7135] Epoch 7136/12000
2024-11-05 01:40:02,848 - INFO - [diffusion][Epoch 7135] diffusion training Loss: 0.05829682480543852
2024-11-05 01:40:02,850 - INFO - [diffusion][Epoch 7135] diffusion learning rate: 0.001
2024-11-05 01:40:02,852 - INFO - [diffusion][Epoch 7135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:02,853 - INFO - [diffusion][Epoch 7136] Epoch 7137/12000
2024-11-05 01:40:07,544 - INFO - [diffusion][Epoch 7136] diffusion training Loss: 0.060380179435014725
2024-11-05 01:40:07,546 - INFO - [diffusion][Epoch 7136] diffusion learning rate: 0.001
2024-11-05 01:40:07,548 - INFO - [diffusion][Epoch 7136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:07,549 - INFO - [diffusion][Epoch 7137] Epoch 7138/12000
2024-11-05 01:40:11,611 - INFO - [diffusion][Epoch 7137] diffusion training Loss: 0.05808279011398554
2024-11-05 01:40:11,614 - INFO - [diffusion][Epoch 7137] diffusion learning rate: 0.001
2024-11-05 01:40:11,616 - INFO - [diffusion][Epoch 7137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:11,618 - INFO - [diffusion][Epoch 7138] Epoch 7139/12000
2024-11-05 01:40:15,669 - INFO - [diffusion][Epoch 7138] diffusion training Loss: 0.0590644720941782
2024-11-05 01:40:15,671 - INFO - [diffusion][Epoch 7138] diffusion learning rate: 0.001
2024-11-05 01:40:15,673 - INFO - [diffusion][Epoch 7138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:15,674 - INFO - [diffusion][Epoch 7139] Epoch 7140/12000
2024-11-05 01:40:19,700 - INFO - [diffusion][Epoch 7139] diffusion training Loss: 0.06365001387894154
2024-11-05 01:40:19,703 - INFO - [diffusion][Epoch 7139] diffusion learning rate: 0.001
2024-11-05 01:40:19,704 - INFO - [diffusion][Epoch 7139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:19,705 - INFO - [diffusion][Epoch 7140] Epoch 7141/12000
2024-11-05 01:40:23,811 - INFO - [diffusion][Epoch 7140] diffusion training Loss: 0.05315403826534748
2024-11-05 01:40:23,813 - INFO - [diffusion][Epoch 7140] diffusion learning rate: 0.001
2024-11-05 01:40:23,814 - INFO - [diffusion][Epoch 7140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:23,816 - INFO - [diffusion][Epoch 7141] Epoch 7142/12000
2024-11-05 01:40:27,975 - INFO - [diffusion][Epoch 7141] diffusion training Loss: 0.0546438954770565
2024-11-05 01:40:27,977 - INFO - [diffusion][Epoch 7141] diffusion learning rate: 0.001
2024-11-05 01:40:27,979 - INFO - [diffusion][Epoch 7141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:27,981 - INFO - [diffusion][Epoch 7142] Epoch 7143/12000
2024-11-05 01:40:32,004 - INFO - [diffusion][Epoch 7142] diffusion training Loss: 0.05349694658070803
2024-11-05 01:40:32,006 - INFO - [diffusion][Epoch 7142] diffusion learning rate: 0.001
2024-11-05 01:40:32,010 - INFO - [diffusion][Epoch 7142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:32,011 - INFO - [diffusion][Epoch 7143] Epoch 7144/12000
2024-11-05 01:40:36,026 - INFO - [diffusion][Epoch 7143] diffusion training Loss: 0.05534053593873978
2024-11-05 01:40:36,029 - INFO - [diffusion][Epoch 7143] diffusion learning rate: 0.001
2024-11-05 01:40:36,031 - INFO - [diffusion][Epoch 7143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:36,032 - INFO - [diffusion][Epoch 7144] Epoch 7145/12000
2024-11-05 01:40:40,155 - INFO - [diffusion][Epoch 7144] diffusion training Loss: 0.05667661689221859
2024-11-05 01:40:40,157 - INFO - [diffusion][Epoch 7144] diffusion learning rate: 0.001
2024-11-05 01:40:40,159 - INFO - [diffusion][Epoch 7144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:40,160 - INFO - [diffusion][Epoch 7145] Epoch 7146/12000
2024-11-05 01:40:44,118 - INFO - [diffusion][Epoch 7145] diffusion training Loss: 0.05623246356844902
2024-11-05 01:40:44,120 - INFO - [diffusion][Epoch 7145] diffusion learning rate: 0.001
2024-11-05 01:40:44,121 - INFO - [diffusion][Epoch 7145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:44,123 - INFO - [diffusion][Epoch 7146] Epoch 7147/12000
2024-11-05 01:40:48,248 - INFO - [diffusion][Epoch 7146] diffusion training Loss: 0.055898348800837994
2024-11-05 01:40:48,250 - INFO - [diffusion][Epoch 7146] diffusion learning rate: 0.001
2024-11-05 01:40:48,252 - INFO - [diffusion][Epoch 7146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:48,254 - INFO - [diffusion][Epoch 7147] Epoch 7148/12000
2024-11-05 01:40:52,377 - INFO - [diffusion][Epoch 7147] diffusion training Loss: 0.058923435397446156
2024-11-05 01:40:52,380 - INFO - [diffusion][Epoch 7147] diffusion learning rate: 0.001
2024-11-05 01:40:52,382 - INFO - [diffusion][Epoch 7147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:52,383 - INFO - [diffusion][Epoch 7148] Epoch 7149/12000
2024-11-05 01:40:56,524 - INFO - [diffusion][Epoch 7148] diffusion training Loss: 0.059192207641899586
2024-11-05 01:40:56,526 - INFO - [diffusion][Epoch 7148] diffusion learning rate: 0.001
2024-11-05 01:40:56,528 - INFO - [diffusion][Epoch 7148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:40:56,529 - INFO - [diffusion][Epoch 7149] Epoch 7150/12000
2024-11-05 01:41:00,538 - INFO - [diffusion][Epoch 7149] diffusion training Loss: 0.06077420152723789
2024-11-05 01:41:00,540 - INFO - [diffusion][Epoch 7149] diffusion learning rate: 0.001
2024-11-05 01:41:00,542 - INFO - [diffusion][Epoch 7149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:00,543 - INFO - [diffusion][Epoch 7150] Epoch 7151/12000
2024-11-05 01:41:04,535 - INFO - [diffusion][Epoch 7150] diffusion training Loss: 0.056400089524686337
2024-11-05 01:41:04,537 - INFO - [diffusion][Epoch 7150] diffusion learning rate: 0.001
2024-11-05 01:41:04,539 - INFO - [diffusion][Epoch 7150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:04,541 - INFO - [diffusion][Epoch 7151] Epoch 7152/12000
2024-11-05 01:41:08,623 - INFO - [diffusion][Epoch 7151] diffusion training Loss: 0.05639640800654888
2024-11-05 01:41:08,625 - INFO - [diffusion][Epoch 7151] diffusion learning rate: 0.001
2024-11-05 01:41:08,627 - INFO - [diffusion][Epoch 7151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:08,628 - INFO - [diffusion][Epoch 7152] Epoch 7153/12000
2024-11-05 01:41:12,615 - INFO - [diffusion][Epoch 7152] diffusion training Loss: 0.05860724113881588
2024-11-05 01:41:12,618 - INFO - [diffusion][Epoch 7152] diffusion learning rate: 0.001
2024-11-05 01:41:12,620 - INFO - [diffusion][Epoch 7152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:12,621 - INFO - [diffusion][Epoch 7153] Epoch 7154/12000
2024-11-05 01:41:16,710 - INFO - [diffusion][Epoch 7153] diffusion training Loss: 0.060534412041306496
2024-11-05 01:41:16,712 - INFO - [diffusion][Epoch 7153] diffusion learning rate: 0.001
2024-11-05 01:41:16,713 - INFO - [diffusion][Epoch 7153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:16,714 - INFO - [diffusion][Epoch 7154] Epoch 7155/12000
2024-11-05 01:41:20,745 - INFO - [diffusion][Epoch 7154] diffusion training Loss: 0.0550028420984745
2024-11-05 01:41:20,747 - INFO - [diffusion][Epoch 7154] diffusion learning rate: 0.001
2024-11-05 01:41:20,801 - INFO - [diffusion][Epoch 7154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:20,802 - INFO - [diffusion][Epoch 7155] Epoch 7156/12000
2024-11-05 01:41:24,810 - INFO - [diffusion][Epoch 7155] diffusion training Loss: 0.052865225821733475
2024-11-05 01:41:24,812 - INFO - [diffusion][Epoch 7155] diffusion learning rate: 0.001
2024-11-05 01:41:24,814 - INFO - [diffusion][Epoch 7155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:24,815 - INFO - [diffusion][Epoch 7156] Epoch 7157/12000
2024-11-05 01:41:28,946 - INFO - [diffusion][Epoch 7156] diffusion training Loss: 0.059086134657263756
2024-11-05 01:41:28,948 - INFO - [diffusion][Epoch 7156] diffusion learning rate: 0.001
2024-11-05 01:41:28,950 - INFO - [diffusion][Epoch 7156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:28,951 - INFO - [diffusion][Epoch 7157] Epoch 7158/12000
2024-11-05 01:41:33,240 - INFO - [diffusion][Epoch 7157] diffusion training Loss: 0.05707270558923483
2024-11-05 01:41:33,242 - INFO - [diffusion][Epoch 7157] diffusion learning rate: 0.001
2024-11-05 01:41:33,243 - INFO - [diffusion][Epoch 7157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:33,245 - INFO - [diffusion][Epoch 7158] Epoch 7159/12000
2024-11-05 01:41:37,273 - INFO - [diffusion][Epoch 7158] diffusion training Loss: 0.055949052795767784
2024-11-05 01:41:37,276 - INFO - [diffusion][Epoch 7158] diffusion learning rate: 0.001
2024-11-05 01:41:37,277 - INFO - [diffusion][Epoch 7158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:37,279 - INFO - [diffusion][Epoch 7159] Epoch 7160/12000
2024-11-05 01:41:41,401 - INFO - [diffusion][Epoch 7159] diffusion training Loss: 0.06138313002884388
2024-11-05 01:41:41,403 - INFO - [diffusion][Epoch 7159] diffusion learning rate: 0.001
2024-11-05 01:41:41,405 - INFO - [diffusion][Epoch 7159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:41,406 - INFO - [diffusion][Epoch 7160] Epoch 7161/12000
2024-11-05 01:41:45,460 - INFO - [diffusion][Epoch 7160] diffusion training Loss: 0.05731114186346531
2024-11-05 01:41:45,463 - INFO - [diffusion][Epoch 7160] diffusion learning rate: 0.001
2024-11-05 01:41:45,465 - INFO - [diffusion][Epoch 7160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:45,466 - INFO - [diffusion][Epoch 7161] Epoch 7162/12000
2024-11-05 01:41:49,470 - INFO - [diffusion][Epoch 7161] diffusion training Loss: 0.054303581826388836
2024-11-05 01:41:49,472 - INFO - [diffusion][Epoch 7161] diffusion learning rate: 0.001
2024-11-05 01:41:49,474 - INFO - [diffusion][Epoch 7161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:49,475 - INFO - [diffusion][Epoch 7162] Epoch 7163/12000
2024-11-05 01:41:53,640 - INFO - [diffusion][Epoch 7162] diffusion training Loss: 0.05713816825300455
2024-11-05 01:41:53,642 - INFO - [diffusion][Epoch 7162] diffusion learning rate: 0.001
2024-11-05 01:41:53,644 - INFO - [diffusion][Epoch 7162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:53,645 - INFO - [diffusion][Epoch 7163] Epoch 7164/12000
2024-11-05 01:41:57,816 - INFO - [diffusion][Epoch 7163] diffusion training Loss: 0.051677340641617775
2024-11-05 01:41:57,818 - INFO - [diffusion][Epoch 7163] diffusion learning rate: 0.001
2024-11-05 01:41:57,820 - INFO - [diffusion][Epoch 7163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:41:57,821 - INFO - [diffusion][Epoch 7164] Epoch 7165/12000
2024-11-05 01:42:01,977 - INFO - [diffusion][Epoch 7164] diffusion training Loss: 0.06263822875916958
2024-11-05 01:42:01,979 - INFO - [diffusion][Epoch 7164] diffusion learning rate: 0.001
2024-11-05 01:42:01,981 - INFO - [diffusion][Epoch 7164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:01,983 - INFO - [diffusion][Epoch 7165] Epoch 7166/12000
2024-11-05 01:42:06,028 - INFO - [diffusion][Epoch 7165] diffusion training Loss: 0.051643649116158485
2024-11-05 01:42:06,030 - INFO - [diffusion][Epoch 7165] diffusion learning rate: 0.001
2024-11-05 01:42:06,032 - INFO - [diffusion][Epoch 7165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:06,033 - INFO - [diffusion][Epoch 7166] Epoch 7167/12000
2024-11-05 01:42:10,120 - INFO - [diffusion][Epoch 7166] diffusion training Loss: 0.06031066179275513
2024-11-05 01:42:10,122 - INFO - [diffusion][Epoch 7166] diffusion learning rate: 0.001
2024-11-05 01:42:10,123 - INFO - [diffusion][Epoch 7166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:10,125 - INFO - [diffusion][Epoch 7167] Epoch 7168/12000
2024-11-05 01:42:14,070 - INFO - [diffusion][Epoch 7167] diffusion training Loss: 0.05144375190138817
2024-11-05 01:42:14,072 - INFO - [diffusion][Epoch 7167] diffusion learning rate: 0.001
2024-11-05 01:42:14,074 - INFO - [diffusion][Epoch 7167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:14,075 - INFO - [diffusion][Epoch 7168] Epoch 7169/12000
2024-11-05 01:42:18,167 - INFO - [diffusion][Epoch 7168] diffusion training Loss: 0.04988264013081789
2024-11-05 01:42:18,169 - INFO - [diffusion][Epoch 7168] diffusion learning rate: 0.001
2024-11-05 01:42:18,171 - INFO - [diffusion][Epoch 7168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:18,172 - INFO - [diffusion][Epoch 7169] Epoch 7170/12000
2024-11-05 01:42:22,175 - INFO - [diffusion][Epoch 7169] diffusion training Loss: 0.06120911054313183
2024-11-05 01:42:22,178 - INFO - [diffusion][Epoch 7169] diffusion learning rate: 0.001
2024-11-05 01:42:22,180 - INFO - [diffusion][Epoch 7169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:22,181 - INFO - [diffusion][Epoch 7170] Epoch 7171/12000
2024-11-05 01:42:26,223 - INFO - [diffusion][Epoch 7170] diffusion training Loss: 0.05568613298237324
2024-11-05 01:42:26,225 - INFO - [diffusion][Epoch 7170] diffusion learning rate: 0.001
2024-11-05 01:42:26,228 - INFO - [diffusion][Epoch 7170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:26,229 - INFO - [diffusion][Epoch 7171] Epoch 7172/12000
2024-11-05 01:42:30,261 - INFO - [diffusion][Epoch 7171] diffusion training Loss: 0.05645922105759382
2024-11-05 01:42:30,264 - INFO - [diffusion][Epoch 7171] diffusion learning rate: 0.001
2024-11-05 01:42:30,266 - INFO - [diffusion][Epoch 7171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:30,268 - INFO - [diffusion][Epoch 7172] Epoch 7173/12000
2024-11-05 01:42:34,308 - INFO - [diffusion][Epoch 7172] diffusion training Loss: 0.05545984487980604
2024-11-05 01:42:34,310 - INFO - [diffusion][Epoch 7172] diffusion learning rate: 0.001
2024-11-05 01:42:34,312 - INFO - [diffusion][Epoch 7172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:34,313 - INFO - [diffusion][Epoch 7173] Epoch 7174/12000
2024-11-05 01:42:38,397 - INFO - [diffusion][Epoch 7173] diffusion training Loss: 0.06353461928665638
2024-11-05 01:42:38,400 - INFO - [diffusion][Epoch 7173] diffusion learning rate: 0.001
2024-11-05 01:42:38,403 - INFO - [diffusion][Epoch 7173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:38,404 - INFO - [diffusion][Epoch 7174] Epoch 7175/12000
2024-11-05 01:42:42,529 - INFO - [diffusion][Epoch 7174] diffusion training Loss: 0.053574858233332634
2024-11-05 01:42:42,531 - INFO - [diffusion][Epoch 7174] diffusion learning rate: 0.001
2024-11-05 01:42:42,533 - INFO - [diffusion][Epoch 7174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:42,534 - INFO - [diffusion][Epoch 7175] Epoch 7176/12000
2024-11-05 01:42:46,628 - INFO - [diffusion][Epoch 7175] diffusion training Loss: 0.05726764816790819
2024-11-05 01:42:46,630 - INFO - [diffusion][Epoch 7175] diffusion learning rate: 0.001
2024-11-05 01:42:46,632 - INFO - [diffusion][Epoch 7175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:46,633 - INFO - [diffusion][Epoch 7176] Epoch 7177/12000
2024-11-05 01:42:50,709 - INFO - [diffusion][Epoch 7176] diffusion training Loss: 0.054984127171337605
2024-11-05 01:42:50,711 - INFO - [diffusion][Epoch 7176] diffusion learning rate: 0.001
2024-11-05 01:42:50,713 - INFO - [diffusion][Epoch 7176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:50,714 - INFO - [diffusion][Epoch 7177] Epoch 7178/12000
2024-11-05 01:42:54,841 - INFO - [diffusion][Epoch 7177] diffusion training Loss: 0.058346742764115334
2024-11-05 01:42:54,843 - INFO - [diffusion][Epoch 7177] diffusion learning rate: 0.001
2024-11-05 01:42:54,845 - INFO - [diffusion][Epoch 7177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:54,846 - INFO - [diffusion][Epoch 7178] Epoch 7179/12000
2024-11-05 01:42:59,242 - INFO - [diffusion][Epoch 7178] diffusion training Loss: 0.053815484046936035
2024-11-05 01:42:59,244 - INFO - [diffusion][Epoch 7178] diffusion learning rate: 0.001
2024-11-05 01:42:59,340 - INFO - [diffusion][Epoch 7178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:42:59,342 - INFO - [diffusion][Epoch 7179] Epoch 7180/12000
2024-11-05 01:43:03,443 - INFO - [diffusion][Epoch 7179] diffusion training Loss: 0.05833239108324051
2024-11-05 01:43:03,446 - INFO - [diffusion][Epoch 7179] diffusion learning rate: 0.001
2024-11-05 01:43:03,447 - INFO - [diffusion][Epoch 7179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:03,449 - INFO - [diffusion][Epoch 7180] Epoch 7181/12000
2024-11-05 01:43:07,560 - INFO - [diffusion][Epoch 7180] diffusion training Loss: 0.05844911467283964
2024-11-05 01:43:07,562 - INFO - [diffusion][Epoch 7180] diffusion learning rate: 0.001
2024-11-05 01:43:07,564 - INFO - [diffusion][Epoch 7180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:07,565 - INFO - [diffusion][Epoch 7181] Epoch 7182/12000
2024-11-05 01:43:11,684 - INFO - [diffusion][Epoch 7181] diffusion training Loss: 0.05526104476302862
2024-11-05 01:43:11,685 - INFO - [diffusion][Epoch 7181] diffusion learning rate: 0.001
2024-11-05 01:43:11,688 - INFO - [diffusion][Epoch 7181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:11,689 - INFO - [diffusion][Epoch 7182] Epoch 7183/12000
2024-11-05 01:43:15,783 - INFO - [diffusion][Epoch 7182] diffusion training Loss: 0.05923354532569647
2024-11-05 01:43:15,786 - INFO - [diffusion][Epoch 7182] diffusion learning rate: 0.001
2024-11-05 01:43:15,788 - INFO - [diffusion][Epoch 7182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:15,790 - INFO - [diffusion][Epoch 7183] Epoch 7184/12000
2024-11-05 01:43:19,728 - INFO - [diffusion][Epoch 7183] diffusion training Loss: 0.058415510691702366
2024-11-05 01:43:19,730 - INFO - [diffusion][Epoch 7183] diffusion learning rate: 0.001
2024-11-05 01:43:19,731 - INFO - [diffusion][Epoch 7183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:19,733 - INFO - [diffusion][Epoch 7184] Epoch 7185/12000
2024-11-05 01:43:23,684 - INFO - [diffusion][Epoch 7184] diffusion training Loss: 0.061276499181985855
2024-11-05 01:43:23,686 - INFO - [diffusion][Epoch 7184] diffusion learning rate: 0.001
2024-11-05 01:43:23,687 - INFO - [diffusion][Epoch 7184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:23,689 - INFO - [diffusion][Epoch 7185] Epoch 7186/12000
2024-11-05 01:43:27,782 - INFO - [diffusion][Epoch 7185] diffusion training Loss: 0.061612834222614765
2024-11-05 01:43:27,784 - INFO - [diffusion][Epoch 7185] diffusion learning rate: 0.001
2024-11-05 01:43:27,786 - INFO - [diffusion][Epoch 7185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:27,788 - INFO - [diffusion][Epoch 7186] Epoch 7187/12000
2024-11-05 01:43:31,875 - INFO - [diffusion][Epoch 7186] diffusion training Loss: 0.054870499297976494
2024-11-05 01:43:31,877 - INFO - [diffusion][Epoch 7186] diffusion learning rate: 0.001
2024-11-05 01:43:31,879 - INFO - [diffusion][Epoch 7186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:31,880 - INFO - [diffusion][Epoch 7187] Epoch 7188/12000
2024-11-05 01:43:35,997 - INFO - [diffusion][Epoch 7187] diffusion training Loss: 0.05580632947385311
2024-11-05 01:43:36,000 - INFO - [diffusion][Epoch 7187] diffusion learning rate: 0.001
2024-11-05 01:43:36,001 - INFO - [diffusion][Epoch 7187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:36,006 - INFO - [diffusion][Epoch 7188] Epoch 7189/12000
2024-11-05 01:43:40,077 - INFO - [diffusion][Epoch 7188] diffusion training Loss: 0.06164648104459047
2024-11-05 01:43:40,079 - INFO - [diffusion][Epoch 7188] diffusion learning rate: 0.001
2024-11-05 01:43:40,081 - INFO - [diffusion][Epoch 7188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:40,083 - INFO - [diffusion][Epoch 7189] Epoch 7190/12000
2024-11-05 01:43:44,210 - INFO - [diffusion][Epoch 7189] diffusion training Loss: 0.06037890072911978
2024-11-05 01:43:44,212 - INFO - [diffusion][Epoch 7189] diffusion learning rate: 0.001
2024-11-05 01:43:44,214 - INFO - [diffusion][Epoch 7189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:44,215 - INFO - [diffusion][Epoch 7190] Epoch 7191/12000
2024-11-05 01:43:48,389 - INFO - [diffusion][Epoch 7190] diffusion training Loss: 0.05700180493295193
2024-11-05 01:43:48,392 - INFO - [diffusion][Epoch 7190] diffusion learning rate: 0.001
2024-11-05 01:43:48,394 - INFO - [diffusion][Epoch 7190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:48,395 - INFO - [diffusion][Epoch 7191] Epoch 7192/12000
2024-11-05 01:43:52,531 - INFO - [diffusion][Epoch 7191] diffusion training Loss: 0.05840962193906307
2024-11-05 01:43:52,533 - INFO - [diffusion][Epoch 7191] diffusion learning rate: 0.001
2024-11-05 01:43:52,535 - INFO - [diffusion][Epoch 7191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:52,536 - INFO - [diffusion][Epoch 7192] Epoch 7193/12000
2024-11-05 01:43:56,663 - INFO - [diffusion][Epoch 7192] diffusion training Loss: 0.05635982472449541
2024-11-05 01:43:56,665 - INFO - [diffusion][Epoch 7192] diffusion learning rate: 0.001
2024-11-05 01:43:56,667 - INFO - [diffusion][Epoch 7192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:43:56,668 - INFO - [diffusion][Epoch 7193] Epoch 7194/12000
2024-11-05 01:44:00,685 - INFO - [diffusion][Epoch 7193] diffusion training Loss: 0.058965958654880524
2024-11-05 01:44:00,688 - INFO - [diffusion][Epoch 7193] diffusion learning rate: 0.001
2024-11-05 01:44:00,690 - INFO - [diffusion][Epoch 7193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:00,692 - INFO - [diffusion][Epoch 7194] Epoch 7195/12000
2024-11-05 01:44:04,781 - INFO - [diffusion][Epoch 7194] diffusion training Loss: 0.05553625151515007
2024-11-05 01:44:04,783 - INFO - [diffusion][Epoch 7194] diffusion learning rate: 0.001
2024-11-05 01:44:04,785 - INFO - [diffusion][Epoch 7194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:04,786 - INFO - [diffusion][Epoch 7195] Epoch 7196/12000
2024-11-05 01:44:08,893 - INFO - [diffusion][Epoch 7195] diffusion training Loss: 0.05437350645661354
2024-11-05 01:44:08,896 - INFO - [diffusion][Epoch 7195] diffusion learning rate: 0.001
2024-11-05 01:44:08,897 - INFO - [diffusion][Epoch 7195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:08,899 - INFO - [diffusion][Epoch 7196] Epoch 7197/12000
2024-11-05 01:44:13,025 - INFO - [diffusion][Epoch 7196] diffusion training Loss: 0.05496414564549923
2024-11-05 01:44:13,027 - INFO - [diffusion][Epoch 7196] diffusion learning rate: 0.001
2024-11-05 01:44:13,028 - INFO - [diffusion][Epoch 7196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:13,029 - INFO - [diffusion][Epoch 7197] Epoch 7198/12000
2024-11-05 01:44:17,134 - INFO - [diffusion][Epoch 7197] diffusion training Loss: 0.06293072830885649
2024-11-05 01:44:17,137 - INFO - [diffusion][Epoch 7197] diffusion learning rate: 0.001
2024-11-05 01:44:17,139 - INFO - [diffusion][Epoch 7197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:17,140 - INFO - [diffusion][Epoch 7198] Epoch 7199/12000
2024-11-05 01:44:21,283 - INFO - [diffusion][Epoch 7198] diffusion training Loss: 0.060211069881916046
2024-11-05 01:44:21,285 - INFO - [diffusion][Epoch 7198] diffusion learning rate: 0.001
2024-11-05 01:44:21,287 - INFO - [diffusion][Epoch 7198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:21,288 - INFO - [diffusion][Epoch 7199] Epoch 7200/12000
2024-11-05 01:44:25,591 - INFO - [diffusion][Epoch 7199] diffusion training Loss: 0.05858846567571163
2024-11-05 01:44:25,593 - INFO - [diffusion][Epoch 7199] diffusion learning rate: 0.001
2024-11-05 01:44:25,655 - INFO - [diffusion][Epoch 7199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:25,656 - INFO - [diffusion][Epoch 7200] Epoch 7201/12000
2024-11-05 01:44:29,779 - INFO - [diffusion][Epoch 7200] diffusion training Loss: 0.05017325095832348
2024-11-05 01:44:29,781 - INFO - [diffusion][Epoch 7200] diffusion learning rate: 0.001
2024-11-05 01:44:29,783 - INFO - [diffusion][Epoch 7200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:29,784 - INFO - [diffusion][Epoch 7201] Epoch 7202/12000
2024-11-05 01:44:33,950 - INFO - [diffusion][Epoch 7201] diffusion training Loss: 0.05226478725671768
2024-11-05 01:44:33,952 - INFO - [diffusion][Epoch 7201] diffusion learning rate: 0.001
2024-11-05 01:44:33,954 - INFO - [diffusion][Epoch 7201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:33,955 - INFO - [diffusion][Epoch 7202] Epoch 7203/12000
2024-11-05 01:44:38,084 - INFO - [diffusion][Epoch 7202] diffusion training Loss: 0.05841511953622103
2024-11-05 01:44:38,086 - INFO - [diffusion][Epoch 7202] diffusion learning rate: 0.001
2024-11-05 01:44:38,088 - INFO - [diffusion][Epoch 7202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:38,090 - INFO - [diffusion][Epoch 7203] Epoch 7204/12000
2024-11-05 01:44:42,196 - INFO - [diffusion][Epoch 7203] diffusion training Loss: 0.05365791544318199
2024-11-05 01:44:42,198 - INFO - [diffusion][Epoch 7203] diffusion learning rate: 0.001
2024-11-05 01:44:42,200 - INFO - [diffusion][Epoch 7203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:42,202 - INFO - [diffusion][Epoch 7204] Epoch 7205/12000
2024-11-05 01:44:46,284 - INFO - [diffusion][Epoch 7204] diffusion training Loss: 0.05317572597414255
2024-11-05 01:44:46,286 - INFO - [diffusion][Epoch 7204] diffusion learning rate: 0.001
2024-11-05 01:44:46,289 - INFO - [diffusion][Epoch 7204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:46,291 - INFO - [diffusion][Epoch 7205] Epoch 7206/12000
2024-11-05 01:44:50,411 - INFO - [diffusion][Epoch 7205] diffusion training Loss: 0.05577641259878874
2024-11-05 01:44:50,413 - INFO - [diffusion][Epoch 7205] diffusion learning rate: 0.001
2024-11-05 01:44:50,414 - INFO - [diffusion][Epoch 7205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:50,416 - INFO - [diffusion][Epoch 7206] Epoch 7207/12000
2024-11-05 01:44:54,541 - INFO - [diffusion][Epoch 7206] diffusion training Loss: 0.05548392981290817
2024-11-05 01:44:54,543 - INFO - [diffusion][Epoch 7206] diffusion learning rate: 0.001
2024-11-05 01:44:54,544 - INFO - [diffusion][Epoch 7206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:54,545 - INFO - [diffusion][Epoch 7207] Epoch 7208/12000
2024-11-05 01:44:58,688 - INFO - [diffusion][Epoch 7207] diffusion training Loss: 0.05327056907117367
2024-11-05 01:44:58,690 - INFO - [diffusion][Epoch 7207] diffusion learning rate: 0.001
2024-11-05 01:44:58,692 - INFO - [diffusion][Epoch 7207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:44:58,693 - INFO - [diffusion][Epoch 7208] Epoch 7209/12000
2024-11-05 01:45:02,809 - INFO - [diffusion][Epoch 7208] diffusion training Loss: 0.058368523605167866
2024-11-05 01:45:02,811 - INFO - [diffusion][Epoch 7208] diffusion learning rate: 0.001
2024-11-05 01:45:02,813 - INFO - [diffusion][Epoch 7208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:02,814 - INFO - [diffusion][Epoch 7209] Epoch 7210/12000
2024-11-05 01:45:06,935 - INFO - [diffusion][Epoch 7209] diffusion training Loss: 0.06124107539653778
2024-11-05 01:45:06,937 - INFO - [diffusion][Epoch 7209] diffusion learning rate: 0.001
2024-11-05 01:45:06,938 - INFO - [diffusion][Epoch 7209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:06,940 - INFO - [diffusion][Epoch 7210] Epoch 7211/12000
2024-11-05 01:45:11,063 - INFO - [diffusion][Epoch 7210] diffusion training Loss: 0.05586748383939266
2024-11-05 01:45:11,066 - INFO - [diffusion][Epoch 7210] diffusion learning rate: 0.001
2024-11-05 01:45:11,068 - INFO - [diffusion][Epoch 7210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:11,069 - INFO - [diffusion][Epoch 7211] Epoch 7212/12000
2024-11-05 01:45:15,133 - INFO - [diffusion][Epoch 7211] diffusion training Loss: 0.06107231508940458
2024-11-05 01:45:15,136 - INFO - [diffusion][Epoch 7211] diffusion learning rate: 0.001
2024-11-05 01:45:15,188 - INFO - [diffusion][Epoch 7211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:15,190 - INFO - [diffusion][Epoch 7212] Epoch 7213/12000
2024-11-05 01:45:19,304 - INFO - [diffusion][Epoch 7212] diffusion training Loss: 0.05784668307751417
2024-11-05 01:45:19,307 - INFO - [diffusion][Epoch 7212] diffusion learning rate: 0.001
2024-11-05 01:45:19,308 - INFO - [diffusion][Epoch 7212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:19,310 - INFO - [diffusion][Epoch 7213] Epoch 7214/12000
2024-11-05 01:45:23,426 - INFO - [diffusion][Epoch 7213] diffusion training Loss: 0.05890813283622265
2024-11-05 01:45:23,428 - INFO - [diffusion][Epoch 7213] diffusion learning rate: 0.001
2024-11-05 01:45:23,430 - INFO - [diffusion][Epoch 7213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:23,431 - INFO - [diffusion][Epoch 7214] Epoch 7215/12000
2024-11-05 01:45:27,536 - INFO - [diffusion][Epoch 7214] diffusion training Loss: 0.051083232276141644
2024-11-05 01:45:27,538 - INFO - [diffusion][Epoch 7214] diffusion learning rate: 0.001
2024-11-05 01:45:27,540 - INFO - [diffusion][Epoch 7214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:27,541 - INFO - [diffusion][Epoch 7215] Epoch 7216/12000
2024-11-05 01:45:31,482 - INFO - [diffusion][Epoch 7215] diffusion training Loss: 0.06069706194102764
2024-11-05 01:45:31,484 - INFO - [diffusion][Epoch 7215] diffusion learning rate: 0.001
2024-11-05 01:45:31,486 - INFO - [diffusion][Epoch 7215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:31,487 - INFO - [diffusion][Epoch 7216] Epoch 7217/12000
2024-11-05 01:45:35,628 - INFO - [diffusion][Epoch 7216] diffusion training Loss: 0.053585246205329895
2024-11-05 01:45:35,631 - INFO - [diffusion][Epoch 7216] diffusion learning rate: 0.001
2024-11-05 01:45:35,632 - INFO - [diffusion][Epoch 7216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:35,633 - INFO - [diffusion][Epoch 7217] Epoch 7218/12000
2024-11-05 01:45:39,572 - INFO - [diffusion][Epoch 7217] diffusion training Loss: 0.050808207131922245
2024-11-05 01:45:39,576 - INFO - [diffusion][Epoch 7217] diffusion learning rate: 0.001
2024-11-05 01:45:39,578 - INFO - [diffusion][Epoch 7217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:39,580 - INFO - [diffusion][Epoch 7218] Epoch 7219/12000
2024-11-05 01:45:43,503 - INFO - [diffusion][Epoch 7218] diffusion training Loss: 0.056400119327008724
2024-11-05 01:45:43,505 - INFO - [diffusion][Epoch 7218] diffusion learning rate: 0.001
2024-11-05 01:45:43,507 - INFO - [diffusion][Epoch 7218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:43,509 - INFO - [diffusion][Epoch 7219] Epoch 7220/12000
2024-11-05 01:45:47,625 - INFO - [diffusion][Epoch 7219] diffusion training Loss: 0.05395223945379257
2024-11-05 01:45:47,628 - INFO - [diffusion][Epoch 7219] diffusion learning rate: 0.001
2024-11-05 01:45:47,631 - INFO - [diffusion][Epoch 7219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:47,632 - INFO - [diffusion][Epoch 7220] Epoch 7221/12000
2024-11-05 01:45:51,905 - INFO - [diffusion][Epoch 7220] diffusion training Loss: 0.056396751664578915
2024-11-05 01:45:51,907 - INFO - [diffusion][Epoch 7220] diffusion learning rate: 0.001
2024-11-05 01:45:51,909 - INFO - [diffusion][Epoch 7220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:51,910 - INFO - [diffusion][Epoch 7221] Epoch 7222/12000
2024-11-05 01:45:56,041 - INFO - [diffusion][Epoch 7221] diffusion training Loss: 0.05425616726279259
2024-11-05 01:45:56,043 - INFO - [diffusion][Epoch 7221] diffusion learning rate: 0.001
2024-11-05 01:45:56,045 - INFO - [diffusion][Epoch 7221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:45:56,046 - INFO - [diffusion][Epoch 7222] Epoch 7223/12000
2024-11-05 01:46:00,155 - INFO - [diffusion][Epoch 7222] diffusion training Loss: 0.053132678382098675
2024-11-05 01:46:00,157 - INFO - [diffusion][Epoch 7222] diffusion learning rate: 0.001
2024-11-05 01:46:00,222 - INFO - [diffusion][Epoch 7222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:00,223 - INFO - [diffusion][Epoch 7223] Epoch 7224/12000
2024-11-05 01:46:04,409 - INFO - [diffusion][Epoch 7223] diffusion training Loss: 0.05689555965363979
2024-11-05 01:46:04,411 - INFO - [diffusion][Epoch 7223] diffusion learning rate: 0.001
2024-11-05 01:46:04,412 - INFO - [diffusion][Epoch 7223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:04,414 - INFO - [diffusion][Epoch 7224] Epoch 7225/12000
2024-11-05 01:46:08,550 - INFO - [diffusion][Epoch 7224] diffusion training Loss: 0.0631033768877387
2024-11-05 01:46:08,552 - INFO - [diffusion][Epoch 7224] diffusion learning rate: 0.001
2024-11-05 01:46:08,554 - INFO - [diffusion][Epoch 7224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:08,555 - INFO - [diffusion][Epoch 7225] Epoch 7226/12000
2024-11-05 01:46:12,578 - INFO - [diffusion][Epoch 7225] diffusion training Loss: 0.06059420481324196
2024-11-05 01:46:12,581 - INFO - [diffusion][Epoch 7225] diffusion learning rate: 0.001
2024-11-05 01:46:12,583 - INFO - [diffusion][Epoch 7225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:12,585 - INFO - [diffusion][Epoch 7226] Epoch 7227/12000
2024-11-05 01:46:16,748 - INFO - [diffusion][Epoch 7226] diffusion training Loss: 0.05414609983563423
2024-11-05 01:46:16,750 - INFO - [diffusion][Epoch 7226] diffusion learning rate: 0.001
2024-11-05 01:46:16,752 - INFO - [diffusion][Epoch 7226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:16,753 - INFO - [diffusion][Epoch 7227] Epoch 7228/12000
2024-11-05 01:46:20,881 - INFO - [diffusion][Epoch 7227] diffusion training Loss: 0.05632575694471598
2024-11-05 01:46:20,884 - INFO - [diffusion][Epoch 7227] diffusion learning rate: 0.001
2024-11-05 01:46:20,886 - INFO - [diffusion][Epoch 7227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:20,887 - INFO - [diffusion][Epoch 7228] Epoch 7229/12000
2024-11-05 01:46:25,006 - INFO - [diffusion][Epoch 7228] diffusion training Loss: 0.05872494913637638
2024-11-05 01:46:25,008 - INFO - [diffusion][Epoch 7228] diffusion learning rate: 0.001
2024-11-05 01:46:25,010 - INFO - [diffusion][Epoch 7228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:25,011 - INFO - [diffusion][Epoch 7229] Epoch 7230/12000
2024-11-05 01:46:29,167 - INFO - [diffusion][Epoch 7229] diffusion training Loss: 0.05771350581198931
2024-11-05 01:46:29,169 - INFO - [diffusion][Epoch 7229] diffusion learning rate: 0.001
2024-11-05 01:46:29,171 - INFO - [diffusion][Epoch 7229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:29,172 - INFO - [diffusion][Epoch 7230] Epoch 7231/12000
2024-11-05 01:46:33,316 - INFO - [diffusion][Epoch 7230] diffusion training Loss: 0.05317252688109875
2024-11-05 01:46:33,318 - INFO - [diffusion][Epoch 7230] diffusion learning rate: 0.001
2024-11-05 01:46:33,320 - INFO - [diffusion][Epoch 7230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:33,321 - INFO - [diffusion][Epoch 7231] Epoch 7232/12000
2024-11-05 01:46:37,354 - INFO - [diffusion][Epoch 7231] diffusion training Loss: 0.054981437511742115
2024-11-05 01:46:37,356 - INFO - [diffusion][Epoch 7231] diffusion learning rate: 0.001
2024-11-05 01:46:37,358 - INFO - [diffusion][Epoch 7231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:37,360 - INFO - [diffusion][Epoch 7232] Epoch 7233/12000
2024-11-05 01:46:41,402 - INFO - [diffusion][Epoch 7232] diffusion training Loss: 0.04879047255963087
2024-11-05 01:46:41,404 - INFO - [diffusion][Epoch 7232] diffusion learning rate: 0.001
2024-11-05 01:46:41,406 - INFO - [diffusion][Epoch 7232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:41,407 - INFO - [diffusion][Epoch 7233] Epoch 7234/12000
2024-11-05 01:46:45,586 - INFO - [diffusion][Epoch 7233] diffusion training Loss: 0.05387386307120323
2024-11-05 01:46:45,588 - INFO - [diffusion][Epoch 7233] diffusion learning rate: 0.001
2024-11-05 01:46:45,591 - INFO - [diffusion][Epoch 7233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:45,593 - INFO - [diffusion][Epoch 7234] Epoch 7235/12000
2024-11-05 01:46:49,731 - INFO - [diffusion][Epoch 7234] diffusion training Loss: 0.0622356329113245
2024-11-05 01:46:49,733 - INFO - [diffusion][Epoch 7234] diffusion learning rate: 0.001
2024-11-05 01:46:49,735 - INFO - [diffusion][Epoch 7234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:49,736 - INFO - [diffusion][Epoch 7235] Epoch 7236/12000
2024-11-05 01:46:53,879 - INFO - [diffusion][Epoch 7235] diffusion training Loss: 0.05513022933155298
2024-11-05 01:46:53,881 - INFO - [diffusion][Epoch 7235] diffusion learning rate: 0.001
2024-11-05 01:46:53,883 - INFO - [diffusion][Epoch 7235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:53,884 - INFO - [diffusion][Epoch 7236] Epoch 7237/12000
2024-11-05 01:46:57,995 - INFO - [diffusion][Epoch 7236] diffusion training Loss: 0.05977002717554569
2024-11-05 01:46:57,997 - INFO - [diffusion][Epoch 7236] diffusion learning rate: 0.001
2024-11-05 01:46:57,999 - INFO - [diffusion][Epoch 7236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:46:58,001 - INFO - [diffusion][Epoch 7237] Epoch 7238/12000
2024-11-05 01:47:02,133 - INFO - [diffusion][Epoch 7237] diffusion training Loss: 0.056074660271406174
2024-11-05 01:47:02,136 - INFO - [diffusion][Epoch 7237] diffusion learning rate: 0.001
2024-11-05 01:47:02,138 - INFO - [diffusion][Epoch 7237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:02,139 - INFO - [diffusion][Epoch 7238] Epoch 7239/12000
2024-11-05 01:47:06,250 - INFO - [diffusion][Epoch 7238] diffusion training Loss: 0.056062969379127026
2024-11-05 01:47:06,252 - INFO - [diffusion][Epoch 7238] diffusion learning rate: 0.001
2024-11-05 01:47:06,254 - INFO - [diffusion][Epoch 7238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:06,255 - INFO - [diffusion][Epoch 7239] Epoch 7240/12000
2024-11-05 01:47:10,384 - INFO - [diffusion][Epoch 7239] diffusion training Loss: 0.051059684716165066
2024-11-05 01:47:10,386 - INFO - [diffusion][Epoch 7239] diffusion learning rate: 0.001
2024-11-05 01:47:10,388 - INFO - [diffusion][Epoch 7239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:10,389 - INFO - [diffusion][Epoch 7240] Epoch 7241/12000
2024-11-05 01:47:14,492 - INFO - [diffusion][Epoch 7240] diffusion training Loss: 0.0562279662117362
2024-11-05 01:47:14,494 - INFO - [diffusion][Epoch 7240] diffusion learning rate: 0.001
2024-11-05 01:47:14,496 - INFO - [diffusion][Epoch 7240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:14,497 - INFO - [diffusion][Epoch 7241] Epoch 7242/12000
2024-11-05 01:47:18,745 - INFO - [diffusion][Epoch 7241] diffusion training Loss: 0.05685001611709595
2024-11-05 01:47:18,747 - INFO - [diffusion][Epoch 7241] diffusion learning rate: 0.001
2024-11-05 01:47:18,749 - INFO - [diffusion][Epoch 7241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:18,750 - INFO - [diffusion][Epoch 7242] Epoch 7243/12000
2024-11-05 01:47:22,915 - INFO - [diffusion][Epoch 7242] diffusion training Loss: 0.054136180318892
2024-11-05 01:47:22,918 - INFO - [diffusion][Epoch 7242] diffusion learning rate: 0.001
2024-11-05 01:47:22,920 - INFO - [diffusion][Epoch 7242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:22,921 - INFO - [diffusion][Epoch 7243] Epoch 7244/12000
2024-11-05 01:47:27,064 - INFO - [diffusion][Epoch 7243] diffusion training Loss: 0.05584698636084795
2024-11-05 01:47:27,066 - INFO - [diffusion][Epoch 7243] diffusion learning rate: 0.001
2024-11-05 01:47:27,068 - INFO - [diffusion][Epoch 7243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:27,069 - INFO - [diffusion][Epoch 7244] Epoch 7245/12000
2024-11-05 01:47:31,223 - INFO - [diffusion][Epoch 7244] diffusion training Loss: 0.0584591468796134
2024-11-05 01:47:31,225 - INFO - [diffusion][Epoch 7244] diffusion learning rate: 0.001
2024-11-05 01:47:31,227 - INFO - [diffusion][Epoch 7244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:31,228 - INFO - [diffusion][Epoch 7245] Epoch 7246/12000
2024-11-05 01:47:35,329 - INFO - [diffusion][Epoch 7245] diffusion training Loss: 0.0550151402130723
2024-11-05 01:47:35,331 - INFO - [diffusion][Epoch 7245] diffusion learning rate: 0.001
2024-11-05 01:47:35,333 - INFO - [diffusion][Epoch 7245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:35,334 - INFO - [diffusion][Epoch 7246] Epoch 7247/12000
2024-11-05 01:47:39,433 - INFO - [diffusion][Epoch 7246] diffusion training Loss: 0.05764593277126551
2024-11-05 01:47:39,435 - INFO - [diffusion][Epoch 7246] diffusion learning rate: 0.001
2024-11-05 01:47:39,437 - INFO - [diffusion][Epoch 7246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:39,439 - INFO - [diffusion][Epoch 7247] Epoch 7248/12000
2024-11-05 01:47:43,589 - INFO - [diffusion][Epoch 7247] diffusion training Loss: 0.056465813890099525
2024-11-05 01:47:43,591 - INFO - [diffusion][Epoch 7247] diffusion learning rate: 0.001
2024-11-05 01:47:43,593 - INFO - [diffusion][Epoch 7247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:43,594 - INFO - [diffusion][Epoch 7248] Epoch 7249/12000
2024-11-05 01:47:47,746 - INFO - [diffusion][Epoch 7248] diffusion training Loss: 0.05762530490756035
2024-11-05 01:47:47,749 - INFO - [diffusion][Epoch 7248] diffusion learning rate: 0.001
2024-11-05 01:47:47,751 - INFO - [diffusion][Epoch 7248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:47,752 - INFO - [diffusion][Epoch 7249] Epoch 7250/12000
2024-11-05 01:47:51,886 - INFO - [diffusion][Epoch 7249] diffusion training Loss: 0.051578693091869354
2024-11-05 01:47:51,888 - INFO - [diffusion][Epoch 7249] diffusion learning rate: 0.001
2024-11-05 01:47:51,891 - INFO - [diffusion][Epoch 7249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:51,892 - INFO - [diffusion][Epoch 7250] Epoch 7251/12000
2024-11-05 01:47:56,020 - INFO - [diffusion][Epoch 7250] diffusion training Loss: 0.05920024216175079
2024-11-05 01:47:56,022 - INFO - [diffusion][Epoch 7250] diffusion learning rate: 0.001
2024-11-05 01:47:56,100 - INFO - [diffusion][Epoch 7250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:47:56,101 - INFO - [diffusion][Epoch 7251] Epoch 7252/12000
2024-11-05 01:48:00,277 - INFO - [diffusion][Epoch 7251] diffusion training Loss: 0.05080484785139561
2024-11-05 01:48:00,279 - INFO - [diffusion][Epoch 7251] diffusion learning rate: 0.001
2024-11-05 01:48:00,281 - INFO - [diffusion][Epoch 7251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:00,282 - INFO - [diffusion][Epoch 7252] Epoch 7253/12000
2024-11-05 01:48:04,435 - INFO - [diffusion][Epoch 7252] diffusion training Loss: 0.05376687739044428
2024-11-05 01:48:04,437 - INFO - [diffusion][Epoch 7252] diffusion learning rate: 0.001
2024-11-05 01:48:04,439 - INFO - [diffusion][Epoch 7252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:04,440 - INFO - [diffusion][Epoch 7253] Epoch 7254/12000
2024-11-05 01:48:08,624 - INFO - [diffusion][Epoch 7253] diffusion training Loss: 0.0564526692032814
2024-11-05 01:48:08,626 - INFO - [diffusion][Epoch 7253] diffusion learning rate: 0.001
2024-11-05 01:48:08,627 - INFO - [diffusion][Epoch 7253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:08,629 - INFO - [diffusion][Epoch 7254] Epoch 7255/12000
2024-11-05 01:48:12,717 - INFO - [diffusion][Epoch 7254] diffusion training Loss: 0.0547081446275115
2024-11-05 01:48:12,719 - INFO - [diffusion][Epoch 7254] diffusion learning rate: 0.001
2024-11-05 01:48:12,720 - INFO - [diffusion][Epoch 7254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:12,721 - INFO - [diffusion][Epoch 7255] Epoch 7256/12000
2024-11-05 01:48:16,728 - INFO - [diffusion][Epoch 7255] diffusion training Loss: 0.05408240482211113
2024-11-05 01:48:16,774 - INFO - [diffusion][Epoch 7255] diffusion learning rate: 0.001
2024-11-05 01:48:16,776 - INFO - [diffusion][Epoch 7255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:16,778 - INFO - [diffusion][Epoch 7256] Epoch 7257/12000
2024-11-05 01:48:20,878 - INFO - [diffusion][Epoch 7256] diffusion training Loss: 0.0534690311178565
2024-11-05 01:48:20,880 - INFO - [diffusion][Epoch 7256] diffusion learning rate: 0.001
2024-11-05 01:48:20,882 - INFO - [diffusion][Epoch 7256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:20,884 - INFO - [diffusion][Epoch 7257] Epoch 7258/12000
2024-11-05 01:48:24,981 - INFO - [diffusion][Epoch 7257] diffusion training Loss: 0.060031685046851635
2024-11-05 01:48:24,984 - INFO - [diffusion][Epoch 7257] diffusion learning rate: 0.001
2024-11-05 01:48:24,985 - INFO - [diffusion][Epoch 7257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:24,987 - INFO - [diffusion][Epoch 7258] Epoch 7259/12000
2024-11-05 01:48:28,993 - INFO - [diffusion][Epoch 7258] diffusion training Loss: 0.05319554544985294
2024-11-05 01:48:28,995 - INFO - [diffusion][Epoch 7258] diffusion learning rate: 0.001
2024-11-05 01:48:28,997 - INFO - [diffusion][Epoch 7258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:28,998 - INFO - [diffusion][Epoch 7259] Epoch 7260/12000
2024-11-05 01:48:33,083 - INFO - [diffusion][Epoch 7259] diffusion training Loss: 0.05516265332698822
2024-11-05 01:48:33,085 - INFO - [diffusion][Epoch 7259] diffusion learning rate: 0.001
2024-11-05 01:48:33,142 - INFO - [diffusion][Epoch 7259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:33,144 - INFO - [diffusion][Epoch 7260] Epoch 7261/12000
2024-11-05 01:48:37,332 - INFO - [diffusion][Epoch 7260] diffusion training Loss: 0.054246922954916954
2024-11-05 01:48:37,334 - INFO - [diffusion][Epoch 7260] diffusion learning rate: 0.001
2024-11-05 01:48:37,336 - INFO - [diffusion][Epoch 7260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:37,337 - INFO - [diffusion][Epoch 7261] Epoch 7262/12000
2024-11-05 01:48:41,458 - INFO - [diffusion][Epoch 7261] diffusion training Loss: 0.056473247706890106
2024-11-05 01:48:41,460 - INFO - [diffusion][Epoch 7261] diffusion learning rate: 0.001
2024-11-05 01:48:41,461 - INFO - [diffusion][Epoch 7261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:41,463 - INFO - [diffusion][Epoch 7262] Epoch 7263/12000
2024-11-05 01:48:45,432 - INFO - [diffusion][Epoch 7262] diffusion training Loss: 0.056324646808207035
2024-11-05 01:48:45,434 - INFO - [diffusion][Epoch 7262] diffusion learning rate: 0.001
2024-11-05 01:48:45,436 - INFO - [diffusion][Epoch 7262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:45,437 - INFO - [diffusion][Epoch 7263] Epoch 7264/12000
2024-11-05 01:48:49,576 - INFO - [diffusion][Epoch 7263] diffusion training Loss: 0.060591135174036026
2024-11-05 01:48:49,578 - INFO - [diffusion][Epoch 7263] diffusion learning rate: 0.001
2024-11-05 01:48:49,580 - INFO - [diffusion][Epoch 7263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:49,581 - INFO - [diffusion][Epoch 7264] Epoch 7265/12000
2024-11-05 01:48:53,707 - INFO - [diffusion][Epoch 7264] diffusion training Loss: 0.05201852694153786
2024-11-05 01:48:53,709 - INFO - [diffusion][Epoch 7264] diffusion learning rate: 0.001
2024-11-05 01:48:53,710 - INFO - [diffusion][Epoch 7264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:53,712 - INFO - [diffusion][Epoch 7265] Epoch 7266/12000
2024-11-05 01:48:57,830 - INFO - [diffusion][Epoch 7265] diffusion training Loss: 0.058923717588186264
2024-11-05 01:48:57,832 - INFO - [diffusion][Epoch 7265] diffusion learning rate: 0.001
2024-11-05 01:48:57,834 - INFO - [diffusion][Epoch 7265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:48:57,835 - INFO - [diffusion][Epoch 7266] Epoch 7267/12000
2024-11-05 01:49:01,964 - INFO - [diffusion][Epoch 7266] diffusion training Loss: 0.05513285472989082
2024-11-05 01:49:01,966 - INFO - [diffusion][Epoch 7266] diffusion learning rate: 0.001
2024-11-05 01:49:01,968 - INFO - [diffusion][Epoch 7266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:01,969 - INFO - [diffusion][Epoch 7267] Epoch 7268/12000
2024-11-05 01:49:06,089 - INFO - [diffusion][Epoch 7267] diffusion training Loss: 0.06200787890702486
2024-11-05 01:49:06,090 - INFO - [diffusion][Epoch 7267] diffusion learning rate: 0.001
2024-11-05 01:49:06,092 - INFO - [diffusion][Epoch 7267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:06,094 - INFO - [diffusion][Epoch 7268] Epoch 7269/12000
2024-11-05 01:49:10,174 - INFO - [diffusion][Epoch 7268] diffusion training Loss: 0.05791551060974598
2024-11-05 01:49:10,176 - INFO - [diffusion][Epoch 7268] diffusion learning rate: 0.001
2024-11-05 01:49:10,178 - INFO - [diffusion][Epoch 7268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:10,179 - INFO - [diffusion][Epoch 7269] Epoch 7270/12000
2024-11-05 01:49:14,289 - INFO - [diffusion][Epoch 7269] diffusion training Loss: 0.05548582971096039
2024-11-05 01:49:14,291 - INFO - [diffusion][Epoch 7269] diffusion learning rate: 0.001
2024-11-05 01:49:14,293 - INFO - [diffusion][Epoch 7269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:14,294 - INFO - [diffusion][Epoch 7270] Epoch 7271/12000
2024-11-05 01:49:18,355 - INFO - [diffusion][Epoch 7270] diffusion training Loss: 0.06061160657554865
2024-11-05 01:49:18,358 - INFO - [diffusion][Epoch 7270] diffusion learning rate: 0.001
2024-11-05 01:49:18,401 - INFO - [diffusion][Epoch 7270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:18,403 - INFO - [diffusion][Epoch 7271] Epoch 7272/12000
2024-11-05 01:49:22,491 - INFO - [diffusion][Epoch 7271] diffusion training Loss: 0.059781430289149284
2024-11-05 01:49:22,493 - INFO - [diffusion][Epoch 7271] diffusion learning rate: 0.001
2024-11-05 01:49:22,495 - INFO - [diffusion][Epoch 7271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:22,496 - INFO - [diffusion][Epoch 7272] Epoch 7273/12000
2024-11-05 01:49:26,565 - INFO - [diffusion][Epoch 7272] diffusion training Loss: 0.06413432396948338
2024-11-05 01:49:26,567 - INFO - [diffusion][Epoch 7272] diffusion learning rate: 0.001
2024-11-05 01:49:26,569 - INFO - [diffusion][Epoch 7272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:26,570 - INFO - [diffusion][Epoch 7273] Epoch 7274/12000
2024-11-05 01:49:30,662 - INFO - [diffusion][Epoch 7273] diffusion training Loss: 0.05710216239094734
2024-11-05 01:49:30,664 - INFO - [diffusion][Epoch 7273] diffusion learning rate: 0.001
2024-11-05 01:49:30,665 - INFO - [diffusion][Epoch 7273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:30,667 - INFO - [diffusion][Epoch 7274] Epoch 7275/12000
2024-11-05 01:49:34,633 - INFO - [diffusion][Epoch 7274] diffusion training Loss: 0.05765129253268242
2024-11-05 01:49:34,635 - INFO - [diffusion][Epoch 7274] diffusion learning rate: 0.001
2024-11-05 01:49:34,684 - INFO - [diffusion][Epoch 7274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:34,686 - INFO - [diffusion][Epoch 7275] Epoch 7276/12000
2024-11-05 01:49:38,686 - INFO - [diffusion][Epoch 7275] diffusion training Loss: 0.05039524659514427
2024-11-05 01:49:38,688 - INFO - [diffusion][Epoch 7275] diffusion learning rate: 0.001
2024-11-05 01:49:38,690 - INFO - [diffusion][Epoch 7275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:38,691 - INFO - [diffusion][Epoch 7276] Epoch 7277/12000
2024-11-05 01:49:42,771 - INFO - [diffusion][Epoch 7276] diffusion training Loss: 0.06072086934000254
2024-11-05 01:49:42,773 - INFO - [diffusion][Epoch 7276] diffusion learning rate: 0.001
2024-11-05 01:49:42,775 - INFO - [diffusion][Epoch 7276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:42,776 - INFO - [diffusion][Epoch 7277] Epoch 7278/12000
2024-11-05 01:49:46,905 - INFO - [diffusion][Epoch 7277] diffusion training Loss: 0.052054887637495995
2024-11-05 01:49:46,907 - INFO - [diffusion][Epoch 7277] diffusion learning rate: 0.001
2024-11-05 01:49:46,909 - INFO - [diffusion][Epoch 7277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:46,910 - INFO - [diffusion][Epoch 7278] Epoch 7279/12000
2024-11-05 01:49:50,970 - INFO - [diffusion][Epoch 7278] diffusion training Loss: 0.05877208802849054
2024-11-05 01:49:50,972 - INFO - [diffusion][Epoch 7278] diffusion learning rate: 0.001
2024-11-05 01:49:50,973 - INFO - [diffusion][Epoch 7278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:50,975 - INFO - [diffusion][Epoch 7279] Epoch 7280/12000
2024-11-05 01:49:55,060 - INFO - [diffusion][Epoch 7279] diffusion training Loss: 0.05351534951478243
2024-11-05 01:49:55,062 - INFO - [diffusion][Epoch 7279] diffusion learning rate: 0.001
2024-11-05 01:49:55,064 - INFO - [diffusion][Epoch 7279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:55,065 - INFO - [diffusion][Epoch 7280] Epoch 7281/12000
2024-11-05 01:49:59,156 - INFO - [diffusion][Epoch 7280] diffusion training Loss: 0.05726365651935339
2024-11-05 01:49:59,158 - INFO - [diffusion][Epoch 7280] diffusion learning rate: 0.001
2024-11-05 01:49:59,160 - INFO - [diffusion][Epoch 7280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:49:59,161 - INFO - [diffusion][Epoch 7281] Epoch 7282/12000
2024-11-05 01:50:03,200 - INFO - [diffusion][Epoch 7281] diffusion training Loss: 0.05711805634200573
2024-11-05 01:50:03,202 - INFO - [diffusion][Epoch 7281] diffusion learning rate: 0.001
2024-11-05 01:50:03,203 - INFO - [diffusion][Epoch 7281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:03,205 - INFO - [diffusion][Epoch 7282] Epoch 7283/12000
2024-11-05 01:50:07,626 - INFO - [diffusion][Epoch 7282] diffusion training Loss: 0.05860178917646408
2024-11-05 01:50:07,628 - INFO - [diffusion][Epoch 7282] diffusion learning rate: 0.001
2024-11-05 01:50:07,630 - INFO - [diffusion][Epoch 7282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:07,632 - INFO - [diffusion][Epoch 7283] Epoch 7284/12000
2024-11-05 01:50:11,723 - INFO - [diffusion][Epoch 7283] diffusion training Loss: 0.05799334216862917
2024-11-05 01:50:11,725 - INFO - [diffusion][Epoch 7283] diffusion learning rate: 0.001
2024-11-05 01:50:11,727 - INFO - [diffusion][Epoch 7283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:11,728 - INFO - [diffusion][Epoch 7284] Epoch 7285/12000
2024-11-05 01:50:15,769 - INFO - [diffusion][Epoch 7284] diffusion training Loss: 0.057186419144272804
2024-11-05 01:50:15,771 - INFO - [diffusion][Epoch 7284] diffusion learning rate: 0.001
2024-11-05 01:50:15,773 - INFO - [diffusion][Epoch 7284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:15,774 - INFO - [diffusion][Epoch 7285] Epoch 7286/12000
2024-11-05 01:50:19,845 - INFO - [diffusion][Epoch 7285] diffusion training Loss: 0.060112529434263706
2024-11-05 01:50:19,847 - INFO - [diffusion][Epoch 7285] diffusion learning rate: 0.001
2024-11-05 01:50:19,849 - INFO - [diffusion][Epoch 7285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:19,851 - INFO - [diffusion][Epoch 7286] Epoch 7287/12000
2024-11-05 01:50:23,993 - INFO - [diffusion][Epoch 7286] diffusion training Loss: 0.05107368901371956
2024-11-05 01:50:23,996 - INFO - [diffusion][Epoch 7286] diffusion learning rate: 0.001
2024-11-05 01:50:23,997 - INFO - [diffusion][Epoch 7286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:23,999 - INFO - [diffusion][Epoch 7287] Epoch 7288/12000
2024-11-05 01:50:28,101 - INFO - [diffusion][Epoch 7287] diffusion training Loss: 0.05412509571760893
2024-11-05 01:50:28,103 - INFO - [diffusion][Epoch 7287] diffusion learning rate: 0.001
2024-11-05 01:50:28,105 - INFO - [diffusion][Epoch 7287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:28,106 - INFO - [diffusion][Epoch 7288] Epoch 7289/12000
2024-11-05 01:50:32,227 - INFO - [diffusion][Epoch 7288] diffusion training Loss: 0.05261599738150835
2024-11-05 01:50:32,229 - INFO - [diffusion][Epoch 7288] diffusion learning rate: 0.001
2024-11-05 01:50:32,230 - INFO - [diffusion][Epoch 7288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:32,231 - INFO - [diffusion][Epoch 7289] Epoch 7290/12000
2024-11-05 01:50:36,318 - INFO - [diffusion][Epoch 7289] diffusion training Loss: 0.05348831322044134
2024-11-05 01:50:36,320 - INFO - [diffusion][Epoch 7289] diffusion learning rate: 0.001
2024-11-05 01:50:36,322 - INFO - [diffusion][Epoch 7289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:36,323 - INFO - [diffusion][Epoch 7290] Epoch 7291/12000
2024-11-05 01:50:40,444 - INFO - [diffusion][Epoch 7290] diffusion training Loss: 0.05849375855177641
2024-11-05 01:50:40,446 - INFO - [diffusion][Epoch 7290] diffusion learning rate: 0.001
2024-11-05 01:50:40,447 - INFO - [diffusion][Epoch 7290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:40,449 - INFO - [diffusion][Epoch 7291] Epoch 7292/12000
2024-11-05 01:50:44,594 - INFO - [diffusion][Epoch 7291] diffusion training Loss: 0.054744841530919075
2024-11-05 01:50:44,596 - INFO - [diffusion][Epoch 7291] diffusion learning rate: 0.001
2024-11-05 01:50:44,647 - INFO - [diffusion][Epoch 7291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:44,648 - INFO - [diffusion][Epoch 7292] Epoch 7293/12000
2024-11-05 01:50:48,773 - INFO - [diffusion][Epoch 7292] diffusion training Loss: 0.05598630104213953
2024-11-05 01:50:48,776 - INFO - [diffusion][Epoch 7292] diffusion learning rate: 0.001
2024-11-05 01:50:48,777 - INFO - [diffusion][Epoch 7292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:48,779 - INFO - [diffusion][Epoch 7293] Epoch 7294/12000
2024-11-05 01:50:52,886 - INFO - [diffusion][Epoch 7293] diffusion training Loss: 0.050439211539924145
2024-11-05 01:50:52,888 - INFO - [diffusion][Epoch 7293] diffusion learning rate: 0.001
2024-11-05 01:50:52,890 - INFO - [diffusion][Epoch 7293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:52,891 - INFO - [diffusion][Epoch 7294] Epoch 7295/12000
2024-11-05 01:50:56,987 - INFO - [diffusion][Epoch 7294] diffusion training Loss: 0.05727003701031208
2024-11-05 01:50:56,989 - INFO - [diffusion][Epoch 7294] diffusion learning rate: 0.001
2024-11-05 01:50:56,991 - INFO - [diffusion][Epoch 7294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:50:56,992 - INFO - [diffusion][Epoch 7295] Epoch 7296/12000
2024-11-05 01:51:01,097 - INFO - [diffusion][Epoch 7295] diffusion training Loss: 0.055253392085433006
2024-11-05 01:51:01,099 - INFO - [diffusion][Epoch 7295] diffusion learning rate: 0.001
2024-11-05 01:51:01,101 - INFO - [diffusion][Epoch 7295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:01,102 - INFO - [diffusion][Epoch 7296] Epoch 7297/12000
2024-11-05 01:51:05,220 - INFO - [diffusion][Epoch 7296] diffusion training Loss: 0.05973821319639683
2024-11-05 01:51:05,222 - INFO - [diffusion][Epoch 7296] diffusion learning rate: 0.001
2024-11-05 01:51:05,225 - INFO - [diffusion][Epoch 7296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:05,226 - INFO - [diffusion][Epoch 7297] Epoch 7298/12000
2024-11-05 01:51:09,356 - INFO - [diffusion][Epoch 7297] diffusion training Loss: 0.06143052875995636
2024-11-05 01:51:09,358 - INFO - [diffusion][Epoch 7297] diffusion learning rate: 0.001
2024-11-05 01:51:09,361 - INFO - [diffusion][Epoch 7297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:09,362 - INFO - [diffusion][Epoch 7298] Epoch 7299/12000
2024-11-05 01:51:13,530 - INFO - [diffusion][Epoch 7298] diffusion training Loss: 0.049061696976423264
2024-11-05 01:51:13,532 - INFO - [diffusion][Epoch 7298] diffusion learning rate: 0.001
2024-11-05 01:51:13,533 - INFO - [diffusion][Epoch 7298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:13,535 - INFO - [diffusion][Epoch 7299] Epoch 7300/12000
2024-11-05 01:51:17,585 - INFO - [diffusion][Epoch 7299] diffusion training Loss: 0.05459904205054045
2024-11-05 01:51:17,587 - INFO - [diffusion][Epoch 7299] diffusion learning rate: 0.001
2024-11-05 01:51:17,588 - INFO - [diffusion][Epoch 7299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:17,590 - INFO - [diffusion][Epoch 7300] Epoch 7301/12000
2024-11-05 01:51:21,736 - INFO - [diffusion][Epoch 7300] diffusion training Loss: 0.05477872025221586
2024-11-05 01:51:21,739 - INFO - [diffusion][Epoch 7300] diffusion learning rate: 0.001
2024-11-05 01:51:21,741 - INFO - [diffusion][Epoch 7300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:21,742 - INFO - [diffusion][Epoch 7301] Epoch 7302/12000
2024-11-05 01:51:25,852 - INFO - [diffusion][Epoch 7301] diffusion training Loss: 0.05807465221732855
2024-11-05 01:51:25,854 - INFO - [diffusion][Epoch 7301] diffusion learning rate: 0.001
2024-11-05 01:51:25,856 - INFO - [diffusion][Epoch 7301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:25,857 - INFO - [diffusion][Epoch 7302] Epoch 7303/12000
2024-11-05 01:51:29,954 - INFO - [diffusion][Epoch 7302] diffusion training Loss: 0.057169792242348194
2024-11-05 01:51:29,956 - INFO - [diffusion][Epoch 7302] diffusion learning rate: 0.001
2024-11-05 01:51:29,958 - INFO - [diffusion][Epoch 7302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:29,959 - INFO - [diffusion][Epoch 7303] Epoch 7304/12000
2024-11-05 01:51:34,382 - INFO - [diffusion][Epoch 7303] diffusion training Loss: 0.05670044757425785
2024-11-05 01:51:34,454 - INFO - [diffusion][Epoch 7303] diffusion learning rate: 0.001
2024-11-05 01:51:34,455 - INFO - [diffusion][Epoch 7303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:34,457 - INFO - [diffusion][Epoch 7304] Epoch 7305/12000
2024-11-05 01:51:38,577 - INFO - [diffusion][Epoch 7304] diffusion training Loss: 0.053491405211389065
2024-11-05 01:51:38,579 - INFO - [diffusion][Epoch 7304] diffusion learning rate: 0.001
2024-11-05 01:51:38,580 - INFO - [diffusion][Epoch 7304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:38,582 - INFO - [diffusion][Epoch 7305] Epoch 7306/12000
2024-11-05 01:51:42,717 - INFO - [diffusion][Epoch 7305] diffusion training Loss: 0.049253945238888264
2024-11-05 01:51:42,719 - INFO - [diffusion][Epoch 7305] diffusion learning rate: 0.001
2024-11-05 01:51:42,721 - INFO - [diffusion][Epoch 7305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:42,722 - INFO - [diffusion][Epoch 7306] Epoch 7307/12000
2024-11-05 01:51:46,831 - INFO - [diffusion][Epoch 7306] diffusion training Loss: 0.053867378272116184
2024-11-05 01:51:46,833 - INFO - [diffusion][Epoch 7306] diffusion learning rate: 0.001
2024-11-05 01:51:46,834 - INFO - [diffusion][Epoch 7306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:46,836 - INFO - [diffusion][Epoch 7307] Epoch 7308/12000
2024-11-05 01:51:50,940 - INFO - [diffusion][Epoch 7307] diffusion training Loss: 0.05216249171644449
2024-11-05 01:51:50,942 - INFO - [diffusion][Epoch 7307] diffusion learning rate: 0.001
2024-11-05 01:51:50,944 - INFO - [diffusion][Epoch 7307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:50,945 - INFO - [diffusion][Epoch 7308] Epoch 7309/12000
2024-11-05 01:51:55,014 - INFO - [diffusion][Epoch 7308] diffusion training Loss: 0.059829894453287125
2024-11-05 01:51:55,016 - INFO - [diffusion][Epoch 7308] diffusion learning rate: 0.001
2024-11-05 01:51:55,018 - INFO - [diffusion][Epoch 7308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:55,019 - INFO - [diffusion][Epoch 7309] Epoch 7310/12000
2024-11-05 01:51:59,121 - INFO - [diffusion][Epoch 7309] diffusion training Loss: 0.05473208613693714
2024-11-05 01:51:59,123 - INFO - [diffusion][Epoch 7309] diffusion learning rate: 0.001
2024-11-05 01:51:59,125 - INFO - [diffusion][Epoch 7309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:51:59,126 - INFO - [diffusion][Epoch 7310] Epoch 7311/12000
2024-11-05 01:52:03,252 - INFO - [diffusion][Epoch 7310] diffusion training Loss: 0.05851642042398453
2024-11-05 01:52:03,254 - INFO - [diffusion][Epoch 7310] diffusion learning rate: 0.001
2024-11-05 01:52:03,256 - INFO - [diffusion][Epoch 7310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:03,257 - INFO - [diffusion][Epoch 7311] Epoch 7312/12000
2024-11-05 01:52:07,396 - INFO - [diffusion][Epoch 7311] diffusion training Loss: 0.05246876273304224
2024-11-05 01:52:07,398 - INFO - [diffusion][Epoch 7311] diffusion learning rate: 0.001
2024-11-05 01:52:07,400 - INFO - [diffusion][Epoch 7311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:07,401 - INFO - [diffusion][Epoch 7312] Epoch 7313/12000
2024-11-05 01:52:11,489 - INFO - [diffusion][Epoch 7312] diffusion training Loss: 0.0544961653649807
2024-11-05 01:52:11,491 - INFO - [diffusion][Epoch 7312] diffusion learning rate: 0.001
2024-11-05 01:52:11,493 - INFO - [diffusion][Epoch 7312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:11,494 - INFO - [diffusion][Epoch 7313] Epoch 7314/12000
2024-11-05 01:52:15,473 - INFO - [diffusion][Epoch 7313] diffusion training Loss: 0.057483602315187454
2024-11-05 01:52:15,475 - INFO - [diffusion][Epoch 7313] diffusion learning rate: 0.001
2024-11-05 01:52:15,477 - INFO - [diffusion][Epoch 7313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:15,478 - INFO - [diffusion][Epoch 7314] Epoch 7315/12000
2024-11-05 01:52:19,645 - INFO - [diffusion][Epoch 7314] diffusion training Loss: 0.04567387793213129
2024-11-05 01:52:19,647 - INFO - [diffusion][Epoch 7314] diffusion learning rate: 0.001
2024-11-05 01:52:19,649 - INFO - [diffusion][Epoch 7314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:19,651 - INFO - [diffusion][Epoch 7315] Epoch 7316/12000
2024-11-05 01:52:23,812 - INFO - [diffusion][Epoch 7315] diffusion training Loss: 0.05845454894006252
2024-11-05 01:52:23,814 - INFO - [diffusion][Epoch 7315] diffusion learning rate: 0.001
2024-11-05 01:52:23,816 - INFO - [diffusion][Epoch 7315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:23,818 - INFO - [diffusion][Epoch 7316] Epoch 7317/12000
2024-11-05 01:52:27,910 - INFO - [diffusion][Epoch 7316] diffusion training Loss: 0.05720472428947687
2024-11-05 01:52:27,912 - INFO - [diffusion][Epoch 7316] diffusion learning rate: 0.001
2024-11-05 01:52:27,914 - INFO - [diffusion][Epoch 7316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:27,915 - INFO - [diffusion][Epoch 7317] Epoch 7318/12000
2024-11-05 01:52:32,022 - INFO - [diffusion][Epoch 7317] diffusion training Loss: 0.05409661494195461
2024-11-05 01:52:32,045 - INFO - [diffusion][Epoch 7317] diffusion learning rate: 0.001
2024-11-05 01:52:32,047 - INFO - [diffusion][Epoch 7317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:32,048 - INFO - [diffusion][Epoch 7318] Epoch 7319/12000
2024-11-05 01:52:36,198 - INFO - [diffusion][Epoch 7318] diffusion training Loss: 0.052075608633458614
2024-11-05 01:52:36,200 - INFO - [diffusion][Epoch 7318] diffusion learning rate: 0.001
2024-11-05 01:52:36,201 - INFO - [diffusion][Epoch 7318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:36,203 - INFO - [diffusion][Epoch 7319] Epoch 7320/12000
2024-11-05 01:52:40,323 - INFO - [diffusion][Epoch 7319] diffusion training Loss: 0.05924726277589798
2024-11-05 01:52:40,325 - INFO - [diffusion][Epoch 7319] diffusion learning rate: 0.001
2024-11-05 01:52:40,327 - INFO - [diffusion][Epoch 7319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:40,328 - INFO - [diffusion][Epoch 7320] Epoch 7321/12000
2024-11-05 01:52:44,403 - INFO - [diffusion][Epoch 7320] diffusion training Loss: 0.059688775800168514
2024-11-05 01:52:44,405 - INFO - [diffusion][Epoch 7320] diffusion learning rate: 0.001
2024-11-05 01:52:44,408 - INFO - [diffusion][Epoch 7320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:44,409 - INFO - [diffusion][Epoch 7321] Epoch 7322/12000
2024-11-05 01:52:48,593 - INFO - [diffusion][Epoch 7321] diffusion training Loss: 0.05913252662867308
2024-11-05 01:52:48,595 - INFO - [diffusion][Epoch 7321] diffusion learning rate: 0.001
2024-11-05 01:52:48,597 - INFO - [diffusion][Epoch 7321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:48,598 - INFO - [diffusion][Epoch 7322] Epoch 7323/12000
2024-11-05 01:52:52,813 - INFO - [diffusion][Epoch 7322] diffusion training Loss: 0.0548302736133337
2024-11-05 01:52:52,815 - INFO - [diffusion][Epoch 7322] diffusion learning rate: 0.001
2024-11-05 01:52:52,817 - INFO - [diffusion][Epoch 7322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:52,818 - INFO - [diffusion][Epoch 7323] Epoch 7324/12000
2024-11-05 01:52:56,955 - INFO - [diffusion][Epoch 7323] diffusion training Loss: 0.05834758561104536
2024-11-05 01:52:56,957 - INFO - [diffusion][Epoch 7323] diffusion learning rate: 0.001
2024-11-05 01:52:56,959 - INFO - [diffusion][Epoch 7323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:52:56,960 - INFO - [diffusion][Epoch 7324] Epoch 7325/12000
2024-11-05 01:53:01,062 - INFO - [diffusion][Epoch 7324] diffusion training Loss: 0.05820412188768387
2024-11-05 01:53:01,064 - INFO - [diffusion][Epoch 7324] diffusion learning rate: 0.001
2024-11-05 01:53:01,066 - INFO - [diffusion][Epoch 7324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:01,067 - INFO - [diffusion][Epoch 7325] Epoch 7326/12000
2024-11-05 01:53:05,846 - INFO - [diffusion][Epoch 7325] diffusion training Loss: 0.051989302970469
2024-11-05 01:53:05,850 - INFO - [diffusion][Epoch 7325] diffusion learning rate: 0.001
2024-11-05 01:53:05,852 - INFO - [diffusion][Epoch 7325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:05,853 - INFO - [diffusion][Epoch 7326] Epoch 7327/12000
2024-11-05 01:53:10,038 - INFO - [diffusion][Epoch 7326] diffusion training Loss: 0.05508496053516865
2024-11-05 01:53:10,040 - INFO - [diffusion][Epoch 7326] diffusion learning rate: 0.001
2024-11-05 01:53:10,042 - INFO - [diffusion][Epoch 7326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:10,043 - INFO - [diffusion][Epoch 7327] Epoch 7328/12000
2024-11-05 01:53:14,163 - INFO - [diffusion][Epoch 7327] diffusion training Loss: 0.06031095143407583
2024-11-05 01:53:14,165 - INFO - [diffusion][Epoch 7327] diffusion learning rate: 0.001
2024-11-05 01:53:14,241 - INFO - [diffusion][Epoch 7327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:14,243 - INFO - [diffusion][Epoch 7328] Epoch 7329/12000
2024-11-05 01:53:18,368 - INFO - [diffusion][Epoch 7328] diffusion training Loss: 0.05415742564946413
2024-11-05 01:53:18,372 - INFO - [diffusion][Epoch 7328] diffusion learning rate: 0.001
2024-11-05 01:53:18,374 - INFO - [diffusion][Epoch 7328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:18,376 - INFO - [diffusion][Epoch 7329] Epoch 7330/12000
2024-11-05 01:53:22,370 - INFO - [diffusion][Epoch 7329] diffusion training Loss: 0.05846522469073534
2024-11-05 01:53:22,372 - INFO - [diffusion][Epoch 7329] diffusion learning rate: 0.001
2024-11-05 01:53:22,374 - INFO - [diffusion][Epoch 7329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:22,375 - INFO - [diffusion][Epoch 7330] Epoch 7331/12000
2024-11-05 01:53:26,527 - INFO - [diffusion][Epoch 7330] diffusion training Loss: 0.055515061132609844
2024-11-05 01:53:26,529 - INFO - [diffusion][Epoch 7330] diffusion learning rate: 0.001
2024-11-05 01:53:26,530 - INFO - [diffusion][Epoch 7330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:26,532 - INFO - [diffusion][Epoch 7331] Epoch 7332/12000
2024-11-05 01:53:30,697 - INFO - [diffusion][Epoch 7331] diffusion training Loss: 0.05611206218600273
2024-11-05 01:53:30,699 - INFO - [diffusion][Epoch 7331] diffusion learning rate: 0.001
2024-11-05 01:53:30,745 - INFO - [diffusion][Epoch 7331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:30,746 - INFO - [diffusion][Epoch 7332] Epoch 7333/12000
2024-11-05 01:53:34,826 - INFO - [diffusion][Epoch 7332] diffusion training Loss: 0.054068997502326965
2024-11-05 01:53:34,828 - INFO - [diffusion][Epoch 7332] diffusion learning rate: 0.001
2024-11-05 01:53:34,830 - INFO - [diffusion][Epoch 7332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:34,833 - INFO - [diffusion][Epoch 7333] Epoch 7334/12000
2024-11-05 01:53:39,001 - INFO - [diffusion][Epoch 7333] diffusion training Loss: 0.059203699231147766
2024-11-05 01:53:39,002 - INFO - [diffusion][Epoch 7333] diffusion learning rate: 0.001
2024-11-05 01:53:39,004 - INFO - [diffusion][Epoch 7333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:39,006 - INFO - [diffusion][Epoch 7334] Epoch 7335/12000
2024-11-05 01:53:43,166 - INFO - [diffusion][Epoch 7334] diffusion training Loss: 0.057695843279361725
2024-11-05 01:53:43,168 - INFO - [diffusion][Epoch 7334] diffusion learning rate: 0.001
2024-11-05 01:53:43,170 - INFO - [diffusion][Epoch 7334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:43,171 - INFO - [diffusion][Epoch 7335] Epoch 7336/12000
2024-11-05 01:53:47,319 - INFO - [diffusion][Epoch 7335] diffusion training Loss: 0.056449469178915024
2024-11-05 01:53:47,321 - INFO - [diffusion][Epoch 7335] diffusion learning rate: 0.001
2024-11-05 01:53:47,323 - INFO - [diffusion][Epoch 7335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:47,324 - INFO - [diffusion][Epoch 7336] Epoch 7337/12000
2024-11-05 01:53:51,464 - INFO - [diffusion][Epoch 7336] diffusion training Loss: 0.05594716966152191
2024-11-05 01:53:51,467 - INFO - [diffusion][Epoch 7336] diffusion learning rate: 0.001
2024-11-05 01:53:51,468 - INFO - [diffusion][Epoch 7336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:51,469 - INFO - [diffusion][Epoch 7337] Epoch 7338/12000
2024-11-05 01:53:55,662 - INFO - [diffusion][Epoch 7337] diffusion training Loss: 0.05643575359135866
2024-11-05 01:53:55,664 - INFO - [diffusion][Epoch 7337] diffusion learning rate: 0.001
2024-11-05 01:53:55,665 - INFO - [diffusion][Epoch 7337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:55,667 - INFO - [diffusion][Epoch 7338] Epoch 7339/12000
2024-11-05 01:53:59,770 - INFO - [diffusion][Epoch 7338] diffusion training Loss: 0.056947494857013226
2024-11-05 01:53:59,800 - INFO - [diffusion][Epoch 7338] diffusion learning rate: 0.001
2024-11-05 01:53:59,801 - INFO - [diffusion][Epoch 7338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:53:59,803 - INFO - [diffusion][Epoch 7339] Epoch 7340/12000
2024-11-05 01:54:03,923 - INFO - [diffusion][Epoch 7339] diffusion training Loss: 0.055850871838629246
2024-11-05 01:54:03,925 - INFO - [diffusion][Epoch 7339] diffusion learning rate: 0.001
2024-11-05 01:54:03,927 - INFO - [diffusion][Epoch 7339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:03,928 - INFO - [diffusion][Epoch 7340] Epoch 7341/12000
2024-11-05 01:54:08,116 - INFO - [diffusion][Epoch 7340] diffusion training Loss: 0.05533919297158718
2024-11-05 01:54:08,118 - INFO - [diffusion][Epoch 7340] diffusion learning rate: 0.001
2024-11-05 01:54:08,120 - INFO - [diffusion][Epoch 7340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:08,121 - INFO - [diffusion][Epoch 7341] Epoch 7342/12000
2024-11-05 01:54:12,284 - INFO - [diffusion][Epoch 7341] diffusion training Loss: 0.06181173771619797
2024-11-05 01:54:12,401 - INFO - [diffusion][Epoch 7341] diffusion learning rate: 0.001
2024-11-05 01:54:12,403 - INFO - [diffusion][Epoch 7341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:12,404 - INFO - [diffusion][Epoch 7342] Epoch 7343/12000
2024-11-05 01:54:16,468 - INFO - [diffusion][Epoch 7342] diffusion training Loss: 0.05473334901034832
2024-11-05 01:54:16,470 - INFO - [diffusion][Epoch 7342] diffusion learning rate: 0.001
2024-11-05 01:54:16,472 - INFO - [diffusion][Epoch 7342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:16,473 - INFO - [diffusion][Epoch 7343] Epoch 7344/12000
2024-11-05 01:54:20,619 - INFO - [diffusion][Epoch 7343] diffusion training Loss: 0.05883930716663599
2024-11-05 01:54:20,621 - INFO - [diffusion][Epoch 7343] diffusion learning rate: 0.001
2024-11-05 01:54:20,701 - INFO - [diffusion][Epoch 7343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:20,702 - INFO - [diffusion][Epoch 7344] Epoch 7345/12000
2024-11-05 01:54:24,910 - INFO - [diffusion][Epoch 7344] diffusion training Loss: 0.062182108871638775
2024-11-05 01:54:24,912 - INFO - [diffusion][Epoch 7344] diffusion learning rate: 0.001
2024-11-05 01:54:24,914 - INFO - [diffusion][Epoch 7344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:24,915 - INFO - [diffusion][Epoch 7345] Epoch 7346/12000
2024-11-05 01:54:29,114 - INFO - [diffusion][Epoch 7345] diffusion training Loss: 0.055771855637431145
2024-11-05 01:54:29,116 - INFO - [diffusion][Epoch 7345] diffusion learning rate: 0.001
2024-11-05 01:54:29,118 - INFO - [diffusion][Epoch 7345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:29,120 - INFO - [diffusion][Epoch 7346] Epoch 7347/12000
2024-11-05 01:54:33,292 - INFO - [diffusion][Epoch 7346] diffusion training Loss: 0.059782263822853565
2024-11-05 01:54:33,294 - INFO - [diffusion][Epoch 7346] diffusion learning rate: 0.001
2024-11-05 01:54:33,296 - INFO - [diffusion][Epoch 7346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:33,297 - INFO - [diffusion][Epoch 7347] Epoch 7348/12000
2024-11-05 01:54:37,328 - INFO - [diffusion][Epoch 7347] diffusion training Loss: 0.05634778365492821
2024-11-05 01:54:37,331 - INFO - [diffusion][Epoch 7347] diffusion learning rate: 0.001
2024-11-05 01:54:37,333 - INFO - [diffusion][Epoch 7347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:37,335 - INFO - [diffusion][Epoch 7348] Epoch 7349/12000
2024-11-05 01:54:41,468 - INFO - [diffusion][Epoch 7348] diffusion training Loss: 0.05984642822295427
2024-11-05 01:54:41,473 - INFO - [diffusion][Epoch 7348] diffusion learning rate: 0.001
2024-11-05 01:54:41,475 - INFO - [diffusion][Epoch 7348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:41,476 - INFO - [diffusion][Epoch 7349] Epoch 7350/12000
2024-11-05 01:54:45,562 - INFO - [diffusion][Epoch 7349] diffusion training Loss: 0.05902636516839266
2024-11-05 01:54:45,564 - INFO - [diffusion][Epoch 7349] diffusion learning rate: 0.001
2024-11-05 01:54:45,566 - INFO - [diffusion][Epoch 7349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:45,567 - INFO - [diffusion][Epoch 7350] Epoch 7351/12000
2024-11-05 01:54:49,757 - INFO - [diffusion][Epoch 7350] diffusion training Loss: 0.056256094947457314
2024-11-05 01:54:49,759 - INFO - [diffusion][Epoch 7350] diffusion learning rate: 0.001
2024-11-05 01:54:49,760 - INFO - [diffusion][Epoch 7350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:49,762 - INFO - [diffusion][Epoch 7351] Epoch 7352/12000
2024-11-05 01:54:53,854 - INFO - [diffusion][Epoch 7351] diffusion training Loss: 0.05205131694674492
2024-11-05 01:54:53,857 - INFO - [diffusion][Epoch 7351] diffusion learning rate: 0.001
2024-11-05 01:54:53,860 - INFO - [diffusion][Epoch 7351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:53,862 - INFO - [diffusion][Epoch 7352] Epoch 7353/12000
2024-11-05 01:54:57,840 - INFO - [diffusion][Epoch 7352] diffusion training Loss: 0.05649022199213505
2024-11-05 01:54:57,842 - INFO - [diffusion][Epoch 7352] diffusion learning rate: 0.001
2024-11-05 01:54:57,844 - INFO - [diffusion][Epoch 7352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:54:57,845 - INFO - [diffusion][Epoch 7353] Epoch 7354/12000
2024-11-05 01:55:01,826 - INFO - [diffusion][Epoch 7353] diffusion training Loss: 0.04971762374043465
2024-11-05 01:55:01,828 - INFO - [diffusion][Epoch 7353] diffusion learning rate: 0.001
2024-11-05 01:55:01,830 - INFO - [diffusion][Epoch 7353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:01,831 - INFO - [diffusion][Epoch 7354] Epoch 7355/12000
2024-11-05 01:55:05,953 - INFO - [diffusion][Epoch 7354] diffusion training Loss: 0.05582645256072283
2024-11-05 01:55:05,956 - INFO - [diffusion][Epoch 7354] diffusion learning rate: 0.001
2024-11-05 01:55:05,958 - INFO - [diffusion][Epoch 7354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:05,959 - INFO - [diffusion][Epoch 7355] Epoch 7356/12000
2024-11-05 01:55:10,079 - INFO - [diffusion][Epoch 7355] diffusion training Loss: 0.05405258946120739
2024-11-05 01:55:10,081 - INFO - [diffusion][Epoch 7355] diffusion learning rate: 0.001
2024-11-05 01:55:10,083 - INFO - [diffusion][Epoch 7355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:10,085 - INFO - [diffusion][Epoch 7356] Epoch 7357/12000
2024-11-05 01:55:14,255 - INFO - [diffusion][Epoch 7356] diffusion training Loss: 0.052690367214381695
2024-11-05 01:55:14,257 - INFO - [diffusion][Epoch 7356] diffusion learning rate: 0.001
2024-11-05 01:55:14,259 - INFO - [diffusion][Epoch 7356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:14,260 - INFO - [diffusion][Epoch 7357] Epoch 7358/12000
2024-11-05 01:55:18,444 - INFO - [diffusion][Epoch 7357] diffusion training Loss: 0.057871319353580475
2024-11-05 01:55:18,446 - INFO - [diffusion][Epoch 7357] diffusion learning rate: 0.001
2024-11-05 01:55:18,448 - INFO - [diffusion][Epoch 7357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:18,450 - INFO - [diffusion][Epoch 7358] Epoch 7359/12000
2024-11-05 01:55:22,590 - INFO - [diffusion][Epoch 7358] diffusion training Loss: 0.058593966998159885
2024-11-05 01:55:22,592 - INFO - [diffusion][Epoch 7358] diffusion learning rate: 0.001
2024-11-05 01:55:22,594 - INFO - [diffusion][Epoch 7358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:22,595 - INFO - [diffusion][Epoch 7359] Epoch 7360/12000
2024-11-05 01:55:26,652 - INFO - [diffusion][Epoch 7359] diffusion training Loss: 0.05877274740487337
2024-11-05 01:55:26,654 - INFO - [diffusion][Epoch 7359] diffusion learning rate: 0.001
2024-11-05 01:55:26,662 - INFO - [diffusion][Epoch 7359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:26,663 - INFO - [diffusion][Epoch 7360] Epoch 7361/12000
2024-11-05 01:55:30,841 - INFO - [diffusion][Epoch 7360] diffusion training Loss: 0.05715739447623491
2024-11-05 01:55:30,844 - INFO - [diffusion][Epoch 7360] diffusion learning rate: 0.001
2024-11-05 01:55:30,846 - INFO - [diffusion][Epoch 7360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:30,847 - INFO - [diffusion][Epoch 7361] Epoch 7362/12000
2024-11-05 01:55:34,989 - INFO - [diffusion][Epoch 7361] diffusion training Loss: 0.05876560136675835
2024-11-05 01:55:34,991 - INFO - [diffusion][Epoch 7361] diffusion learning rate: 0.001
2024-11-05 01:55:34,993 - INFO - [diffusion][Epoch 7361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:34,994 - INFO - [diffusion][Epoch 7362] Epoch 7363/12000
2024-11-05 01:55:39,078 - INFO - [diffusion][Epoch 7362] diffusion training Loss: 0.051028598099946976
2024-11-05 01:55:39,080 - INFO - [diffusion][Epoch 7362] diffusion learning rate: 0.001
2024-11-05 01:55:39,082 - INFO - [diffusion][Epoch 7362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:39,083 - INFO - [diffusion][Epoch 7363] Epoch 7364/12000
2024-11-05 01:55:43,191 - INFO - [diffusion][Epoch 7363] diffusion training Loss: 0.053025360219180584
2024-11-05 01:55:43,193 - INFO - [diffusion][Epoch 7363] diffusion learning rate: 0.001
2024-11-05 01:55:43,195 - INFO - [diffusion][Epoch 7363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:43,197 - INFO - [diffusion][Epoch 7364] Epoch 7365/12000
2024-11-05 01:55:47,354 - INFO - [diffusion][Epoch 7364] diffusion training Loss: 0.05739159323275089
2024-11-05 01:55:47,356 - INFO - [diffusion][Epoch 7364] diffusion learning rate: 0.001
2024-11-05 01:55:47,358 - INFO - [diffusion][Epoch 7364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:47,359 - INFO - [diffusion][Epoch 7365] Epoch 7366/12000
2024-11-05 01:55:51,688 - INFO - [diffusion][Epoch 7365] diffusion training Loss: 0.05769713595509529
2024-11-05 01:55:51,690 - INFO - [diffusion][Epoch 7365] diffusion learning rate: 0.001
2024-11-05 01:55:51,691 - INFO - [diffusion][Epoch 7365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:51,693 - INFO - [diffusion][Epoch 7366] Epoch 7367/12000
2024-11-05 01:55:55,811 - INFO - [diffusion][Epoch 7366] diffusion training Loss: 0.05950605869293213
2024-11-05 01:55:55,813 - INFO - [diffusion][Epoch 7366] diffusion learning rate: 0.001
2024-11-05 01:55:55,815 - INFO - [diffusion][Epoch 7366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:55,816 - INFO - [diffusion][Epoch 7367] Epoch 7368/12000
2024-11-05 01:55:59,939 - INFO - [diffusion][Epoch 7367] diffusion training Loss: 0.0500314449891448
2024-11-05 01:55:59,941 - INFO - [diffusion][Epoch 7367] diffusion learning rate: 0.001
2024-11-05 01:55:59,943 - INFO - [diffusion][Epoch 7367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:55:59,944 - INFO - [diffusion][Epoch 7368] Epoch 7369/12000
2024-11-05 01:56:04,032 - INFO - [diffusion][Epoch 7368] diffusion training Loss: 0.05820920038968325
2024-11-05 01:56:04,035 - INFO - [diffusion][Epoch 7368] diffusion learning rate: 0.001
2024-11-05 01:56:04,038 - INFO - [diffusion][Epoch 7368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:04,039 - INFO - [diffusion][Epoch 7369] Epoch 7370/12000
2024-11-05 01:56:08,205 - INFO - [diffusion][Epoch 7369] diffusion training Loss: 0.05517710279673338
2024-11-05 01:56:08,207 - INFO - [diffusion][Epoch 7369] diffusion learning rate: 0.001
2024-11-05 01:56:08,209 - INFO - [diffusion][Epoch 7369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:08,210 - INFO - [diffusion][Epoch 7370] Epoch 7371/12000
2024-11-05 01:56:12,371 - INFO - [diffusion][Epoch 7370] diffusion training Loss: 0.05898925196379423
2024-11-05 01:56:12,373 - INFO - [diffusion][Epoch 7370] diffusion learning rate: 0.001
2024-11-05 01:56:12,375 - INFO - [diffusion][Epoch 7370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:12,376 - INFO - [diffusion][Epoch 7371] Epoch 7372/12000
2024-11-05 01:56:16,500 - INFO - [diffusion][Epoch 7371] diffusion training Loss: 0.05690091289579868
2024-11-05 01:56:16,502 - INFO - [diffusion][Epoch 7371] diffusion learning rate: 0.001
2024-11-05 01:56:16,538 - INFO - [diffusion][Epoch 7371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:16,539 - INFO - [diffusion][Epoch 7372] Epoch 7373/12000
2024-11-05 01:56:20,612 - INFO - [diffusion][Epoch 7372] diffusion training Loss: 0.06304713524878025
2024-11-05 01:56:20,614 - INFO - [diffusion][Epoch 7372] diffusion learning rate: 0.001
2024-11-05 01:56:20,616 - INFO - [diffusion][Epoch 7372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:20,617 - INFO - [diffusion][Epoch 7373] Epoch 7374/12000
2024-11-05 01:56:24,728 - INFO - [diffusion][Epoch 7373] diffusion training Loss: 0.058890195563435555
2024-11-05 01:56:24,730 - INFO - [diffusion][Epoch 7373] diffusion learning rate: 0.001
2024-11-05 01:56:24,732 - INFO - [diffusion][Epoch 7373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:24,733 - INFO - [diffusion][Epoch 7374] Epoch 7375/12000
2024-11-05 01:56:28,875 - INFO - [diffusion][Epoch 7374] diffusion training Loss: 0.05451659858226776
2024-11-05 01:56:28,877 - INFO - [diffusion][Epoch 7374] diffusion learning rate: 0.001
2024-11-05 01:56:28,879 - INFO - [diffusion][Epoch 7374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:28,880 - INFO - [diffusion][Epoch 7375] Epoch 7376/12000
2024-11-05 01:56:33,036 - INFO - [diffusion][Epoch 7375] diffusion training Loss: 0.0602361811324954
2024-11-05 01:56:33,038 - INFO - [diffusion][Epoch 7375] diffusion learning rate: 0.001
2024-11-05 01:56:33,040 - INFO - [diffusion][Epoch 7375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:33,042 - INFO - [diffusion][Epoch 7376] Epoch 7377/12000
2024-11-05 01:56:37,151 - INFO - [diffusion][Epoch 7376] diffusion training Loss: 0.058995780535042286
2024-11-05 01:56:37,153 - INFO - [diffusion][Epoch 7376] diffusion learning rate: 0.001
2024-11-05 01:56:37,212 - INFO - [diffusion][Epoch 7376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:37,213 - INFO - [diffusion][Epoch 7377] Epoch 7378/12000
2024-11-05 01:56:41,317 - INFO - [diffusion][Epoch 7377] diffusion training Loss: 0.05485583655536175
2024-11-05 01:56:41,319 - INFO - [diffusion][Epoch 7377] diffusion learning rate: 0.001
2024-11-05 01:56:41,321 - INFO - [diffusion][Epoch 7377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:41,323 - INFO - [diffusion][Epoch 7378] Epoch 7379/12000
2024-11-05 01:56:45,472 - INFO - [diffusion][Epoch 7378] diffusion training Loss: 0.04965732432901859
2024-11-05 01:56:45,474 - INFO - [diffusion][Epoch 7378] diffusion learning rate: 0.001
2024-11-05 01:56:45,476 - INFO - [diffusion][Epoch 7378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:45,477 - INFO - [diffusion][Epoch 7379] Epoch 7380/12000
2024-11-05 01:56:49,635 - INFO - [diffusion][Epoch 7379] diffusion training Loss: 0.05853988043963909
2024-11-05 01:56:49,637 - INFO - [diffusion][Epoch 7379] diffusion learning rate: 0.001
2024-11-05 01:56:49,639 - INFO - [diffusion][Epoch 7379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:49,640 - INFO - [diffusion][Epoch 7380] Epoch 7381/12000
2024-11-05 01:56:53,759 - INFO - [diffusion][Epoch 7380] diffusion training Loss: 0.05704790726304054
2024-11-05 01:56:53,761 - INFO - [diffusion][Epoch 7380] diffusion learning rate: 0.001
2024-11-05 01:56:53,796 - INFO - [diffusion][Epoch 7380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:53,797 - INFO - [diffusion][Epoch 7381] Epoch 7382/12000
2024-11-05 01:56:57,910 - INFO - [diffusion][Epoch 7381] diffusion training Loss: 0.05317949503660202
2024-11-05 01:56:57,912 - INFO - [diffusion][Epoch 7381] diffusion learning rate: 0.001
2024-11-05 01:56:57,914 - INFO - [diffusion][Epoch 7381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:56:57,915 - INFO - [diffusion][Epoch 7382] Epoch 7383/12000
2024-11-05 01:57:02,028 - INFO - [diffusion][Epoch 7382] diffusion training Loss: 0.05396384745836258
2024-11-05 01:57:02,030 - INFO - [diffusion][Epoch 7382] diffusion learning rate: 0.001
2024-11-05 01:57:02,032 - INFO - [diffusion][Epoch 7382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:02,033 - INFO - [diffusion][Epoch 7383] Epoch 7384/12000
2024-11-05 01:57:06,077 - INFO - [diffusion][Epoch 7383] diffusion training Loss: 0.05901193059980869
2024-11-05 01:57:06,079 - INFO - [diffusion][Epoch 7383] diffusion learning rate: 0.001
2024-11-05 01:57:06,080 - INFO - [diffusion][Epoch 7383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:06,082 - INFO - [diffusion][Epoch 7384] Epoch 7385/12000
2024-11-05 01:57:10,190 - INFO - [diffusion][Epoch 7384] diffusion training Loss: 0.05591991450637579
2024-11-05 01:57:10,192 - INFO - [diffusion][Epoch 7384] diffusion learning rate: 0.001
2024-11-05 01:57:10,193 - INFO - [diffusion][Epoch 7384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:10,195 - INFO - [diffusion][Epoch 7385] Epoch 7386/12000
2024-11-05 01:57:14,170 - INFO - [diffusion][Epoch 7385] diffusion training Loss: 0.05981803499162197
2024-11-05 01:57:14,172 - INFO - [diffusion][Epoch 7385] diffusion learning rate: 0.001
2024-11-05 01:57:14,173 - INFO - [diffusion][Epoch 7385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:14,175 - INFO - [diffusion][Epoch 7386] Epoch 7387/12000
2024-11-05 01:57:18,794 - INFO - [diffusion][Epoch 7386] diffusion training Loss: 0.05183870159089565
2024-11-05 01:57:18,795 - INFO - [diffusion][Epoch 7386] diffusion learning rate: 0.001
2024-11-05 01:57:18,797 - INFO - [diffusion][Epoch 7386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:18,798 - INFO - [diffusion][Epoch 7387] Epoch 7388/12000
2024-11-05 01:57:22,804 - INFO - [diffusion][Epoch 7387] diffusion training Loss: 0.056250994093716145
2024-11-05 01:57:22,805 - INFO - [diffusion][Epoch 7387] diffusion learning rate: 0.001
2024-11-05 01:57:22,807 - INFO - [diffusion][Epoch 7387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:22,808 - INFO - [diffusion][Epoch 7388] Epoch 7389/12000
2024-11-05 01:57:26,927 - INFO - [diffusion][Epoch 7388] diffusion training Loss: 0.05404758732765913
2024-11-05 01:57:26,928 - INFO - [diffusion][Epoch 7388] diffusion learning rate: 0.001
2024-11-05 01:57:26,931 - INFO - [diffusion][Epoch 7388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:26,932 - INFO - [diffusion][Epoch 7389] Epoch 7390/12000
2024-11-05 01:57:31,066 - INFO - [diffusion][Epoch 7389] diffusion training Loss: 0.0586983896791935
2024-11-05 01:57:31,069 - INFO - [diffusion][Epoch 7389] diffusion learning rate: 0.001
2024-11-05 01:57:31,070 - INFO - [diffusion][Epoch 7389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:31,071 - INFO - [diffusion][Epoch 7390] Epoch 7391/12000
2024-11-05 01:57:35,159 - INFO - [diffusion][Epoch 7390] diffusion training Loss: 0.05646236054599285
2024-11-05 01:57:35,161 - INFO - [diffusion][Epoch 7390] diffusion learning rate: 0.001
2024-11-05 01:57:35,162 - INFO - [diffusion][Epoch 7390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:35,164 - INFO - [diffusion][Epoch 7391] Epoch 7392/12000
2024-11-05 01:57:39,283 - INFO - [diffusion][Epoch 7391] diffusion training Loss: 0.06081884540617466
2024-11-05 01:57:39,285 - INFO - [diffusion][Epoch 7391] diffusion learning rate: 0.001
2024-11-05 01:57:39,287 - INFO - [diffusion][Epoch 7391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:39,288 - INFO - [diffusion][Epoch 7392] Epoch 7393/12000
2024-11-05 01:57:43,413 - INFO - [diffusion][Epoch 7392] diffusion training Loss: 0.05196286924183369
2024-11-05 01:57:43,416 - INFO - [diffusion][Epoch 7392] diffusion learning rate: 0.001
2024-11-05 01:57:43,417 - INFO - [diffusion][Epoch 7392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:43,419 - INFO - [diffusion][Epoch 7393] Epoch 7394/12000
2024-11-05 01:57:47,526 - INFO - [diffusion][Epoch 7393] diffusion training Loss: 0.05600486509501934
2024-11-05 01:57:47,528 - INFO - [diffusion][Epoch 7393] diffusion learning rate: 0.001
2024-11-05 01:57:47,530 - INFO - [diffusion][Epoch 7393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:47,531 - INFO - [diffusion][Epoch 7394] Epoch 7395/12000
2024-11-05 01:57:51,605 - INFO - [diffusion][Epoch 7394] diffusion training Loss: 0.05490199010819197
2024-11-05 01:57:51,607 - INFO - [diffusion][Epoch 7394] diffusion learning rate: 0.001
2024-11-05 01:57:51,609 - INFO - [diffusion][Epoch 7394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:51,610 - INFO - [diffusion][Epoch 7395] Epoch 7396/12000
2024-11-05 01:57:55,743 - INFO - [diffusion][Epoch 7395] diffusion training Loss: 0.05746632441878319
2024-11-05 01:57:55,745 - INFO - [diffusion][Epoch 7395] diffusion learning rate: 0.001
2024-11-05 01:57:55,747 - INFO - [diffusion][Epoch 7395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:55,748 - INFO - [diffusion][Epoch 7396] Epoch 7397/12000
2024-11-05 01:57:59,864 - INFO - [diffusion][Epoch 7396] diffusion training Loss: 0.0556069016456604
2024-11-05 01:57:59,867 - INFO - [diffusion][Epoch 7396] diffusion learning rate: 0.001
2024-11-05 01:57:59,905 - INFO - [diffusion][Epoch 7396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:57:59,906 - INFO - [diffusion][Epoch 7397] Epoch 7398/12000
2024-11-05 01:58:04,025 - INFO - [diffusion][Epoch 7397] diffusion training Loss: 0.04992342367768288
2024-11-05 01:58:04,027 - INFO - [diffusion][Epoch 7397] diffusion learning rate: 0.001
2024-11-05 01:58:04,029 - INFO - [diffusion][Epoch 7397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:04,030 - INFO - [diffusion][Epoch 7398] Epoch 7399/12000
2024-11-05 01:58:08,152 - INFO - [diffusion][Epoch 7398] diffusion training Loss: 0.05556466616690159
2024-11-05 01:58:08,154 - INFO - [diffusion][Epoch 7398] diffusion learning rate: 0.001
2024-11-05 01:58:08,156 - INFO - [diffusion][Epoch 7398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:08,157 - INFO - [diffusion][Epoch 7399] Epoch 7400/12000
2024-11-05 01:58:12,279 - INFO - [diffusion][Epoch 7399] diffusion training Loss: 0.05173556134104729
2024-11-05 01:58:12,281 - INFO - [diffusion][Epoch 7399] diffusion learning rate: 0.001
2024-11-05 01:58:12,282 - INFO - [diffusion][Epoch 7399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:12,284 - INFO - [diffusion][Epoch 7400] Epoch 7401/12000
2024-11-05 01:58:16,409 - INFO - [diffusion][Epoch 7400] diffusion training Loss: 0.05602056812494993
2024-11-05 01:58:16,411 - INFO - [diffusion][Epoch 7400] diffusion learning rate: 0.001
2024-11-05 01:58:16,413 - INFO - [diffusion][Epoch 7400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:16,414 - INFO - [diffusion][Epoch 7401] Epoch 7402/12000
2024-11-05 01:58:20,544 - INFO - [diffusion][Epoch 7401] diffusion training Loss: 0.056740669533610344
2024-11-05 01:58:20,546 - INFO - [diffusion][Epoch 7401] diffusion learning rate: 0.001
2024-11-05 01:58:20,547 - INFO - [diffusion][Epoch 7401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:20,548 - INFO - [diffusion][Epoch 7402] Epoch 7403/12000
2024-11-05 01:58:24,676 - INFO - [diffusion][Epoch 7402] diffusion training Loss: 0.05181080009788275
2024-11-05 01:58:24,678 - INFO - [diffusion][Epoch 7402] diffusion learning rate: 0.001
2024-11-05 01:58:24,680 - INFO - [diffusion][Epoch 7402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:24,681 - INFO - [diffusion][Epoch 7403] Epoch 7404/12000
2024-11-05 01:58:28,750 - INFO - [diffusion][Epoch 7403] diffusion training Loss: 0.06050044950097799
2024-11-05 01:58:28,752 - INFO - [diffusion][Epoch 7403] diffusion learning rate: 0.001
2024-11-05 01:58:28,755 - INFO - [diffusion][Epoch 7403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:28,757 - INFO - [diffusion][Epoch 7404] Epoch 7405/12000
2024-11-05 01:58:32,878 - INFO - [diffusion][Epoch 7404] diffusion training Loss: 0.0562870679423213
2024-11-05 01:58:32,880 - INFO - [diffusion][Epoch 7404] diffusion learning rate: 0.001
2024-11-05 01:58:32,882 - INFO - [diffusion][Epoch 7404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:32,883 - INFO - [diffusion][Epoch 7405] Epoch 7406/12000
2024-11-05 01:58:36,818 - INFO - [diffusion][Epoch 7405] diffusion training Loss: 0.05838009901344776
2024-11-05 01:58:36,820 - INFO - [diffusion][Epoch 7405] diffusion learning rate: 0.001
2024-11-05 01:58:36,821 - INFO - [diffusion][Epoch 7405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:36,822 - INFO - [diffusion][Epoch 7406] Epoch 7407/12000
2024-11-05 01:58:40,774 - INFO - [diffusion][Epoch 7406] diffusion training Loss: 0.047647545114159584
2024-11-05 01:58:40,776 - INFO - [diffusion][Epoch 7406] diffusion learning rate: 0.001
2024-11-05 01:58:40,778 - INFO - [diffusion][Epoch 7406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:40,779 - INFO - [diffusion][Epoch 7407] Epoch 7408/12000
2024-11-05 01:58:44,858 - INFO - [diffusion][Epoch 7407] diffusion training Loss: 0.05478507932275534
2024-11-05 01:58:44,860 - INFO - [diffusion][Epoch 7407] diffusion learning rate: 0.001
2024-11-05 01:58:44,862 - INFO - [diffusion][Epoch 7407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:44,863 - INFO - [diffusion][Epoch 7408] Epoch 7409/12000
2024-11-05 01:58:49,447 - INFO - [diffusion][Epoch 7408] diffusion training Loss: 0.05532524921000004
2024-11-05 01:58:49,449 - INFO - [diffusion][Epoch 7408] diffusion learning rate: 0.001
2024-11-05 01:58:49,451 - INFO - [diffusion][Epoch 7408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:49,452 - INFO - [diffusion][Epoch 7409] Epoch 7410/12000
2024-11-05 01:58:53,601 - INFO - [diffusion][Epoch 7409] diffusion training Loss: 0.055431569926440716
2024-11-05 01:58:53,603 - INFO - [diffusion][Epoch 7409] diffusion learning rate: 0.001
2024-11-05 01:58:53,605 - INFO - [diffusion][Epoch 7409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:53,606 - INFO - [diffusion][Epoch 7410] Epoch 7411/12000
2024-11-05 01:58:57,677 - INFO - [diffusion][Epoch 7410] diffusion training Loss: 0.05453168507665396
2024-11-05 01:58:57,679 - INFO - [diffusion][Epoch 7410] diffusion learning rate: 0.001
2024-11-05 01:58:57,681 - INFO - [diffusion][Epoch 7410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:58:57,682 - INFO - [diffusion][Epoch 7411] Epoch 7412/12000
2024-11-05 01:59:01,674 - INFO - [diffusion][Epoch 7411] diffusion training Loss: 0.0570211810991168
2024-11-05 01:59:01,677 - INFO - [diffusion][Epoch 7411] diffusion learning rate: 0.001
2024-11-05 01:59:01,679 - INFO - [diffusion][Epoch 7411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:01,680 - INFO - [diffusion][Epoch 7412] Epoch 7413/12000
2024-11-05 01:59:05,712 - INFO - [diffusion][Epoch 7412] diffusion training Loss: 0.05303171928972006
2024-11-05 01:59:05,714 - INFO - [diffusion][Epoch 7412] diffusion learning rate: 0.001
2024-11-05 01:59:05,716 - INFO - [diffusion][Epoch 7412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:05,717 - INFO - [diffusion][Epoch 7413] Epoch 7414/12000
2024-11-05 01:59:09,688 - INFO - [diffusion][Epoch 7413] diffusion training Loss: 0.053925396874547005
2024-11-05 01:59:09,690 - INFO - [diffusion][Epoch 7413] diffusion learning rate: 0.001
2024-11-05 01:59:09,691 - INFO - [diffusion][Epoch 7413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:09,693 - INFO - [diffusion][Epoch 7414] Epoch 7415/12000
2024-11-05 01:59:13,680 - INFO - [diffusion][Epoch 7414] diffusion training Loss: 0.06064942944794893
2024-11-05 01:59:13,682 - INFO - [diffusion][Epoch 7414] diffusion learning rate: 0.001
2024-11-05 01:59:13,683 - INFO - [diffusion][Epoch 7414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:13,685 - INFO - [diffusion][Epoch 7415] Epoch 7416/12000
2024-11-05 01:59:17,715 - INFO - [diffusion][Epoch 7415] diffusion training Loss: 0.05518859997391701
2024-11-05 01:59:17,717 - INFO - [diffusion][Epoch 7415] diffusion learning rate: 0.001
2024-11-05 01:59:17,719 - INFO - [diffusion][Epoch 7415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:17,720 - INFO - [diffusion][Epoch 7416] Epoch 7417/12000
2024-11-05 01:59:21,835 - INFO - [diffusion][Epoch 7416] diffusion training Loss: 0.057527597062289715
2024-11-05 01:59:21,837 - INFO - [diffusion][Epoch 7416] diffusion learning rate: 0.001
2024-11-05 01:59:21,839 - INFO - [diffusion][Epoch 7416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:21,841 - INFO - [diffusion][Epoch 7417] Epoch 7418/12000
2024-11-05 01:59:25,826 - INFO - [diffusion][Epoch 7417] diffusion training Loss: 0.05416610557585955
2024-11-05 01:59:25,828 - INFO - [diffusion][Epoch 7417] diffusion learning rate: 0.001
2024-11-05 01:59:25,830 - INFO - [diffusion][Epoch 7417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:25,832 - INFO - [diffusion][Epoch 7418] Epoch 7419/12000
2024-11-05 01:59:29,804 - INFO - [diffusion][Epoch 7418] diffusion training Loss: 0.05686871241778135
2024-11-05 01:59:29,806 - INFO - [diffusion][Epoch 7418] diffusion learning rate: 0.001
2024-11-05 01:59:29,808 - INFO - [diffusion][Epoch 7418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:29,809 - INFO - [diffusion][Epoch 7419] Epoch 7420/12000
2024-11-05 01:59:33,904 - INFO - [diffusion][Epoch 7419] diffusion training Loss: 0.056317998096346855
2024-11-05 01:59:33,906 - INFO - [diffusion][Epoch 7419] diffusion learning rate: 0.001
2024-11-05 01:59:33,908 - INFO - [diffusion][Epoch 7419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:33,909 - INFO - [diffusion][Epoch 7420] Epoch 7421/12000
2024-11-05 01:59:38,010 - INFO - [diffusion][Epoch 7420] diffusion training Loss: 0.05504818353801966
2024-11-05 01:59:38,012 - INFO - [diffusion][Epoch 7420] diffusion learning rate: 0.001
2024-11-05 01:59:38,014 - INFO - [diffusion][Epoch 7420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:38,016 - INFO - [diffusion][Epoch 7421] Epoch 7422/12000
2024-11-05 01:59:42,168 - INFO - [diffusion][Epoch 7421] diffusion training Loss: 0.055458469316363335
2024-11-05 01:59:42,170 - INFO - [diffusion][Epoch 7421] diffusion learning rate: 0.001
2024-11-05 01:59:42,172 - INFO - [diffusion][Epoch 7421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:42,173 - INFO - [diffusion][Epoch 7422] Epoch 7423/12000
2024-11-05 01:59:46,129 - INFO - [diffusion][Epoch 7422] diffusion training Loss: 0.05549388751387596
2024-11-05 01:59:46,135 - INFO - [diffusion][Epoch 7422] diffusion learning rate: 0.001
2024-11-05 01:59:46,137 - INFO - [diffusion][Epoch 7422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:46,139 - INFO - [diffusion][Epoch 7423] Epoch 7424/12000
2024-11-05 01:59:50,180 - INFO - [diffusion][Epoch 7423] diffusion training Loss: 0.05748515110462904
2024-11-05 01:59:50,182 - INFO - [diffusion][Epoch 7423] diffusion learning rate: 0.001
2024-11-05 01:59:50,184 - INFO - [diffusion][Epoch 7423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:50,185 - INFO - [diffusion][Epoch 7424] Epoch 7425/12000
2024-11-05 01:59:54,018 - INFO - [diffusion][Epoch 7424] diffusion training Loss: 0.049087016843259335
2024-11-05 01:59:54,020 - INFO - [diffusion][Epoch 7424] diffusion learning rate: 0.001
2024-11-05 01:59:54,022 - INFO - [diffusion][Epoch 7424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:54,023 - INFO - [diffusion][Epoch 7425] Epoch 7426/12000
2024-11-05 01:59:57,994 - INFO - [diffusion][Epoch 7425] diffusion training Loss: 0.056187271140515804
2024-11-05 01:59:57,996 - INFO - [diffusion][Epoch 7425] diffusion learning rate: 0.001
2024-11-05 01:59:57,998 - INFO - [diffusion][Epoch 7425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 01:59:58,000 - INFO - [diffusion][Epoch 7426] Epoch 7427/12000
2024-11-05 02:00:02,035 - INFO - [diffusion][Epoch 7426] diffusion training Loss: 0.05764160118997097
2024-11-05 02:00:02,039 - INFO - [diffusion][Epoch 7426] diffusion learning rate: 0.001
2024-11-05 02:00:02,041 - INFO - [diffusion][Epoch 7426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:02,042 - INFO - [diffusion][Epoch 7427] Epoch 7428/12000
2024-11-05 02:00:06,029 - INFO - [diffusion][Epoch 7427] diffusion training Loss: 0.05982917360961437
2024-11-05 02:00:06,031 - INFO - [diffusion][Epoch 7427] diffusion learning rate: 0.001
2024-11-05 02:00:06,033 - INFO - [diffusion][Epoch 7427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:06,034 - INFO - [diffusion][Epoch 7428] Epoch 7429/12000
2024-11-05 02:00:10,697 - INFO - [diffusion][Epoch 7428] diffusion training Loss: 0.05554976500570774
2024-11-05 02:00:10,700 - INFO - [diffusion][Epoch 7428] diffusion learning rate: 0.001
2024-11-05 02:00:10,702 - INFO - [diffusion][Epoch 7428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:10,703 - INFO - [diffusion][Epoch 7429] Epoch 7430/12000
2024-11-05 02:00:14,670 - INFO - [diffusion][Epoch 7429] diffusion training Loss: 0.05791633762419224
2024-11-05 02:00:14,672 - INFO - [diffusion][Epoch 7429] diffusion learning rate: 0.001
2024-11-05 02:00:14,673 - INFO - [diffusion][Epoch 7429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:14,674 - INFO - [diffusion][Epoch 7430] Epoch 7431/12000
2024-11-05 02:00:18,737 - INFO - [diffusion][Epoch 7430] diffusion training Loss: 0.0562650328502059
2024-11-05 02:00:18,740 - INFO - [diffusion][Epoch 7430] diffusion learning rate: 0.001
2024-11-05 02:00:18,770 - INFO - [diffusion][Epoch 7430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:18,772 - INFO - [diffusion][Epoch 7431] Epoch 7432/12000
2024-11-05 02:00:22,921 - INFO - [diffusion][Epoch 7431] diffusion training Loss: 0.055616024881601334
2024-11-05 02:00:22,923 - INFO - [diffusion][Epoch 7431] diffusion learning rate: 0.001
2024-11-05 02:00:22,925 - INFO - [diffusion][Epoch 7431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:22,926 - INFO - [diffusion][Epoch 7432] Epoch 7433/12000
2024-11-05 02:00:27,005 - INFO - [diffusion][Epoch 7432] diffusion training Loss: 0.05327482521533966
2024-11-05 02:00:27,007 - INFO - [diffusion][Epoch 7432] diffusion learning rate: 0.001
2024-11-05 02:00:27,009 - INFO - [diffusion][Epoch 7432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:27,010 - INFO - [diffusion][Epoch 7433] Epoch 7434/12000
2024-11-05 02:00:31,105 - INFO - [diffusion][Epoch 7433] diffusion training Loss: 0.05234746914356947
2024-11-05 02:00:31,107 - INFO - [diffusion][Epoch 7433] diffusion learning rate: 0.001
2024-11-05 02:00:31,108 - INFO - [diffusion][Epoch 7433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:31,110 - INFO - [diffusion][Epoch 7434] Epoch 7435/12000
2024-11-05 02:00:35,252 - INFO - [diffusion][Epoch 7434] diffusion training Loss: 0.05592312663793564
2024-11-05 02:00:35,254 - INFO - [diffusion][Epoch 7434] diffusion learning rate: 0.001
2024-11-05 02:00:35,255 - INFO - [diffusion][Epoch 7434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:35,257 - INFO - [diffusion][Epoch 7435] Epoch 7436/12000
2024-11-05 02:00:39,388 - INFO - [diffusion][Epoch 7435] diffusion training Loss: 0.05376838892698288
2024-11-05 02:00:39,530 - INFO - [diffusion][Epoch 7435] diffusion learning rate: 0.001
2024-11-05 02:00:39,669 - INFO - [diffusion][Epoch 7435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:39,753 - INFO - [diffusion][Epoch 7436] Epoch 7437/12000
2024-11-05 02:00:43,877 - INFO - [diffusion][Epoch 7436] diffusion training Loss: 0.053208258002996445
2024-11-05 02:00:43,879 - INFO - [diffusion][Epoch 7436] diffusion learning rate: 0.001
2024-11-05 02:00:43,881 - INFO - [diffusion][Epoch 7436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:43,882 - INFO - [diffusion][Epoch 7437] Epoch 7438/12000
2024-11-05 02:00:47,930 - INFO - [diffusion][Epoch 7437] diffusion training Loss: 0.05439181998372078
2024-11-05 02:00:47,933 - INFO - [diffusion][Epoch 7437] diffusion learning rate: 0.001
2024-11-05 02:00:47,935 - INFO - [diffusion][Epoch 7437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:47,936 - INFO - [diffusion][Epoch 7438] Epoch 7439/12000
2024-11-05 02:00:52,081 - INFO - [diffusion][Epoch 7438] diffusion training Loss: 0.05337089207023382
2024-11-05 02:00:52,083 - INFO - [diffusion][Epoch 7438] diffusion learning rate: 0.001
2024-11-05 02:00:52,085 - INFO - [diffusion][Epoch 7438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:52,086 - INFO - [diffusion][Epoch 7439] Epoch 7440/12000
2024-11-05 02:00:56,238 - INFO - [diffusion][Epoch 7439] diffusion training Loss: 0.05429246183484793
2024-11-05 02:00:56,241 - INFO - [diffusion][Epoch 7439] diffusion learning rate: 0.001
2024-11-05 02:00:56,243 - INFO - [diffusion][Epoch 7439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:00:56,244 - INFO - [diffusion][Epoch 7440] Epoch 7441/12000
2024-11-05 02:01:00,402 - INFO - [diffusion][Epoch 7440] diffusion training Loss: 0.054063587449491024
2024-11-05 02:01:00,404 - INFO - [diffusion][Epoch 7440] diffusion learning rate: 0.001
2024-11-05 02:01:00,406 - INFO - [diffusion][Epoch 7440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:00,407 - INFO - [diffusion][Epoch 7441] Epoch 7442/12000
2024-11-05 02:01:04,544 - INFO - [diffusion][Epoch 7441] diffusion training Loss: 0.054982840083539486
2024-11-05 02:01:04,547 - INFO - [diffusion][Epoch 7441] diffusion learning rate: 0.001
2024-11-05 02:01:04,549 - INFO - [diffusion][Epoch 7441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:04,550 - INFO - [diffusion][Epoch 7442] Epoch 7443/12000
2024-11-05 02:01:08,483 - INFO - [diffusion][Epoch 7442] diffusion training Loss: 0.04895415250211954
2024-11-05 02:01:08,645 - INFO - [diffusion][Epoch 7442] diffusion learning rate: 0.001
2024-11-05 02:01:08,647 - INFO - [diffusion][Epoch 7442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:08,648 - INFO - [diffusion][Epoch 7443] Epoch 7444/12000
2024-11-05 02:01:12,766 - INFO - [diffusion][Epoch 7443] diffusion training Loss: 0.059372616931796074
2024-11-05 02:01:12,768 - INFO - [diffusion][Epoch 7443] diffusion learning rate: 0.001
2024-11-05 02:01:12,769 - INFO - [diffusion][Epoch 7443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:12,771 - INFO - [diffusion][Epoch 7444] Epoch 7445/12000
2024-11-05 02:01:16,889 - INFO - [diffusion][Epoch 7444] diffusion training Loss: 0.05647939909249544
2024-11-05 02:01:16,891 - INFO - [diffusion][Epoch 7444] diffusion learning rate: 0.001
2024-11-05 02:01:16,893 - INFO - [diffusion][Epoch 7444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:16,894 - INFO - [diffusion][Epoch 7445] Epoch 7446/12000
2024-11-05 02:01:21,002 - INFO - [diffusion][Epoch 7445] diffusion training Loss: 0.06100761890411377
2024-11-05 02:01:21,003 - INFO - [diffusion][Epoch 7445] diffusion learning rate: 0.001
2024-11-05 02:01:21,005 - INFO - [diffusion][Epoch 7445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:21,006 - INFO - [diffusion][Epoch 7446] Epoch 7447/12000
2024-11-05 02:01:25,028 - INFO - [diffusion][Epoch 7446] diffusion training Loss: 0.0595151474699378
2024-11-05 02:01:25,030 - INFO - [diffusion][Epoch 7446] diffusion learning rate: 0.001
2024-11-05 02:01:25,032 - INFO - [diffusion][Epoch 7446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:25,033 - INFO - [diffusion][Epoch 7447] Epoch 7448/12000
2024-11-05 02:01:29,310 - INFO - [diffusion][Epoch 7447] diffusion training Loss: 0.057039570063352585
2024-11-05 02:01:29,312 - INFO - [diffusion][Epoch 7447] diffusion learning rate: 0.001
2024-11-05 02:01:29,314 - INFO - [diffusion][Epoch 7447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:29,315 - INFO - [diffusion][Epoch 7448] Epoch 7449/12000
2024-11-05 02:01:33,304 - INFO - [diffusion][Epoch 7448] diffusion training Loss: 0.05515828914940357
2024-11-05 02:01:33,306 - INFO - [diffusion][Epoch 7448] diffusion learning rate: 0.001
2024-11-05 02:01:33,307 - INFO - [diffusion][Epoch 7448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:33,309 - INFO - [diffusion][Epoch 7449] Epoch 7450/12000
2024-11-05 02:01:37,410 - INFO - [diffusion][Epoch 7449] diffusion training Loss: 0.05768088437616825
2024-11-05 02:01:37,412 - INFO - [diffusion][Epoch 7449] diffusion learning rate: 0.001
2024-11-05 02:01:37,414 - INFO - [diffusion][Epoch 7449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:37,415 - INFO - [diffusion][Epoch 7450] Epoch 7451/12000
2024-11-05 02:01:41,504 - INFO - [diffusion][Epoch 7450] diffusion training Loss: 0.05177583731710911
2024-11-05 02:01:41,506 - INFO - [diffusion][Epoch 7450] diffusion learning rate: 0.001
2024-11-05 02:01:41,507 - INFO - [diffusion][Epoch 7450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:41,509 - INFO - [diffusion][Epoch 7451] Epoch 7452/12000
2024-11-05 02:01:45,622 - INFO - [diffusion][Epoch 7451] diffusion training Loss: 0.053448256105184555
2024-11-05 02:01:45,624 - INFO - [diffusion][Epoch 7451] diffusion learning rate: 0.001
2024-11-05 02:01:45,626 - INFO - [diffusion][Epoch 7451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:45,628 - INFO - [diffusion][Epoch 7452] Epoch 7453/12000
2024-11-05 02:01:49,739 - INFO - [diffusion][Epoch 7452] diffusion training Loss: 0.05676346831023693
2024-11-05 02:01:49,743 - INFO - [diffusion][Epoch 7452] diffusion learning rate: 0.001
2024-11-05 02:01:49,745 - INFO - [diffusion][Epoch 7452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:49,746 - INFO - [diffusion][Epoch 7453] Epoch 7454/12000
2024-11-05 02:01:53,827 - INFO - [diffusion][Epoch 7453] diffusion training Loss: 0.06037758383899927
2024-11-05 02:01:53,829 - INFO - [diffusion][Epoch 7453] diffusion learning rate: 0.001
2024-11-05 02:01:53,831 - INFO - [diffusion][Epoch 7453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:53,832 - INFO - [diffusion][Epoch 7454] Epoch 7455/12000
2024-11-05 02:01:57,942 - INFO - [diffusion][Epoch 7454] diffusion training Loss: 0.05469621904194355
2024-11-05 02:01:57,944 - INFO - [diffusion][Epoch 7454] diffusion learning rate: 0.001
2024-11-05 02:01:57,945 - INFO - [diffusion][Epoch 7454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:01:57,947 - INFO - [diffusion][Epoch 7455] Epoch 7456/12000
2024-11-05 02:02:02,041 - INFO - [diffusion][Epoch 7455] diffusion training Loss: 0.05099972151219845
2024-11-05 02:02:02,043 - INFO - [diffusion][Epoch 7455] diffusion learning rate: 0.001
2024-11-05 02:02:02,046 - INFO - [diffusion][Epoch 7455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:02,047 - INFO - [diffusion][Epoch 7456] Epoch 7457/12000
2024-11-05 02:02:06,161 - INFO - [diffusion][Epoch 7456] diffusion training Loss: 0.06213129684329033
2024-11-05 02:02:06,163 - INFO - [diffusion][Epoch 7456] diffusion learning rate: 0.001
2024-11-05 02:02:06,166 - INFO - [diffusion][Epoch 7456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:06,167 - INFO - [diffusion][Epoch 7457] Epoch 7458/12000
2024-11-05 02:02:10,308 - INFO - [diffusion][Epoch 7457] diffusion training Loss: 0.052854583598673344
2024-11-05 02:02:10,310 - INFO - [diffusion][Epoch 7457] diffusion learning rate: 0.001
2024-11-05 02:02:10,312 - INFO - [diffusion][Epoch 7457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:10,314 - INFO - [diffusion][Epoch 7458] Epoch 7459/12000
2024-11-05 02:02:14,419 - INFO - [diffusion][Epoch 7458] diffusion training Loss: 0.05762781109660864
2024-11-05 02:02:14,421 - INFO - [diffusion][Epoch 7458] diffusion learning rate: 0.001
2024-11-05 02:02:14,423 - INFO - [diffusion][Epoch 7458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:14,424 - INFO - [diffusion][Epoch 7459] Epoch 7460/12000
2024-11-05 02:02:18,543 - INFO - [diffusion][Epoch 7459] diffusion training Loss: 0.05585031770169735
2024-11-05 02:02:18,545 - INFO - [diffusion][Epoch 7459] diffusion learning rate: 0.001
2024-11-05 02:02:18,547 - INFO - [diffusion][Epoch 7459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:18,548 - INFO - [diffusion][Epoch 7460] Epoch 7461/12000
2024-11-05 02:02:22,689 - INFO - [diffusion][Epoch 7460] diffusion training Loss: 0.05799448490142822
2024-11-05 02:02:22,691 - INFO - [diffusion][Epoch 7460] diffusion learning rate: 0.001
2024-11-05 02:02:22,693 - INFO - [diffusion][Epoch 7460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:22,695 - INFO - [diffusion][Epoch 7461] Epoch 7462/12000
2024-11-05 02:02:26,778 - INFO - [diffusion][Epoch 7461] diffusion training Loss: 0.05650510638952255
2024-11-05 02:02:26,780 - INFO - [diffusion][Epoch 7461] diffusion learning rate: 0.001
2024-11-05 02:02:26,782 - INFO - [diffusion][Epoch 7461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:26,783 - INFO - [diffusion][Epoch 7462] Epoch 7463/12000
2024-11-05 02:02:30,812 - INFO - [diffusion][Epoch 7462] diffusion training Loss: 0.05195998307317495
2024-11-05 02:02:30,814 - INFO - [diffusion][Epoch 7462] diffusion learning rate: 0.001
2024-11-05 02:02:30,816 - INFO - [diffusion][Epoch 7462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:30,817 - INFO - [diffusion][Epoch 7463] Epoch 7464/12000
2024-11-05 02:02:34,953 - INFO - [diffusion][Epoch 7463] diffusion training Loss: 0.055347198620438576
2024-11-05 02:02:34,955 - INFO - [diffusion][Epoch 7463] diffusion learning rate: 0.001
2024-11-05 02:02:34,957 - INFO - [diffusion][Epoch 7463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:34,958 - INFO - [diffusion][Epoch 7464] Epoch 7465/12000
2024-11-05 02:02:39,003 - INFO - [diffusion][Epoch 7464] diffusion training Loss: 0.05578821711242199
2024-11-05 02:02:39,005 - INFO - [diffusion][Epoch 7464] diffusion learning rate: 0.001
2024-11-05 02:02:39,007 - INFO - [diffusion][Epoch 7464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:39,009 - INFO - [diffusion][Epoch 7465] Epoch 7466/12000
2024-11-05 02:02:43,085 - INFO - [diffusion][Epoch 7465] diffusion training Loss: 0.06014075595885515
2024-11-05 02:02:43,088 - INFO - [diffusion][Epoch 7465] diffusion learning rate: 0.001
2024-11-05 02:02:43,090 - INFO - [diffusion][Epoch 7465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:43,093 - INFO - [diffusion][Epoch 7466] Epoch 7467/12000
2024-11-05 02:02:47,268 - INFO - [diffusion][Epoch 7466] diffusion training Loss: 0.05535100307315588
2024-11-05 02:02:47,270 - INFO - [diffusion][Epoch 7466] diffusion learning rate: 0.001
2024-11-05 02:02:47,272 - INFO - [diffusion][Epoch 7466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:47,274 - INFO - [diffusion][Epoch 7467] Epoch 7468/12000
2024-11-05 02:02:51,395 - INFO - [diffusion][Epoch 7467] diffusion training Loss: 0.0543248038738966
2024-11-05 02:02:51,398 - INFO - [diffusion][Epoch 7467] diffusion learning rate: 0.001
2024-11-05 02:02:51,399 - INFO - [diffusion][Epoch 7467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:51,401 - INFO - [diffusion][Epoch 7468] Epoch 7469/12000
2024-11-05 02:02:55,524 - INFO - [diffusion][Epoch 7468] diffusion training Loss: 0.05200920253992081
2024-11-05 02:02:55,526 - INFO - [diffusion][Epoch 7468] diffusion learning rate: 0.001
2024-11-05 02:02:55,528 - INFO - [diffusion][Epoch 7468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:55,529 - INFO - [diffusion][Epoch 7469] Epoch 7470/12000
2024-11-05 02:02:59,716 - INFO - [diffusion][Epoch 7469] diffusion training Loss: 0.05928628891706467
2024-11-05 02:02:59,718 - INFO - [diffusion][Epoch 7469] diffusion learning rate: 0.001
2024-11-05 02:02:59,719 - INFO - [diffusion][Epoch 7469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:02:59,722 - INFO - [diffusion][Epoch 7470] Epoch 7471/12000
2024-11-05 02:03:03,855 - INFO - [diffusion][Epoch 7470] diffusion training Loss: 0.056912398897111416
2024-11-05 02:03:03,857 - INFO - [diffusion][Epoch 7470] diffusion learning rate: 0.001
2024-11-05 02:03:03,859 - INFO - [diffusion][Epoch 7470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:03,860 - INFO - [diffusion][Epoch 7471] Epoch 7472/12000
2024-11-05 02:03:07,989 - INFO - [diffusion][Epoch 7471] diffusion training Loss: 0.06021552439779043
2024-11-05 02:03:07,994 - INFO - [diffusion][Epoch 7471] diffusion learning rate: 0.001
2024-11-05 02:03:07,996 - INFO - [diffusion][Epoch 7471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:07,997 - INFO - [diffusion][Epoch 7472] Epoch 7473/12000
2024-11-05 02:03:12,109 - INFO - [diffusion][Epoch 7472] diffusion training Loss: 0.05354295019060373
2024-11-05 02:03:12,398 - INFO - [diffusion][Epoch 7472] diffusion learning rate: 0.001
2024-11-05 02:03:12,400 - INFO - [diffusion][Epoch 7472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:12,401 - INFO - [diffusion][Epoch 7473] Epoch 7474/12000
2024-11-05 02:03:16,511 - INFO - [diffusion][Epoch 7473] diffusion training Loss: 0.053622232750058174
2024-11-05 02:03:16,513 - INFO - [diffusion][Epoch 7473] diffusion learning rate: 0.001
2024-11-05 02:03:16,515 - INFO - [diffusion][Epoch 7473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:16,517 - INFO - [diffusion][Epoch 7474] Epoch 7475/12000
2024-11-05 02:03:20,679 - INFO - [diffusion][Epoch 7474] diffusion training Loss: 0.05507180467247963
2024-11-05 02:03:20,681 - INFO - [diffusion][Epoch 7474] diffusion learning rate: 0.001
2024-11-05 02:03:20,683 - INFO - [diffusion][Epoch 7474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:20,684 - INFO - [diffusion][Epoch 7475] Epoch 7476/12000
2024-11-05 02:03:24,799 - INFO - [diffusion][Epoch 7475] diffusion training Loss: 0.05772325582802296
2024-11-05 02:03:24,801 - INFO - [diffusion][Epoch 7475] diffusion learning rate: 0.001
2024-11-05 02:03:24,803 - INFO - [diffusion][Epoch 7475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:24,805 - INFO - [diffusion][Epoch 7476] Epoch 7477/12000
2024-11-05 02:03:28,782 - INFO - [diffusion][Epoch 7476] diffusion training Loss: 0.05802756641060114
2024-11-05 02:03:28,784 - INFO - [diffusion][Epoch 7476] diffusion learning rate: 0.001
2024-11-05 02:03:28,786 - INFO - [diffusion][Epoch 7476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:28,788 - INFO - [diffusion][Epoch 7477] Epoch 7478/12000
2024-11-05 02:03:32,922 - INFO - [diffusion][Epoch 7477] diffusion training Loss: 0.05211609788239002
2024-11-05 02:03:32,924 - INFO - [diffusion][Epoch 7477] diffusion learning rate: 0.001
2024-11-05 02:03:32,926 - INFO - [diffusion][Epoch 7477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:32,928 - INFO - [diffusion][Epoch 7478] Epoch 7479/12000
2024-11-05 02:03:37,098 - INFO - [diffusion][Epoch 7478] diffusion training Loss: 0.06092085316777229
2024-11-05 02:03:37,100 - INFO - [diffusion][Epoch 7478] diffusion learning rate: 0.001
2024-11-05 02:03:37,103 - INFO - [diffusion][Epoch 7478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:37,104 - INFO - [diffusion][Epoch 7479] Epoch 7480/12000
2024-11-05 02:03:41,265 - INFO - [diffusion][Epoch 7479] diffusion training Loss: 0.06196779292076826
2024-11-05 02:03:41,267 - INFO - [diffusion][Epoch 7479] diffusion learning rate: 0.001
2024-11-05 02:03:41,269 - INFO - [diffusion][Epoch 7479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:41,271 - INFO - [diffusion][Epoch 7480] Epoch 7481/12000
2024-11-05 02:03:45,424 - INFO - [diffusion][Epoch 7480] diffusion training Loss: 0.06036854162812233
2024-11-05 02:03:45,426 - INFO - [diffusion][Epoch 7480] diffusion learning rate: 0.001
2024-11-05 02:03:45,428 - INFO - [diffusion][Epoch 7480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:45,429 - INFO - [diffusion][Epoch 7481] Epoch 7482/12000
2024-11-05 02:03:49,550 - INFO - [diffusion][Epoch 7481] diffusion training Loss: 0.05459022428840399
2024-11-05 02:03:49,553 - INFO - [diffusion][Epoch 7481] diffusion learning rate: 0.001
2024-11-05 02:03:49,555 - INFO - [diffusion][Epoch 7481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:49,556 - INFO - [diffusion][Epoch 7482] Epoch 7483/12000
2024-11-05 02:03:53,655 - INFO - [diffusion][Epoch 7482] diffusion training Loss: 0.06025948282331228
2024-11-05 02:03:53,658 - INFO - [diffusion][Epoch 7482] diffusion learning rate: 0.001
2024-11-05 02:03:53,660 - INFO - [diffusion][Epoch 7482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:53,661 - INFO - [diffusion][Epoch 7483] Epoch 7484/12000
2024-11-05 02:03:57,678 - INFO - [diffusion][Epoch 7483] diffusion training Loss: 0.05155238602310419
2024-11-05 02:03:57,681 - INFO - [diffusion][Epoch 7483] diffusion learning rate: 0.001
2024-11-05 02:03:57,682 - INFO - [diffusion][Epoch 7483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:03:57,684 - INFO - [diffusion][Epoch 7484] Epoch 7485/12000
2024-11-05 02:04:01,800 - INFO - [diffusion][Epoch 7484] diffusion training Loss: 0.054653117433190346
2024-11-05 02:04:01,802 - INFO - [diffusion][Epoch 7484] diffusion learning rate: 0.001
2024-11-05 02:04:01,804 - INFO - [diffusion][Epoch 7484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:01,805 - INFO - [diffusion][Epoch 7485] Epoch 7486/12000
2024-11-05 02:04:05,781 - INFO - [diffusion][Epoch 7485] diffusion training Loss: 0.05777324829250574
2024-11-05 02:04:05,783 - INFO - [diffusion][Epoch 7485] diffusion learning rate: 0.001
2024-11-05 02:04:05,785 - INFO - [diffusion][Epoch 7485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:05,786 - INFO - [diffusion][Epoch 7486] Epoch 7487/12000
2024-11-05 02:04:09,781 - INFO - [diffusion][Epoch 7486] diffusion training Loss: 0.05771707650274038
2024-11-05 02:04:09,783 - INFO - [diffusion][Epoch 7486] diffusion learning rate: 0.001
2024-11-05 02:04:09,785 - INFO - [diffusion][Epoch 7486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:09,786 - INFO - [diffusion][Epoch 7487] Epoch 7488/12000
2024-11-05 02:04:13,850 - INFO - [diffusion][Epoch 7487] diffusion training Loss: 0.052233210764825344
2024-11-05 02:04:13,852 - INFO - [diffusion][Epoch 7487] diffusion learning rate: 0.001
2024-11-05 02:04:13,854 - INFO - [diffusion][Epoch 7487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:13,855 - INFO - [diffusion][Epoch 7488] Epoch 7489/12000
2024-11-05 02:04:17,994 - INFO - [diffusion][Epoch 7488] diffusion training Loss: 0.05748037341982126
2024-11-05 02:04:17,996 - INFO - [diffusion][Epoch 7488] diffusion learning rate: 0.001
2024-11-05 02:04:17,998 - INFO - [diffusion][Epoch 7488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:17,999 - INFO - [diffusion][Epoch 7489] Epoch 7490/12000
2024-11-05 02:04:22,078 - INFO - [diffusion][Epoch 7489] diffusion training Loss: 0.05532955192029476
2024-11-05 02:04:22,080 - INFO - [diffusion][Epoch 7489] diffusion learning rate: 0.001
2024-11-05 02:04:22,082 - INFO - [diffusion][Epoch 7489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:22,083 - INFO - [diffusion][Epoch 7490] Epoch 7491/12000
2024-11-05 02:04:26,459 - INFO - [diffusion][Epoch 7490] diffusion training Loss: 0.05878730025142431
2024-11-05 02:04:26,461 - INFO - [diffusion][Epoch 7490] diffusion learning rate: 0.001
2024-11-05 02:04:26,462 - INFO - [diffusion][Epoch 7490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:26,464 - INFO - [diffusion][Epoch 7491] Epoch 7492/12000
2024-11-05 02:04:30,534 - INFO - [diffusion][Epoch 7491] diffusion training Loss: 0.056811558082699776
2024-11-05 02:04:30,536 - INFO - [diffusion][Epoch 7491] diffusion learning rate: 0.001
2024-11-05 02:04:30,538 - INFO - [diffusion][Epoch 7491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:30,539 - INFO - [diffusion][Epoch 7492] Epoch 7493/12000
2024-11-05 02:04:34,681 - INFO - [diffusion][Epoch 7492] diffusion training Loss: 0.061727166175842285
2024-11-05 02:04:34,683 - INFO - [diffusion][Epoch 7492] diffusion learning rate: 0.001
2024-11-05 02:04:34,716 - INFO - [diffusion][Epoch 7492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:34,717 - INFO - [diffusion][Epoch 7493] Epoch 7494/12000
2024-11-05 02:04:38,838 - INFO - [diffusion][Epoch 7493] diffusion training Loss: 0.05520503129810095
2024-11-05 02:04:38,840 - INFO - [diffusion][Epoch 7493] diffusion learning rate: 0.001
2024-11-05 02:04:38,842 - INFO - [diffusion][Epoch 7493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:38,843 - INFO - [diffusion][Epoch 7494] Epoch 7495/12000
2024-11-05 02:04:42,946 - INFO - [diffusion][Epoch 7494] diffusion training Loss: 0.05436666123569012
2024-11-05 02:04:42,948 - INFO - [diffusion][Epoch 7494] diffusion learning rate: 0.001
2024-11-05 02:04:42,949 - INFO - [diffusion][Epoch 7494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:42,951 - INFO - [diffusion][Epoch 7495] Epoch 7496/12000
2024-11-05 02:04:47,079 - INFO - [diffusion][Epoch 7495] diffusion training Loss: 0.05543920397758484
2024-11-05 02:04:47,081 - INFO - [diffusion][Epoch 7495] diffusion learning rate: 0.001
2024-11-05 02:04:47,083 - INFO - [diffusion][Epoch 7495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:47,084 - INFO - [diffusion][Epoch 7496] Epoch 7497/12000
2024-11-05 02:04:51,203 - INFO - [diffusion][Epoch 7496] diffusion training Loss: 0.055323027074337006
2024-11-05 02:04:51,205 - INFO - [diffusion][Epoch 7496] diffusion learning rate: 0.001
2024-11-05 02:04:51,207 - INFO - [diffusion][Epoch 7496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:51,208 - INFO - [diffusion][Epoch 7497] Epoch 7498/12000
2024-11-05 02:04:55,277 - INFO - [diffusion][Epoch 7497] diffusion training Loss: 0.05649624951183796
2024-11-05 02:04:55,280 - INFO - [diffusion][Epoch 7497] diffusion learning rate: 0.001
2024-11-05 02:04:55,282 - INFO - [diffusion][Epoch 7497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:55,283 - INFO - [diffusion][Epoch 7498] Epoch 7499/12000
2024-11-05 02:04:59,393 - INFO - [diffusion][Epoch 7498] diffusion training Loss: 0.053024619817733765
2024-11-05 02:04:59,396 - INFO - [diffusion][Epoch 7498] diffusion learning rate: 0.001
2024-11-05 02:04:59,397 - INFO - [diffusion][Epoch 7498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:04:59,399 - INFO - [diffusion][Epoch 7499] Epoch 7500/12000
2024-11-05 02:05:03,485 - INFO - [diffusion][Epoch 7499] diffusion training Loss: 0.0542746102437377
2024-11-05 02:05:03,487 - INFO - [diffusion][Epoch 7499] diffusion learning rate: 0.001
2024-11-05 02:05:03,489 - INFO - [diffusion][Epoch 7499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:03,490 - INFO - [diffusion][Epoch 7500] Epoch 7501/12000
2024-11-05 02:05:07,622 - INFO - [diffusion][Epoch 7500] diffusion training Loss: 0.055557490326464176
2024-11-05 02:05:07,624 - INFO - [diffusion][Epoch 7500] diffusion learning rate: 0.001
2024-11-05 02:05:07,650 - INFO - [diffusion][Epoch 7500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:07,652 - INFO - [diffusion][Epoch 7501] Epoch 7502/12000
2024-11-05 02:05:11,611 - INFO - [diffusion][Epoch 7501] diffusion training Loss: 0.05748267285525799
2024-11-05 02:05:11,613 - INFO - [diffusion][Epoch 7501] diffusion learning rate: 0.001
2024-11-05 02:05:11,615 - INFO - [diffusion][Epoch 7501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:11,616 - INFO - [diffusion][Epoch 7502] Epoch 7503/12000
2024-11-05 02:05:15,737 - INFO - [diffusion][Epoch 7502] diffusion training Loss: 0.05854251142591238
2024-11-05 02:05:15,739 - INFO - [diffusion][Epoch 7502] diffusion learning rate: 0.001
2024-11-05 02:05:15,741 - INFO - [diffusion][Epoch 7502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:15,742 - INFO - [diffusion][Epoch 7503] Epoch 7504/12000
2024-11-05 02:05:19,829 - INFO - [diffusion][Epoch 7503] diffusion training Loss: 0.056980203837156296
2024-11-05 02:05:19,831 - INFO - [diffusion][Epoch 7503] diffusion learning rate: 0.001
2024-11-05 02:05:19,833 - INFO - [diffusion][Epoch 7503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:19,834 - INFO - [diffusion][Epoch 7504] Epoch 7505/12000
2024-11-05 02:05:23,941 - INFO - [diffusion][Epoch 7504] diffusion training Loss: 0.05476795043796301
2024-11-05 02:05:23,943 - INFO - [diffusion][Epoch 7504] diffusion learning rate: 0.001
2024-11-05 02:05:23,969 - INFO - [diffusion][Epoch 7504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:23,971 - INFO - [diffusion][Epoch 7505] Epoch 7506/12000
2024-11-05 02:05:28,096 - INFO - [diffusion][Epoch 7505] diffusion training Loss: 0.05966933909803629
2024-11-05 02:05:28,098 - INFO - [diffusion][Epoch 7505] diffusion learning rate: 0.001
2024-11-05 02:05:28,100 - INFO - [diffusion][Epoch 7505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:28,101 - INFO - [diffusion][Epoch 7506] Epoch 7507/12000
2024-11-05 02:05:32,203 - INFO - [diffusion][Epoch 7506] diffusion training Loss: 0.054927424527704716
2024-11-05 02:05:32,205 - INFO - [diffusion][Epoch 7506] diffusion learning rate: 0.001
2024-11-05 02:05:32,206 - INFO - [diffusion][Epoch 7506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:32,208 - INFO - [diffusion][Epoch 7507] Epoch 7508/12000
2024-11-05 02:05:36,302 - INFO - [diffusion][Epoch 7507] diffusion training Loss: 0.0484384223818779
2024-11-05 02:05:36,305 - INFO - [diffusion][Epoch 7507] diffusion learning rate: 0.001
2024-11-05 02:05:36,306 - INFO - [diffusion][Epoch 7507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:36,308 - INFO - [diffusion][Epoch 7508] Epoch 7509/12000
2024-11-05 02:05:40,477 - INFO - [diffusion][Epoch 7508] diffusion training Loss: 0.0596314063295722
2024-11-05 02:05:40,479 - INFO - [diffusion][Epoch 7508] diffusion learning rate: 0.001
2024-11-05 02:05:40,518 - INFO - [diffusion][Epoch 7508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:40,519 - INFO - [diffusion][Epoch 7509] Epoch 7510/12000
2024-11-05 02:05:44,477 - INFO - [diffusion][Epoch 7509] diffusion training Loss: 0.0595571082085371
2024-11-05 02:05:44,479 - INFO - [diffusion][Epoch 7509] diffusion learning rate: 0.001
2024-11-05 02:05:44,481 - INFO - [diffusion][Epoch 7509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:44,482 - INFO - [diffusion][Epoch 7510] Epoch 7511/12000
2024-11-05 02:05:48,521 - INFO - [diffusion][Epoch 7510] diffusion training Loss: 0.05292809009552002
2024-11-05 02:05:48,523 - INFO - [diffusion][Epoch 7510] diffusion learning rate: 0.001
2024-11-05 02:05:48,525 - INFO - [diffusion][Epoch 7510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:48,526 - INFO - [diffusion][Epoch 7511] Epoch 7512/12000
2024-11-05 02:05:52,643 - INFO - [diffusion][Epoch 7511] diffusion training Loss: 0.05592257622629404
2024-11-05 02:05:52,794 - INFO - [diffusion][Epoch 7511] diffusion learning rate: 0.001
2024-11-05 02:05:52,796 - INFO - [diffusion][Epoch 7511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:52,797 - INFO - [diffusion][Epoch 7512] Epoch 7513/12000
2024-11-05 02:05:56,905 - INFO - [diffusion][Epoch 7512] diffusion training Loss: 0.05152325518429279
2024-11-05 02:05:56,908 - INFO - [diffusion][Epoch 7512] diffusion learning rate: 0.001
2024-11-05 02:05:56,909 - INFO - [diffusion][Epoch 7512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:05:56,911 - INFO - [diffusion][Epoch 7513] Epoch 7514/12000
2024-11-05 02:06:00,963 - INFO - [diffusion][Epoch 7513] diffusion training Loss: 0.06206375826150179
2024-11-05 02:06:00,965 - INFO - [diffusion][Epoch 7513] diffusion learning rate: 0.001
2024-11-05 02:06:00,967 - INFO - [diffusion][Epoch 7513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:00,968 - INFO - [diffusion][Epoch 7514] Epoch 7515/12000
2024-11-05 02:06:05,066 - INFO - [diffusion][Epoch 7514] diffusion training Loss: 0.05913256574422121
2024-11-05 02:06:05,068 - INFO - [diffusion][Epoch 7514] diffusion learning rate: 0.001
2024-11-05 02:06:05,070 - INFO - [diffusion][Epoch 7514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:05,071 - INFO - [diffusion][Epoch 7515] Epoch 7516/12000
2024-11-05 02:06:09,208 - INFO - [diffusion][Epoch 7515] diffusion training Loss: 0.0577993281185627
2024-11-05 02:06:09,210 - INFO - [diffusion][Epoch 7515] diffusion learning rate: 0.001
2024-11-05 02:06:09,212 - INFO - [diffusion][Epoch 7515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:09,213 - INFO - [diffusion][Epoch 7516] Epoch 7517/12000
2024-11-05 02:06:13,397 - INFO - [diffusion][Epoch 7516] diffusion training Loss: 0.05737857706844807
2024-11-05 02:06:13,399 - INFO - [diffusion][Epoch 7516] diffusion learning rate: 0.001
2024-11-05 02:06:13,435 - INFO - [diffusion][Epoch 7516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:13,436 - INFO - [diffusion][Epoch 7517] Epoch 7518/12000
2024-11-05 02:06:17,543 - INFO - [diffusion][Epoch 7517] diffusion training Loss: 0.05894263740628958
2024-11-05 02:06:17,545 - INFO - [diffusion][Epoch 7517] diffusion learning rate: 0.001
2024-11-05 02:06:17,546 - INFO - [diffusion][Epoch 7517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:17,548 - INFO - [diffusion][Epoch 7518] Epoch 7519/12000
2024-11-05 02:06:21,513 - INFO - [diffusion][Epoch 7518] diffusion training Loss: 0.05216178297996521
2024-11-05 02:06:21,515 - INFO - [diffusion][Epoch 7518] diffusion learning rate: 0.001
2024-11-05 02:06:21,517 - INFO - [diffusion][Epoch 7518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:21,518 - INFO - [diffusion][Epoch 7519] Epoch 7520/12000
2024-11-05 02:06:25,645 - INFO - [diffusion][Epoch 7519] diffusion training Loss: 0.05335234850645065
2024-11-05 02:06:25,647 - INFO - [diffusion][Epoch 7519] diffusion learning rate: 0.001
2024-11-05 02:06:25,648 - INFO - [diffusion][Epoch 7519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:25,649 - INFO - [diffusion][Epoch 7520] Epoch 7521/12000
2024-11-05 02:06:29,713 - INFO - [diffusion][Epoch 7520] diffusion training Loss: 0.05306009668856859
2024-11-05 02:06:29,715 - INFO - [diffusion][Epoch 7520] diffusion learning rate: 0.001
2024-11-05 02:06:29,752 - INFO - [diffusion][Epoch 7520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:29,753 - INFO - [diffusion][Epoch 7521] Epoch 7522/12000
2024-11-05 02:06:33,890 - INFO - [diffusion][Epoch 7521] diffusion training Loss: 0.055024417117238045
2024-11-05 02:06:33,892 - INFO - [diffusion][Epoch 7521] diffusion learning rate: 0.001
2024-11-05 02:06:33,894 - INFO - [diffusion][Epoch 7521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:33,896 - INFO - [diffusion][Epoch 7522] Epoch 7523/12000
2024-11-05 02:06:37,956 - INFO - [diffusion][Epoch 7522] diffusion training Loss: 0.05467588920146227
2024-11-05 02:06:37,958 - INFO - [diffusion][Epoch 7522] diffusion learning rate: 0.001
2024-11-05 02:06:37,960 - INFO - [diffusion][Epoch 7522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:37,961 - INFO - [diffusion][Epoch 7523] Epoch 7524/12000
2024-11-05 02:06:42,100 - INFO - [diffusion][Epoch 7523] diffusion training Loss: 0.051159799098968506
2024-11-05 02:06:42,102 - INFO - [diffusion][Epoch 7523] diffusion learning rate: 0.001
2024-11-05 02:06:42,105 - INFO - [diffusion][Epoch 7523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:42,106 - INFO - [diffusion][Epoch 7524] Epoch 7525/12000
2024-11-05 02:06:46,178 - INFO - [diffusion][Epoch 7524] diffusion training Loss: 0.05663660913705826
2024-11-05 02:06:46,180 - INFO - [diffusion][Epoch 7524] diffusion learning rate: 0.001
2024-11-05 02:06:46,213 - INFO - [diffusion][Epoch 7524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:46,214 - INFO - [diffusion][Epoch 7525] Epoch 7526/12000
2024-11-05 02:06:50,282 - INFO - [diffusion][Epoch 7525] diffusion training Loss: 0.055505664087831974
2024-11-05 02:06:50,285 - INFO - [diffusion][Epoch 7525] diffusion learning rate: 0.001
2024-11-05 02:06:50,286 - INFO - [diffusion][Epoch 7525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:50,288 - INFO - [diffusion][Epoch 7526] Epoch 7527/12000
2024-11-05 02:06:54,343 - INFO - [diffusion][Epoch 7526] diffusion training Loss: 0.06022263690829277
2024-11-05 02:06:54,345 - INFO - [diffusion][Epoch 7526] diffusion learning rate: 0.001
2024-11-05 02:06:54,347 - INFO - [diffusion][Epoch 7526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:54,349 - INFO - [diffusion][Epoch 7527] Epoch 7528/12000
2024-11-05 02:06:58,419 - INFO - [diffusion][Epoch 7527] diffusion training Loss: 0.055295394733548164
2024-11-05 02:06:58,421 - INFO - [diffusion][Epoch 7527] diffusion learning rate: 0.001
2024-11-05 02:06:58,423 - INFO - [diffusion][Epoch 7527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:06:58,425 - INFO - [diffusion][Epoch 7528] Epoch 7529/12000
2024-11-05 02:07:02,516 - INFO - [diffusion][Epoch 7528] diffusion training Loss: 0.062082989141345024
2024-11-05 02:07:02,518 - INFO - [diffusion][Epoch 7528] diffusion learning rate: 0.001
2024-11-05 02:07:02,520 - INFO - [diffusion][Epoch 7528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:02,521 - INFO - [diffusion][Epoch 7529] Epoch 7530/12000
2024-11-05 02:07:06,527 - INFO - [diffusion][Epoch 7529] diffusion training Loss: 0.0561112891882658
2024-11-05 02:07:06,529 - INFO - [diffusion][Epoch 7529] diffusion learning rate: 0.001
2024-11-05 02:07:06,532 - INFO - [diffusion][Epoch 7529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:06,533 - INFO - [diffusion][Epoch 7530] Epoch 7531/12000
2024-11-05 02:07:10,633 - INFO - [diffusion][Epoch 7530] diffusion training Loss: 0.051876368932425976
2024-11-05 02:07:10,635 - INFO - [diffusion][Epoch 7530] diffusion learning rate: 0.001
2024-11-05 02:07:10,637 - INFO - [diffusion][Epoch 7530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:10,639 - INFO - [diffusion][Epoch 7531] Epoch 7532/12000
2024-11-05 02:07:15,535 - INFO - [diffusion][Epoch 7531] diffusion training Loss: 0.05613045394420624
2024-11-05 02:07:15,537 - INFO - [diffusion][Epoch 7531] diffusion learning rate: 0.001
2024-11-05 02:07:15,539 - INFO - [diffusion][Epoch 7531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:15,540 - INFO - [diffusion][Epoch 7532] Epoch 7533/12000
2024-11-05 02:07:19,550 - INFO - [diffusion][Epoch 7532] diffusion training Loss: 0.058537816628813744
2024-11-05 02:07:19,552 - INFO - [diffusion][Epoch 7532] diffusion learning rate: 0.001
2024-11-05 02:07:19,555 - INFO - [diffusion][Epoch 7532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:19,556 - INFO - [diffusion][Epoch 7533] Epoch 7534/12000
2024-11-05 02:07:23,586 - INFO - [diffusion][Epoch 7533] diffusion training Loss: 0.056167989037930965
2024-11-05 02:07:23,588 - INFO - [diffusion][Epoch 7533] diffusion learning rate: 0.001
2024-11-05 02:07:23,590 - INFO - [diffusion][Epoch 7533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:23,591 - INFO - [diffusion][Epoch 7534] Epoch 7535/12000
2024-11-05 02:07:27,512 - INFO - [diffusion][Epoch 7534] diffusion training Loss: 0.054460871033370495
2024-11-05 02:07:27,514 - INFO - [diffusion][Epoch 7534] diffusion learning rate: 0.001
2024-11-05 02:07:27,516 - INFO - [diffusion][Epoch 7534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:27,517 - INFO - [diffusion][Epoch 7535] Epoch 7536/12000
2024-11-05 02:07:31,600 - INFO - [diffusion][Epoch 7535] diffusion training Loss: 0.05323134083300829
2024-11-05 02:07:31,601 - INFO - [diffusion][Epoch 7535] diffusion learning rate: 0.001
2024-11-05 02:07:31,603 - INFO - [diffusion][Epoch 7535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:31,604 - INFO - [diffusion][Epoch 7536] Epoch 7537/12000
2024-11-05 02:07:35,559 - INFO - [diffusion][Epoch 7536] diffusion training Loss: 0.056733474135398865
2024-11-05 02:07:35,561 - INFO - [diffusion][Epoch 7536] diffusion learning rate: 0.001
2024-11-05 02:07:35,563 - INFO - [diffusion][Epoch 7536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:35,564 - INFO - [diffusion][Epoch 7537] Epoch 7538/12000
2024-11-05 02:07:39,603 - INFO - [diffusion][Epoch 7537] diffusion training Loss: 0.05848322622478008
2024-11-05 02:07:39,604 - INFO - [diffusion][Epoch 7537] diffusion learning rate: 0.001
2024-11-05 02:07:39,606 - INFO - [diffusion][Epoch 7537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:39,608 - INFO - [diffusion][Epoch 7538] Epoch 7539/12000
2024-11-05 02:07:43,739 - INFO - [diffusion][Epoch 7538] diffusion training Loss: 0.05943744815886021
2024-11-05 02:07:43,742 - INFO - [diffusion][Epoch 7538] diffusion learning rate: 0.001
2024-11-05 02:07:43,743 - INFO - [diffusion][Epoch 7538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:43,745 - INFO - [diffusion][Epoch 7539] Epoch 7540/12000
2024-11-05 02:07:47,832 - INFO - [diffusion][Epoch 7539] diffusion training Loss: 0.05214161146432161
2024-11-05 02:07:47,834 - INFO - [diffusion][Epoch 7539] diffusion learning rate: 0.001
2024-11-05 02:07:47,862 - INFO - [diffusion][Epoch 7539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:47,863 - INFO - [diffusion][Epoch 7540] Epoch 7541/12000
2024-11-05 02:07:51,948 - INFO - [diffusion][Epoch 7540] diffusion training Loss: 0.0554902758449316
2024-11-05 02:07:51,950 - INFO - [diffusion][Epoch 7540] diffusion learning rate: 0.001
2024-11-05 02:07:51,952 - INFO - [diffusion][Epoch 7540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:51,953 - INFO - [diffusion][Epoch 7541] Epoch 7542/12000
2024-11-05 02:07:56,097 - INFO - [diffusion][Epoch 7541] diffusion training Loss: 0.05486610718071461
2024-11-05 02:07:56,099 - INFO - [diffusion][Epoch 7541] diffusion learning rate: 0.001
2024-11-05 02:07:56,100 - INFO - [diffusion][Epoch 7541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:07:56,102 - INFO - [diffusion][Epoch 7542] Epoch 7543/12000
2024-11-05 02:08:00,200 - INFO - [diffusion][Epoch 7542] diffusion training Loss: 0.04814181197434664
2024-11-05 02:08:00,203 - INFO - [diffusion][Epoch 7542] diffusion learning rate: 0.001
2024-11-05 02:08:00,230 - INFO - [diffusion][Epoch 7542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:00,233 - INFO - [diffusion][Epoch 7543] Epoch 7544/12000
2024-11-05 02:08:04,237 - INFO - [diffusion][Epoch 7543] diffusion training Loss: 0.05397649481892586
2024-11-05 02:08:04,239 - INFO - [diffusion][Epoch 7543] diffusion learning rate: 0.001
2024-11-05 02:08:04,241 - INFO - [diffusion][Epoch 7543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:04,242 - INFO - [diffusion][Epoch 7544] Epoch 7545/12000
2024-11-05 02:08:08,405 - INFO - [diffusion][Epoch 7544] diffusion training Loss: 0.055434051901102066
2024-11-05 02:08:08,407 - INFO - [diffusion][Epoch 7544] diffusion learning rate: 0.001
2024-11-05 02:08:08,409 - INFO - [diffusion][Epoch 7544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:08,410 - INFO - [diffusion][Epoch 7545] Epoch 7546/12000
2024-11-05 02:08:12,401 - INFO - [diffusion][Epoch 7545] diffusion training Loss: 0.05680665094405413
2024-11-05 02:08:12,403 - INFO - [diffusion][Epoch 7545] diffusion learning rate: 0.001
2024-11-05 02:08:12,405 - INFO - [diffusion][Epoch 7545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:12,407 - INFO - [diffusion][Epoch 7546] Epoch 7547/12000
2024-11-05 02:08:16,415 - INFO - [diffusion][Epoch 7546] diffusion training Loss: 0.053695328533649445
2024-11-05 02:08:16,417 - INFO - [diffusion][Epoch 7546] diffusion learning rate: 0.001
2024-11-05 02:08:16,419 - INFO - [diffusion][Epoch 7546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:16,420 - INFO - [diffusion][Epoch 7547] Epoch 7548/12000
2024-11-05 02:08:20,648 - INFO - [diffusion][Epoch 7547] diffusion training Loss: 0.05732914712280035
2024-11-05 02:08:20,650 - INFO - [diffusion][Epoch 7547] diffusion learning rate: 0.001
2024-11-05 02:08:20,652 - INFO - [diffusion][Epoch 7547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:20,654 - INFO - [diffusion][Epoch 7548] Epoch 7549/12000
2024-11-05 02:08:24,748 - INFO - [diffusion][Epoch 7548] diffusion training Loss: 0.054851370863616467
2024-11-05 02:08:24,750 - INFO - [diffusion][Epoch 7548] diffusion learning rate: 0.001
2024-11-05 02:08:24,752 - INFO - [diffusion][Epoch 7548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:24,753 - INFO - [diffusion][Epoch 7549] Epoch 7550/12000
2024-11-05 02:08:28,945 - INFO - [diffusion][Epoch 7549] diffusion training Loss: 0.05238873511552811
2024-11-05 02:08:28,947 - INFO - [diffusion][Epoch 7549] diffusion learning rate: 0.001
2024-11-05 02:08:28,949 - INFO - [diffusion][Epoch 7549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:28,950 - INFO - [diffusion][Epoch 7550] Epoch 7551/12000
2024-11-05 02:08:33,107 - INFO - [diffusion][Epoch 7550] diffusion training Loss: 0.051724803633987904
2024-11-05 02:08:33,109 - INFO - [diffusion][Epoch 7550] diffusion learning rate: 0.001
2024-11-05 02:08:33,111 - INFO - [diffusion][Epoch 7550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:33,112 - INFO - [diffusion][Epoch 7551] Epoch 7552/12000
2024-11-05 02:08:37,282 - INFO - [diffusion][Epoch 7551] diffusion training Loss: 0.06004172936081886
2024-11-05 02:08:37,285 - INFO - [diffusion][Epoch 7551] diffusion learning rate: 0.001
2024-11-05 02:08:37,286 - INFO - [diffusion][Epoch 7551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:37,288 - INFO - [diffusion][Epoch 7552] Epoch 7553/12000
2024-11-05 02:08:41,450 - INFO - [diffusion][Epoch 7552] diffusion training Loss: 0.05378607753664255
2024-11-05 02:08:41,452 - INFO - [diffusion][Epoch 7552] diffusion learning rate: 0.001
2024-11-05 02:08:41,454 - INFO - [diffusion][Epoch 7552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:41,455 - INFO - [diffusion][Epoch 7553] Epoch 7554/12000
2024-11-05 02:08:45,426 - INFO - [diffusion][Epoch 7553] diffusion training Loss: 0.058055746369063854
2024-11-05 02:08:45,428 - INFO - [diffusion][Epoch 7553] diffusion learning rate: 0.001
2024-11-05 02:08:45,430 - INFO - [diffusion][Epoch 7553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:45,431 - INFO - [diffusion][Epoch 7554] Epoch 7555/12000
2024-11-05 02:08:49,525 - INFO - [diffusion][Epoch 7554] diffusion training Loss: 0.056663088500499725
2024-11-05 02:08:49,527 - INFO - [diffusion][Epoch 7554] diffusion learning rate: 0.001
2024-11-05 02:08:49,529 - INFO - [diffusion][Epoch 7554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:49,530 - INFO - [diffusion][Epoch 7555] Epoch 7556/12000
2024-11-05 02:08:53,448 - INFO - [diffusion][Epoch 7555] diffusion training Loss: 0.04967606998980045
2024-11-05 02:08:53,450 - INFO - [diffusion][Epoch 7555] diffusion learning rate: 0.001
2024-11-05 02:08:53,452 - INFO - [diffusion][Epoch 7555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:53,453 - INFO - [diffusion][Epoch 7556] Epoch 7557/12000
2024-11-05 02:08:57,342 - INFO - [diffusion][Epoch 7556] diffusion training Loss: 0.058368461206555367
2024-11-05 02:08:57,344 - INFO - [diffusion][Epoch 7556] diffusion learning rate: 0.001
2024-11-05 02:08:57,345 - INFO - [diffusion][Epoch 7556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:08:57,347 - INFO - [diffusion][Epoch 7557] Epoch 7558/12000
2024-11-05 02:09:01,481 - INFO - [diffusion][Epoch 7557] diffusion training Loss: 0.05601683631539345
2024-11-05 02:09:01,484 - INFO - [diffusion][Epoch 7557] diffusion learning rate: 0.001
2024-11-05 02:09:01,486 - INFO - [diffusion][Epoch 7557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:01,487 - INFO - [diffusion][Epoch 7558] Epoch 7559/12000
2024-11-05 02:09:05,541 - INFO - [diffusion][Epoch 7558] diffusion training Loss: 0.054437131620943546
2024-11-05 02:09:05,543 - INFO - [diffusion][Epoch 7558] diffusion learning rate: 0.001
2024-11-05 02:09:05,582 - INFO - [diffusion][Epoch 7558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:05,583 - INFO - [diffusion][Epoch 7559] Epoch 7560/12000
2024-11-05 02:09:09,664 - INFO - [diffusion][Epoch 7559] diffusion training Loss: 0.05756604950875044
2024-11-05 02:09:09,666 - INFO - [diffusion][Epoch 7559] diffusion learning rate: 0.001
2024-11-05 02:09:09,667 - INFO - [diffusion][Epoch 7559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:09,669 - INFO - [diffusion][Epoch 7560] Epoch 7561/12000
2024-11-05 02:09:13,771 - INFO - [diffusion][Epoch 7560] diffusion training Loss: 0.0557722682133317
2024-11-05 02:09:13,774 - INFO - [diffusion][Epoch 7560] diffusion learning rate: 0.001
2024-11-05 02:09:13,776 - INFO - [diffusion][Epoch 7560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:13,777 - INFO - [diffusion][Epoch 7561] Epoch 7562/12000
2024-11-05 02:09:17,896 - INFO - [diffusion][Epoch 7561] diffusion training Loss: 0.05496617220342159
2024-11-05 02:09:17,898 - INFO - [diffusion][Epoch 7561] diffusion learning rate: 0.001
2024-11-05 02:09:17,899 - INFO - [diffusion][Epoch 7561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:17,901 - INFO - [diffusion][Epoch 7562] Epoch 7563/12000
2024-11-05 02:09:21,957 - INFO - [diffusion][Epoch 7562] diffusion training Loss: 0.05595205258578062
2024-11-05 02:09:21,959 - INFO - [diffusion][Epoch 7562] diffusion learning rate: 0.001
2024-11-05 02:09:21,961 - INFO - [diffusion][Epoch 7562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:21,962 - INFO - [diffusion][Epoch 7563] Epoch 7564/12000
2024-11-05 02:09:25,995 - INFO - [diffusion][Epoch 7563] diffusion training Loss: 0.0544429300352931
2024-11-05 02:09:25,997 - INFO - [diffusion][Epoch 7563] diffusion learning rate: 0.001
2024-11-05 02:09:25,999 - INFO - [diffusion][Epoch 7563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:26,001 - INFO - [diffusion][Epoch 7564] Epoch 7565/12000
2024-11-05 02:09:30,139 - INFO - [diffusion][Epoch 7564] diffusion training Loss: 0.05908176116645336
2024-11-05 02:09:30,141 - INFO - [diffusion][Epoch 7564] diffusion learning rate: 0.001
2024-11-05 02:09:30,143 - INFO - [diffusion][Epoch 7564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:30,144 - INFO - [diffusion][Epoch 7565] Epoch 7566/12000
2024-11-05 02:09:34,160 - INFO - [diffusion][Epoch 7565] diffusion training Loss: 0.0569754708558321
2024-11-05 02:09:34,162 - INFO - [diffusion][Epoch 7565] diffusion learning rate: 0.001
2024-11-05 02:09:34,164 - INFO - [diffusion][Epoch 7565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:34,165 - INFO - [diffusion][Epoch 7566] Epoch 7567/12000
2024-11-05 02:09:38,305 - INFO - [diffusion][Epoch 7566] diffusion training Loss: 0.060570359230041504
2024-11-05 02:09:38,307 - INFO - [diffusion][Epoch 7566] diffusion learning rate: 0.001
2024-11-05 02:09:38,327 - INFO - [diffusion][Epoch 7566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:38,328 - INFO - [diffusion][Epoch 7567] Epoch 7568/12000
2024-11-05 02:09:42,407 - INFO - [diffusion][Epoch 7567] diffusion training Loss: 0.05532020889222622
2024-11-05 02:09:42,410 - INFO - [diffusion][Epoch 7567] diffusion learning rate: 0.001
2024-11-05 02:09:42,412 - INFO - [diffusion][Epoch 7567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:42,413 - INFO - [diffusion][Epoch 7568] Epoch 7569/12000
2024-11-05 02:09:46,414 - INFO - [diffusion][Epoch 7568] diffusion training Loss: 0.05159658379852772
2024-11-05 02:09:46,417 - INFO - [diffusion][Epoch 7568] diffusion learning rate: 0.001
2024-11-05 02:09:46,418 - INFO - [diffusion][Epoch 7568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:46,420 - INFO - [diffusion][Epoch 7569] Epoch 7570/12000
2024-11-05 02:09:50,389 - INFO - [diffusion][Epoch 7569] diffusion training Loss: 0.05661414284259081
2024-11-05 02:09:50,391 - INFO - [diffusion][Epoch 7569] diffusion learning rate: 0.001
2024-11-05 02:09:50,393 - INFO - [diffusion][Epoch 7569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:50,394 - INFO - [diffusion][Epoch 7570] Epoch 7571/12000
2024-11-05 02:09:54,584 - INFO - [diffusion][Epoch 7570] diffusion training Loss: 0.05401342548429966
2024-11-05 02:09:54,586 - INFO - [diffusion][Epoch 7570] diffusion learning rate: 0.001
2024-11-05 02:09:54,588 - INFO - [diffusion][Epoch 7570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:54,590 - INFO - [diffusion][Epoch 7571] Epoch 7572/12000
2024-11-05 02:09:58,680 - INFO - [diffusion][Epoch 7571] diffusion training Loss: 0.05242591071873903
2024-11-05 02:09:58,682 - INFO - [diffusion][Epoch 7571] diffusion learning rate: 0.001
2024-11-05 02:09:58,684 - INFO - [diffusion][Epoch 7571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:09:58,685 - INFO - [diffusion][Epoch 7572] Epoch 7573/12000
2024-11-05 02:10:03,082 - INFO - [diffusion][Epoch 7572] diffusion training Loss: 0.0586458882316947
2024-11-05 02:10:03,085 - INFO - [diffusion][Epoch 7572] diffusion learning rate: 0.001
2024-11-05 02:10:03,086 - INFO - [diffusion][Epoch 7572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:03,088 - INFO - [diffusion][Epoch 7573] Epoch 7574/12000
2024-11-05 02:10:07,190 - INFO - [diffusion][Epoch 7573] diffusion training Loss: 0.05258474312722683
2024-11-05 02:10:07,192 - INFO - [diffusion][Epoch 7573] diffusion learning rate: 0.001
2024-11-05 02:10:07,194 - INFO - [diffusion][Epoch 7573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:07,195 - INFO - [diffusion][Epoch 7574] Epoch 7575/12000
2024-11-05 02:10:11,219 - INFO - [diffusion][Epoch 7574] diffusion training Loss: 0.06273103225976229
2024-11-05 02:10:11,221 - INFO - [diffusion][Epoch 7574] diffusion learning rate: 0.001
2024-11-05 02:10:11,223 - INFO - [diffusion][Epoch 7574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:11,224 - INFO - [diffusion][Epoch 7575] Epoch 7576/12000
2024-11-05 02:10:15,263 - INFO - [diffusion][Epoch 7575] diffusion training Loss: 0.05626142956316471
2024-11-05 02:10:15,265 - INFO - [diffusion][Epoch 7575] diffusion learning rate: 0.001
2024-11-05 02:10:15,267 - INFO - [diffusion][Epoch 7575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:15,268 - INFO - [diffusion][Epoch 7576] Epoch 7577/12000
2024-11-05 02:10:19,377 - INFO - [diffusion][Epoch 7576] diffusion training Loss: 0.05429730378091335
2024-11-05 02:10:19,379 - INFO - [diffusion][Epoch 7576] diffusion learning rate: 0.001
2024-11-05 02:10:19,381 - INFO - [diffusion][Epoch 7576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:19,382 - INFO - [diffusion][Epoch 7577] Epoch 7578/12000
2024-11-05 02:10:23,337 - INFO - [diffusion][Epoch 7577] diffusion training Loss: 0.06276606302708387
2024-11-05 02:10:23,339 - INFO - [diffusion][Epoch 7577] diffusion learning rate: 0.001
2024-11-05 02:10:23,341 - INFO - [diffusion][Epoch 7577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:23,342 - INFO - [diffusion][Epoch 7578] Epoch 7579/12000
2024-11-05 02:10:27,398 - INFO - [diffusion][Epoch 7578] diffusion training Loss: 0.0575891574844718
2024-11-05 02:10:27,400 - INFO - [diffusion][Epoch 7578] diffusion learning rate: 0.001
2024-11-05 02:10:27,402 - INFO - [diffusion][Epoch 7578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:27,403 - INFO - [diffusion][Epoch 7579] Epoch 7580/12000
2024-11-05 02:10:31,498 - INFO - [diffusion][Epoch 7579] diffusion training Loss: 0.06191211938858032
2024-11-05 02:10:31,500 - INFO - [diffusion][Epoch 7579] diffusion learning rate: 0.001
2024-11-05 02:10:31,501 - INFO - [diffusion][Epoch 7579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:31,503 - INFO - [diffusion][Epoch 7580] Epoch 7581/12000
2024-11-05 02:10:35,574 - INFO - [diffusion][Epoch 7580] diffusion training Loss: 0.0527872359380126
2024-11-05 02:10:35,577 - INFO - [diffusion][Epoch 7580] diffusion learning rate: 0.001
2024-11-05 02:10:35,579 - INFO - [diffusion][Epoch 7580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:35,580 - INFO - [diffusion][Epoch 7581] Epoch 7582/12000
2024-11-05 02:10:39,668 - INFO - [diffusion][Epoch 7581] diffusion training Loss: 0.05667123571038246
2024-11-05 02:10:39,670 - INFO - [diffusion][Epoch 7581] diffusion learning rate: 0.001
2024-11-05 02:10:39,671 - INFO - [diffusion][Epoch 7581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:39,673 - INFO - [diffusion][Epoch 7582] Epoch 7583/12000
2024-11-05 02:10:43,733 - INFO - [diffusion][Epoch 7582] diffusion training Loss: 0.05661205481737852
2024-11-05 02:10:43,736 - INFO - [diffusion][Epoch 7582] diffusion learning rate: 0.001
2024-11-05 02:10:43,738 - INFO - [diffusion][Epoch 7582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:43,740 - INFO - [diffusion][Epoch 7583] Epoch 7584/12000
2024-11-05 02:10:47,783 - INFO - [diffusion][Epoch 7583] diffusion training Loss: 0.055565303191542625
2024-11-05 02:10:47,785 - INFO - [diffusion][Epoch 7583] diffusion learning rate: 0.001
2024-11-05 02:10:47,787 - INFO - [diffusion][Epoch 7583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:47,788 - INFO - [diffusion][Epoch 7584] Epoch 7585/12000
2024-11-05 02:10:51,689 - INFO - [diffusion][Epoch 7584] diffusion training Loss: 0.05725132580846548
2024-11-05 02:10:51,690 - INFO - [diffusion][Epoch 7584] diffusion learning rate: 0.001
2024-11-05 02:10:51,692 - INFO - [diffusion][Epoch 7584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:51,693 - INFO - [diffusion][Epoch 7585] Epoch 7586/12000
2024-11-05 02:10:55,767 - INFO - [diffusion][Epoch 7585] diffusion training Loss: 0.05378996767103672
2024-11-05 02:10:55,769 - INFO - [diffusion][Epoch 7585] diffusion learning rate: 0.001
2024-11-05 02:10:55,771 - INFO - [diffusion][Epoch 7585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:55,772 - INFO - [diffusion][Epoch 7586] Epoch 7587/12000
2024-11-05 02:10:59,710 - INFO - [diffusion][Epoch 7586] diffusion training Loss: 0.05254777707159519
2024-11-05 02:10:59,712 - INFO - [diffusion][Epoch 7586] diffusion learning rate: 0.001
2024-11-05 02:10:59,754 - INFO - [diffusion][Epoch 7586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:10:59,755 - INFO - [diffusion][Epoch 7587] Epoch 7588/12000
2024-11-05 02:11:03,835 - INFO - [diffusion][Epoch 7587] diffusion training Loss: 0.05172654241323471
2024-11-05 02:11:03,838 - INFO - [diffusion][Epoch 7587] diffusion learning rate: 0.001
2024-11-05 02:11:03,840 - INFO - [diffusion][Epoch 7587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:03,841 - INFO - [diffusion][Epoch 7588] Epoch 7589/12000
2024-11-05 02:11:07,943 - INFO - [diffusion][Epoch 7588] diffusion training Loss: 0.04830482415854931
2024-11-05 02:11:07,945 - INFO - [diffusion][Epoch 7588] diffusion learning rate: 0.001
2024-11-05 02:11:07,947 - INFO - [diffusion][Epoch 7588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:07,948 - INFO - [diffusion][Epoch 7589] Epoch 7590/12000
2024-11-05 02:11:12,002 - INFO - [diffusion][Epoch 7589] diffusion training Loss: 0.05302552320063114
2024-11-05 02:11:12,004 - INFO - [diffusion][Epoch 7589] diffusion learning rate: 0.001
2024-11-05 02:11:12,006 - INFO - [diffusion][Epoch 7589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:12,007 - INFO - [diffusion][Epoch 7590] Epoch 7591/12000
2024-11-05 02:11:16,051 - INFO - [diffusion][Epoch 7590] diffusion training Loss: 0.05870420951396227
2024-11-05 02:11:16,053 - INFO - [diffusion][Epoch 7590] diffusion learning rate: 0.001
2024-11-05 02:11:16,055 - INFO - [diffusion][Epoch 7590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:16,057 - INFO - [diffusion][Epoch 7591] Epoch 7592/12000
2024-11-05 02:11:20,171 - INFO - [diffusion][Epoch 7591] diffusion training Loss: 0.05135761108249426
2024-11-05 02:11:20,173 - INFO - [diffusion][Epoch 7591] diffusion learning rate: 0.001
2024-11-05 02:11:20,175 - INFO - [diffusion][Epoch 7591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:20,176 - INFO - [diffusion][Epoch 7592] Epoch 7593/12000
2024-11-05 02:11:24,272 - INFO - [diffusion][Epoch 7592] diffusion training Loss: 0.05538247339427471
2024-11-05 02:11:24,274 - INFO - [diffusion][Epoch 7592] diffusion learning rate: 0.001
2024-11-05 02:11:24,276 - INFO - [diffusion][Epoch 7592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:24,277 - INFO - [diffusion][Epoch 7593] Epoch 7594/12000
2024-11-05 02:11:28,335 - INFO - [diffusion][Epoch 7593] diffusion training Loss: 0.05153026897460222
2024-11-05 02:11:28,338 - INFO - [diffusion][Epoch 7593] diffusion learning rate: 0.001
2024-11-05 02:11:28,340 - INFO - [diffusion][Epoch 7593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:28,341 - INFO - [diffusion][Epoch 7594] Epoch 7595/12000
2024-11-05 02:11:32,480 - INFO - [diffusion][Epoch 7594] diffusion training Loss: 0.0607607401907444
2024-11-05 02:11:32,482 - INFO - [diffusion][Epoch 7594] diffusion learning rate: 0.001
2024-11-05 02:11:32,484 - INFO - [diffusion][Epoch 7594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:32,485 - INFO - [diffusion][Epoch 7595] Epoch 7596/12000
2024-11-05 02:11:36,459 - INFO - [diffusion][Epoch 7595] diffusion training Loss: 0.05441016051918268
2024-11-05 02:11:36,462 - INFO - [diffusion][Epoch 7595] diffusion learning rate: 0.001
2024-11-05 02:11:36,463 - INFO - [diffusion][Epoch 7595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:36,465 - INFO - [diffusion][Epoch 7596] Epoch 7597/12000
2024-11-05 02:11:40,549 - INFO - [diffusion][Epoch 7596] diffusion training Loss: 0.055908878333866596
2024-11-05 02:11:40,551 - INFO - [diffusion][Epoch 7596] diffusion learning rate: 0.001
2024-11-05 02:11:40,553 - INFO - [diffusion][Epoch 7596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:40,554 - INFO - [diffusion][Epoch 7597] Epoch 7598/12000
2024-11-05 02:11:44,599 - INFO - [diffusion][Epoch 7597] diffusion training Loss: 0.05563144199550152
2024-11-05 02:11:44,601 - INFO - [diffusion][Epoch 7597] diffusion learning rate: 0.001
2024-11-05 02:11:44,603 - INFO - [diffusion][Epoch 7597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:44,604 - INFO - [diffusion][Epoch 7598] Epoch 7599/12000
2024-11-05 02:11:48,672 - INFO - [diffusion][Epoch 7598] diffusion training Loss: 0.05897959787398577
2024-11-05 02:11:48,674 - INFO - [diffusion][Epoch 7598] diffusion learning rate: 0.001
2024-11-05 02:11:48,704 - INFO - [diffusion][Epoch 7598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:48,705 - INFO - [diffusion][Epoch 7599] Epoch 7600/12000
2024-11-05 02:11:52,764 - INFO - [diffusion][Epoch 7599] diffusion training Loss: 0.05704430490732193
2024-11-05 02:11:52,766 - INFO - [diffusion][Epoch 7599] diffusion learning rate: 0.001
2024-11-05 02:11:52,767 - INFO - [diffusion][Epoch 7599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:52,769 - INFO - [diffusion][Epoch 7600] Epoch 7601/12000
2024-11-05 02:11:56,759 - INFO - [diffusion][Epoch 7600] diffusion training Loss: 0.059303331188857555
2024-11-05 02:11:56,761 - INFO - [diffusion][Epoch 7600] diffusion learning rate: 0.001
2024-11-05 02:11:56,762 - INFO - [diffusion][Epoch 7600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:11:56,764 - INFO - [diffusion][Epoch 7601] Epoch 7602/12000
2024-11-05 02:12:00,593 - INFO - [diffusion][Epoch 7601] diffusion training Loss: 0.059091401286423206
2024-11-05 02:12:00,595 - INFO - [diffusion][Epoch 7601] diffusion learning rate: 0.001
2024-11-05 02:12:00,597 - INFO - [diffusion][Epoch 7601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:00,598 - INFO - [diffusion][Epoch 7602] Epoch 7603/12000
2024-11-05 02:12:04,648 - INFO - [diffusion][Epoch 7602] diffusion training Loss: 0.05915577057749033
2024-11-05 02:12:04,651 - INFO - [diffusion][Epoch 7602] diffusion learning rate: 0.001
2024-11-05 02:12:04,653 - INFO - [diffusion][Epoch 7602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:04,654 - INFO - [diffusion][Epoch 7603] Epoch 7604/12000
2024-11-05 02:12:08,636 - INFO - [diffusion][Epoch 7603] diffusion training Loss: 0.0554964579641819
2024-11-05 02:12:08,638 - INFO - [diffusion][Epoch 7603] diffusion learning rate: 0.001
2024-11-05 02:12:08,640 - INFO - [diffusion][Epoch 7603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:08,641 - INFO - [diffusion][Epoch 7604] Epoch 7605/12000
2024-11-05 02:12:12,666 - INFO - [diffusion][Epoch 7604] diffusion training Loss: 0.05455712787806988
2024-11-05 02:12:12,669 - INFO - [diffusion][Epoch 7604] diffusion learning rate: 0.001
2024-11-05 02:12:12,670 - INFO - [diffusion][Epoch 7604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:12,671 - INFO - [diffusion][Epoch 7605] Epoch 7606/12000
2024-11-05 02:12:16,797 - INFO - [diffusion][Epoch 7605] diffusion training Loss: 0.05542246997356415
2024-11-05 02:12:16,799 - INFO - [diffusion][Epoch 7605] diffusion learning rate: 0.001
2024-11-05 02:12:16,801 - INFO - [diffusion][Epoch 7605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:16,802 - INFO - [diffusion][Epoch 7606] Epoch 7607/12000
2024-11-05 02:12:20,819 - INFO - [diffusion][Epoch 7606] diffusion training Loss: 0.0556236207485199
2024-11-05 02:12:20,821 - INFO - [diffusion][Epoch 7606] diffusion learning rate: 0.001
2024-11-05 02:12:20,823 - INFO - [diffusion][Epoch 7606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:20,824 - INFO - [diffusion][Epoch 7607] Epoch 7608/12000
2024-11-05 02:12:24,756 - INFO - [diffusion][Epoch 7607] diffusion training Loss: 0.05730806943029165
2024-11-05 02:12:24,758 - INFO - [diffusion][Epoch 7607] diffusion learning rate: 0.001
2024-11-05 02:12:24,759 - INFO - [diffusion][Epoch 7607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:24,760 - INFO - [diffusion][Epoch 7608] Epoch 7609/12000
2024-11-05 02:12:28,836 - INFO - [diffusion][Epoch 7608] diffusion training Loss: 0.052685908041894436
2024-11-05 02:12:28,838 - INFO - [diffusion][Epoch 7608] diffusion learning rate: 0.001
2024-11-05 02:12:28,840 - INFO - [diffusion][Epoch 7608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:28,841 - INFO - [diffusion][Epoch 7609] Epoch 7610/12000
2024-11-05 02:12:32,873 - INFO - [diffusion][Epoch 7609] diffusion training Loss: 0.05787523649632931
2024-11-05 02:12:32,874 - INFO - [diffusion][Epoch 7609] diffusion learning rate: 0.001
2024-11-05 02:12:32,876 - INFO - [diffusion][Epoch 7609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:32,878 - INFO - [diffusion][Epoch 7610] Epoch 7611/12000
2024-11-05 02:12:36,976 - INFO - [diffusion][Epoch 7610] diffusion training Loss: 0.05543111637234688
2024-11-05 02:12:36,978 - INFO - [diffusion][Epoch 7610] diffusion learning rate: 0.001
2024-11-05 02:12:36,980 - INFO - [diffusion][Epoch 7610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:36,981 - INFO - [diffusion][Epoch 7611] Epoch 7612/12000
2024-11-05 02:12:41,085 - INFO - [diffusion][Epoch 7611] diffusion training Loss: 0.05504399538040161
2024-11-05 02:12:41,087 - INFO - [diffusion][Epoch 7611] diffusion learning rate: 0.001
2024-11-05 02:12:41,089 - INFO - [diffusion][Epoch 7611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:41,090 - INFO - [diffusion][Epoch 7612] Epoch 7613/12000
2024-11-05 02:12:45,225 - INFO - [diffusion][Epoch 7612] diffusion training Loss: 0.05586471315473318
2024-11-05 02:12:45,227 - INFO - [diffusion][Epoch 7612] diffusion learning rate: 0.001
2024-11-05 02:12:45,228 - INFO - [diffusion][Epoch 7612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:45,230 - INFO - [diffusion][Epoch 7613] Epoch 7614/12000
2024-11-05 02:12:49,374 - INFO - [diffusion][Epoch 7613] diffusion training Loss: 0.05777233652770519
2024-11-05 02:12:49,376 - INFO - [diffusion][Epoch 7613] diffusion learning rate: 0.001
2024-11-05 02:12:49,378 - INFO - [diffusion][Epoch 7613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:49,379 - INFO - [diffusion][Epoch 7614] Epoch 7615/12000
2024-11-05 02:12:53,481 - INFO - [diffusion][Epoch 7614] diffusion training Loss: 0.05678724590688944
2024-11-05 02:12:53,483 - INFO - [diffusion][Epoch 7614] diffusion learning rate: 0.001
2024-11-05 02:12:53,485 - INFO - [diffusion][Epoch 7614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:53,487 - INFO - [diffusion][Epoch 7615] Epoch 7616/12000
2024-11-05 02:12:57,477 - INFO - [diffusion][Epoch 7615] diffusion training Loss: 0.06368487235158682
2024-11-05 02:12:57,479 - INFO - [diffusion][Epoch 7615] diffusion learning rate: 0.001
2024-11-05 02:12:57,481 - INFO - [diffusion][Epoch 7615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:12:57,483 - INFO - [diffusion][Epoch 7616] Epoch 7617/12000
2024-11-05 02:13:01,582 - INFO - [diffusion][Epoch 7616] diffusion training Loss: 0.05742338113486767
2024-11-05 02:13:01,584 - INFO - [diffusion][Epoch 7616] diffusion learning rate: 0.001
2024-11-05 02:13:01,586 - INFO - [diffusion][Epoch 7616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:01,587 - INFO - [diffusion][Epoch 7617] Epoch 7618/12000
2024-11-05 02:13:05,582 - INFO - [diffusion][Epoch 7617] diffusion training Loss: 0.055304888635873795
2024-11-05 02:13:05,585 - INFO - [diffusion][Epoch 7617] diffusion learning rate: 0.001
2024-11-05 02:13:05,587 - INFO - [diffusion][Epoch 7617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:05,588 - INFO - [diffusion][Epoch 7618] Epoch 7619/12000
2024-11-05 02:13:09,708 - INFO - [diffusion][Epoch 7618] diffusion training Loss: 0.05428073834627867
2024-11-05 02:13:09,710 - INFO - [diffusion][Epoch 7618] diffusion learning rate: 0.001
2024-11-05 02:13:09,712 - INFO - [diffusion][Epoch 7618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:09,714 - INFO - [diffusion][Epoch 7619] Epoch 7620/12000
2024-11-05 02:13:13,694 - INFO - [diffusion][Epoch 7619] diffusion training Loss: 0.054357463493943214
2024-11-05 02:13:13,696 - INFO - [diffusion][Epoch 7619] diffusion learning rate: 0.001
2024-11-05 02:13:13,698 - INFO - [diffusion][Epoch 7619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:13,699 - INFO - [diffusion][Epoch 7620] Epoch 7621/12000
2024-11-05 02:13:17,716 - INFO - [diffusion][Epoch 7620] diffusion training Loss: 0.060667469166219234
2024-11-05 02:13:17,719 - INFO - [diffusion][Epoch 7620] diffusion learning rate: 0.001
2024-11-05 02:13:17,721 - INFO - [diffusion][Epoch 7620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:17,722 - INFO - [diffusion][Epoch 7621] Epoch 7622/12000
2024-11-05 02:13:21,890 - INFO - [diffusion][Epoch 7621] diffusion training Loss: 0.05405987519770861
2024-11-05 02:13:21,892 - INFO - [diffusion][Epoch 7621] diffusion learning rate: 0.001
2024-11-05 02:13:21,894 - INFO - [diffusion][Epoch 7621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:21,895 - INFO - [diffusion][Epoch 7622] Epoch 7623/12000
2024-11-05 02:13:26,041 - INFO - [diffusion][Epoch 7622] diffusion training Loss: 0.055026852525770664
2024-11-05 02:13:26,044 - INFO - [diffusion][Epoch 7622] diffusion learning rate: 0.001
2024-11-05 02:13:26,046 - INFO - [diffusion][Epoch 7622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:26,047 - INFO - [diffusion][Epoch 7623] Epoch 7624/12000
2024-11-05 02:13:30,028 - INFO - [diffusion][Epoch 7623] diffusion training Loss: 0.05508990306407213
2024-11-05 02:13:30,030 - INFO - [diffusion][Epoch 7623] diffusion learning rate: 0.001
2024-11-05 02:13:30,031 - INFO - [diffusion][Epoch 7623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:30,033 - INFO - [diffusion][Epoch 7624] Epoch 7625/12000
2024-11-05 02:13:33,957 - INFO - [diffusion][Epoch 7624] diffusion training Loss: 0.057926807552576065
2024-11-05 02:13:33,960 - INFO - [diffusion][Epoch 7624] diffusion learning rate: 0.001
2024-11-05 02:13:33,961 - INFO - [diffusion][Epoch 7624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:33,963 - INFO - [diffusion][Epoch 7625] Epoch 7626/12000
2024-11-05 02:13:38,066 - INFO - [diffusion][Epoch 7625] diffusion training Loss: 0.057173057459294796
2024-11-05 02:13:38,069 - INFO - [diffusion][Epoch 7625] diffusion learning rate: 0.001
2024-11-05 02:13:38,071 - INFO - [diffusion][Epoch 7625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:38,072 - INFO - [diffusion][Epoch 7626] Epoch 7627/12000
2024-11-05 02:13:42,100 - INFO - [diffusion][Epoch 7626] diffusion training Loss: 0.05899322498589754
2024-11-05 02:13:42,102 - INFO - [diffusion][Epoch 7626] diffusion learning rate: 0.001
2024-11-05 02:13:42,124 - INFO - [diffusion][Epoch 7626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:42,126 - INFO - [diffusion][Epoch 7627] Epoch 7628/12000
2024-11-05 02:13:46,132 - INFO - [diffusion][Epoch 7627] diffusion training Loss: 0.05384328402578831
2024-11-05 02:13:46,134 - INFO - [diffusion][Epoch 7627] diffusion learning rate: 0.001
2024-11-05 02:13:46,136 - INFO - [diffusion][Epoch 7627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:46,138 - INFO - [diffusion][Epoch 7628] Epoch 7629/12000
2024-11-05 02:13:50,063 - INFO - [diffusion][Epoch 7628] diffusion training Loss: 0.05484389141201973
2024-11-05 02:13:50,065 - INFO - [diffusion][Epoch 7628] diffusion learning rate: 0.001
2024-11-05 02:13:50,067 - INFO - [diffusion][Epoch 7628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:50,068 - INFO - [diffusion][Epoch 7629] Epoch 7630/12000
2024-11-05 02:13:54,219 - INFO - [diffusion][Epoch 7629] diffusion training Loss: 0.058003867976367474
2024-11-05 02:13:54,221 - INFO - [diffusion][Epoch 7629] diffusion learning rate: 0.001
2024-11-05 02:13:54,222 - INFO - [diffusion][Epoch 7629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:54,224 - INFO - [diffusion][Epoch 7630] Epoch 7631/12000
2024-11-05 02:13:58,342 - INFO - [diffusion][Epoch 7630] diffusion training Loss: 0.05398898106068373
2024-11-05 02:13:58,345 - INFO - [diffusion][Epoch 7630] diffusion learning rate: 0.001
2024-11-05 02:13:58,347 - INFO - [diffusion][Epoch 7630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:13:58,348 - INFO - [diffusion][Epoch 7631] Epoch 7632/12000
2024-11-05 02:14:02,358 - INFO - [diffusion][Epoch 7631] diffusion training Loss: 0.05450609978288412
2024-11-05 02:14:02,360 - INFO - [diffusion][Epoch 7631] diffusion learning rate: 0.001
2024-11-05 02:14:02,362 - INFO - [diffusion][Epoch 7631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:02,364 - INFO - [diffusion][Epoch 7632] Epoch 7633/12000
2024-11-05 02:14:06,269 - INFO - [diffusion][Epoch 7632] diffusion training Loss: 0.05936506483703852
2024-11-05 02:14:06,272 - INFO - [diffusion][Epoch 7632] diffusion learning rate: 0.001
2024-11-05 02:14:06,274 - INFO - [diffusion][Epoch 7632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:06,275 - INFO - [diffusion][Epoch 7633] Epoch 7634/12000
2024-11-05 02:14:10,400 - INFO - [diffusion][Epoch 7633] diffusion training Loss: 0.05500948801636696
2024-11-05 02:14:10,402 - INFO - [diffusion][Epoch 7633] diffusion learning rate: 0.001
2024-11-05 02:14:10,404 - INFO - [diffusion][Epoch 7633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:10,406 - INFO - [diffusion][Epoch 7634] Epoch 7635/12000
2024-11-05 02:14:15,199 - INFO - [diffusion][Epoch 7634] diffusion training Loss: 0.05916620325297117
2024-11-05 02:14:15,202 - INFO - [diffusion][Epoch 7634] diffusion learning rate: 0.001
2024-11-05 02:14:15,203 - INFO - [diffusion][Epoch 7634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:15,205 - INFO - [diffusion][Epoch 7635] Epoch 7636/12000
2024-11-05 02:14:19,305 - INFO - [diffusion][Epoch 7635] diffusion training Loss: 0.059985287487506866
2024-11-05 02:14:19,307 - INFO - [diffusion][Epoch 7635] diffusion learning rate: 0.001
2024-11-05 02:14:19,308 - INFO - [diffusion][Epoch 7635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:19,310 - INFO - [diffusion][Epoch 7636] Epoch 7637/12000
2024-11-05 02:14:23,261 - INFO - [diffusion][Epoch 7636] diffusion training Loss: 0.04805745370686054
2024-11-05 02:14:23,263 - INFO - [diffusion][Epoch 7636] diffusion learning rate: 0.001
2024-11-05 02:14:23,265 - INFO - [diffusion][Epoch 7636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:23,266 - INFO - [diffusion][Epoch 7637] Epoch 7638/12000
2024-11-05 02:14:27,139 - INFO - [diffusion][Epoch 7637] diffusion training Loss: 0.05959901958703995
2024-11-05 02:14:27,141 - INFO - [diffusion][Epoch 7637] diffusion learning rate: 0.001
2024-11-05 02:14:27,143 - INFO - [diffusion][Epoch 7637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:27,144 - INFO - [diffusion][Epoch 7638] Epoch 7639/12000
2024-11-05 02:14:31,294 - INFO - [diffusion][Epoch 7638] diffusion training Loss: 0.05545484460890293
2024-11-05 02:14:31,295 - INFO - [diffusion][Epoch 7638] diffusion learning rate: 0.001
2024-11-05 02:14:31,297 - INFO - [diffusion][Epoch 7638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:31,298 - INFO - [diffusion][Epoch 7639] Epoch 7640/12000
2024-11-05 02:14:35,358 - INFO - [diffusion][Epoch 7639] diffusion training Loss: 0.05853448435664177
2024-11-05 02:14:35,361 - INFO - [diffusion][Epoch 7639] diffusion learning rate: 0.001
2024-11-05 02:14:35,362 - INFO - [diffusion][Epoch 7639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:35,364 - INFO - [diffusion][Epoch 7640] Epoch 7641/12000
2024-11-05 02:14:39,472 - INFO - [diffusion][Epoch 7640] diffusion training Loss: 0.06136493105441332
2024-11-05 02:14:39,474 - INFO - [diffusion][Epoch 7640] diffusion learning rate: 0.001
2024-11-05 02:14:39,476 - INFO - [diffusion][Epoch 7640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:39,477 - INFO - [diffusion][Epoch 7641] Epoch 7642/12000
2024-11-05 02:14:43,415 - INFO - [diffusion][Epoch 7641] diffusion training Loss: 0.056485273875296116
2024-11-05 02:14:43,417 - INFO - [diffusion][Epoch 7641] diffusion learning rate: 0.001
2024-11-05 02:14:43,419 - INFO - [diffusion][Epoch 7641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:43,421 - INFO - [diffusion][Epoch 7642] Epoch 7643/12000
2024-11-05 02:14:47,514 - INFO - [diffusion][Epoch 7642] diffusion training Loss: 0.05816542077809572
2024-11-05 02:14:47,516 - INFO - [diffusion][Epoch 7642] diffusion learning rate: 0.001
2024-11-05 02:14:47,518 - INFO - [diffusion][Epoch 7642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:47,520 - INFO - [diffusion][Epoch 7643] Epoch 7644/12000
2024-11-05 02:14:51,582 - INFO - [diffusion][Epoch 7643] diffusion training Loss: 0.054017567075788975
2024-11-05 02:14:51,584 - INFO - [diffusion][Epoch 7643] diffusion learning rate: 0.001
2024-11-05 02:14:51,585 - INFO - [diffusion][Epoch 7643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:51,587 - INFO - [diffusion][Epoch 7644] Epoch 7645/12000
2024-11-05 02:14:55,660 - INFO - [diffusion][Epoch 7644] diffusion training Loss: 0.059481593780219555
2024-11-05 02:14:55,662 - INFO - [diffusion][Epoch 7644] diffusion learning rate: 0.001
2024-11-05 02:14:55,664 - INFO - [diffusion][Epoch 7644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:55,665 - INFO - [diffusion][Epoch 7645] Epoch 7646/12000
2024-11-05 02:14:59,557 - INFO - [diffusion][Epoch 7645] diffusion training Loss: 0.056860510259866714
2024-11-05 02:14:59,566 - INFO - [diffusion][Epoch 7645] diffusion learning rate: 0.001
2024-11-05 02:14:59,568 - INFO - [diffusion][Epoch 7645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:14:59,569 - INFO - [diffusion][Epoch 7646] Epoch 7647/12000
2024-11-05 02:15:03,569 - INFO - [diffusion][Epoch 7646] diffusion training Loss: 0.05691920593380928
2024-11-05 02:15:03,571 - INFO - [diffusion][Epoch 7646] diffusion learning rate: 0.001
2024-11-05 02:15:03,573 - INFO - [diffusion][Epoch 7646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:03,574 - INFO - [diffusion][Epoch 7647] Epoch 7648/12000
2024-11-05 02:15:07,697 - INFO - [diffusion][Epoch 7647] diffusion training Loss: 0.05335388332605362
2024-11-05 02:15:07,700 - INFO - [diffusion][Epoch 7647] diffusion learning rate: 0.001
2024-11-05 02:15:07,701 - INFO - [diffusion][Epoch 7647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:07,703 - INFO - [diffusion][Epoch 7648] Epoch 7649/12000
2024-11-05 02:15:11,875 - INFO - [diffusion][Epoch 7648] diffusion training Loss: 0.0692304726690054
2024-11-05 02:15:11,877 - INFO - [diffusion][Epoch 7648] diffusion learning rate: 0.001
2024-11-05 02:15:11,879 - INFO - [diffusion][Epoch 7648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:11,880 - INFO - [diffusion][Epoch 7649] Epoch 7650/12000
2024-11-05 02:15:15,988 - INFO - [diffusion][Epoch 7649] diffusion training Loss: 0.05270737037062645
2024-11-05 02:15:15,990 - INFO - [diffusion][Epoch 7649] diffusion learning rate: 0.001
2024-11-05 02:15:15,991 - INFO - [diffusion][Epoch 7649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:15,993 - INFO - [diffusion][Epoch 7650] Epoch 7651/12000
2024-11-05 02:15:20,092 - INFO - [diffusion][Epoch 7650] diffusion training Loss: 0.05400298535823822
2024-11-05 02:15:20,094 - INFO - [diffusion][Epoch 7650] diffusion learning rate: 0.001
2024-11-05 02:15:20,096 - INFO - [diffusion][Epoch 7650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:20,097 - INFO - [diffusion][Epoch 7651] Epoch 7652/12000
2024-11-05 02:15:24,234 - INFO - [diffusion][Epoch 7651] diffusion training Loss: 0.055957186967134476
2024-11-05 02:15:24,236 - INFO - [diffusion][Epoch 7651] diffusion learning rate: 0.001
2024-11-05 02:15:24,238 - INFO - [diffusion][Epoch 7651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:24,239 - INFO - [diffusion][Epoch 7652] Epoch 7653/12000
2024-11-05 02:15:28,380 - INFO - [diffusion][Epoch 7652] diffusion training Loss: 0.05160544067621231
2024-11-05 02:15:28,382 - INFO - [diffusion][Epoch 7652] diffusion learning rate: 0.001
2024-11-05 02:15:28,384 - INFO - [diffusion][Epoch 7652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:28,386 - INFO - [diffusion][Epoch 7653] Epoch 7654/12000
2024-11-05 02:15:32,789 - INFO - [diffusion][Epoch 7653] diffusion training Loss: 0.05853935144841671
2024-11-05 02:15:32,791 - INFO - [diffusion][Epoch 7653] diffusion learning rate: 0.001
2024-11-05 02:15:32,793 - INFO - [diffusion][Epoch 7653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:32,795 - INFO - [diffusion][Epoch 7654] Epoch 7655/12000
2024-11-05 02:15:36,852 - INFO - [diffusion][Epoch 7654] diffusion training Loss: 0.05505827907472849
2024-11-05 02:15:36,854 - INFO - [diffusion][Epoch 7654] diffusion learning rate: 0.001
2024-11-05 02:15:36,882 - INFO - [diffusion][Epoch 7654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:36,883 - INFO - [diffusion][Epoch 7655] Epoch 7656/12000
2024-11-05 02:15:40,978 - INFO - [diffusion][Epoch 7655] diffusion training Loss: 0.06033098232001066
2024-11-05 02:15:40,980 - INFO - [diffusion][Epoch 7655] diffusion learning rate: 0.001
2024-11-05 02:15:40,982 - INFO - [diffusion][Epoch 7655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:40,983 - INFO - [diffusion][Epoch 7656] Epoch 7657/12000
2024-11-05 02:15:45,070 - INFO - [diffusion][Epoch 7656] diffusion training Loss: 0.05667924601584673
2024-11-05 02:15:45,072 - INFO - [diffusion][Epoch 7656] diffusion learning rate: 0.001
2024-11-05 02:15:45,074 - INFO - [diffusion][Epoch 7656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:45,075 - INFO - [diffusion][Epoch 7657] Epoch 7658/12000
2024-11-05 02:15:49,204 - INFO - [diffusion][Epoch 7657] diffusion training Loss: 0.05463862046599388
2024-11-05 02:15:49,206 - INFO - [diffusion][Epoch 7657] diffusion learning rate: 0.001
2024-11-05 02:15:49,208 - INFO - [diffusion][Epoch 7657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:49,209 - INFO - [diffusion][Epoch 7658] Epoch 7659/12000
2024-11-05 02:15:53,253 - INFO - [diffusion][Epoch 7658] diffusion training Loss: 0.05769328400492668
2024-11-05 02:15:53,255 - INFO - [diffusion][Epoch 7658] diffusion learning rate: 0.001
2024-11-05 02:15:53,258 - INFO - [diffusion][Epoch 7658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:53,259 - INFO - [diffusion][Epoch 7659] Epoch 7660/12000
2024-11-05 02:15:57,307 - INFO - [diffusion][Epoch 7659] diffusion training Loss: 0.05772078689187765
2024-11-05 02:15:57,309 - INFO - [diffusion][Epoch 7659] diffusion learning rate: 0.001
2024-11-05 02:15:57,311 - INFO - [diffusion][Epoch 7659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:15:57,312 - INFO - [diffusion][Epoch 7660] Epoch 7661/12000
2024-11-05 02:16:01,338 - INFO - [diffusion][Epoch 7660] diffusion training Loss: 0.05632086470723152
2024-11-05 02:16:01,340 - INFO - [diffusion][Epoch 7660] diffusion learning rate: 0.001
2024-11-05 02:16:01,341 - INFO - [diffusion][Epoch 7660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:01,343 - INFO - [diffusion][Epoch 7661] Epoch 7662/12000
2024-11-05 02:16:05,295 - INFO - [diffusion][Epoch 7661] diffusion training Loss: 0.06255136709660292
2024-11-05 02:16:05,297 - INFO - [diffusion][Epoch 7661] diffusion learning rate: 0.001
2024-11-05 02:16:05,299 - INFO - [diffusion][Epoch 7661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:05,300 - INFO - [diffusion][Epoch 7662] Epoch 7663/12000
2024-11-05 02:16:09,332 - INFO - [diffusion][Epoch 7662] diffusion training Loss: 0.05292102228850126
2024-11-05 02:16:09,335 - INFO - [diffusion][Epoch 7662] diffusion learning rate: 0.001
2024-11-05 02:16:09,337 - INFO - [diffusion][Epoch 7662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:09,338 - INFO - [diffusion][Epoch 7663] Epoch 7664/12000
2024-11-05 02:16:13,401 - INFO - [diffusion][Epoch 7663] diffusion training Loss: 0.05499439500272274
2024-11-05 02:16:13,403 - INFO - [diffusion][Epoch 7663] diffusion learning rate: 0.001
2024-11-05 02:16:13,405 - INFO - [diffusion][Epoch 7663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:13,406 - INFO - [diffusion][Epoch 7664] Epoch 7665/12000
2024-11-05 02:16:17,408 - INFO - [diffusion][Epoch 7664] diffusion training Loss: 0.05602641496807337
2024-11-05 02:16:17,410 - INFO - [diffusion][Epoch 7664] diffusion learning rate: 0.001
2024-11-05 02:16:17,412 - INFO - [diffusion][Epoch 7664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:17,413 - INFO - [diffusion][Epoch 7665] Epoch 7666/12000
2024-11-05 02:16:21,500 - INFO - [diffusion][Epoch 7665] diffusion training Loss: 0.061415236443281174
2024-11-05 02:16:21,502 - INFO - [diffusion][Epoch 7665] diffusion learning rate: 0.001
2024-11-05 02:16:21,504 - INFO - [diffusion][Epoch 7665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:21,505 - INFO - [diffusion][Epoch 7666] Epoch 7667/12000
2024-11-05 02:16:25,647 - INFO - [diffusion][Epoch 7666] diffusion training Loss: 0.06125862058252096
2024-11-05 02:16:25,649 - INFO - [diffusion][Epoch 7666] diffusion learning rate: 0.001
2024-11-05 02:16:25,651 - INFO - [diffusion][Epoch 7666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:25,652 - INFO - [diffusion][Epoch 7667] Epoch 7668/12000
2024-11-05 02:16:29,604 - INFO - [diffusion][Epoch 7667] diffusion training Loss: 0.05475948564708233
2024-11-05 02:16:29,606 - INFO - [diffusion][Epoch 7667] diffusion learning rate: 0.001
2024-11-05 02:16:29,607 - INFO - [diffusion][Epoch 7667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:29,608 - INFO - [diffusion][Epoch 7668] Epoch 7669/12000
2024-11-05 02:16:33,716 - INFO - [diffusion][Epoch 7668] diffusion training Loss: 0.054517634212970734
2024-11-05 02:16:33,718 - INFO - [diffusion][Epoch 7668] diffusion learning rate: 0.001
2024-11-05 02:16:33,720 - INFO - [diffusion][Epoch 7668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:33,721 - INFO - [diffusion][Epoch 7669] Epoch 7670/12000
2024-11-05 02:16:37,684 - INFO - [diffusion][Epoch 7669] diffusion training Loss: 0.05834271386265755
2024-11-05 02:16:37,686 - INFO - [diffusion][Epoch 7669] diffusion learning rate: 0.001
2024-11-05 02:16:37,688 - INFO - [diffusion][Epoch 7669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:37,690 - INFO - [diffusion][Epoch 7670] Epoch 7671/12000
2024-11-05 02:16:41,727 - INFO - [diffusion][Epoch 7670] diffusion training Loss: 0.052169375121593475
2024-11-05 02:16:41,729 - INFO - [diffusion][Epoch 7670] diffusion learning rate: 0.001
2024-11-05 02:16:41,731 - INFO - [diffusion][Epoch 7670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:41,732 - INFO - [diffusion][Epoch 7671] Epoch 7672/12000
2024-11-05 02:16:45,854 - INFO - [diffusion][Epoch 7671] diffusion training Loss: 0.05323699302971363
2024-11-05 02:16:45,856 - INFO - [diffusion][Epoch 7671] diffusion learning rate: 0.001
2024-11-05 02:16:45,858 - INFO - [diffusion][Epoch 7671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:45,859 - INFO - [diffusion][Epoch 7672] Epoch 7673/12000
2024-11-05 02:16:50,028 - INFO - [diffusion][Epoch 7672] diffusion training Loss: 0.056489452719688416
2024-11-05 02:16:50,030 - INFO - [diffusion][Epoch 7672] diffusion learning rate: 0.001
2024-11-05 02:16:50,032 - INFO - [diffusion][Epoch 7672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:50,033 - INFO - [diffusion][Epoch 7673] Epoch 7674/12000
2024-11-05 02:16:54,300 - INFO - [diffusion][Epoch 7673] diffusion training Loss: 0.05306955240666866
2024-11-05 02:16:54,302 - INFO - [diffusion][Epoch 7673] diffusion learning rate: 0.001
2024-11-05 02:16:54,303 - INFO - [diffusion][Epoch 7673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:54,305 - INFO - [diffusion][Epoch 7674] Epoch 7675/12000
2024-11-05 02:16:58,372 - INFO - [diffusion][Epoch 7674] diffusion training Loss: 0.049955506809055805
2024-11-05 02:16:58,374 - INFO - [diffusion][Epoch 7674] diffusion learning rate: 0.001
2024-11-05 02:16:58,375 - INFO - [diffusion][Epoch 7674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:16:58,377 - INFO - [diffusion][Epoch 7675] Epoch 7676/12000
2024-11-05 02:17:02,483 - INFO - [diffusion][Epoch 7675] diffusion training Loss: 0.050789239816367626
2024-11-05 02:17:02,486 - INFO - [diffusion][Epoch 7675] diffusion learning rate: 0.001
2024-11-05 02:17:02,513 - INFO - [diffusion][Epoch 7675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:02,515 - INFO - [diffusion][Epoch 7676] Epoch 7677/12000
2024-11-05 02:17:06,554 - INFO - [diffusion][Epoch 7676] diffusion training Loss: 0.05386145133525133
2024-11-05 02:17:06,556 - INFO - [diffusion][Epoch 7676] diffusion learning rate: 0.001
2024-11-05 02:17:06,558 - INFO - [diffusion][Epoch 7676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:06,559 - INFO - [diffusion][Epoch 7677] Epoch 7678/12000
2024-11-05 02:17:10,662 - INFO - [diffusion][Epoch 7677] diffusion training Loss: 0.055037968792021275
2024-11-05 02:17:10,665 - INFO - [diffusion][Epoch 7677] diffusion learning rate: 0.001
2024-11-05 02:17:10,667 - INFO - [diffusion][Epoch 7677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:10,668 - INFO - [diffusion][Epoch 7678] Epoch 7679/12000
2024-11-05 02:17:14,715 - INFO - [diffusion][Epoch 7678] diffusion training Loss: 0.056811748072505
2024-11-05 02:17:14,717 - INFO - [diffusion][Epoch 7678] diffusion learning rate: 0.001
2024-11-05 02:17:14,719 - INFO - [diffusion][Epoch 7678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:14,720 - INFO - [diffusion][Epoch 7679] Epoch 7680/12000
2024-11-05 02:17:18,639 - INFO - [diffusion][Epoch 7679] diffusion training Loss: 0.053511472418904305
2024-11-05 02:17:18,642 - INFO - [diffusion][Epoch 7679] diffusion learning rate: 0.001
2024-11-05 02:17:18,644 - INFO - [diffusion][Epoch 7679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:18,645 - INFO - [diffusion][Epoch 7680] Epoch 7681/12000
2024-11-05 02:17:22,655 - INFO - [diffusion][Epoch 7680] diffusion training Loss: 0.05411872919648886
2024-11-05 02:17:22,658 - INFO - [diffusion][Epoch 7680] diffusion learning rate: 0.001
2024-11-05 02:17:22,659 - INFO - [diffusion][Epoch 7680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:22,661 - INFO - [diffusion][Epoch 7681] Epoch 7682/12000
2024-11-05 02:17:26,635 - INFO - [diffusion][Epoch 7681] diffusion training Loss: 0.054986500181257725
2024-11-05 02:17:26,638 - INFO - [diffusion][Epoch 7681] diffusion learning rate: 0.001
2024-11-05 02:17:26,640 - INFO - [diffusion][Epoch 7681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:26,641 - INFO - [diffusion][Epoch 7682] Epoch 7683/12000
2024-11-05 02:17:30,543 - INFO - [diffusion][Epoch 7682] diffusion training Loss: 0.054287562146782875
2024-11-05 02:17:30,544 - INFO - [diffusion][Epoch 7682] diffusion learning rate: 0.001
2024-11-05 02:17:30,546 - INFO - [diffusion][Epoch 7682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:30,548 - INFO - [diffusion][Epoch 7683] Epoch 7684/12000
2024-11-05 02:17:34,633 - INFO - [diffusion][Epoch 7683] diffusion training Loss: 0.06100674346089363
2024-11-05 02:17:34,635 - INFO - [diffusion][Epoch 7683] diffusion learning rate: 0.001
2024-11-05 02:17:34,666 - INFO - [diffusion][Epoch 7683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:34,668 - INFO - [diffusion][Epoch 7684] Epoch 7685/12000
2024-11-05 02:17:38,808 - INFO - [diffusion][Epoch 7684] diffusion training Loss: 0.055868896655738354
2024-11-05 02:17:38,810 - INFO - [diffusion][Epoch 7684] diffusion learning rate: 0.001
2024-11-05 02:17:38,812 - INFO - [diffusion][Epoch 7684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:38,814 - INFO - [diffusion][Epoch 7685] Epoch 7686/12000
2024-11-05 02:17:42,901 - INFO - [diffusion][Epoch 7685] diffusion training Loss: 0.055035268887877464
2024-11-05 02:17:42,903 - INFO - [diffusion][Epoch 7685] diffusion learning rate: 0.001
2024-11-05 02:17:42,905 - INFO - [diffusion][Epoch 7685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:42,906 - INFO - [diffusion][Epoch 7686] Epoch 7687/12000
2024-11-05 02:17:47,081 - INFO - [diffusion][Epoch 7686] diffusion training Loss: 0.050893181934952736
2024-11-05 02:17:47,084 - INFO - [diffusion][Epoch 7686] diffusion learning rate: 0.001
2024-11-05 02:17:47,085 - INFO - [diffusion][Epoch 7686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:47,087 - INFO - [diffusion][Epoch 7687] Epoch 7688/12000
2024-11-05 02:17:51,195 - INFO - [diffusion][Epoch 7687] diffusion training Loss: 0.05401234235614538
2024-11-05 02:17:51,197 - INFO - [diffusion][Epoch 7687] diffusion learning rate: 0.001
2024-11-05 02:17:51,199 - INFO - [diffusion][Epoch 7687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:51,200 - INFO - [diffusion][Epoch 7688] Epoch 7689/12000
2024-11-05 02:17:55,141 - INFO - [diffusion][Epoch 7688] diffusion training Loss: 0.050526452250778675
2024-11-05 02:17:55,143 - INFO - [diffusion][Epoch 7688] diffusion learning rate: 0.001
2024-11-05 02:17:55,144 - INFO - [diffusion][Epoch 7688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:55,146 - INFO - [diffusion][Epoch 7689] Epoch 7690/12000
2024-11-05 02:17:59,157 - INFO - [diffusion][Epoch 7689] diffusion training Loss: 0.055587612092494965
2024-11-05 02:17:59,160 - INFO - [diffusion][Epoch 7689] diffusion learning rate: 0.001
2024-11-05 02:17:59,162 - INFO - [diffusion][Epoch 7689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:17:59,164 - INFO - [diffusion][Epoch 7690] Epoch 7691/12000
2024-11-05 02:18:03,189 - INFO - [diffusion][Epoch 7690] diffusion training Loss: 0.05487284529954195
2024-11-05 02:18:03,190 - INFO - [diffusion][Epoch 7690] diffusion learning rate: 0.001
2024-11-05 02:18:03,192 - INFO - [diffusion][Epoch 7690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:03,193 - INFO - [diffusion][Epoch 7691] Epoch 7692/12000
2024-11-05 02:18:07,159 - INFO - [diffusion][Epoch 7691] diffusion training Loss: 0.055941538885235786
2024-11-05 02:18:07,161 - INFO - [diffusion][Epoch 7691] diffusion learning rate: 0.001
2024-11-05 02:18:07,163 - INFO - [diffusion][Epoch 7691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:07,164 - INFO - [diffusion][Epoch 7692] Epoch 7693/12000
2024-11-05 02:18:11,308 - INFO - [diffusion][Epoch 7692] diffusion training Loss: 0.05314455274492502
2024-11-05 02:18:11,310 - INFO - [diffusion][Epoch 7692] diffusion learning rate: 0.001
2024-11-05 02:18:11,312 - INFO - [diffusion][Epoch 7692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:11,313 - INFO - [diffusion][Epoch 7693] Epoch 7694/12000
2024-11-05 02:18:15,716 - INFO - [diffusion][Epoch 7693] diffusion training Loss: 0.05049803853034973
2024-11-05 02:18:15,718 - INFO - [diffusion][Epoch 7693] diffusion learning rate: 0.001
2024-11-05 02:18:15,720 - INFO - [diffusion][Epoch 7693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:15,721 - INFO - [diffusion][Epoch 7694] Epoch 7695/12000
2024-11-05 02:18:19,800 - INFO - [diffusion][Epoch 7694] diffusion training Loss: 0.05683371890336275
2024-11-05 02:18:19,802 - INFO - [diffusion][Epoch 7694] diffusion learning rate: 0.001
2024-11-05 02:18:19,804 - INFO - [diffusion][Epoch 7694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:19,805 - INFO - [diffusion][Epoch 7695] Epoch 7696/12000
2024-11-05 02:18:23,912 - INFO - [diffusion][Epoch 7695] diffusion training Loss: 0.05371971055865288
2024-11-05 02:18:23,914 - INFO - [diffusion][Epoch 7695] diffusion learning rate: 0.001
2024-11-05 02:18:23,915 - INFO - [diffusion][Epoch 7695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:23,917 - INFO - [diffusion][Epoch 7696] Epoch 7697/12000
2024-11-05 02:18:28,017 - INFO - [diffusion][Epoch 7696] diffusion training Loss: 0.05318955518305302
2024-11-05 02:18:28,019 - INFO - [diffusion][Epoch 7696] diffusion learning rate: 0.001
2024-11-05 02:18:28,021 - INFO - [diffusion][Epoch 7696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:28,022 - INFO - [diffusion][Epoch 7697] Epoch 7698/12000
2024-11-05 02:18:32,047 - INFO - [diffusion][Epoch 7697] diffusion training Loss: 0.05445422977209091
2024-11-05 02:18:32,049 - INFO - [diffusion][Epoch 7697] diffusion learning rate: 0.001
2024-11-05 02:18:32,051 - INFO - [diffusion][Epoch 7697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:32,053 - INFO - [diffusion][Epoch 7698] Epoch 7699/12000
2024-11-05 02:18:35,998 - INFO - [diffusion][Epoch 7698] diffusion training Loss: 0.05498398281633854
2024-11-05 02:18:36,000 - INFO - [diffusion][Epoch 7698] diffusion learning rate: 0.001
2024-11-05 02:18:36,002 - INFO - [diffusion][Epoch 7698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:36,003 - INFO - [diffusion][Epoch 7699] Epoch 7700/12000
2024-11-05 02:18:40,133 - INFO - [diffusion][Epoch 7699] diffusion training Loss: 0.05645695514976978
2024-11-05 02:18:40,135 - INFO - [diffusion][Epoch 7699] diffusion learning rate: 0.001
2024-11-05 02:18:40,137 - INFO - [diffusion][Epoch 7699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:40,138 - INFO - [diffusion][Epoch 7700] Epoch 7701/12000
2024-11-05 02:18:44,159 - INFO - [diffusion][Epoch 7700] diffusion training Loss: 0.050827126018702984
2024-11-05 02:18:44,161 - INFO - [diffusion][Epoch 7700] diffusion learning rate: 0.001
2024-11-05 02:18:44,163 - INFO - [diffusion][Epoch 7700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:44,164 - INFO - [diffusion][Epoch 7701] Epoch 7702/12000
2024-11-05 02:18:48,245 - INFO - [diffusion][Epoch 7701] diffusion training Loss: 0.05699625704437494
2024-11-05 02:18:48,248 - INFO - [diffusion][Epoch 7701] diffusion learning rate: 0.001
2024-11-05 02:18:48,250 - INFO - [diffusion][Epoch 7701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:48,252 - INFO - [diffusion][Epoch 7702] Epoch 7703/12000
2024-11-05 02:18:52,199 - INFO - [diffusion][Epoch 7702] diffusion training Loss: 0.05733160488307476
2024-11-05 02:18:52,200 - INFO - [diffusion][Epoch 7702] diffusion learning rate: 0.001
2024-11-05 02:18:52,202 - INFO - [diffusion][Epoch 7702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:52,203 - INFO - [diffusion][Epoch 7703] Epoch 7704/12000
2024-11-05 02:18:56,170 - INFO - [diffusion][Epoch 7703] diffusion training Loss: 0.05482830014079809
2024-11-05 02:18:56,172 - INFO - [diffusion][Epoch 7703] diffusion learning rate: 0.001
2024-11-05 02:18:56,173 - INFO - [diffusion][Epoch 7703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:18:56,175 - INFO - [diffusion][Epoch 7704] Epoch 7705/12000
2024-11-05 02:19:00,082 - INFO - [diffusion][Epoch 7704] diffusion training Loss: 0.05780600570142269
2024-11-05 02:19:00,084 - INFO - [diffusion][Epoch 7704] diffusion learning rate: 0.001
2024-11-05 02:19:00,086 - INFO - [diffusion][Epoch 7704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:00,087 - INFO - [diffusion][Epoch 7705] Epoch 7706/12000
2024-11-05 02:19:04,083 - INFO - [diffusion][Epoch 7705] diffusion training Loss: 0.055107382126152515
2024-11-05 02:19:04,085 - INFO - [diffusion][Epoch 7705] diffusion learning rate: 0.001
2024-11-05 02:19:04,087 - INFO - [diffusion][Epoch 7705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:04,089 - INFO - [diffusion][Epoch 7706] Epoch 7707/12000
2024-11-05 02:19:08,090 - INFO - [diffusion][Epoch 7706] diffusion training Loss: 0.057126354426145554
2024-11-05 02:19:08,092 - INFO - [diffusion][Epoch 7706] diffusion learning rate: 0.001
2024-11-05 02:19:08,094 - INFO - [diffusion][Epoch 7706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:08,095 - INFO - [diffusion][Epoch 7707] Epoch 7708/12000
2024-11-05 02:19:12,251 - INFO - [diffusion][Epoch 7707] diffusion training Loss: 0.055833520367741585
2024-11-05 02:19:12,253 - INFO - [diffusion][Epoch 7707] diffusion learning rate: 0.001
2024-11-05 02:19:12,255 - INFO - [diffusion][Epoch 7707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:12,257 - INFO - [diffusion][Epoch 7708] Epoch 7709/12000
2024-11-05 02:19:16,258 - INFO - [diffusion][Epoch 7708] diffusion training Loss: 0.059179085306823254
2024-11-05 02:19:16,260 - INFO - [diffusion][Epoch 7708] diffusion learning rate: 0.001
2024-11-05 02:19:16,262 - INFO - [diffusion][Epoch 7708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:16,263 - INFO - [diffusion][Epoch 7709] Epoch 7710/12000
2024-11-05 02:19:20,372 - INFO - [diffusion][Epoch 7709] diffusion training Loss: 0.052606490440666676
2024-11-05 02:19:20,375 - INFO - [diffusion][Epoch 7709] diffusion learning rate: 0.001
2024-11-05 02:19:20,377 - INFO - [diffusion][Epoch 7709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:20,379 - INFO - [diffusion][Epoch 7710] Epoch 7711/12000
2024-11-05 02:19:24,413 - INFO - [diffusion][Epoch 7710] diffusion training Loss: 0.05681504588574171
2024-11-05 02:19:24,415 - INFO - [diffusion][Epoch 7710] diffusion learning rate: 0.001
2024-11-05 02:19:24,417 - INFO - [diffusion][Epoch 7710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:24,419 - INFO - [diffusion][Epoch 7711] Epoch 7712/12000
2024-11-05 02:19:28,405 - INFO - [diffusion][Epoch 7711] diffusion training Loss: 0.053770399652421474
2024-11-05 02:19:28,409 - INFO - [diffusion][Epoch 7711] diffusion learning rate: 0.001
2024-11-05 02:19:28,411 - INFO - [diffusion][Epoch 7711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:28,412 - INFO - [diffusion][Epoch 7712] Epoch 7713/12000
2024-11-05 02:19:32,619 - INFO - [diffusion][Epoch 7712] diffusion training Loss: 0.055241161957383156
2024-11-05 02:19:32,622 - INFO - [diffusion][Epoch 7712] diffusion learning rate: 0.001
2024-11-05 02:19:32,624 - INFO - [diffusion][Epoch 7712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:32,625 - INFO - [diffusion][Epoch 7713] Epoch 7714/12000
2024-11-05 02:19:36,815 - INFO - [diffusion][Epoch 7713] diffusion training Loss: 0.051303726620972157
2024-11-05 02:19:36,817 - INFO - [diffusion][Epoch 7713] diffusion learning rate: 0.001
2024-11-05 02:19:36,819 - INFO - [diffusion][Epoch 7713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:36,820 - INFO - [diffusion][Epoch 7714] Epoch 7715/12000
2024-11-05 02:19:40,831 - INFO - [diffusion][Epoch 7714] diffusion training Loss: 0.05399109423160553
2024-11-05 02:19:40,833 - INFO - [diffusion][Epoch 7714] diffusion learning rate: 0.001
2024-11-05 02:19:40,836 - INFO - [diffusion][Epoch 7714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:40,837 - INFO - [diffusion][Epoch 7715] Epoch 7716/12000
2024-11-05 02:19:45,548 - INFO - [diffusion][Epoch 7715] diffusion training Loss: 0.05362859647721052
2024-11-05 02:19:45,550 - INFO - [diffusion][Epoch 7715] diffusion learning rate: 0.001
2024-11-05 02:19:45,552 - INFO - [diffusion][Epoch 7715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:45,554 - INFO - [diffusion][Epoch 7716] Epoch 7717/12000
2024-11-05 02:19:49,648 - INFO - [diffusion][Epoch 7716] diffusion training Loss: 0.056406112387776375
2024-11-05 02:19:49,650 - INFO - [diffusion][Epoch 7716] diffusion learning rate: 0.001
2024-11-05 02:19:49,652 - INFO - [diffusion][Epoch 7716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:49,653 - INFO - [diffusion][Epoch 7717] Epoch 7718/12000
2024-11-05 02:19:53,831 - INFO - [diffusion][Epoch 7717] diffusion training Loss: 0.04824885539710522
2024-11-05 02:19:53,833 - INFO - [diffusion][Epoch 7717] diffusion learning rate: 0.001
2024-11-05 02:19:53,834 - INFO - [diffusion][Epoch 7717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:53,836 - INFO - [diffusion][Epoch 7718] Epoch 7719/12000
2024-11-05 02:19:57,928 - INFO - [diffusion][Epoch 7718] diffusion training Loss: 0.0560046024620533
2024-11-05 02:19:57,930 - INFO - [diffusion][Epoch 7718] diffusion learning rate: 0.001
2024-11-05 02:19:57,957 - INFO - [diffusion][Epoch 7718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:19:57,958 - INFO - [diffusion][Epoch 7719] Epoch 7720/12000
2024-11-05 02:20:02,084 - INFO - [diffusion][Epoch 7719] diffusion training Loss: 0.05560420546680689
2024-11-05 02:20:02,086 - INFO - [diffusion][Epoch 7719] diffusion learning rate: 0.001
2024-11-05 02:20:02,087 - INFO - [diffusion][Epoch 7719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:02,089 - INFO - [diffusion][Epoch 7720] Epoch 7721/12000
2024-11-05 02:20:06,262 - INFO - [diffusion][Epoch 7720] diffusion training Loss: 0.053927463479340076
2024-11-05 02:20:06,264 - INFO - [diffusion][Epoch 7720] diffusion learning rate: 0.001
2024-11-05 02:20:06,266 - INFO - [diffusion][Epoch 7720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:06,268 - INFO - [diffusion][Epoch 7721] Epoch 7722/12000
2024-11-05 02:20:10,253 - INFO - [diffusion][Epoch 7721] diffusion training Loss: 0.058322492986917496
2024-11-05 02:20:10,255 - INFO - [diffusion][Epoch 7721] diffusion learning rate: 0.001
2024-11-05 02:20:10,257 - INFO - [diffusion][Epoch 7721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:10,258 - INFO - [diffusion][Epoch 7722] Epoch 7723/12000
2024-11-05 02:20:14,409 - INFO - [diffusion][Epoch 7722] diffusion training Loss: 0.056185413151979446
2024-11-05 02:20:14,411 - INFO - [diffusion][Epoch 7722] diffusion learning rate: 0.001
2024-11-05 02:20:14,413 - INFO - [diffusion][Epoch 7722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:14,414 - INFO - [diffusion][Epoch 7723] Epoch 7724/12000
2024-11-05 02:20:18,507 - INFO - [diffusion][Epoch 7723] diffusion training Loss: 0.056726982817053795
2024-11-05 02:20:18,509 - INFO - [diffusion][Epoch 7723] diffusion learning rate: 0.001
2024-11-05 02:20:18,511 - INFO - [diffusion][Epoch 7723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:18,512 - INFO - [diffusion][Epoch 7724] Epoch 7725/12000
2024-11-05 02:20:22,625 - INFO - [diffusion][Epoch 7724] diffusion training Loss: 0.05263993516564369
2024-11-05 02:20:22,628 - INFO - [diffusion][Epoch 7724] diffusion learning rate: 0.001
2024-11-05 02:20:22,630 - INFO - [diffusion][Epoch 7724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:22,631 - INFO - [diffusion][Epoch 7725] Epoch 7726/12000
2024-11-05 02:20:26,759 - INFO - [diffusion][Epoch 7725] diffusion training Loss: 0.05034528486430645
2024-11-05 02:20:26,761 - INFO - [diffusion][Epoch 7725] diffusion learning rate: 0.001
2024-11-05 02:20:26,763 - INFO - [diffusion][Epoch 7725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:26,764 - INFO - [diffusion][Epoch 7726] Epoch 7727/12000
2024-11-05 02:20:30,711 - INFO - [diffusion][Epoch 7726] diffusion training Loss: 0.05652204342186451
2024-11-05 02:20:30,714 - INFO - [diffusion][Epoch 7726] diffusion learning rate: 0.001
2024-11-05 02:20:30,716 - INFO - [diffusion][Epoch 7726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:30,717 - INFO - [diffusion][Epoch 7727] Epoch 7728/12000
2024-11-05 02:20:34,794 - INFO - [diffusion][Epoch 7727] diffusion training Loss: 0.05389230605214834
2024-11-05 02:20:34,796 - INFO - [diffusion][Epoch 7727] diffusion learning rate: 0.001
2024-11-05 02:20:34,798 - INFO - [diffusion][Epoch 7727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:34,799 - INFO - [diffusion][Epoch 7728] Epoch 7729/12000
2024-11-05 02:20:39,009 - INFO - [diffusion][Epoch 7728] diffusion training Loss: 0.05730289500206709
2024-11-05 02:20:39,011 - INFO - [diffusion][Epoch 7728] diffusion learning rate: 0.001
2024-11-05 02:20:39,012 - INFO - [diffusion][Epoch 7728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:39,014 - INFO - [diffusion][Epoch 7729] Epoch 7730/12000
2024-11-05 02:20:43,149 - INFO - [diffusion][Epoch 7729] diffusion training Loss: 0.05973824206739664
2024-11-05 02:20:43,151 - INFO - [diffusion][Epoch 7729] diffusion learning rate: 0.001
2024-11-05 02:20:43,153 - INFO - [diffusion][Epoch 7729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:43,154 - INFO - [diffusion][Epoch 7730] Epoch 7731/12000
2024-11-05 02:20:47,330 - INFO - [diffusion][Epoch 7730] diffusion training Loss: 0.051216659136116505
2024-11-05 02:20:47,331 - INFO - [diffusion][Epoch 7730] diffusion learning rate: 0.001
2024-11-05 02:20:47,333 - INFO - [diffusion][Epoch 7730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:47,334 - INFO - [diffusion][Epoch 7731] Epoch 7732/12000
2024-11-05 02:20:51,482 - INFO - [diffusion][Epoch 7731] diffusion training Loss: 0.051127344369888306
2024-11-05 02:20:51,484 - INFO - [diffusion][Epoch 7731] diffusion learning rate: 0.001
2024-11-05 02:20:51,486 - INFO - [diffusion][Epoch 7731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:51,488 - INFO - [diffusion][Epoch 7732] Epoch 7733/12000
2024-11-05 02:20:55,601 - INFO - [diffusion][Epoch 7732] diffusion training Loss: 0.05336623080074787
2024-11-05 02:20:55,603 - INFO - [diffusion][Epoch 7732] diffusion learning rate: 0.001
2024-11-05 02:20:55,605 - INFO - [diffusion][Epoch 7732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:55,606 - INFO - [diffusion][Epoch 7733] Epoch 7734/12000
2024-11-05 02:20:59,740 - INFO - [diffusion][Epoch 7733] diffusion training Loss: 0.05455040093511343
2024-11-05 02:20:59,742 - INFO - [diffusion][Epoch 7733] diffusion learning rate: 0.001
2024-11-05 02:20:59,744 - INFO - [diffusion][Epoch 7733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:20:59,745 - INFO - [diffusion][Epoch 7734] Epoch 7735/12000
2024-11-05 02:21:03,714 - INFO - [diffusion][Epoch 7734] diffusion training Loss: 0.05510485917329788
2024-11-05 02:21:03,716 - INFO - [diffusion][Epoch 7734] diffusion learning rate: 0.001
2024-11-05 02:21:03,718 - INFO - [diffusion][Epoch 7734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:03,719 - INFO - [diffusion][Epoch 7735] Epoch 7736/12000
2024-11-05 02:21:07,849 - INFO - [diffusion][Epoch 7735] diffusion training Loss: 0.05605931021273136
2024-11-05 02:21:07,851 - INFO - [diffusion][Epoch 7735] diffusion learning rate: 0.001
2024-11-05 02:21:07,853 - INFO - [diffusion][Epoch 7735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:07,854 - INFO - [diffusion][Epoch 7736] Epoch 7737/12000
2024-11-05 02:21:12,414 - INFO - [diffusion][Epoch 7736] diffusion training Loss: 0.0524988379329443
2024-11-05 02:21:12,416 - INFO - [diffusion][Epoch 7736] diffusion learning rate: 0.001
2024-11-05 02:21:12,454 - INFO - [diffusion][Epoch 7736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:12,456 - INFO - [diffusion][Epoch 7737] Epoch 7738/12000
2024-11-05 02:21:16,512 - INFO - [diffusion][Epoch 7737] diffusion training Loss: 0.05549575015902519
2024-11-05 02:21:16,514 - INFO - [diffusion][Epoch 7737] diffusion learning rate: 0.001
2024-11-05 02:21:16,516 - INFO - [diffusion][Epoch 7737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:16,517 - INFO - [diffusion][Epoch 7738] Epoch 7739/12000
2024-11-05 02:21:20,629 - INFO - [diffusion][Epoch 7738] diffusion training Loss: 0.05481265764683485
2024-11-05 02:21:20,631 - INFO - [diffusion][Epoch 7738] diffusion learning rate: 0.001
2024-11-05 02:21:20,633 - INFO - [diffusion][Epoch 7738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:20,634 - INFO - [diffusion][Epoch 7739] Epoch 7740/12000
2024-11-05 02:21:24,827 - INFO - [diffusion][Epoch 7739] diffusion training Loss: 0.052726355381309986
2024-11-05 02:21:24,829 - INFO - [diffusion][Epoch 7739] diffusion learning rate: 0.001
2024-11-05 02:21:24,831 - INFO - [diffusion][Epoch 7739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:24,832 - INFO - [diffusion][Epoch 7740] Epoch 7741/12000
2024-11-05 02:21:28,951 - INFO - [diffusion][Epoch 7740] diffusion training Loss: 0.05401791352778673
2024-11-05 02:21:28,953 - INFO - [diffusion][Epoch 7740] diffusion learning rate: 0.001
2024-11-05 02:21:28,955 - INFO - [diffusion][Epoch 7740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:28,956 - INFO - [diffusion][Epoch 7741] Epoch 7742/12000
2024-11-05 02:21:33,069 - INFO - [diffusion][Epoch 7741] diffusion training Loss: 0.05319064576178789
2024-11-05 02:21:33,071 - INFO - [diffusion][Epoch 7741] diffusion learning rate: 0.001
2024-11-05 02:21:33,073 - INFO - [diffusion][Epoch 7741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:33,074 - INFO - [diffusion][Epoch 7742] Epoch 7743/12000
2024-11-05 02:21:37,206 - INFO - [diffusion][Epoch 7742] diffusion training Loss: 0.0560744097456336
2024-11-05 02:21:37,208 - INFO - [diffusion][Epoch 7742] diffusion learning rate: 0.001
2024-11-05 02:21:37,211 - INFO - [diffusion][Epoch 7742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:37,212 - INFO - [diffusion][Epoch 7743] Epoch 7744/12000
2024-11-05 02:21:41,365 - INFO - [diffusion][Epoch 7743] diffusion training Loss: 0.060943576507270336
2024-11-05 02:21:41,367 - INFO - [diffusion][Epoch 7743] diffusion learning rate: 0.001
2024-11-05 02:21:41,369 - INFO - [diffusion][Epoch 7743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:41,370 - INFO - [diffusion][Epoch 7744] Epoch 7745/12000
2024-11-05 02:21:45,359 - INFO - [diffusion][Epoch 7744] diffusion training Loss: 0.05460662767291069
2024-11-05 02:21:45,361 - INFO - [diffusion][Epoch 7744] diffusion learning rate: 0.001
2024-11-05 02:21:45,363 - INFO - [diffusion][Epoch 7744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:45,364 - INFO - [diffusion][Epoch 7745] Epoch 7746/12000
2024-11-05 02:21:49,539 - INFO - [diffusion][Epoch 7745] diffusion training Loss: 0.057006497867405415
2024-11-05 02:21:49,542 - INFO - [diffusion][Epoch 7745] diffusion learning rate: 0.001
2024-11-05 02:21:49,544 - INFO - [diffusion][Epoch 7745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:49,545 - INFO - [diffusion][Epoch 7746] Epoch 7747/12000
2024-11-05 02:21:53,598 - INFO - [diffusion][Epoch 7746] diffusion training Loss: 0.05033620912581682
2024-11-05 02:21:53,600 - INFO - [diffusion][Epoch 7746] diffusion learning rate: 0.001
2024-11-05 02:21:53,601 - INFO - [diffusion][Epoch 7746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:53,603 - INFO - [diffusion][Epoch 7747] Epoch 7748/12000
2024-11-05 02:21:57,678 - INFO - [diffusion][Epoch 7747] diffusion training Loss: 0.05457072891294956
2024-11-05 02:21:57,680 - INFO - [diffusion][Epoch 7747] diffusion learning rate: 0.001
2024-11-05 02:21:57,682 - INFO - [diffusion][Epoch 7747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:21:57,683 - INFO - [diffusion][Epoch 7748] Epoch 7749/12000
2024-11-05 02:22:01,611 - INFO - [diffusion][Epoch 7748] diffusion training Loss: 0.05570069048553705
2024-11-05 02:22:01,613 - INFO - [diffusion][Epoch 7748] diffusion learning rate: 0.001
2024-11-05 02:22:01,614 - INFO - [diffusion][Epoch 7748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:01,616 - INFO - [diffusion][Epoch 7749] Epoch 7750/12000
2024-11-05 02:22:05,709 - INFO - [diffusion][Epoch 7749] diffusion training Loss: 0.05900442786514759
2024-11-05 02:22:05,711 - INFO - [diffusion][Epoch 7749] diffusion learning rate: 0.001
2024-11-05 02:22:05,713 - INFO - [diffusion][Epoch 7749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:05,714 - INFO - [diffusion][Epoch 7750] Epoch 7751/12000
2024-11-05 02:22:09,823 - INFO - [diffusion][Epoch 7750] diffusion training Loss: 0.05652388837188482
2024-11-05 02:22:09,825 - INFO - [diffusion][Epoch 7750] diffusion learning rate: 0.001
2024-11-05 02:22:09,827 - INFO - [diffusion][Epoch 7750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:09,828 - INFO - [diffusion][Epoch 7751] Epoch 7752/12000
2024-11-05 02:22:13,952 - INFO - [diffusion][Epoch 7751] diffusion training Loss: 0.05421960446983576
2024-11-05 02:22:13,954 - INFO - [diffusion][Epoch 7751] diffusion learning rate: 0.001
2024-11-05 02:22:13,956 - INFO - [diffusion][Epoch 7751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:13,957 - INFO - [diffusion][Epoch 7752] Epoch 7753/12000
2024-11-05 02:22:18,039 - INFO - [diffusion][Epoch 7752] diffusion training Loss: 0.05512840673327446
2024-11-05 02:22:18,043 - INFO - [diffusion][Epoch 7752] diffusion learning rate: 0.001
2024-11-05 02:22:18,045 - INFO - [diffusion][Epoch 7752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:18,046 - INFO - [diffusion][Epoch 7753] Epoch 7754/12000
2024-11-05 02:22:21,920 - INFO - [diffusion][Epoch 7753] diffusion training Loss: 0.049951995722949505
2024-11-05 02:22:21,922 - INFO - [diffusion][Epoch 7753] diffusion learning rate: 0.001
2024-11-05 02:22:21,924 - INFO - [diffusion][Epoch 7753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:21,925 - INFO - [diffusion][Epoch 7754] Epoch 7755/12000
2024-11-05 02:22:25,989 - INFO - [diffusion][Epoch 7754] diffusion training Loss: 0.050124216824769974
2024-11-05 02:22:25,991 - INFO - [diffusion][Epoch 7754] diffusion learning rate: 0.001
2024-11-05 02:22:25,993 - INFO - [diffusion][Epoch 7754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:25,995 - INFO - [diffusion][Epoch 7755] Epoch 7756/12000
2024-11-05 02:22:30,074 - INFO - [diffusion][Epoch 7755] diffusion training Loss: 0.05380482319742441
2024-11-05 02:22:30,077 - INFO - [diffusion][Epoch 7755] diffusion learning rate: 0.001
2024-11-05 02:22:30,078 - INFO - [diffusion][Epoch 7755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:30,079 - INFO - [diffusion][Epoch 7756] Epoch 7757/12000
2024-11-05 02:22:34,385 - INFO - [diffusion][Epoch 7756] diffusion training Loss: 0.0502258762717247
2024-11-05 02:22:34,387 - INFO - [diffusion][Epoch 7756] diffusion learning rate: 0.001
2024-11-05 02:22:34,388 - INFO - [diffusion][Epoch 7756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:34,389 - INFO - [diffusion][Epoch 7757] Epoch 7758/12000
2024-11-05 02:22:38,499 - INFO - [diffusion][Epoch 7757] diffusion training Loss: 0.04910121392458677
2024-11-05 02:22:38,501 - INFO - [diffusion][Epoch 7757] diffusion learning rate: 0.001
2024-11-05 02:22:38,503 - INFO - [diffusion][Epoch 7757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:38,504 - INFO - [diffusion][Epoch 7758] Epoch 7759/12000
2024-11-05 02:22:42,490 - INFO - [diffusion][Epoch 7758] diffusion training Loss: 0.05730499420315027
2024-11-05 02:22:42,492 - INFO - [diffusion][Epoch 7758] diffusion learning rate: 0.001
2024-11-05 02:22:42,494 - INFO - [diffusion][Epoch 7758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:42,495 - INFO - [diffusion][Epoch 7759] Epoch 7760/12000
2024-11-05 02:22:46,504 - INFO - [diffusion][Epoch 7759] diffusion training Loss: 0.051913502626121044
2024-11-05 02:22:46,506 - INFO - [diffusion][Epoch 7759] diffusion learning rate: 0.001
2024-11-05 02:22:46,508 - INFO - [diffusion][Epoch 7759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:46,509 - INFO - [diffusion][Epoch 7760] Epoch 7761/12000
2024-11-05 02:22:50,547 - INFO - [diffusion][Epoch 7760] diffusion training Loss: 0.05693592689931393
2024-11-05 02:22:50,549 - INFO - [diffusion][Epoch 7760] diffusion learning rate: 0.001
2024-11-05 02:22:50,599 - INFO - [diffusion][Epoch 7760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:50,600 - INFO - [diffusion][Epoch 7761] Epoch 7762/12000
2024-11-05 02:22:54,734 - INFO - [diffusion][Epoch 7761] diffusion training Loss: 0.05034642107784748
2024-11-05 02:22:54,736 - INFO - [diffusion][Epoch 7761] diffusion learning rate: 0.001
2024-11-05 02:22:54,738 - INFO - [diffusion][Epoch 7761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:54,739 - INFO - [diffusion][Epoch 7762] Epoch 7763/12000
2024-11-05 02:22:58,796 - INFO - [diffusion][Epoch 7762] diffusion training Loss: 0.052989291958510876
2024-11-05 02:22:58,798 - INFO - [diffusion][Epoch 7762] diffusion learning rate: 0.001
2024-11-05 02:22:58,800 - INFO - [diffusion][Epoch 7762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:22:58,802 - INFO - [diffusion][Epoch 7763] Epoch 7764/12000
2024-11-05 02:23:02,918 - INFO - [diffusion][Epoch 7763] diffusion training Loss: 0.05525883845984936
2024-11-05 02:23:02,920 - INFO - [diffusion][Epoch 7763] diffusion learning rate: 0.001
2024-11-05 02:23:02,922 - INFO - [diffusion][Epoch 7763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:02,923 - INFO - [diffusion][Epoch 7764] Epoch 7765/12000
2024-11-05 02:23:07,044 - INFO - [diffusion][Epoch 7764] diffusion training Loss: 0.05457375943660736
2024-11-05 02:23:07,046 - INFO - [diffusion][Epoch 7764] diffusion learning rate: 0.001
2024-11-05 02:23:07,048 - INFO - [diffusion][Epoch 7764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:07,049 - INFO - [diffusion][Epoch 7765] Epoch 7766/12000
2024-11-05 02:23:11,164 - INFO - [diffusion][Epoch 7765] diffusion training Loss: 0.05645274557173252
2024-11-05 02:23:11,166 - INFO - [diffusion][Epoch 7765] diffusion learning rate: 0.001
2024-11-05 02:23:11,168 - INFO - [diffusion][Epoch 7765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:11,169 - INFO - [diffusion][Epoch 7766] Epoch 7767/12000
2024-11-05 02:23:15,285 - INFO - [diffusion][Epoch 7766] diffusion training Loss: 0.056660289876163006
2024-11-05 02:23:15,287 - INFO - [diffusion][Epoch 7766] diffusion learning rate: 0.001
2024-11-05 02:23:15,289 - INFO - [diffusion][Epoch 7766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:15,290 - INFO - [diffusion][Epoch 7767] Epoch 7768/12000
2024-11-05 02:23:19,430 - INFO - [diffusion][Epoch 7767] diffusion training Loss: 0.05097901728004217
2024-11-05 02:23:19,433 - INFO - [diffusion][Epoch 7767] diffusion learning rate: 0.001
2024-11-05 02:23:19,435 - INFO - [diffusion][Epoch 7767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:19,436 - INFO - [diffusion][Epoch 7768] Epoch 7769/12000
2024-11-05 02:23:23,562 - INFO - [diffusion][Epoch 7768] diffusion training Loss: 0.05409316532313824
2024-11-05 02:23:23,564 - INFO - [diffusion][Epoch 7768] diffusion learning rate: 0.001
2024-11-05 02:23:23,566 - INFO - [diffusion][Epoch 7768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:23,568 - INFO - [diffusion][Epoch 7769] Epoch 7770/12000
2024-11-05 02:23:27,621 - INFO - [diffusion][Epoch 7769] diffusion training Loss: 0.05149004701524973
2024-11-05 02:23:27,623 - INFO - [diffusion][Epoch 7769] diffusion learning rate: 0.001
2024-11-05 02:23:27,625 - INFO - [diffusion][Epoch 7769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:27,626 - INFO - [diffusion][Epoch 7770] Epoch 7771/12000
2024-11-05 02:23:31,666 - INFO - [diffusion][Epoch 7770] diffusion training Loss: 0.05817934684455395
2024-11-05 02:23:31,668 - INFO - [diffusion][Epoch 7770] diffusion learning rate: 0.001
2024-11-05 02:23:31,670 - INFO - [diffusion][Epoch 7770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:31,671 - INFO - [diffusion][Epoch 7771] Epoch 7772/12000
2024-11-05 02:23:35,593 - INFO - [diffusion][Epoch 7771] diffusion training Loss: 0.05360070709139109
2024-11-05 02:23:35,595 - INFO - [diffusion][Epoch 7771] diffusion learning rate: 0.001
2024-11-05 02:23:35,597 - INFO - [diffusion][Epoch 7771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:35,598 - INFO - [diffusion][Epoch 7772] Epoch 7773/12000
2024-11-05 02:23:39,471 - INFO - [diffusion][Epoch 7772] diffusion training Loss: 0.06047527864575386
2024-11-05 02:23:39,473 - INFO - [diffusion][Epoch 7772] diffusion learning rate: 0.001
2024-11-05 02:23:39,474 - INFO - [diffusion][Epoch 7772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:39,476 - INFO - [diffusion][Epoch 7773] Epoch 7774/12000
2024-11-05 02:23:43,465 - INFO - [diffusion][Epoch 7773] diffusion training Loss: 0.05254985764622688
2024-11-05 02:23:43,467 - INFO - [diffusion][Epoch 7773] diffusion learning rate: 0.001
2024-11-05 02:23:43,469 - INFO - [diffusion][Epoch 7773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:43,471 - INFO - [diffusion][Epoch 7774] Epoch 7775/12000
2024-11-05 02:23:47,450 - INFO - [diffusion][Epoch 7774] diffusion training Loss: 0.05699493549764156
2024-11-05 02:23:47,453 - INFO - [diffusion][Epoch 7774] diffusion learning rate: 0.001
2024-11-05 02:23:47,454 - INFO - [diffusion][Epoch 7774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:47,456 - INFO - [diffusion][Epoch 7775] Epoch 7776/12000
2024-11-05 02:23:51,343 - INFO - [diffusion][Epoch 7775] diffusion training Loss: 0.0614443551748991
2024-11-05 02:23:51,345 - INFO - [diffusion][Epoch 7775] diffusion learning rate: 0.001
2024-11-05 02:23:51,347 - INFO - [diffusion][Epoch 7775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:51,348 - INFO - [diffusion][Epoch 7776] Epoch 7777/12000
2024-11-05 02:23:55,369 - INFO - [diffusion][Epoch 7776] diffusion training Loss: 0.06022472679615021
2024-11-05 02:23:55,371 - INFO - [diffusion][Epoch 7776] diffusion learning rate: 0.001
2024-11-05 02:23:55,373 - INFO - [diffusion][Epoch 7776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:55,374 - INFO - [diffusion][Epoch 7777] Epoch 7778/12000
2024-11-05 02:23:59,383 - INFO - [diffusion][Epoch 7777] diffusion training Loss: 0.05212419852614403
2024-11-05 02:23:59,385 - INFO - [diffusion][Epoch 7777] diffusion learning rate: 0.001
2024-11-05 02:23:59,387 - INFO - [diffusion][Epoch 7777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:23:59,388 - INFO - [diffusion][Epoch 7778] Epoch 7779/12000
2024-11-05 02:24:03,576 - INFO - [diffusion][Epoch 7778] diffusion training Loss: 0.05873924307525158
2024-11-05 02:24:03,578 - INFO - [diffusion][Epoch 7778] diffusion learning rate: 0.001
2024-11-05 02:24:03,605 - INFO - [diffusion][Epoch 7778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:03,607 - INFO - [diffusion][Epoch 7779] Epoch 7780/12000
2024-11-05 02:24:07,567 - INFO - [diffusion][Epoch 7779] diffusion training Loss: 0.05096896551549435
2024-11-05 02:24:07,569 - INFO - [diffusion][Epoch 7779] diffusion learning rate: 0.001
2024-11-05 02:24:07,571 - INFO - [diffusion][Epoch 7779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:07,572 - INFO - [diffusion][Epoch 7780] Epoch 7781/12000
2024-11-05 02:24:11,543 - INFO - [diffusion][Epoch 7780] diffusion training Loss: 0.05821299646049738
2024-11-05 02:24:11,545 - INFO - [diffusion][Epoch 7780] diffusion learning rate: 0.001
2024-11-05 02:24:11,547 - INFO - [diffusion][Epoch 7780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:11,548 - INFO - [diffusion][Epoch 7781] Epoch 7782/12000
2024-11-05 02:24:15,586 - INFO - [diffusion][Epoch 7781] diffusion training Loss: 0.05477804318070412
2024-11-05 02:24:15,588 - INFO - [diffusion][Epoch 7781] diffusion learning rate: 0.001
2024-11-05 02:24:15,589 - INFO - [diffusion][Epoch 7781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:15,591 - INFO - [diffusion][Epoch 7782] Epoch 7783/12000
2024-11-05 02:24:19,689 - INFO - [diffusion][Epoch 7782] diffusion training Loss: 0.054910642094910145
2024-11-05 02:24:19,692 - INFO - [diffusion][Epoch 7782] diffusion learning rate: 0.001
2024-11-05 02:24:19,694 - INFO - [diffusion][Epoch 7782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:19,695 - INFO - [diffusion][Epoch 7783] Epoch 7784/12000
2024-11-05 02:24:23,769 - INFO - [diffusion][Epoch 7783] diffusion training Loss: 0.05760672129690647
2024-11-05 02:24:23,772 - INFO - [diffusion][Epoch 7783] diffusion learning rate: 0.001
2024-11-05 02:24:23,773 - INFO - [diffusion][Epoch 7783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:23,775 - INFO - [diffusion][Epoch 7784] Epoch 7785/12000
2024-11-05 02:24:27,912 - INFO - [diffusion][Epoch 7784] diffusion training Loss: 0.05500873923301697
2024-11-05 02:24:27,914 - INFO - [diffusion][Epoch 7784] diffusion learning rate: 0.001
2024-11-05 02:24:27,916 - INFO - [diffusion][Epoch 7784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:27,917 - INFO - [diffusion][Epoch 7785] Epoch 7786/12000
2024-11-05 02:24:32,022 - INFO - [diffusion][Epoch 7785] diffusion training Loss: 0.05599992908537388
2024-11-05 02:24:32,024 - INFO - [diffusion][Epoch 7785] diffusion learning rate: 0.001
2024-11-05 02:24:32,025 - INFO - [diffusion][Epoch 7785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:32,027 - INFO - [diffusion][Epoch 7786] Epoch 7787/12000
2024-11-05 02:24:36,133 - INFO - [diffusion][Epoch 7786] diffusion training Loss: 0.05324946064502001
2024-11-05 02:24:36,135 - INFO - [diffusion][Epoch 7786] diffusion learning rate: 0.001
2024-11-05 02:24:36,184 - INFO - [diffusion][Epoch 7786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:36,185 - INFO - [diffusion][Epoch 7787] Epoch 7788/12000
2024-11-05 02:24:40,288 - INFO - [diffusion][Epoch 7787] diffusion training Loss: 0.05775810219347477
2024-11-05 02:24:40,290 - INFO - [diffusion][Epoch 7787] diffusion learning rate: 0.001
2024-11-05 02:24:40,292 - INFO - [diffusion][Epoch 7787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:40,294 - INFO - [diffusion][Epoch 7788] Epoch 7789/12000
2024-11-05 02:24:44,432 - INFO - [diffusion][Epoch 7788] diffusion training Loss: 0.056071871891617775
2024-11-05 02:24:44,434 - INFO - [diffusion][Epoch 7788] diffusion learning rate: 0.001
2024-11-05 02:24:44,436 - INFO - [diffusion][Epoch 7788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:44,438 - INFO - [diffusion][Epoch 7789] Epoch 7790/12000
2024-11-05 02:24:48,570 - INFO - [diffusion][Epoch 7789] diffusion training Loss: 0.05823349114507437
2024-11-05 02:24:48,572 - INFO - [diffusion][Epoch 7789] diffusion learning rate: 0.001
2024-11-05 02:24:48,574 - INFO - [diffusion][Epoch 7789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:48,575 - INFO - [diffusion][Epoch 7790] Epoch 7791/12000
2024-11-05 02:24:52,540 - INFO - [diffusion][Epoch 7790] diffusion training Loss: 0.05696479789912701
2024-11-05 02:24:52,543 - INFO - [diffusion][Epoch 7790] diffusion learning rate: 0.001
2024-11-05 02:24:52,545 - INFO - [diffusion][Epoch 7790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:52,547 - INFO - [diffusion][Epoch 7791] Epoch 7792/12000
2024-11-05 02:24:56,711 - INFO - [diffusion][Epoch 7791] diffusion training Loss: 0.05854908190667629
2024-11-05 02:24:56,713 - INFO - [diffusion][Epoch 7791] diffusion learning rate: 0.001
2024-11-05 02:24:56,715 - INFO - [diffusion][Epoch 7791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:24:56,717 - INFO - [diffusion][Epoch 7792] Epoch 7793/12000
2024-11-05 02:25:00,724 - INFO - [diffusion][Epoch 7792] diffusion training Loss: 0.06072916276752949
2024-11-05 02:25:00,726 - INFO - [diffusion][Epoch 7792] diffusion learning rate: 0.001
2024-11-05 02:25:00,728 - INFO - [diffusion][Epoch 7792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:00,729 - INFO - [diffusion][Epoch 7793] Epoch 7794/12000
2024-11-05 02:25:04,756 - INFO - [diffusion][Epoch 7793] diffusion training Loss: 0.055936249904334545
2024-11-05 02:25:04,758 - INFO - [diffusion][Epoch 7793] diffusion learning rate: 0.001
2024-11-05 02:25:04,760 - INFO - [diffusion][Epoch 7793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:04,761 - INFO - [diffusion][Epoch 7794] Epoch 7795/12000
2024-11-05 02:25:08,713 - INFO - [diffusion][Epoch 7794] diffusion training Loss: 0.06070136558264494
2024-11-05 02:25:08,715 - INFO - [diffusion][Epoch 7794] diffusion learning rate: 0.001
2024-11-05 02:25:08,757 - INFO - [diffusion][Epoch 7794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:08,761 - INFO - [diffusion][Epoch 7795] Epoch 7796/12000
2024-11-05 02:25:12,881 - INFO - [diffusion][Epoch 7795] diffusion training Loss: 0.05383652448654175
2024-11-05 02:25:12,884 - INFO - [diffusion][Epoch 7795] diffusion learning rate: 0.001
2024-11-05 02:25:12,887 - INFO - [diffusion][Epoch 7795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:12,888 - INFO - [diffusion][Epoch 7796] Epoch 7797/12000
2024-11-05 02:25:16,989 - INFO - [diffusion][Epoch 7796] diffusion training Loss: 0.05368410516530275
2024-11-05 02:25:16,991 - INFO - [diffusion][Epoch 7796] diffusion learning rate: 0.001
2024-11-05 02:25:16,993 - INFO - [diffusion][Epoch 7796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:16,995 - INFO - [diffusion][Epoch 7797] Epoch 7798/12000
2024-11-05 02:25:21,250 - INFO - [diffusion][Epoch 7797] diffusion training Loss: 0.05413006339222193
2024-11-05 02:25:21,253 - INFO - [diffusion][Epoch 7797] diffusion learning rate: 0.001
2024-11-05 02:25:21,255 - INFO - [diffusion][Epoch 7797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:21,256 - INFO - [diffusion][Epoch 7798] Epoch 7799/12000
2024-11-05 02:25:25,415 - INFO - [diffusion][Epoch 7798] diffusion training Loss: 0.05266136955469847
2024-11-05 02:25:25,417 - INFO - [diffusion][Epoch 7798] diffusion learning rate: 0.001
2024-11-05 02:25:25,419 - INFO - [diffusion][Epoch 7798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:25,420 - INFO - [diffusion][Epoch 7799] Epoch 7800/12000
2024-11-05 02:25:29,433 - INFO - [diffusion][Epoch 7799] diffusion training Loss: 0.05534293409436941
2024-11-05 02:25:29,435 - INFO - [diffusion][Epoch 7799] diffusion learning rate: 0.001
2024-11-05 02:25:29,436 - INFO - [diffusion][Epoch 7799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:29,438 - INFO - [diffusion][Epoch 7800] Epoch 7801/12000
2024-11-05 02:25:33,494 - INFO - [diffusion][Epoch 7800] diffusion training Loss: 0.057426183484494686
2024-11-05 02:25:33,496 - INFO - [diffusion][Epoch 7800] diffusion learning rate: 0.001
2024-11-05 02:25:33,498 - INFO - [diffusion][Epoch 7800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:33,500 - INFO - [diffusion][Epoch 7801] Epoch 7802/12000
2024-11-05 02:25:37,465 - INFO - [diffusion][Epoch 7801] diffusion training Loss: 0.058534324169158936
2024-11-05 02:25:37,467 - INFO - [diffusion][Epoch 7801] diffusion learning rate: 0.001
2024-11-05 02:25:37,469 - INFO - [diffusion][Epoch 7801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:37,470 - INFO - [diffusion][Epoch 7802] Epoch 7803/12000
2024-11-05 02:25:41,609 - INFO - [diffusion][Epoch 7802] diffusion training Loss: 0.05418125540018082
2024-11-05 02:25:41,611 - INFO - [diffusion][Epoch 7802] diffusion learning rate: 0.001
2024-11-05 02:25:41,639 - INFO - [diffusion][Epoch 7802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:41,641 - INFO - [diffusion][Epoch 7803] Epoch 7804/12000
2024-11-05 02:25:45,672 - INFO - [diffusion][Epoch 7803] diffusion training Loss: 0.05808780435472727
2024-11-05 02:25:45,674 - INFO - [diffusion][Epoch 7803] diffusion learning rate: 0.001
2024-11-05 02:25:45,675 - INFO - [diffusion][Epoch 7803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:45,677 - INFO - [diffusion][Epoch 7804] Epoch 7805/12000
2024-11-05 02:25:49,773 - INFO - [diffusion][Epoch 7804] diffusion training Loss: 0.06157172378152609
2024-11-05 02:25:49,775 - INFO - [diffusion][Epoch 7804] diffusion learning rate: 0.001
2024-11-05 02:25:49,777 - INFO - [diffusion][Epoch 7804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:49,778 - INFO - [diffusion][Epoch 7805] Epoch 7806/12000
2024-11-05 02:25:53,875 - INFO - [diffusion][Epoch 7805] diffusion training Loss: 0.057652974501252174
2024-11-05 02:25:53,878 - INFO - [diffusion][Epoch 7805] diffusion learning rate: 0.001
2024-11-05 02:25:53,880 - INFO - [diffusion][Epoch 7805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:53,881 - INFO - [diffusion][Epoch 7806] Epoch 7807/12000
2024-11-05 02:25:57,818 - INFO - [diffusion][Epoch 7806] diffusion training Loss: 0.06540757603943348
2024-11-05 02:25:57,820 - INFO - [diffusion][Epoch 7806] diffusion learning rate: 0.001
2024-11-05 02:25:57,861 - INFO - [diffusion][Epoch 7806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:25:57,862 - INFO - [diffusion][Epoch 7807] Epoch 7808/12000
2024-11-05 02:26:01,886 - INFO - [diffusion][Epoch 7807] diffusion training Loss: 0.05400333274155855
2024-11-05 02:26:01,888 - INFO - [diffusion][Epoch 7807] diffusion learning rate: 0.001
2024-11-05 02:26:01,890 - INFO - [diffusion][Epoch 7807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:01,891 - INFO - [diffusion][Epoch 7808] Epoch 7809/12000
2024-11-05 02:26:05,860 - INFO - [diffusion][Epoch 7808] diffusion training Loss: 0.052856285125017166
2024-11-05 02:26:05,861 - INFO - [diffusion][Epoch 7808] diffusion learning rate: 0.001
2024-11-05 02:26:05,863 - INFO - [diffusion][Epoch 7808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:05,864 - INFO - [diffusion][Epoch 7809] Epoch 7810/12000
2024-11-05 02:26:09,999 - INFO - [diffusion][Epoch 7809] diffusion training Loss: 0.05640192702412605
2024-11-05 02:26:10,001 - INFO - [diffusion][Epoch 7809] diffusion learning rate: 0.001
2024-11-05 02:26:10,003 - INFO - [diffusion][Epoch 7809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:10,005 - INFO - [diffusion][Epoch 7810] Epoch 7811/12000
2024-11-05 02:26:14,133 - INFO - [diffusion][Epoch 7810] diffusion training Loss: 0.05006356071680784
2024-11-05 02:26:14,135 - INFO - [diffusion][Epoch 7810] diffusion learning rate: 0.001
2024-11-05 02:26:14,138 - INFO - [diffusion][Epoch 7810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:14,139 - INFO - [diffusion][Epoch 7811] Epoch 7812/12000
2024-11-05 02:26:18,264 - INFO - [diffusion][Epoch 7811] diffusion training Loss: 0.05439379531890154
2024-11-05 02:26:18,266 - INFO - [diffusion][Epoch 7811] diffusion learning rate: 0.001
2024-11-05 02:26:18,267 - INFO - [diffusion][Epoch 7811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:18,269 - INFO - [diffusion][Epoch 7812] Epoch 7813/12000
2024-11-05 02:26:22,364 - INFO - [diffusion][Epoch 7812] diffusion training Loss: 0.05454099830240011
2024-11-05 02:26:22,366 - INFO - [diffusion][Epoch 7812] diffusion learning rate: 0.001
2024-11-05 02:26:22,368 - INFO - [diffusion][Epoch 7812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:22,369 - INFO - [diffusion][Epoch 7813] Epoch 7814/12000
2024-11-05 02:26:26,356 - INFO - [diffusion][Epoch 7813] diffusion training Loss: 0.052136645652353764
2024-11-05 02:26:26,358 - INFO - [diffusion][Epoch 7813] diffusion learning rate: 0.001
2024-11-05 02:26:26,360 - INFO - [diffusion][Epoch 7813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:26,361 - INFO - [diffusion][Epoch 7814] Epoch 7815/12000
2024-11-05 02:26:30,412 - INFO - [diffusion][Epoch 7814] diffusion training Loss: 0.05737505480647087
2024-11-05 02:26:30,414 - INFO - [diffusion][Epoch 7814] diffusion learning rate: 0.001
2024-11-05 02:26:30,416 - INFO - [diffusion][Epoch 7814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:30,418 - INFO - [diffusion][Epoch 7815] Epoch 7816/12000
2024-11-05 02:26:34,360 - INFO - [diffusion][Epoch 7815] diffusion training Loss: 0.05527870263904333
2024-11-05 02:26:34,362 - INFO - [diffusion][Epoch 7815] diffusion learning rate: 0.001
2024-11-05 02:26:34,363 - INFO - [diffusion][Epoch 7815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:34,365 - INFO - [diffusion][Epoch 7816] Epoch 7817/12000
2024-11-05 02:26:38,473 - INFO - [diffusion][Epoch 7816] diffusion training Loss: 0.0530818235129118
2024-11-05 02:26:38,475 - INFO - [diffusion][Epoch 7816] diffusion learning rate: 0.001
2024-11-05 02:26:38,477 - INFO - [diffusion][Epoch 7816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:38,478 - INFO - [diffusion][Epoch 7817] Epoch 7818/12000
2024-11-05 02:26:42,589 - INFO - [diffusion][Epoch 7817] diffusion training Loss: 0.0546164670959115
2024-11-05 02:26:42,590 - INFO - [diffusion][Epoch 7817] diffusion learning rate: 0.001
2024-11-05 02:26:42,592 - INFO - [diffusion][Epoch 7817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:42,593 - INFO - [diffusion][Epoch 7818] Epoch 7819/12000
2024-11-05 02:26:46,742 - INFO - [diffusion][Epoch 7818] diffusion training Loss: 0.05507174599915743
2024-11-05 02:26:46,744 - INFO - [diffusion][Epoch 7818] diffusion learning rate: 0.001
2024-11-05 02:26:46,746 - INFO - [diffusion][Epoch 7818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:46,747 - INFO - [diffusion][Epoch 7819] Epoch 7820/12000
2024-11-05 02:26:50,884 - INFO - [diffusion][Epoch 7819] diffusion training Loss: 0.05279536359012127
2024-11-05 02:26:50,886 - INFO - [diffusion][Epoch 7819] diffusion learning rate: 0.001
2024-11-05 02:26:50,928 - INFO - [diffusion][Epoch 7819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:50,929 - INFO - [diffusion][Epoch 7820] Epoch 7821/12000
2024-11-05 02:26:55,094 - INFO - [diffusion][Epoch 7820] diffusion training Loss: 0.053462148644030094
2024-11-05 02:26:55,096 - INFO - [diffusion][Epoch 7820] diffusion learning rate: 0.001
2024-11-05 02:26:55,098 - INFO - [diffusion][Epoch 7820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:55,100 - INFO - [diffusion][Epoch 7821] Epoch 7822/12000
2024-11-05 02:26:59,209 - INFO - [diffusion][Epoch 7821] diffusion training Loss: 0.058353615924715996
2024-11-05 02:26:59,211 - INFO - [diffusion][Epoch 7821] diffusion learning rate: 0.001
2024-11-05 02:26:59,213 - INFO - [diffusion][Epoch 7821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:26:59,214 - INFO - [diffusion][Epoch 7822] Epoch 7823/12000
2024-11-05 02:27:03,355 - INFO - [diffusion][Epoch 7822] diffusion training Loss: 0.05893953703343868
2024-11-05 02:27:03,357 - INFO - [diffusion][Epoch 7822] diffusion learning rate: 0.001
2024-11-05 02:27:03,358 - INFO - [diffusion][Epoch 7822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:03,360 - INFO - [diffusion][Epoch 7823] Epoch 7824/12000
2024-11-05 02:27:07,520 - INFO - [diffusion][Epoch 7823] diffusion training Loss: 0.0576260220259428
2024-11-05 02:27:07,522 - INFO - [diffusion][Epoch 7823] diffusion learning rate: 0.001
2024-11-05 02:27:07,523 - INFO - [diffusion][Epoch 7823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:07,525 - INFO - [diffusion][Epoch 7824] Epoch 7825/12000
2024-11-05 02:27:11,676 - INFO - [diffusion][Epoch 7824] diffusion training Loss: 0.05760325212031603
2024-11-05 02:27:11,678 - INFO - [diffusion][Epoch 7824] diffusion learning rate: 0.001
2024-11-05 02:27:11,680 - INFO - [diffusion][Epoch 7824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:11,681 - INFO - [diffusion][Epoch 7825] Epoch 7826/12000
2024-11-05 02:27:15,767 - INFO - [diffusion][Epoch 7825] diffusion training Loss: 0.05467917583882809
2024-11-05 02:27:15,770 - INFO - [diffusion][Epoch 7825] diffusion learning rate: 0.001
2024-11-05 02:27:15,772 - INFO - [diffusion][Epoch 7825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:15,773 - INFO - [diffusion][Epoch 7826] Epoch 7827/12000
2024-11-05 02:27:19,910 - INFO - [diffusion][Epoch 7826] diffusion training Loss: 0.05290129501372576
2024-11-05 02:27:19,912 - INFO - [diffusion][Epoch 7826] diffusion learning rate: 0.001
2024-11-05 02:27:19,914 - INFO - [diffusion][Epoch 7826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:19,916 - INFO - [diffusion][Epoch 7827] Epoch 7828/12000
2024-11-05 02:27:24,047 - INFO - [diffusion][Epoch 7827] diffusion training Loss: 0.05866079684346914
2024-11-05 02:27:24,049 - INFO - [diffusion][Epoch 7827] diffusion learning rate: 0.001
2024-11-05 02:27:24,051 - INFO - [diffusion][Epoch 7827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:24,052 - INFO - [diffusion][Epoch 7828] Epoch 7829/12000
2024-11-05 02:27:28,129 - INFO - [diffusion][Epoch 7828] diffusion training Loss: 0.05927823483943939
2024-11-05 02:27:28,132 - INFO - [diffusion][Epoch 7828] diffusion learning rate: 0.001
2024-11-05 02:27:28,133 - INFO - [diffusion][Epoch 7828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:28,135 - INFO - [diffusion][Epoch 7829] Epoch 7830/12000
2024-11-05 02:27:32,262 - INFO - [diffusion][Epoch 7829] diffusion training Loss: 0.05408529471606016
2024-11-05 02:27:32,264 - INFO - [diffusion][Epoch 7829] diffusion learning rate: 0.001
2024-11-05 02:27:32,266 - INFO - [diffusion][Epoch 7829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:32,267 - INFO - [diffusion][Epoch 7830] Epoch 7831/12000
2024-11-05 02:27:36,432 - INFO - [diffusion][Epoch 7830] diffusion training Loss: 0.05590261146426201
2024-11-05 02:27:36,434 - INFO - [diffusion][Epoch 7830] diffusion learning rate: 0.001
2024-11-05 02:27:36,436 - INFO - [diffusion][Epoch 7830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:36,437 - INFO - [diffusion][Epoch 7831] Epoch 7832/12000
2024-11-05 02:27:40,513 - INFO - [diffusion][Epoch 7831] diffusion training Loss: 0.05657148361206055
2024-11-05 02:27:40,515 - INFO - [diffusion][Epoch 7831] diffusion learning rate: 0.001
2024-11-05 02:27:40,517 - INFO - [diffusion][Epoch 7831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:40,519 - INFO - [diffusion][Epoch 7832] Epoch 7833/12000
2024-11-05 02:27:44,644 - INFO - [diffusion][Epoch 7832] diffusion training Loss: 0.052569434978067875
2024-11-05 02:27:44,647 - INFO - [diffusion][Epoch 7832] diffusion learning rate: 0.001
2024-11-05 02:27:44,648 - INFO - [diffusion][Epoch 7832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:44,649 - INFO - [diffusion][Epoch 7833] Epoch 7834/12000
2024-11-05 02:27:48,742 - INFO - [diffusion][Epoch 7833] diffusion training Loss: 0.054007984697818756
2024-11-05 02:27:48,744 - INFO - [diffusion][Epoch 7833] diffusion learning rate: 0.001
2024-11-05 02:27:48,747 - INFO - [diffusion][Epoch 7833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:48,749 - INFO - [diffusion][Epoch 7834] Epoch 7835/12000
2024-11-05 02:27:52,750 - INFO - [diffusion][Epoch 7834] diffusion training Loss: 0.05420544184744358
2024-11-05 02:27:52,753 - INFO - [diffusion][Epoch 7834] diffusion learning rate: 0.001
2024-11-05 02:27:52,756 - INFO - [diffusion][Epoch 7834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:52,757 - INFO - [diffusion][Epoch 7835] Epoch 7836/12000
2024-11-05 02:27:56,827 - INFO - [diffusion][Epoch 7835] diffusion training Loss: 0.05486644525080919
2024-11-05 02:27:56,829 - INFO - [diffusion][Epoch 7835] diffusion learning rate: 0.001
2024-11-05 02:27:56,831 - INFO - [diffusion][Epoch 7835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:27:56,832 - INFO - [diffusion][Epoch 7836] Epoch 7837/12000
2024-11-05 02:28:00,963 - INFO - [diffusion][Epoch 7836] diffusion training Loss: 0.054389593191444874
2024-11-05 02:28:00,965 - INFO - [diffusion][Epoch 7836] diffusion learning rate: 0.001
2024-11-05 02:28:00,967 - INFO - [diffusion][Epoch 7836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:00,968 - INFO - [diffusion][Epoch 7837] Epoch 7838/12000
2024-11-05 02:28:04,939 - INFO - [diffusion][Epoch 7837] diffusion training Loss: 0.05443881079554558
2024-11-05 02:28:04,941 - INFO - [diffusion][Epoch 7837] diffusion learning rate: 0.001
2024-11-05 02:28:04,942 - INFO - [diffusion][Epoch 7837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:04,944 - INFO - [diffusion][Epoch 7838] Epoch 7839/12000
2024-11-05 02:28:08,987 - INFO - [diffusion][Epoch 7838] diffusion training Loss: 0.04693808313459158
2024-11-05 02:28:08,989 - INFO - [diffusion][Epoch 7838] diffusion learning rate: 0.001
2024-11-05 02:28:08,991 - INFO - [diffusion][Epoch 7838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:08,992 - INFO - [diffusion][Epoch 7839] Epoch 7840/12000
2024-11-05 02:28:13,301 - INFO - [diffusion][Epoch 7839] diffusion training Loss: 0.0563761331140995
2024-11-05 02:28:13,303 - INFO - [diffusion][Epoch 7839] diffusion learning rate: 0.001
2024-11-05 02:28:13,305 - INFO - [diffusion][Epoch 7839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:13,307 - INFO - [diffusion][Epoch 7840] Epoch 7841/12000
2024-11-05 02:28:17,464 - INFO - [diffusion][Epoch 7840] diffusion training Loss: 0.06093348376452923
2024-11-05 02:28:17,466 - INFO - [diffusion][Epoch 7840] diffusion learning rate: 0.001
2024-11-05 02:28:17,469 - INFO - [diffusion][Epoch 7840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:17,470 - INFO - [diffusion][Epoch 7841] Epoch 7842/12000
2024-11-05 02:28:21,532 - INFO - [diffusion][Epoch 7841] diffusion training Loss: 0.05945087317377329
2024-11-05 02:28:21,534 - INFO - [diffusion][Epoch 7841] diffusion learning rate: 0.001
2024-11-05 02:28:21,536 - INFO - [diffusion][Epoch 7841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:21,537 - INFO - [diffusion][Epoch 7842] Epoch 7843/12000
2024-11-05 02:28:25,564 - INFO - [diffusion][Epoch 7842] diffusion training Loss: 0.05942438542842865
2024-11-05 02:28:25,567 - INFO - [diffusion][Epoch 7842] diffusion learning rate: 0.001
2024-11-05 02:28:25,569 - INFO - [diffusion][Epoch 7842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:25,570 - INFO - [diffusion][Epoch 7843] Epoch 7844/12000
2024-11-05 02:28:29,710 - INFO - [diffusion][Epoch 7843] diffusion training Loss: 0.05020137969404459
2024-11-05 02:28:29,713 - INFO - [diffusion][Epoch 7843] diffusion learning rate: 0.001
2024-11-05 02:28:29,714 - INFO - [diffusion][Epoch 7843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:29,716 - INFO - [diffusion][Epoch 7844] Epoch 7845/12000
2024-11-05 02:28:33,864 - INFO - [diffusion][Epoch 7844] diffusion training Loss: 0.05853492580354214
2024-11-05 02:28:33,866 - INFO - [diffusion][Epoch 7844] diffusion learning rate: 0.001
2024-11-05 02:28:33,868 - INFO - [diffusion][Epoch 7844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:33,869 - INFO - [diffusion][Epoch 7845] Epoch 7846/12000
2024-11-05 02:28:37,942 - INFO - [diffusion][Epoch 7845] diffusion training Loss: 0.055873023346066475
2024-11-05 02:28:37,944 - INFO - [diffusion][Epoch 7845] diffusion learning rate: 0.001
2024-11-05 02:28:37,946 - INFO - [diffusion][Epoch 7845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:37,947 - INFO - [diffusion][Epoch 7846] Epoch 7847/12000
2024-11-05 02:28:41,968 - INFO - [diffusion][Epoch 7846] diffusion training Loss: 0.05779497139155865
2024-11-05 02:28:41,971 - INFO - [diffusion][Epoch 7846] diffusion learning rate: 0.001
2024-11-05 02:28:41,973 - INFO - [diffusion][Epoch 7846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:41,974 - INFO - [diffusion][Epoch 7847] Epoch 7848/12000
2024-11-05 02:28:45,945 - INFO - [diffusion][Epoch 7847] diffusion training Loss: 0.05440008733421564
2024-11-05 02:28:45,947 - INFO - [diffusion][Epoch 7847] diffusion learning rate: 0.001
2024-11-05 02:28:45,949 - INFO - [diffusion][Epoch 7847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:45,950 - INFO - [diffusion][Epoch 7848] Epoch 7849/12000
2024-11-05 02:28:50,018 - INFO - [diffusion][Epoch 7848] diffusion training Loss: 0.05988843645900488
2024-11-05 02:28:50,020 - INFO - [diffusion][Epoch 7848] diffusion learning rate: 0.001
2024-11-05 02:28:50,022 - INFO - [diffusion][Epoch 7848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:50,023 - INFO - [diffusion][Epoch 7849] Epoch 7850/12000
2024-11-05 02:28:54,092 - INFO - [diffusion][Epoch 7849] diffusion training Loss: 0.05674188770353794
2024-11-05 02:28:54,094 - INFO - [diffusion][Epoch 7849] diffusion learning rate: 0.001
2024-11-05 02:28:54,096 - INFO - [diffusion][Epoch 7849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:54,097 - INFO - [diffusion][Epoch 7850] Epoch 7851/12000
2024-11-05 02:28:58,244 - INFO - [diffusion][Epoch 7850] diffusion training Loss: 0.05136277060955763
2024-11-05 02:28:58,246 - INFO - [diffusion][Epoch 7850] diffusion learning rate: 0.001
2024-11-05 02:28:58,248 - INFO - [diffusion][Epoch 7850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:28:58,250 - INFO - [diffusion][Epoch 7851] Epoch 7852/12000
2024-11-05 02:29:02,392 - INFO - [diffusion][Epoch 7851] diffusion training Loss: 0.0506544541567564
2024-11-05 02:29:02,394 - INFO - [diffusion][Epoch 7851] diffusion learning rate: 0.001
2024-11-05 02:29:02,396 - INFO - [diffusion][Epoch 7851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:02,397 - INFO - [diffusion][Epoch 7852] Epoch 7853/12000
2024-11-05 02:29:06,530 - INFO - [diffusion][Epoch 7852] diffusion training Loss: 0.05432022921741009
2024-11-05 02:29:06,532 - INFO - [diffusion][Epoch 7852] diffusion learning rate: 0.001
2024-11-05 02:29:06,534 - INFO - [diffusion][Epoch 7852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:06,535 - INFO - [diffusion][Epoch 7853] Epoch 7854/12000
2024-11-05 02:29:10,596 - INFO - [diffusion][Epoch 7853] diffusion training Loss: 0.061224594712257385
2024-11-05 02:29:10,599 - INFO - [diffusion][Epoch 7853] diffusion learning rate: 0.001
2024-11-05 02:29:10,602 - INFO - [diffusion][Epoch 7853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:10,604 - INFO - [diffusion][Epoch 7854] Epoch 7855/12000
2024-11-05 02:29:14,738 - INFO - [diffusion][Epoch 7854] diffusion training Loss: 0.05578843969851732
2024-11-05 02:29:14,740 - INFO - [diffusion][Epoch 7854] diffusion learning rate: 0.001
2024-11-05 02:29:14,742 - INFO - [diffusion][Epoch 7854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:14,743 - INFO - [diffusion][Epoch 7855] Epoch 7856/12000
2024-11-05 02:29:18,806 - INFO - [diffusion][Epoch 7855] diffusion training Loss: 0.05196401942521334
2024-11-05 02:29:18,808 - INFO - [diffusion][Epoch 7855] diffusion learning rate: 0.001
2024-11-05 02:29:18,810 - INFO - [diffusion][Epoch 7855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:18,811 - INFO - [diffusion][Epoch 7856] Epoch 7857/12000
2024-11-05 02:29:22,913 - INFO - [diffusion][Epoch 7856] diffusion training Loss: 0.054928895086050034
2024-11-05 02:29:22,915 - INFO - [diffusion][Epoch 7856] diffusion learning rate: 0.001
2024-11-05 02:29:22,917 - INFO - [diffusion][Epoch 7856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:22,918 - INFO - [diffusion][Epoch 7857] Epoch 7858/12000
2024-11-05 02:29:26,931 - INFO - [diffusion][Epoch 7857] diffusion training Loss: 0.060860589146614075
2024-11-05 02:29:26,933 - INFO - [diffusion][Epoch 7857] diffusion learning rate: 0.001
2024-11-05 02:29:26,935 - INFO - [diffusion][Epoch 7857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:26,936 - INFO - [diffusion][Epoch 7858] Epoch 7859/12000
2024-11-05 02:29:30,984 - INFO - [diffusion][Epoch 7858] diffusion training Loss: 0.05382432974874973
2024-11-05 02:29:30,986 - INFO - [diffusion][Epoch 7858] diffusion learning rate: 0.001
2024-11-05 02:29:30,988 - INFO - [diffusion][Epoch 7858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:30,989 - INFO - [diffusion][Epoch 7859] Epoch 7860/12000
2024-11-05 02:29:35,091 - INFO - [diffusion][Epoch 7859] diffusion training Loss: 0.050189897418022156
2024-11-05 02:29:35,093 - INFO - [diffusion][Epoch 7859] diffusion learning rate: 0.001
2024-11-05 02:29:35,095 - INFO - [diffusion][Epoch 7859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:35,096 - INFO - [diffusion][Epoch 7860] Epoch 7861/12000
2024-11-05 02:29:39,191 - INFO - [diffusion][Epoch 7860] diffusion training Loss: 0.04851943161338568
2024-11-05 02:29:39,192 - INFO - [diffusion][Epoch 7860] diffusion learning rate: 0.001
2024-11-05 02:29:39,194 - INFO - [diffusion][Epoch 7860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:39,195 - INFO - [diffusion][Epoch 7861] Epoch 7862/12000
2024-11-05 02:29:43,249 - INFO - [diffusion][Epoch 7861] diffusion training Loss: 0.05157632101327181
2024-11-05 02:29:43,251 - INFO - [diffusion][Epoch 7861] diffusion learning rate: 0.001
2024-11-05 02:29:43,252 - INFO - [diffusion][Epoch 7861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:43,254 - INFO - [diffusion][Epoch 7862] Epoch 7863/12000
2024-11-05 02:29:47,205 - INFO - [diffusion][Epoch 7862] diffusion training Loss: 0.052413420751690865
2024-11-05 02:29:47,207 - INFO - [diffusion][Epoch 7862] diffusion learning rate: 0.001
2024-11-05 02:29:47,209 - INFO - [diffusion][Epoch 7862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:47,210 - INFO - [diffusion][Epoch 7863] Epoch 7864/12000
2024-11-05 02:29:51,357 - INFO - [diffusion][Epoch 7863] diffusion training Loss: 0.05510726384818554
2024-11-05 02:29:51,359 - INFO - [diffusion][Epoch 7863] diffusion learning rate: 0.001
2024-11-05 02:29:51,361 - INFO - [diffusion][Epoch 7863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:51,362 - INFO - [diffusion][Epoch 7864] Epoch 7865/12000
2024-11-05 02:29:55,467 - INFO - [diffusion][Epoch 7864] diffusion training Loss: 0.0522830905392766
2024-11-05 02:29:55,469 - INFO - [diffusion][Epoch 7864] diffusion learning rate: 0.001
2024-11-05 02:29:55,471 - INFO - [diffusion][Epoch 7864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:55,473 - INFO - [diffusion][Epoch 7865] Epoch 7866/12000
2024-11-05 02:29:59,592 - INFO - [diffusion][Epoch 7865] diffusion training Loss: 0.054061892442405224
2024-11-05 02:29:59,594 - INFO - [diffusion][Epoch 7865] diffusion learning rate: 0.001
2024-11-05 02:29:59,596 - INFO - [diffusion][Epoch 7865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:29:59,598 - INFO - [diffusion][Epoch 7866] Epoch 7867/12000
2024-11-05 02:30:03,738 - INFO - [diffusion][Epoch 7866] diffusion training Loss: 0.04787731450051069
2024-11-05 02:30:03,758 - INFO - [diffusion][Epoch 7866] diffusion learning rate: 0.001
2024-11-05 02:30:03,760 - INFO - [diffusion][Epoch 7866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:03,762 - INFO - [diffusion][Epoch 7867] Epoch 7868/12000
2024-11-05 02:30:07,790 - INFO - [diffusion][Epoch 7867] diffusion training Loss: 0.06229094136506319
2024-11-05 02:30:07,792 - INFO - [diffusion][Epoch 7867] diffusion learning rate: 0.001
2024-11-05 02:30:07,794 - INFO - [diffusion][Epoch 7867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:07,795 - INFO - [diffusion][Epoch 7868] Epoch 7869/12000
2024-11-05 02:30:11,874 - INFO - [diffusion][Epoch 7868] diffusion training Loss: 0.058608464896678925
2024-11-05 02:30:11,876 - INFO - [diffusion][Epoch 7868] diffusion learning rate: 0.001
2024-11-05 02:30:11,878 - INFO - [diffusion][Epoch 7868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:11,879 - INFO - [diffusion][Epoch 7869] Epoch 7870/12000
2024-11-05 02:30:15,937 - INFO - [diffusion][Epoch 7869] diffusion training Loss: 0.05777534190565348
2024-11-05 02:30:15,939 - INFO - [diffusion][Epoch 7869] diffusion learning rate: 0.001
2024-11-05 02:30:15,941 - INFO - [diffusion][Epoch 7869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:15,942 - INFO - [diffusion][Epoch 7870] Epoch 7871/12000
2024-11-05 02:30:20,046 - INFO - [diffusion][Epoch 7870] diffusion training Loss: 0.05505605787038803
2024-11-05 02:30:20,048 - INFO - [diffusion][Epoch 7870] diffusion learning rate: 0.001
2024-11-05 02:30:20,076 - INFO - [diffusion][Epoch 7870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:20,077 - INFO - [diffusion][Epoch 7871] Epoch 7872/12000
2024-11-05 02:30:24,161 - INFO - [diffusion][Epoch 7871] diffusion training Loss: 0.05073391180485487
2024-11-05 02:30:24,163 - INFO - [diffusion][Epoch 7871] diffusion learning rate: 0.001
2024-11-05 02:30:24,165 - INFO - [diffusion][Epoch 7871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:24,166 - INFO - [diffusion][Epoch 7872] Epoch 7873/12000
2024-11-05 02:30:28,255 - INFO - [diffusion][Epoch 7872] diffusion training Loss: 0.05219798441976309
2024-11-05 02:30:28,258 - INFO - [diffusion][Epoch 7872] diffusion learning rate: 0.001
2024-11-05 02:30:28,260 - INFO - [diffusion][Epoch 7872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:28,261 - INFO - [diffusion][Epoch 7873] Epoch 7874/12000
2024-11-05 02:30:32,241 - INFO - [diffusion][Epoch 7873] diffusion training Loss: 0.05607387609779835
2024-11-05 02:30:32,243 - INFO - [diffusion][Epoch 7873] diffusion learning rate: 0.001
2024-11-05 02:30:32,245 - INFO - [diffusion][Epoch 7873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:32,246 - INFO - [diffusion][Epoch 7874] Epoch 7875/12000
2024-11-05 02:30:36,321 - INFO - [diffusion][Epoch 7874] diffusion training Loss: 0.05668920557945967
2024-11-05 02:30:36,323 - INFO - [diffusion][Epoch 7874] diffusion learning rate: 0.001
2024-11-05 02:30:36,325 - INFO - [diffusion][Epoch 7874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:36,326 - INFO - [diffusion][Epoch 7875] Epoch 7876/12000
2024-11-05 02:30:40,229 - INFO - [diffusion][Epoch 7875] diffusion training Loss: 0.05744185019284487
2024-11-05 02:30:40,231 - INFO - [diffusion][Epoch 7875] diffusion learning rate: 0.001
2024-11-05 02:30:40,233 - INFO - [diffusion][Epoch 7875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:40,234 - INFO - [diffusion][Epoch 7876] Epoch 7877/12000
2024-11-05 02:30:44,292 - INFO - [diffusion][Epoch 7876] diffusion training Loss: 0.05145479924976826
2024-11-05 02:30:44,294 - INFO - [diffusion][Epoch 7876] diffusion learning rate: 0.001
2024-11-05 02:30:44,296 - INFO - [diffusion][Epoch 7876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:44,297 - INFO - [diffusion][Epoch 7877] Epoch 7878/12000
2024-11-05 02:30:48,359 - INFO - [diffusion][Epoch 7877] diffusion training Loss: 0.0523930573835969
2024-11-05 02:30:48,361 - INFO - [diffusion][Epoch 7877] diffusion learning rate: 0.001
2024-11-05 02:30:48,363 - INFO - [diffusion][Epoch 7877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:48,364 - INFO - [diffusion][Epoch 7878] Epoch 7879/12000
2024-11-05 02:30:52,355 - INFO - [diffusion][Epoch 7878] diffusion training Loss: 0.05303853750228882
2024-11-05 02:30:52,357 - INFO - [diffusion][Epoch 7878] diffusion learning rate: 0.001
2024-11-05 02:30:52,359 - INFO - [diffusion][Epoch 7878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:52,360 - INFO - [diffusion][Epoch 7879] Epoch 7880/12000
2024-11-05 02:30:56,406 - INFO - [diffusion][Epoch 7879] diffusion training Loss: 0.05816953629255295
2024-11-05 02:30:56,408 - INFO - [diffusion][Epoch 7879] diffusion learning rate: 0.001
2024-11-05 02:30:56,410 - INFO - [diffusion][Epoch 7879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:30:56,411 - INFO - [diffusion][Epoch 7880] Epoch 7881/12000
2024-11-05 02:31:00,749 - INFO - [diffusion][Epoch 7880] diffusion training Loss: 0.05632467567920685
2024-11-05 02:31:00,751 - INFO - [diffusion][Epoch 7880] diffusion learning rate: 0.001
2024-11-05 02:31:00,752 - INFO - [diffusion][Epoch 7880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:00,753 - INFO - [diffusion][Epoch 7881] Epoch 7882/12000
2024-11-05 02:31:04,806 - INFO - [diffusion][Epoch 7881] diffusion training Loss: 0.05301753245294094
2024-11-05 02:31:04,808 - INFO - [diffusion][Epoch 7881] diffusion learning rate: 0.001
2024-11-05 02:31:04,809 - INFO - [diffusion][Epoch 7881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:04,810 - INFO - [diffusion][Epoch 7882] Epoch 7883/12000
2024-11-05 02:31:08,913 - INFO - [diffusion][Epoch 7882] diffusion training Loss: 0.058914425782859325
2024-11-05 02:31:08,915 - INFO - [diffusion][Epoch 7882] diffusion learning rate: 0.001
2024-11-05 02:31:08,917 - INFO - [diffusion][Epoch 7882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:08,918 - INFO - [diffusion][Epoch 7883] Epoch 7884/12000
2024-11-05 02:31:13,062 - INFO - [diffusion][Epoch 7883] diffusion training Loss: 0.05676161590963602
2024-11-05 02:31:13,064 - INFO - [diffusion][Epoch 7883] diffusion learning rate: 0.001
2024-11-05 02:31:13,102 - INFO - [diffusion][Epoch 7883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:13,103 - INFO - [diffusion][Epoch 7884] Epoch 7885/12000
2024-11-05 02:31:17,051 - INFO - [diffusion][Epoch 7884] diffusion training Loss: 0.050404924899339676
2024-11-05 02:31:17,053 - INFO - [diffusion][Epoch 7884] diffusion learning rate: 0.001
2024-11-05 02:31:17,055 - INFO - [diffusion][Epoch 7884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:17,056 - INFO - [diffusion][Epoch 7885] Epoch 7886/12000
2024-11-05 02:31:21,148 - INFO - [diffusion][Epoch 7885] diffusion training Loss: 0.057424239814281464
2024-11-05 02:31:21,150 - INFO - [diffusion][Epoch 7885] diffusion learning rate: 0.001
2024-11-05 02:31:21,152 - INFO - [diffusion][Epoch 7885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:21,153 - INFO - [diffusion][Epoch 7886] Epoch 7887/12000
2024-11-05 02:31:25,178 - INFO - [diffusion][Epoch 7886] diffusion training Loss: 0.061736369505524635
2024-11-05 02:31:25,180 - INFO - [diffusion][Epoch 7886] diffusion learning rate: 0.001
2024-11-05 02:31:25,181 - INFO - [diffusion][Epoch 7886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:25,182 - INFO - [diffusion][Epoch 7887] Epoch 7888/12000
2024-11-05 02:31:29,081 - INFO - [diffusion][Epoch 7887] diffusion training Loss: 0.057215651497244835
2024-11-05 02:31:29,083 - INFO - [diffusion][Epoch 7887] diffusion learning rate: 0.001
2024-11-05 02:31:29,085 - INFO - [diffusion][Epoch 7887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:29,086 - INFO - [diffusion][Epoch 7888] Epoch 7889/12000
2024-11-05 02:31:33,049 - INFO - [diffusion][Epoch 7888] diffusion training Loss: 0.05393981747329235
2024-11-05 02:31:33,051 - INFO - [diffusion][Epoch 7888] diffusion learning rate: 0.001
2024-11-05 02:31:33,053 - INFO - [diffusion][Epoch 7888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:33,055 - INFO - [diffusion][Epoch 7889] Epoch 7890/12000
2024-11-05 02:31:37,157 - INFO - [diffusion][Epoch 7889] diffusion training Loss: 0.052369832061231136
2024-11-05 02:31:37,159 - INFO - [diffusion][Epoch 7889] diffusion learning rate: 0.001
2024-11-05 02:31:37,161 - INFO - [diffusion][Epoch 7889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:37,162 - INFO - [diffusion][Epoch 7890] Epoch 7891/12000
2024-11-05 02:31:41,213 - INFO - [diffusion][Epoch 7890] diffusion training Loss: 0.053940681740641594
2024-11-05 02:31:41,215 - INFO - [diffusion][Epoch 7890] diffusion learning rate: 0.001
2024-11-05 02:31:41,217 - INFO - [diffusion][Epoch 7890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:41,218 - INFO - [diffusion][Epoch 7891] Epoch 7892/12000
2024-11-05 02:31:45,182 - INFO - [diffusion][Epoch 7891] diffusion training Loss: 0.05428688321262598
2024-11-05 02:31:45,184 - INFO - [diffusion][Epoch 7891] diffusion learning rate: 0.001
2024-11-05 02:31:45,186 - INFO - [diffusion][Epoch 7891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:45,188 - INFO - [diffusion][Epoch 7892] Epoch 7893/12000
2024-11-05 02:31:49,235 - INFO - [diffusion][Epoch 7892] diffusion training Loss: 0.056749509647488594
2024-11-05 02:31:49,237 - INFO - [diffusion][Epoch 7892] diffusion learning rate: 0.001
2024-11-05 02:31:49,239 - INFO - [diffusion][Epoch 7892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:49,240 - INFO - [diffusion][Epoch 7893] Epoch 7894/12000
2024-11-05 02:31:53,334 - INFO - [diffusion][Epoch 7893] diffusion training Loss: 0.05214853025972843
2024-11-05 02:31:53,336 - INFO - [diffusion][Epoch 7893] diffusion learning rate: 0.001
2024-11-05 02:31:53,338 - INFO - [diffusion][Epoch 7893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:53,340 - INFO - [diffusion][Epoch 7894] Epoch 7895/12000
2024-11-05 02:31:57,341 - INFO - [diffusion][Epoch 7894] diffusion training Loss: 0.05877879448235035
2024-11-05 02:31:57,343 - INFO - [diffusion][Epoch 7894] diffusion learning rate: 0.001
2024-11-05 02:31:57,345 - INFO - [diffusion][Epoch 7894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:31:57,346 - INFO - [diffusion][Epoch 7895] Epoch 7896/12000
2024-11-05 02:32:01,351 - INFO - [diffusion][Epoch 7895] diffusion training Loss: 0.05326807498931885
2024-11-05 02:32:01,353 - INFO - [diffusion][Epoch 7895] diffusion learning rate: 0.001
2024-11-05 02:32:01,355 - INFO - [diffusion][Epoch 7895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:01,356 - INFO - [diffusion][Epoch 7896] Epoch 7897/12000
2024-11-05 02:32:05,362 - INFO - [diffusion][Epoch 7896] diffusion training Loss: 0.05491787102073431
2024-11-05 02:32:05,365 - INFO - [diffusion][Epoch 7896] diffusion learning rate: 0.001
2024-11-05 02:32:05,367 - INFO - [diffusion][Epoch 7896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:05,368 - INFO - [diffusion][Epoch 7897] Epoch 7898/12000
2024-11-05 02:32:09,538 - INFO - [diffusion][Epoch 7897] diffusion training Loss: 0.05715085379779339
2024-11-05 02:32:09,540 - INFO - [diffusion][Epoch 7897] diffusion learning rate: 0.001
2024-11-05 02:32:09,542 - INFO - [diffusion][Epoch 7897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:09,543 - INFO - [diffusion][Epoch 7898] Epoch 7899/12000
2024-11-05 02:32:13,594 - INFO - [diffusion][Epoch 7898] diffusion training Loss: 0.054796913638710976
2024-11-05 02:32:13,596 - INFO - [diffusion][Epoch 7898] diffusion learning rate: 0.001
2024-11-05 02:32:13,598 - INFO - [diffusion][Epoch 7898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:13,599 - INFO - [diffusion][Epoch 7899] Epoch 7900/12000
2024-11-05 02:32:17,563 - INFO - [diffusion][Epoch 7899] diffusion training Loss: 0.05281903222203255
2024-11-05 02:32:17,565 - INFO - [diffusion][Epoch 7899] diffusion learning rate: 0.001
2024-11-05 02:32:17,567 - INFO - [diffusion][Epoch 7899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:17,569 - INFO - [diffusion][Epoch 7900] Epoch 7901/12000
2024-11-05 02:32:21,960 - INFO - [diffusion][Epoch 7900] diffusion training Loss: 0.05662401486188173
2024-11-05 02:32:21,963 - INFO - [diffusion][Epoch 7900] diffusion learning rate: 0.001
2024-11-05 02:32:21,965 - INFO - [diffusion][Epoch 7900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:21,967 - INFO - [diffusion][Epoch 7901] Epoch 7902/12000
2024-11-05 02:32:26,091 - INFO - [diffusion][Epoch 7901] diffusion training Loss: 0.05896202754229307
2024-11-05 02:32:26,092 - INFO - [diffusion][Epoch 7901] diffusion learning rate: 0.001
2024-11-05 02:32:26,094 - INFO - [diffusion][Epoch 7901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:26,095 - INFO - [diffusion][Epoch 7902] Epoch 7903/12000
2024-11-05 02:32:30,182 - INFO - [diffusion][Epoch 7902] diffusion training Loss: 0.057744643650949
2024-11-05 02:32:30,184 - INFO - [diffusion][Epoch 7902] diffusion learning rate: 0.001
2024-11-05 02:32:30,186 - INFO - [diffusion][Epoch 7902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:30,187 - INFO - [diffusion][Epoch 7903] Epoch 7904/12000
2024-11-05 02:32:34,249 - INFO - [diffusion][Epoch 7903] diffusion training Loss: 0.054358954541385174
2024-11-05 02:32:34,251 - INFO - [diffusion][Epoch 7903] diffusion learning rate: 0.001
2024-11-05 02:32:34,254 - INFO - [diffusion][Epoch 7903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:34,255 - INFO - [diffusion][Epoch 7904] Epoch 7905/12000
2024-11-05 02:32:38,360 - INFO - [diffusion][Epoch 7904] diffusion training Loss: 0.05414054822176695
2024-11-05 02:32:38,362 - INFO - [diffusion][Epoch 7904] diffusion learning rate: 0.001
2024-11-05 02:32:38,364 - INFO - [diffusion][Epoch 7904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:38,365 - INFO - [diffusion][Epoch 7905] Epoch 7906/12000
2024-11-05 02:32:42,548 - INFO - [diffusion][Epoch 7905] diffusion training Loss: 0.05077015049755573
2024-11-05 02:32:42,551 - INFO - [diffusion][Epoch 7905] diffusion learning rate: 0.001
2024-11-05 02:32:42,553 - INFO - [diffusion][Epoch 7905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:42,554 - INFO - [diffusion][Epoch 7906] Epoch 7907/12000
2024-11-05 02:32:46,678 - INFO - [diffusion][Epoch 7906] diffusion training Loss: 0.05253567639738321
2024-11-05 02:32:46,680 - INFO - [diffusion][Epoch 7906] diffusion learning rate: 0.001
2024-11-05 02:32:46,682 - INFO - [diffusion][Epoch 7906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:46,683 - INFO - [diffusion][Epoch 7907] Epoch 7908/12000
2024-11-05 02:32:50,757 - INFO - [diffusion][Epoch 7907] diffusion training Loss: 0.05007594730705023
2024-11-05 02:32:50,759 - INFO - [diffusion][Epoch 7907] diffusion learning rate: 0.001
2024-11-05 02:32:50,761 - INFO - [diffusion][Epoch 7907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:50,762 - INFO - [diffusion][Epoch 7908] Epoch 7909/12000
2024-11-05 02:32:54,888 - INFO - [diffusion][Epoch 7908] diffusion training Loss: 0.05498672090470791
2024-11-05 02:32:54,891 - INFO - [diffusion][Epoch 7908] diffusion learning rate: 0.001
2024-11-05 02:32:54,893 - INFO - [diffusion][Epoch 7908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:54,894 - INFO - [diffusion][Epoch 7909] Epoch 7910/12000
2024-11-05 02:32:59,000 - INFO - [diffusion][Epoch 7909] diffusion training Loss: 0.05490401666611433
2024-11-05 02:32:59,002 - INFO - [diffusion][Epoch 7909] diffusion learning rate: 0.001
2024-11-05 02:32:59,004 - INFO - [diffusion][Epoch 7909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:32:59,006 - INFO - [diffusion][Epoch 7910] Epoch 7911/12000
2024-11-05 02:33:03,103 - INFO - [diffusion][Epoch 7910] diffusion training Loss: 0.050996698439121246
2024-11-05 02:33:03,105 - INFO - [diffusion][Epoch 7910] diffusion learning rate: 0.001
2024-11-05 02:33:03,107 - INFO - [diffusion][Epoch 7910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:03,108 - INFO - [diffusion][Epoch 7911] Epoch 7912/12000
2024-11-05 02:33:07,190 - INFO - [diffusion][Epoch 7911] diffusion training Loss: 0.052924404852092266
2024-11-05 02:33:07,192 - INFO - [diffusion][Epoch 7911] diffusion learning rate: 0.001
2024-11-05 02:33:07,194 - INFO - [diffusion][Epoch 7911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:07,195 - INFO - [diffusion][Epoch 7912] Epoch 7913/12000
2024-11-05 02:33:11,366 - INFO - [diffusion][Epoch 7912] diffusion training Loss: 0.04882199037820101
2024-11-05 02:33:11,368 - INFO - [diffusion][Epoch 7912] diffusion learning rate: 0.001
2024-11-05 02:33:11,369 - INFO - [diffusion][Epoch 7912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:11,371 - INFO - [diffusion][Epoch 7913] Epoch 7914/12000
2024-11-05 02:33:15,481 - INFO - [diffusion][Epoch 7913] diffusion training Loss: 0.05261569004505873
2024-11-05 02:33:15,483 - INFO - [diffusion][Epoch 7913] diffusion learning rate: 0.001
2024-11-05 02:33:15,484 - INFO - [diffusion][Epoch 7913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:15,486 - INFO - [diffusion][Epoch 7914] Epoch 7915/12000
2024-11-05 02:33:19,584 - INFO - [diffusion][Epoch 7914] diffusion training Loss: 0.05913341511040926
2024-11-05 02:33:19,586 - INFO - [diffusion][Epoch 7914] diffusion learning rate: 0.001
2024-11-05 02:33:19,588 - INFO - [diffusion][Epoch 7914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:19,589 - INFO - [diffusion][Epoch 7915] Epoch 7916/12000
2024-11-05 02:33:23,686 - INFO - [diffusion][Epoch 7915] diffusion training Loss: 0.04924710001796484
2024-11-05 02:33:23,688 - INFO - [diffusion][Epoch 7915] diffusion learning rate: 0.001
2024-11-05 02:33:23,690 - INFO - [diffusion][Epoch 7915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:23,691 - INFO - [diffusion][Epoch 7916] Epoch 7917/12000
2024-11-05 02:33:27,796 - INFO - [diffusion][Epoch 7916] diffusion training Loss: 0.0524735189974308
2024-11-05 02:33:27,798 - INFO - [diffusion][Epoch 7916] diffusion learning rate: 0.001
2024-11-05 02:33:27,799 - INFO - [diffusion][Epoch 7916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:27,801 - INFO - [diffusion][Epoch 7917] Epoch 7918/12000
2024-11-05 02:33:31,823 - INFO - [diffusion][Epoch 7917] diffusion training Loss: 0.05202810000628233
2024-11-05 02:33:31,825 - INFO - [diffusion][Epoch 7917] diffusion learning rate: 0.001
2024-11-05 02:33:31,827 - INFO - [diffusion][Epoch 7917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:31,828 - INFO - [diffusion][Epoch 7918] Epoch 7919/12000
2024-11-05 02:33:35,924 - INFO - [diffusion][Epoch 7918] diffusion training Loss: 0.05734125338494778
2024-11-05 02:33:35,927 - INFO - [diffusion][Epoch 7918] diffusion learning rate: 0.001
2024-11-05 02:33:35,929 - INFO - [diffusion][Epoch 7918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:35,930 - INFO - [diffusion][Epoch 7919] Epoch 7920/12000
2024-11-05 02:33:40,085 - INFO - [diffusion][Epoch 7919] diffusion training Loss: 0.05325326882302761
2024-11-05 02:33:40,087 - INFO - [diffusion][Epoch 7919] diffusion learning rate: 0.001
2024-11-05 02:33:40,089 - INFO - [diffusion][Epoch 7919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:40,090 - INFO - [diffusion][Epoch 7920] Epoch 7921/12000
2024-11-05 02:33:44,143 - INFO - [diffusion][Epoch 7920] diffusion training Loss: 0.05339368712157011
2024-11-05 02:33:44,145 - INFO - [diffusion][Epoch 7920] diffusion learning rate: 0.001
2024-11-05 02:33:44,147 - INFO - [diffusion][Epoch 7920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:44,148 - INFO - [diffusion][Epoch 7921] Epoch 7922/12000
2024-11-05 02:33:48,263 - INFO - [diffusion][Epoch 7921] diffusion training Loss: 0.05391490552574396
2024-11-05 02:33:48,265 - INFO - [diffusion][Epoch 7921] diffusion learning rate: 0.001
2024-11-05 02:33:48,267 - INFO - [diffusion][Epoch 7921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:48,268 - INFO - [diffusion][Epoch 7922] Epoch 7923/12000
2024-11-05 02:33:52,433 - INFO - [diffusion][Epoch 7922] diffusion training Loss: 0.05414362717419863
2024-11-05 02:33:52,435 - INFO - [diffusion][Epoch 7922] diffusion learning rate: 0.001
2024-11-05 02:33:52,436 - INFO - [diffusion][Epoch 7922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:52,438 - INFO - [diffusion][Epoch 7923] Epoch 7924/12000
2024-11-05 02:33:56,533 - INFO - [diffusion][Epoch 7923] diffusion training Loss: 0.05286882910877466
2024-11-05 02:33:56,535 - INFO - [diffusion][Epoch 7923] diffusion learning rate: 0.001
2024-11-05 02:33:56,537 - INFO - [diffusion][Epoch 7923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:33:56,538 - INFO - [diffusion][Epoch 7924] Epoch 7925/12000
2024-11-05 02:34:00,498 - INFO - [diffusion][Epoch 7924] diffusion training Loss: 0.05091241933405399
2024-11-05 02:34:00,500 - INFO - [diffusion][Epoch 7924] diffusion learning rate: 0.001
2024-11-05 02:34:00,502 - INFO - [diffusion][Epoch 7924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:00,503 - INFO - [diffusion][Epoch 7925] Epoch 7926/12000
2024-11-05 02:34:04,590 - INFO - [diffusion][Epoch 7925] diffusion training Loss: 0.06270483508706093
2024-11-05 02:34:04,592 - INFO - [diffusion][Epoch 7925] diffusion learning rate: 0.001
2024-11-05 02:34:04,593 - INFO - [diffusion][Epoch 7925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:04,595 - INFO - [diffusion][Epoch 7926] Epoch 7927/12000
2024-11-05 02:34:08,704 - INFO - [diffusion][Epoch 7926] diffusion training Loss: 0.05694737657904625
2024-11-05 02:34:08,706 - INFO - [diffusion][Epoch 7926] diffusion learning rate: 0.001
2024-11-05 02:34:08,708 - INFO - [diffusion][Epoch 7926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:08,709 - INFO - [diffusion][Epoch 7927] Epoch 7928/12000
2024-11-05 02:34:12,760 - INFO - [diffusion][Epoch 7927] diffusion training Loss: 0.0524950735270977
2024-11-05 02:34:12,762 - INFO - [diffusion][Epoch 7927] diffusion learning rate: 0.001
2024-11-05 02:34:12,765 - INFO - [diffusion][Epoch 7927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:12,766 - INFO - [diffusion][Epoch 7928] Epoch 7929/12000
2024-11-05 02:34:16,809 - INFO - [diffusion][Epoch 7928] diffusion training Loss: 0.060793185606598854
2024-11-05 02:34:16,811 - INFO - [diffusion][Epoch 7928] diffusion learning rate: 0.001
2024-11-05 02:34:16,814 - INFO - [diffusion][Epoch 7928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:16,815 - INFO - [diffusion][Epoch 7929] Epoch 7930/12000
2024-11-05 02:34:20,920 - INFO - [diffusion][Epoch 7929] diffusion training Loss: 0.05751522071659565
2024-11-05 02:34:20,921 - INFO - [diffusion][Epoch 7929] diffusion learning rate: 0.001
2024-11-05 02:34:20,923 - INFO - [diffusion][Epoch 7929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:20,924 - INFO - [diffusion][Epoch 7930] Epoch 7931/12000
2024-11-05 02:34:25,013 - INFO - [diffusion][Epoch 7930] diffusion training Loss: 0.05740573722869158
2024-11-05 02:34:25,015 - INFO - [diffusion][Epoch 7930] diffusion learning rate: 0.001
2024-11-05 02:34:25,017 - INFO - [diffusion][Epoch 7930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:25,018 - INFO - [diffusion][Epoch 7931] Epoch 7932/12000
2024-11-05 02:34:29,153 - INFO - [diffusion][Epoch 7931] diffusion training Loss: 0.054917678236961365
2024-11-05 02:34:29,155 - INFO - [diffusion][Epoch 7931] diffusion learning rate: 0.001
2024-11-05 02:34:29,157 - INFO - [diffusion][Epoch 7931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:29,158 - INFO - [diffusion][Epoch 7932] Epoch 7933/12000
2024-11-05 02:34:33,229 - INFO - [diffusion][Epoch 7932] diffusion training Loss: 0.060331239365041256
2024-11-05 02:34:33,232 - INFO - [diffusion][Epoch 7932] diffusion learning rate: 0.001
2024-11-05 02:34:33,234 - INFO - [diffusion][Epoch 7932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:33,235 - INFO - [diffusion][Epoch 7933] Epoch 7934/12000
2024-11-05 02:34:37,256 - INFO - [diffusion][Epoch 7933] diffusion training Loss: 0.05200923793017864
2024-11-05 02:34:37,259 - INFO - [diffusion][Epoch 7933] diffusion learning rate: 0.001
2024-11-05 02:34:37,261 - INFO - [diffusion][Epoch 7933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:37,262 - INFO - [diffusion][Epoch 7934] Epoch 7935/12000
2024-11-05 02:34:41,339 - INFO - [diffusion][Epoch 7934] diffusion training Loss: 0.06273312587291002
2024-11-05 02:34:41,341 - INFO - [diffusion][Epoch 7934] diffusion learning rate: 0.001
2024-11-05 02:34:41,343 - INFO - [diffusion][Epoch 7934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:41,344 - INFO - [diffusion][Epoch 7935] Epoch 7936/12000
2024-11-05 02:34:45,292 - INFO - [diffusion][Epoch 7935] diffusion training Loss: 0.05219610594213009
2024-11-05 02:34:45,295 - INFO - [diffusion][Epoch 7935] diffusion learning rate: 0.001
2024-11-05 02:34:45,296 - INFO - [diffusion][Epoch 7935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:45,298 - INFO - [diffusion][Epoch 7936] Epoch 7937/12000
2024-11-05 02:34:49,281 - INFO - [diffusion][Epoch 7936] diffusion training Loss: 0.05555360205471516
2024-11-05 02:34:49,283 - INFO - [diffusion][Epoch 7936] diffusion learning rate: 0.001
2024-11-05 02:34:49,285 - INFO - [diffusion][Epoch 7936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:49,287 - INFO - [diffusion][Epoch 7937] Epoch 7938/12000
2024-11-05 02:34:53,356 - INFO - [diffusion][Epoch 7937] diffusion training Loss: 0.049874681048095226
2024-11-05 02:34:53,358 - INFO - [diffusion][Epoch 7937] diffusion learning rate: 0.001
2024-11-05 02:34:53,360 - INFO - [diffusion][Epoch 7937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:53,361 - INFO - [diffusion][Epoch 7938] Epoch 7939/12000
2024-11-05 02:34:57,497 - INFO - [diffusion][Epoch 7938] diffusion training Loss: 0.05315663944929838
2024-11-05 02:34:57,499 - INFO - [diffusion][Epoch 7938] diffusion learning rate: 0.001
2024-11-05 02:34:57,501 - INFO - [diffusion][Epoch 7938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:34:57,502 - INFO - [diffusion][Epoch 7939] Epoch 7940/12000
2024-11-05 02:35:01,585 - INFO - [diffusion][Epoch 7939] diffusion training Loss: 0.05047753918915987
2024-11-05 02:35:01,587 - INFO - [diffusion][Epoch 7939] diffusion learning rate: 0.001
2024-11-05 02:35:01,589 - INFO - [diffusion][Epoch 7939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:01,590 - INFO - [diffusion][Epoch 7940] Epoch 7941/12000
2024-11-05 02:35:05,642 - INFO - [diffusion][Epoch 7940] diffusion training Loss: 0.05448274873197079
2024-11-05 02:35:05,644 - INFO - [diffusion][Epoch 7940] diffusion learning rate: 0.001
2024-11-05 02:35:05,646 - INFO - [diffusion][Epoch 7940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:05,647 - INFO - [diffusion][Epoch 7941] Epoch 7942/12000
2024-11-05 02:35:09,810 - INFO - [diffusion][Epoch 7941] diffusion training Loss: 0.05199682340025902
2024-11-05 02:35:09,812 - INFO - [diffusion][Epoch 7941] diffusion learning rate: 0.001
2024-11-05 02:35:09,813 - INFO - [diffusion][Epoch 7941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:09,815 - INFO - [diffusion][Epoch 7942] Epoch 7943/12000
2024-11-05 02:35:13,966 - INFO - [diffusion][Epoch 7942] diffusion training Loss: 0.05246251914650202
2024-11-05 02:35:13,968 - INFO - [diffusion][Epoch 7942] diffusion learning rate: 0.001
2024-11-05 02:35:13,970 - INFO - [diffusion][Epoch 7942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:13,971 - INFO - [diffusion][Epoch 7943] Epoch 7944/12000
2024-11-05 02:35:17,984 - INFO - [diffusion][Epoch 7943] diffusion training Loss: 0.058590665459632874
2024-11-05 02:35:17,986 - INFO - [diffusion][Epoch 7943] diffusion learning rate: 0.001
2024-11-05 02:35:17,988 - INFO - [diffusion][Epoch 7943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:17,989 - INFO - [diffusion][Epoch 7944] Epoch 7945/12000
2024-11-05 02:35:22,090 - INFO - [diffusion][Epoch 7944] diffusion training Loss: 0.0550058837980032
2024-11-05 02:35:22,092 - INFO - [diffusion][Epoch 7944] diffusion learning rate: 0.001
2024-11-05 02:35:22,094 - INFO - [diffusion][Epoch 7944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:22,095 - INFO - [diffusion][Epoch 7945] Epoch 7946/12000
2024-11-05 02:35:26,159 - INFO - [diffusion][Epoch 7945] diffusion training Loss: 0.052857584320008755
2024-11-05 02:35:26,168 - INFO - [diffusion][Epoch 7945] diffusion learning rate: 0.001
2024-11-05 02:35:26,170 - INFO - [diffusion][Epoch 7945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:26,172 - INFO - [diffusion][Epoch 7946] Epoch 7947/12000
2024-11-05 02:35:30,161 - INFO - [diffusion][Epoch 7946] diffusion training Loss: 0.056330849416553974
2024-11-05 02:35:30,163 - INFO - [diffusion][Epoch 7946] diffusion learning rate: 0.001
2024-11-05 02:35:30,165 - INFO - [diffusion][Epoch 7946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:30,166 - INFO - [diffusion][Epoch 7947] Epoch 7948/12000
2024-11-05 02:35:34,175 - INFO - [diffusion][Epoch 7947] diffusion training Loss: 0.052297831512987614
2024-11-05 02:35:34,177 - INFO - [diffusion][Epoch 7947] diffusion learning rate: 0.001
2024-11-05 02:35:34,179 - INFO - [diffusion][Epoch 7947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:34,180 - INFO - [diffusion][Epoch 7948] Epoch 7949/12000
2024-11-05 02:35:38,427 - INFO - [diffusion][Epoch 7948] diffusion training Loss: 0.05370714794844389
2024-11-05 02:35:38,429 - INFO - [diffusion][Epoch 7948] diffusion learning rate: 0.001
2024-11-05 02:35:38,431 - INFO - [diffusion][Epoch 7948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:38,432 - INFO - [diffusion][Epoch 7949] Epoch 7950/12000
2024-11-05 02:35:42,557 - INFO - [diffusion][Epoch 7949] diffusion training Loss: 0.05137320049107075
2024-11-05 02:35:42,560 - INFO - [diffusion][Epoch 7949] diffusion learning rate: 0.001
2024-11-05 02:35:42,562 - INFO - [diffusion][Epoch 7949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:42,563 - INFO - [diffusion][Epoch 7950] Epoch 7951/12000
2024-11-05 02:35:46,748 - INFO - [diffusion][Epoch 7950] diffusion training Loss: 0.05454606935381889
2024-11-05 02:35:46,750 - INFO - [diffusion][Epoch 7950] diffusion learning rate: 0.001
2024-11-05 02:35:46,752 - INFO - [diffusion][Epoch 7950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:46,753 - INFO - [diffusion][Epoch 7951] Epoch 7952/12000
2024-11-05 02:35:50,888 - INFO - [diffusion][Epoch 7951] diffusion training Loss: 0.05433669686317444
2024-11-05 02:35:50,890 - INFO - [diffusion][Epoch 7951] diffusion learning rate: 0.001
2024-11-05 02:35:50,892 - INFO - [diffusion][Epoch 7951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:50,893 - INFO - [diffusion][Epoch 7952] Epoch 7953/12000
2024-11-05 02:35:55,022 - INFO - [diffusion][Epoch 7952] diffusion training Loss: 0.0569934593513608
2024-11-05 02:35:55,024 - INFO - [diffusion][Epoch 7952] diffusion learning rate: 0.001
2024-11-05 02:35:55,026 - INFO - [diffusion][Epoch 7952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:55,028 - INFO - [diffusion][Epoch 7953] Epoch 7954/12000
2024-11-05 02:35:59,178 - INFO - [diffusion][Epoch 7953] diffusion training Loss: 0.05367174465209246
2024-11-05 02:35:59,180 - INFO - [diffusion][Epoch 7953] diffusion learning rate: 0.001
2024-11-05 02:35:59,182 - INFO - [diffusion][Epoch 7953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:35:59,184 - INFO - [diffusion][Epoch 7954] Epoch 7955/12000
2024-11-05 02:36:03,323 - INFO - [diffusion][Epoch 7954] diffusion training Loss: 0.0543962437659502
2024-11-05 02:36:03,325 - INFO - [diffusion][Epoch 7954] diffusion learning rate: 0.001
2024-11-05 02:36:03,327 - INFO - [diffusion][Epoch 7954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:03,328 - INFO - [diffusion][Epoch 7955] Epoch 7956/12000
2024-11-05 02:36:07,452 - INFO - [diffusion][Epoch 7955] diffusion training Loss: 0.05569890607148409
2024-11-05 02:36:07,454 - INFO - [diffusion][Epoch 7955] diffusion learning rate: 0.001
2024-11-05 02:36:07,456 - INFO - [diffusion][Epoch 7955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:07,457 - INFO - [diffusion][Epoch 7956] Epoch 7957/12000
2024-11-05 02:36:11,673 - INFO - [diffusion][Epoch 7956] diffusion training Loss: 0.052969856187701225
2024-11-05 02:36:11,675 - INFO - [diffusion][Epoch 7956] diffusion learning rate: 0.001
2024-11-05 02:36:11,677 - INFO - [diffusion][Epoch 7956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:11,678 - INFO - [diffusion][Epoch 7957] Epoch 7958/12000
2024-11-05 02:36:15,749 - INFO - [diffusion][Epoch 7957] diffusion training Loss: 0.053215259686112404
2024-11-05 02:36:15,751 - INFO - [diffusion][Epoch 7957] diffusion learning rate: 0.001
2024-11-05 02:36:15,752 - INFO - [diffusion][Epoch 7957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:15,754 - INFO - [diffusion][Epoch 7958] Epoch 7959/12000
2024-11-05 02:36:19,711 - INFO - [diffusion][Epoch 7958] diffusion training Loss: 0.05561578180640936
2024-11-05 02:36:19,714 - INFO - [diffusion][Epoch 7958] diffusion learning rate: 0.001
2024-11-05 02:36:19,715 - INFO - [diffusion][Epoch 7958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:19,717 - INFO - [diffusion][Epoch 7959] Epoch 7960/12000
2024-11-05 02:36:23,746 - INFO - [diffusion][Epoch 7959] diffusion training Loss: 0.05131559260189533
2024-11-05 02:36:23,748 - INFO - [diffusion][Epoch 7959] diffusion learning rate: 0.001
2024-11-05 02:36:23,750 - INFO - [diffusion][Epoch 7959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:23,751 - INFO - [diffusion][Epoch 7960] Epoch 7961/12000
2024-11-05 02:36:27,899 - INFO - [diffusion][Epoch 7960] diffusion training Loss: 0.0524451220408082
2024-11-05 02:36:27,901 - INFO - [diffusion][Epoch 7960] diffusion learning rate: 0.001
2024-11-05 02:36:27,903 - INFO - [diffusion][Epoch 7960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:27,904 - INFO - [diffusion][Epoch 7961] Epoch 7962/12000
2024-11-05 02:36:31,975 - INFO - [diffusion][Epoch 7961] diffusion training Loss: 0.05432390421628952
2024-11-05 02:36:31,977 - INFO - [diffusion][Epoch 7961] diffusion learning rate: 0.001
2024-11-05 02:36:31,978 - INFO - [diffusion][Epoch 7961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:31,980 - INFO - [diffusion][Epoch 7962] Epoch 7963/12000
2024-11-05 02:36:36,166 - INFO - [diffusion][Epoch 7962] diffusion training Loss: 0.06214647274464369
2024-11-05 02:36:36,169 - INFO - [diffusion][Epoch 7962] diffusion learning rate: 0.001
2024-11-05 02:36:36,170 - INFO - [diffusion][Epoch 7962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:36,172 - INFO - [diffusion][Epoch 7963] Epoch 7964/12000
2024-11-05 02:36:40,261 - INFO - [diffusion][Epoch 7963] diffusion training Loss: 0.05698457919061184
2024-11-05 02:36:40,263 - INFO - [diffusion][Epoch 7963] diffusion learning rate: 0.001
2024-11-05 02:36:40,265 - INFO - [diffusion][Epoch 7963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:40,267 - INFO - [diffusion][Epoch 7964] Epoch 7965/12000
2024-11-05 02:36:44,392 - INFO - [diffusion][Epoch 7964] diffusion training Loss: 0.05683149676769972
2024-11-05 02:36:44,394 - INFO - [diffusion][Epoch 7964] diffusion learning rate: 0.001
2024-11-05 02:36:44,396 - INFO - [diffusion][Epoch 7964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:44,397 - INFO - [diffusion][Epoch 7965] Epoch 7966/12000
2024-11-05 02:36:48,353 - INFO - [diffusion][Epoch 7965] diffusion training Loss: 0.05313768144696951
2024-11-05 02:36:48,355 - INFO - [diffusion][Epoch 7965] diffusion learning rate: 0.001
2024-11-05 02:36:48,356 - INFO - [diffusion][Epoch 7965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:48,358 - INFO - [diffusion][Epoch 7966] Epoch 7967/12000
2024-11-05 02:36:52,465 - INFO - [diffusion][Epoch 7966] diffusion training Loss: 0.05452090408653021
2024-11-05 02:36:52,468 - INFO - [diffusion][Epoch 7966] diffusion learning rate: 0.001
2024-11-05 02:36:52,470 - INFO - [diffusion][Epoch 7966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:52,471 - INFO - [diffusion][Epoch 7967] Epoch 7968/12000
2024-11-05 02:36:56,447 - INFO - [diffusion][Epoch 7967] diffusion training Loss: 0.056293497793376446
2024-11-05 02:36:56,449 - INFO - [diffusion][Epoch 7967] diffusion learning rate: 0.001
2024-11-05 02:36:56,451 - INFO - [diffusion][Epoch 7967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:36:56,452 - INFO - [diffusion][Epoch 7968] Epoch 7969/12000
2024-11-05 02:37:00,539 - INFO - [diffusion][Epoch 7968] diffusion training Loss: 0.05830857623368502
2024-11-05 02:37:00,541 - INFO - [diffusion][Epoch 7968] diffusion learning rate: 0.001
2024-11-05 02:37:00,543 - INFO - [diffusion][Epoch 7968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:00,544 - INFO - [diffusion][Epoch 7969] Epoch 7970/12000
2024-11-05 02:37:04,600 - INFO - [diffusion][Epoch 7969] diffusion training Loss: 0.05498220305889845
2024-11-05 02:37:04,602 - INFO - [diffusion][Epoch 7969] diffusion learning rate: 0.001
2024-11-05 02:37:04,604 - INFO - [diffusion][Epoch 7969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:04,606 - INFO - [diffusion][Epoch 7970] Epoch 7971/12000
2024-11-05 02:37:08,602 - INFO - [diffusion][Epoch 7970] diffusion training Loss: 0.05894695594906807
2024-11-05 02:37:08,604 - INFO - [diffusion][Epoch 7970] diffusion learning rate: 0.001
2024-11-05 02:37:08,606 - INFO - [diffusion][Epoch 7970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:08,607 - INFO - [diffusion][Epoch 7971] Epoch 7972/12000
2024-11-05 02:37:12,667 - INFO - [diffusion][Epoch 7971] diffusion training Loss: 0.0545523427426815
2024-11-05 02:37:12,875 - INFO - [diffusion][Epoch 7971] diffusion learning rate: 0.001
2024-11-05 02:37:12,877 - INFO - [diffusion][Epoch 7971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:12,878 - INFO - [diffusion][Epoch 7972] Epoch 7973/12000
2024-11-05 02:37:16,951 - INFO - [diffusion][Epoch 7972] diffusion training Loss: 0.05342399887740612
2024-11-05 02:37:16,953 - INFO - [diffusion][Epoch 7972] diffusion learning rate: 0.001
2024-11-05 02:37:16,955 - INFO - [diffusion][Epoch 7972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:16,956 - INFO - [diffusion][Epoch 7973] Epoch 7974/12000
2024-11-05 02:37:21,027 - INFO - [diffusion][Epoch 7973] diffusion training Loss: 0.05127372592687607
2024-11-05 02:37:21,029 - INFO - [diffusion][Epoch 7973] diffusion learning rate: 0.001
2024-11-05 02:37:21,031 - INFO - [diffusion][Epoch 7973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:21,032 - INFO - [diffusion][Epoch 7974] Epoch 7975/12000
2024-11-05 02:37:25,135 - INFO - [diffusion][Epoch 7974] diffusion training Loss: 0.056420937180519104
2024-11-05 02:37:25,138 - INFO - [diffusion][Epoch 7974] diffusion learning rate: 0.001
2024-11-05 02:37:25,139 - INFO - [diffusion][Epoch 7974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:25,141 - INFO - [diffusion][Epoch 7975] Epoch 7976/12000
2024-11-05 02:37:29,294 - INFO - [diffusion][Epoch 7975] diffusion training Loss: 0.05003676190972328
2024-11-05 02:37:29,296 - INFO - [diffusion][Epoch 7975] diffusion learning rate: 0.001
2024-11-05 02:37:29,298 - INFO - [diffusion][Epoch 7975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:29,300 - INFO - [diffusion][Epoch 7976] Epoch 7977/12000
2024-11-05 02:37:33,366 - INFO - [diffusion][Epoch 7976] diffusion training Loss: 0.05183858145028353
2024-11-05 02:37:33,368 - INFO - [diffusion][Epoch 7976] diffusion learning rate: 0.001
2024-11-05 02:37:33,370 - INFO - [diffusion][Epoch 7976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:33,372 - INFO - [diffusion][Epoch 7977] Epoch 7978/12000
2024-11-05 02:37:37,513 - INFO - [diffusion][Epoch 7977] diffusion training Loss: 0.05739895533770323
2024-11-05 02:37:37,515 - INFO - [diffusion][Epoch 7977] diffusion learning rate: 0.001
2024-11-05 02:37:37,517 - INFO - [diffusion][Epoch 7977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:37,518 - INFO - [diffusion][Epoch 7978] Epoch 7979/12000
2024-11-05 02:37:41,578 - INFO - [diffusion][Epoch 7978] diffusion training Loss: 0.060591522604227066
2024-11-05 02:37:41,579 - INFO - [diffusion][Epoch 7978] diffusion learning rate: 0.001
2024-11-05 02:37:41,581 - INFO - [diffusion][Epoch 7978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:41,582 - INFO - [diffusion][Epoch 7979] Epoch 7980/12000
2024-11-05 02:37:45,677 - INFO - [diffusion][Epoch 7979] diffusion training Loss: 0.05522277485579252
2024-11-05 02:37:45,680 - INFO - [diffusion][Epoch 7979] diffusion learning rate: 0.001
2024-11-05 02:37:45,682 - INFO - [diffusion][Epoch 7979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:45,683 - INFO - [diffusion][Epoch 7980] Epoch 7981/12000
2024-11-05 02:37:49,747 - INFO - [diffusion][Epoch 7980] diffusion training Loss: 0.05584876611828804
2024-11-05 02:37:49,749 - INFO - [diffusion][Epoch 7980] diffusion learning rate: 0.001
2024-11-05 02:37:49,750 - INFO - [diffusion][Epoch 7980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:49,752 - INFO - [diffusion][Epoch 7981] Epoch 7982/12000
2024-11-05 02:37:53,921 - INFO - [diffusion][Epoch 7981] diffusion training Loss: 0.053522903472185135
2024-11-05 02:37:53,923 - INFO - [diffusion][Epoch 7981] diffusion learning rate: 0.001
2024-11-05 02:37:53,925 - INFO - [diffusion][Epoch 7981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:53,926 - INFO - [diffusion][Epoch 7982] Epoch 7983/12000
2024-11-05 02:37:58,151 - INFO - [diffusion][Epoch 7982] diffusion training Loss: 0.05540409032255411
2024-11-05 02:37:58,153 - INFO - [diffusion][Epoch 7982] diffusion learning rate: 0.001
2024-11-05 02:37:58,154 - INFO - [diffusion][Epoch 7982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:37:58,156 - INFO - [diffusion][Epoch 7983] Epoch 7984/12000
2024-11-05 02:38:02,215 - INFO - [diffusion][Epoch 7983] diffusion training Loss: 0.052189518697559834
2024-11-05 02:38:02,217 - INFO - [diffusion][Epoch 7983] diffusion learning rate: 0.001
2024-11-05 02:38:02,245 - INFO - [diffusion][Epoch 7983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:02,247 - INFO - [diffusion][Epoch 7984] Epoch 7985/12000
2024-11-05 02:38:06,361 - INFO - [diffusion][Epoch 7984] diffusion training Loss: 0.05543221440166235
2024-11-05 02:38:06,363 - INFO - [diffusion][Epoch 7984] diffusion learning rate: 0.001
2024-11-05 02:38:06,365 - INFO - [diffusion][Epoch 7984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:06,366 - INFO - [diffusion][Epoch 7985] Epoch 7986/12000
2024-11-05 02:38:10,538 - INFO - [diffusion][Epoch 7985] diffusion training Loss: 0.0505458926782012
2024-11-05 02:38:10,540 - INFO - [diffusion][Epoch 7985] diffusion learning rate: 0.001
2024-11-05 02:38:10,542 - INFO - [diffusion][Epoch 7985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:10,543 - INFO - [diffusion][Epoch 7986] Epoch 7987/12000
2024-11-05 02:38:14,614 - INFO - [diffusion][Epoch 7986] diffusion training Loss: 0.05557669047266245
2024-11-05 02:38:14,616 - INFO - [diffusion][Epoch 7986] diffusion learning rate: 0.001
2024-11-05 02:38:14,618 - INFO - [diffusion][Epoch 7986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:14,619 - INFO - [diffusion][Epoch 7987] Epoch 7988/12000
2024-11-05 02:38:18,729 - INFO - [diffusion][Epoch 7987] diffusion training Loss: 0.05108164809644222
2024-11-05 02:38:18,731 - INFO - [diffusion][Epoch 7987] diffusion learning rate: 0.001
2024-11-05 02:38:18,733 - INFO - [diffusion][Epoch 7987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:18,735 - INFO - [diffusion][Epoch 7988] Epoch 7989/12000
2024-11-05 02:38:22,855 - INFO - [diffusion][Epoch 7988] diffusion training Loss: 0.050719231367111206
2024-11-05 02:38:22,857 - INFO - [diffusion][Epoch 7988] diffusion learning rate: 0.001
2024-11-05 02:38:22,858 - INFO - [diffusion][Epoch 7988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:22,860 - INFO - [diffusion][Epoch 7989] Epoch 7990/12000
2024-11-05 02:38:26,970 - INFO - [diffusion][Epoch 7989] diffusion training Loss: 0.05711894854903221
2024-11-05 02:38:26,972 - INFO - [diffusion][Epoch 7989] diffusion learning rate: 0.001
2024-11-05 02:38:26,973 - INFO - [diffusion][Epoch 7989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:26,975 - INFO - [diffusion][Epoch 7990] Epoch 7991/12000
2024-11-05 02:38:31,116 - INFO - [diffusion][Epoch 7990] diffusion training Loss: 0.054218025878071785
2024-11-05 02:38:31,118 - INFO - [diffusion][Epoch 7990] diffusion learning rate: 0.001
2024-11-05 02:38:31,120 - INFO - [diffusion][Epoch 7990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:31,121 - INFO - [diffusion][Epoch 7991] Epoch 7992/12000
2024-11-05 02:38:35,164 - INFO - [diffusion][Epoch 7991] diffusion training Loss: 0.05615533050149679
2024-11-05 02:38:35,165 - INFO - [diffusion][Epoch 7991] diffusion learning rate: 0.001
2024-11-05 02:38:35,167 - INFO - [diffusion][Epoch 7991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:35,168 - INFO - [diffusion][Epoch 7992] Epoch 7993/12000
2024-11-05 02:38:39,198 - INFO - [diffusion][Epoch 7992] diffusion training Loss: 0.05375590920448303
2024-11-05 02:38:39,201 - INFO - [diffusion][Epoch 7992] diffusion learning rate: 0.001
2024-11-05 02:38:39,204 - INFO - [diffusion][Epoch 7992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:39,205 - INFO - [diffusion][Epoch 7993] Epoch 7994/12000
2024-11-05 02:38:43,288 - INFO - [diffusion][Epoch 7993] diffusion training Loss: 0.056591542437672615
2024-11-05 02:38:43,291 - INFO - [diffusion][Epoch 7993] diffusion learning rate: 0.001
2024-11-05 02:38:43,293 - INFO - [diffusion][Epoch 7993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:43,294 - INFO - [diffusion][Epoch 7994] Epoch 7995/12000
2024-11-05 02:38:47,369 - INFO - [diffusion][Epoch 7994] diffusion training Loss: 0.053126380778849125
2024-11-05 02:38:47,371 - INFO - [diffusion][Epoch 7994] diffusion learning rate: 0.001
2024-11-05 02:38:47,373 - INFO - [diffusion][Epoch 7994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:47,374 - INFO - [diffusion][Epoch 7995] Epoch 7996/12000
2024-11-05 02:38:51,467 - INFO - [diffusion][Epoch 7995] diffusion training Loss: 0.05140805244445801
2024-11-05 02:38:51,469 - INFO - [diffusion][Epoch 7995] diffusion learning rate: 0.001
2024-11-05 02:38:51,471 - INFO - [diffusion][Epoch 7995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:51,472 - INFO - [diffusion][Epoch 7996] Epoch 7997/12000
2024-11-05 02:38:55,603 - INFO - [diffusion][Epoch 7996] diffusion training Loss: 0.04808571469038725
2024-11-05 02:38:55,605 - INFO - [diffusion][Epoch 7996] diffusion learning rate: 0.001
2024-11-05 02:38:55,607 - INFO - [diffusion][Epoch 7996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:55,608 - INFO - [diffusion][Epoch 7997] Epoch 7998/12000
2024-11-05 02:38:59,727 - INFO - [diffusion][Epoch 7997] diffusion training Loss: 0.057288115844130516
2024-11-05 02:38:59,731 - INFO - [diffusion][Epoch 7997] diffusion learning rate: 0.001
2024-11-05 02:38:59,733 - INFO - [diffusion][Epoch 7997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:38:59,734 - INFO - [diffusion][Epoch 7998] Epoch 7999/12000
2024-11-05 02:39:03,837 - INFO - [diffusion][Epoch 7998] diffusion training Loss: 0.04965735785663128
2024-11-05 02:39:03,839 - INFO - [diffusion][Epoch 7998] diffusion learning rate: 0.001
2024-11-05 02:39:03,841 - INFO - [diffusion][Epoch 7998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:03,842 - INFO - [diffusion][Epoch 7999] Epoch 8000/12000
2024-11-05 02:39:07,923 - INFO - [diffusion][Epoch 7999] diffusion training Loss: 0.0579299358651042
2024-11-05 02:39:07,925 - INFO - [diffusion][Epoch 7999] diffusion learning rate: 0.001
2024-11-05 02:39:08,115 - INFO - [diffusion][Epoch 7999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:08,117 - INFO - [diffusion][Epoch 8000] Epoch 8001/12000
2024-11-05 02:39:12,177 - INFO - [diffusion][Epoch 8000] diffusion training Loss: 0.05261017847806215
2024-11-05 02:39:12,312 - INFO - [diffusion][Epoch 8000] diffusion learning rate: 0.001
2024-11-05 02:39:12,314 - INFO - [diffusion][Epoch 8000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:12,315 - INFO - [diffusion][Epoch 8001] Epoch 8002/12000
2024-11-05 02:39:16,278 - INFO - [diffusion][Epoch 8001] diffusion training Loss: 0.05331613961607218
2024-11-05 02:39:16,280 - INFO - [diffusion][Epoch 8001] diffusion learning rate: 0.001
2024-11-05 02:39:16,282 - INFO - [diffusion][Epoch 8001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:16,283 - INFO - [diffusion][Epoch 8002] Epoch 8003/12000
2024-11-05 02:39:20,327 - INFO - [diffusion][Epoch 8002] diffusion training Loss: 0.05688661336898804
2024-11-05 02:39:20,329 - INFO - [diffusion][Epoch 8002] diffusion learning rate: 0.001
2024-11-05 02:39:20,330 - INFO - [diffusion][Epoch 8002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:20,332 - INFO - [diffusion][Epoch 8003] Epoch 8004/12000
2024-11-05 02:39:24,442 - INFO - [diffusion][Epoch 8003] diffusion training Loss: 0.05308537557721138
2024-11-05 02:39:24,444 - INFO - [diffusion][Epoch 8003] diffusion learning rate: 0.001
2024-11-05 02:39:24,445 - INFO - [diffusion][Epoch 8003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:24,447 - INFO - [diffusion][Epoch 8004] Epoch 8005/12000
2024-11-05 02:39:28,585 - INFO - [diffusion][Epoch 8004] diffusion training Loss: 0.05435098987072706
2024-11-05 02:39:28,588 - INFO - [diffusion][Epoch 8004] diffusion learning rate: 0.001
2024-11-05 02:39:28,589 - INFO - [diffusion][Epoch 8004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:28,591 - INFO - [diffusion][Epoch 8005] Epoch 8006/12000
2024-11-05 02:39:32,757 - INFO - [diffusion][Epoch 8005] diffusion training Loss: 0.05613811407238245
2024-11-05 02:39:32,759 - INFO - [diffusion][Epoch 8005] diffusion learning rate: 0.001
2024-11-05 02:39:32,786 - INFO - [diffusion][Epoch 8005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:32,787 - INFO - [diffusion][Epoch 8006] Epoch 8007/12000
2024-11-05 02:39:36,908 - INFO - [diffusion][Epoch 8006] diffusion training Loss: 0.05232109967619181
2024-11-05 02:39:36,911 - INFO - [diffusion][Epoch 8006] diffusion learning rate: 0.001
2024-11-05 02:39:36,913 - INFO - [diffusion][Epoch 8006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:36,914 - INFO - [diffusion][Epoch 8007] Epoch 8008/12000
2024-11-05 02:39:41,018 - INFO - [diffusion][Epoch 8007] diffusion training Loss: 0.06025996431708336
2024-11-05 02:39:41,021 - INFO - [diffusion][Epoch 8007] diffusion learning rate: 0.001
2024-11-05 02:39:41,023 - INFO - [diffusion][Epoch 8007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:41,024 - INFO - [diffusion][Epoch 8008] Epoch 8009/12000
2024-11-05 02:39:45,109 - INFO - [diffusion][Epoch 8008] diffusion training Loss: 0.06383874639868736
2024-11-05 02:39:45,112 - INFO - [diffusion][Epoch 8008] diffusion learning rate: 0.001
2024-11-05 02:39:45,114 - INFO - [diffusion][Epoch 8008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:45,115 - INFO - [diffusion][Epoch 8009] Epoch 8010/12000
2024-11-05 02:39:49,232 - INFO - [diffusion][Epoch 8009] diffusion training Loss: 0.04809770919382572
2024-11-05 02:39:49,234 - INFO - [diffusion][Epoch 8009] diffusion learning rate: 0.001
2024-11-05 02:39:49,236 - INFO - [diffusion][Epoch 8009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:49,237 - INFO - [diffusion][Epoch 8010] Epoch 8011/12000
2024-11-05 02:39:53,319 - INFO - [diffusion][Epoch 8010] diffusion training Loss: 0.05843939073383808
2024-11-05 02:39:53,349 - INFO - [diffusion][Epoch 8010] diffusion learning rate: 0.001
2024-11-05 02:39:53,351 - INFO - [diffusion][Epoch 8010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:53,352 - INFO - [diffusion][Epoch 8011] Epoch 8012/12000
2024-11-05 02:39:57,481 - INFO - [diffusion][Epoch 8011] diffusion training Loss: 0.05823822133243084
2024-11-05 02:39:57,484 - INFO - [diffusion][Epoch 8011] diffusion learning rate: 0.001
2024-11-05 02:39:57,489 - INFO - [diffusion][Epoch 8011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:39:57,491 - INFO - [diffusion][Epoch 8012] Epoch 8013/12000
2024-11-05 02:40:01,472 - INFO - [diffusion][Epoch 8012] diffusion training Loss: 0.05155771877616644
2024-11-05 02:40:01,474 - INFO - [diffusion][Epoch 8012] diffusion learning rate: 0.001
2024-11-05 02:40:01,476 - INFO - [diffusion][Epoch 8012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:01,477 - INFO - [diffusion][Epoch 8013] Epoch 8014/12000
2024-11-05 02:40:05,589 - INFO - [diffusion][Epoch 8013] diffusion training Loss: 0.05876218434423208
2024-11-05 02:40:05,592 - INFO - [diffusion][Epoch 8013] diffusion learning rate: 0.001
2024-11-05 02:40:05,594 - INFO - [diffusion][Epoch 8013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:05,595 - INFO - [diffusion][Epoch 8014] Epoch 8015/12000
2024-11-05 02:40:09,710 - INFO - [diffusion][Epoch 8014] diffusion training Loss: 0.05186655092984438
2024-11-05 02:40:09,712 - INFO - [diffusion][Epoch 8014] diffusion learning rate: 0.001
2024-11-05 02:40:09,715 - INFO - [diffusion][Epoch 8014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:09,716 - INFO - [diffusion][Epoch 8015] Epoch 8016/12000
2024-11-05 02:40:13,914 - INFO - [diffusion][Epoch 8015] diffusion training Loss: 0.05743846297264099
2024-11-05 02:40:13,916 - INFO - [diffusion][Epoch 8015] diffusion learning rate: 0.001
2024-11-05 02:40:13,960 - INFO - [diffusion][Epoch 8015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:13,961 - INFO - [diffusion][Epoch 8016] Epoch 8017/12000
2024-11-05 02:40:18,134 - INFO - [diffusion][Epoch 8016] diffusion training Loss: 0.05532977916300297
2024-11-05 02:40:18,136 - INFO - [diffusion][Epoch 8016] diffusion learning rate: 0.001
2024-11-05 02:40:18,138 - INFO - [diffusion][Epoch 8016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:18,139 - INFO - [diffusion][Epoch 8017] Epoch 8018/12000
2024-11-05 02:40:22,285 - INFO - [diffusion][Epoch 8017] diffusion training Loss: 0.05798941198736429
2024-11-05 02:40:22,288 - INFO - [diffusion][Epoch 8017] diffusion learning rate: 0.001
2024-11-05 02:40:22,289 - INFO - [diffusion][Epoch 8017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:22,291 - INFO - [diffusion][Epoch 8018] Epoch 8019/12000
2024-11-05 02:40:26,367 - INFO - [diffusion][Epoch 8018] diffusion training Loss: 0.05292028561234474
2024-11-05 02:40:26,369 - INFO - [diffusion][Epoch 8018] diffusion learning rate: 0.001
2024-11-05 02:40:26,371 - INFO - [diffusion][Epoch 8018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:26,372 - INFO - [diffusion][Epoch 8019] Epoch 8020/12000
2024-11-05 02:40:30,319 - INFO - [diffusion][Epoch 8019] diffusion training Loss: 0.05563355144113302
2024-11-05 02:40:30,321 - INFO - [diffusion][Epoch 8019] diffusion learning rate: 0.001
2024-11-05 02:40:30,323 - INFO - [diffusion][Epoch 8019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:30,324 - INFO - [diffusion][Epoch 8020] Epoch 8021/12000
2024-11-05 02:40:34,341 - INFO - [diffusion][Epoch 8020] diffusion training Loss: 0.0521415751427412
2024-11-05 02:40:34,343 - INFO - [diffusion][Epoch 8020] diffusion learning rate: 0.001
2024-11-05 02:40:34,345 - INFO - [diffusion][Epoch 8020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:34,347 - INFO - [diffusion][Epoch 8021] Epoch 8022/12000
2024-11-05 02:40:38,420 - INFO - [diffusion][Epoch 8021] diffusion training Loss: 0.05277799628674984
2024-11-05 02:40:38,422 - INFO - [diffusion][Epoch 8021] diffusion learning rate: 0.001
2024-11-05 02:40:38,424 - INFO - [diffusion][Epoch 8021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:38,425 - INFO - [diffusion][Epoch 8022] Epoch 8023/12000
2024-11-05 02:40:42,538 - INFO - [diffusion][Epoch 8022] diffusion training Loss: 0.05589801445603371
2024-11-05 02:40:42,541 - INFO - [diffusion][Epoch 8022] diffusion learning rate: 0.001
2024-11-05 02:40:42,542 - INFO - [diffusion][Epoch 8022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:42,544 - INFO - [diffusion][Epoch 8023] Epoch 8024/12000
2024-11-05 02:40:46,960 - INFO - [diffusion][Epoch 8023] diffusion training Loss: 0.04933311324566603
2024-11-05 02:40:46,962 - INFO - [diffusion][Epoch 8023] diffusion learning rate: 0.001
2024-11-05 02:40:46,963 - INFO - [diffusion][Epoch 8023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:46,965 - INFO - [diffusion][Epoch 8024] Epoch 8025/12000
2024-11-05 02:40:51,031 - INFO - [diffusion][Epoch 8024] diffusion training Loss: 0.053551994264125824
2024-11-05 02:40:51,033 - INFO - [diffusion][Epoch 8024] diffusion learning rate: 0.001
2024-11-05 02:40:51,035 - INFO - [diffusion][Epoch 8024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:51,036 - INFO - [diffusion][Epoch 8025] Epoch 8026/12000
2024-11-05 02:40:55,041 - INFO - [diffusion][Epoch 8025] diffusion training Loss: 0.05400964058935642
2024-11-05 02:40:55,043 - INFO - [diffusion][Epoch 8025] diffusion learning rate: 0.001
2024-11-05 02:40:55,045 - INFO - [diffusion][Epoch 8025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:55,046 - INFO - [diffusion][Epoch 8026] Epoch 8027/12000
2024-11-05 02:40:58,921 - INFO - [diffusion][Epoch 8026] diffusion training Loss: 0.04867887031286955
2024-11-05 02:40:58,923 - INFO - [diffusion][Epoch 8026] diffusion learning rate: 0.001
2024-11-05 02:40:58,925 - INFO - [diffusion][Epoch 8026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:40:58,927 - INFO - [diffusion][Epoch 8027] Epoch 8028/12000
2024-11-05 02:41:02,988 - INFO - [diffusion][Epoch 8027] diffusion training Loss: 0.05313854571431875
2024-11-05 02:41:02,991 - INFO - [diffusion][Epoch 8027] diffusion learning rate: 0.001
2024-11-05 02:41:02,993 - INFO - [diffusion][Epoch 8027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:02,994 - INFO - [diffusion][Epoch 8028] Epoch 8029/12000
2024-11-05 02:41:06,960 - INFO - [diffusion][Epoch 8028] diffusion training Loss: 0.05387434735894203
2024-11-05 02:41:06,963 - INFO - [diffusion][Epoch 8028] diffusion learning rate: 0.001
2024-11-05 02:41:07,004 - INFO - [diffusion][Epoch 8028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:07,006 - INFO - [diffusion][Epoch 8029] Epoch 8030/12000
2024-11-05 02:41:11,087 - INFO - [diffusion][Epoch 8029] diffusion training Loss: 0.05498649552464485
2024-11-05 02:41:11,089 - INFO - [diffusion][Epoch 8029] diffusion learning rate: 0.001
2024-11-05 02:41:11,091 - INFO - [diffusion][Epoch 8029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:11,092 - INFO - [diffusion][Epoch 8030] Epoch 8031/12000
2024-11-05 02:41:15,213 - INFO - [diffusion][Epoch 8030] diffusion training Loss: 0.04946043714880943
2024-11-05 02:41:15,215 - INFO - [diffusion][Epoch 8030] diffusion learning rate: 0.001
2024-11-05 02:41:15,217 - INFO - [diffusion][Epoch 8030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:15,219 - INFO - [diffusion][Epoch 8031] Epoch 8032/12000
2024-11-05 02:41:19,237 - INFO - [diffusion][Epoch 8031] diffusion training Loss: 0.05143522843718529
2024-11-05 02:41:19,239 - INFO - [diffusion][Epoch 8031] diffusion learning rate: 0.001
2024-11-05 02:41:19,241 - INFO - [diffusion][Epoch 8031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:19,243 - INFO - [diffusion][Epoch 8032] Epoch 8033/12000
2024-11-05 02:41:23,296 - INFO - [diffusion][Epoch 8032] diffusion training Loss: 0.0597445759922266
2024-11-05 02:41:23,298 - INFO - [diffusion][Epoch 8032] diffusion learning rate: 0.001
2024-11-05 02:41:23,300 - INFO - [diffusion][Epoch 8032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:23,302 - INFO - [diffusion][Epoch 8033] Epoch 8034/12000
2024-11-05 02:41:27,434 - INFO - [diffusion][Epoch 8033] diffusion training Loss: 0.05230991821736097
2024-11-05 02:41:27,436 - INFO - [diffusion][Epoch 8033] diffusion learning rate: 0.001
2024-11-05 02:41:27,438 - INFO - [diffusion][Epoch 8033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:27,439 - INFO - [diffusion][Epoch 8034] Epoch 8035/12000
2024-11-05 02:41:31,369 - INFO - [diffusion][Epoch 8034] diffusion training Loss: 0.052885377779603004
2024-11-05 02:41:31,371 - INFO - [diffusion][Epoch 8034] diffusion learning rate: 0.001
2024-11-05 02:41:31,372 - INFO - [diffusion][Epoch 8034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:31,374 - INFO - [diffusion][Epoch 8035] Epoch 8036/12000
2024-11-05 02:41:35,483 - INFO - [diffusion][Epoch 8035] diffusion training Loss: 0.05351839307695627
2024-11-05 02:41:35,485 - INFO - [diffusion][Epoch 8035] diffusion learning rate: 0.001
2024-11-05 02:41:35,487 - INFO - [diffusion][Epoch 8035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:35,489 - INFO - [diffusion][Epoch 8036] Epoch 8037/12000
2024-11-05 02:41:39,500 - INFO - [diffusion][Epoch 8036] diffusion training Loss: 0.052364631555974483
2024-11-05 02:41:39,502 - INFO - [diffusion][Epoch 8036] diffusion learning rate: 0.001
2024-11-05 02:41:39,504 - INFO - [diffusion][Epoch 8036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:39,505 - INFO - [diffusion][Epoch 8037] Epoch 8038/12000
2024-11-05 02:41:43,519 - INFO - [diffusion][Epoch 8037] diffusion training Loss: 0.05390764493495226
2024-11-05 02:41:43,521 - INFO - [diffusion][Epoch 8037] diffusion learning rate: 0.001
2024-11-05 02:41:43,524 - INFO - [diffusion][Epoch 8037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:43,525 - INFO - [diffusion][Epoch 8038] Epoch 8039/12000
2024-11-05 02:41:47,638 - INFO - [diffusion][Epoch 8038] diffusion training Loss: 0.05469202343374491
2024-11-05 02:41:47,640 - INFO - [diffusion][Epoch 8038] diffusion learning rate: 0.001
2024-11-05 02:41:47,642 - INFO - [diffusion][Epoch 8038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:47,643 - INFO - [diffusion][Epoch 8039] Epoch 8040/12000
2024-11-05 02:41:51,699 - INFO - [diffusion][Epoch 8039] diffusion training Loss: 0.05273118335753679
2024-11-05 02:41:51,701 - INFO - [diffusion][Epoch 8039] diffusion learning rate: 0.001
2024-11-05 02:41:51,726 - INFO - [diffusion][Epoch 8039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:51,727 - INFO - [diffusion][Epoch 8040] Epoch 8041/12000
2024-11-05 02:41:55,825 - INFO - [diffusion][Epoch 8040] diffusion training Loss: 0.05480777472257614
2024-11-05 02:41:55,827 - INFO - [diffusion][Epoch 8040] diffusion learning rate: 0.001
2024-11-05 02:41:55,828 - INFO - [diffusion][Epoch 8040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:55,830 - INFO - [diffusion][Epoch 8041] Epoch 8042/12000
2024-11-05 02:41:59,935 - INFO - [diffusion][Epoch 8041] diffusion training Loss: 0.04942656494677067
2024-11-05 02:41:59,937 - INFO - [diffusion][Epoch 8041] diffusion learning rate: 0.001
2024-11-05 02:41:59,939 - INFO - [diffusion][Epoch 8041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:41:59,941 - INFO - [diffusion][Epoch 8042] Epoch 8043/12000
2024-11-05 02:42:04,232 - INFO - [diffusion][Epoch 8042] diffusion training Loss: 0.052575740963220596
2024-11-05 02:42:04,234 - INFO - [diffusion][Epoch 8042] diffusion learning rate: 0.001
2024-11-05 02:42:04,236 - INFO - [diffusion][Epoch 8042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:04,237 - INFO - [diffusion][Epoch 8043] Epoch 8044/12000
2024-11-05 02:42:08,201 - INFO - [diffusion][Epoch 8043] diffusion training Loss: 0.0562519459053874
2024-11-05 02:42:08,204 - INFO - [diffusion][Epoch 8043] diffusion learning rate: 0.001
2024-11-05 02:42:08,206 - INFO - [diffusion][Epoch 8043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:08,207 - INFO - [diffusion][Epoch 8044] Epoch 8045/12000
2024-11-05 02:42:12,339 - INFO - [diffusion][Epoch 8044] diffusion training Loss: 0.057162958197295666
2024-11-05 02:42:12,342 - INFO - [diffusion][Epoch 8044] diffusion learning rate: 0.001
2024-11-05 02:42:12,343 - INFO - [diffusion][Epoch 8044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:12,345 - INFO - [diffusion][Epoch 8045] Epoch 8046/12000
2024-11-05 02:42:16,300 - INFO - [diffusion][Epoch 8045] diffusion training Loss: 0.05305423494428396
2024-11-05 02:42:16,302 - INFO - [diffusion][Epoch 8045] diffusion learning rate: 0.001
2024-11-05 02:42:16,304 - INFO - [diffusion][Epoch 8045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:16,305 - INFO - [diffusion][Epoch 8046] Epoch 8047/12000
2024-11-05 02:42:20,467 - INFO - [diffusion][Epoch 8046] diffusion training Loss: 0.053411202505230904
2024-11-05 02:42:20,470 - INFO - [diffusion][Epoch 8046] diffusion learning rate: 0.001
2024-11-05 02:42:20,472 - INFO - [diffusion][Epoch 8046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:20,473 - INFO - [diffusion][Epoch 8047] Epoch 8048/12000
2024-11-05 02:42:24,438 - INFO - [diffusion][Epoch 8047] diffusion training Loss: 0.053533244878053665
2024-11-05 02:42:24,440 - INFO - [diffusion][Epoch 8047] diffusion learning rate: 0.001
2024-11-05 02:42:24,444 - INFO - [diffusion][Epoch 8047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:24,445 - INFO - [diffusion][Epoch 8048] Epoch 8049/12000
2024-11-05 02:42:28,458 - INFO - [diffusion][Epoch 8048] diffusion training Loss: 0.05617978144437075
2024-11-05 02:42:28,460 - INFO - [diffusion][Epoch 8048] diffusion learning rate: 0.001
2024-11-05 02:42:28,462 - INFO - [diffusion][Epoch 8048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:28,463 - INFO - [diffusion][Epoch 8049] Epoch 8050/12000
2024-11-05 02:42:32,547 - INFO - [diffusion][Epoch 8049] diffusion training Loss: 0.05918224435299635
2024-11-05 02:42:32,549 - INFO - [diffusion][Epoch 8049] diffusion learning rate: 0.001
2024-11-05 02:42:32,551 - INFO - [diffusion][Epoch 8049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:32,553 - INFO - [diffusion][Epoch 8050] Epoch 8051/12000
2024-11-05 02:42:36,608 - INFO - [diffusion][Epoch 8050] diffusion training Loss: 0.05126893240958452
2024-11-05 02:42:36,610 - INFO - [diffusion][Epoch 8050] diffusion learning rate: 0.001
2024-11-05 02:42:36,612 - INFO - [diffusion][Epoch 8050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:36,613 - INFO - [diffusion][Epoch 8051] Epoch 8052/12000
2024-11-05 02:42:40,728 - INFO - [diffusion][Epoch 8051] diffusion training Loss: 0.05501867085695267
2024-11-05 02:42:40,729 - INFO - [diffusion][Epoch 8051] diffusion learning rate: 0.001
2024-11-05 02:42:40,731 - INFO - [diffusion][Epoch 8051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:40,732 - INFO - [diffusion][Epoch 8052] Epoch 8053/12000
2024-11-05 02:42:44,807 - INFO - [diffusion][Epoch 8052] diffusion training Loss: 0.050609578378498554
2024-11-05 02:42:44,811 - INFO - [diffusion][Epoch 8052] diffusion learning rate: 0.001
2024-11-05 02:42:44,812 - INFO - [diffusion][Epoch 8052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:44,814 - INFO - [diffusion][Epoch 8053] Epoch 8054/12000
2024-11-05 02:42:48,889 - INFO - [diffusion][Epoch 8053] diffusion training Loss: 0.05395464226603508
2024-11-05 02:42:48,891 - INFO - [diffusion][Epoch 8053] diffusion learning rate: 0.001
2024-11-05 02:42:48,893 - INFO - [diffusion][Epoch 8053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:48,895 - INFO - [diffusion][Epoch 8054] Epoch 8055/12000
2024-11-05 02:42:52,938 - INFO - [diffusion][Epoch 8054] diffusion training Loss: 0.05858656205236912
2024-11-05 02:42:52,940 - INFO - [diffusion][Epoch 8054] diffusion learning rate: 0.001
2024-11-05 02:42:52,942 - INFO - [diffusion][Epoch 8054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:52,943 - INFO - [diffusion][Epoch 8055] Epoch 8056/12000
2024-11-05 02:42:57,067 - INFO - [diffusion][Epoch 8055] diffusion training Loss: 0.054691984318196774
2024-11-05 02:42:57,069 - INFO - [diffusion][Epoch 8055] diffusion learning rate: 0.001
2024-11-05 02:42:57,086 - INFO - [diffusion][Epoch 8055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:42:57,087 - INFO - [diffusion][Epoch 8056] Epoch 8057/12000
2024-11-05 02:43:01,137 - INFO - [diffusion][Epoch 8056] diffusion training Loss: 0.05703597143292427
2024-11-05 02:43:01,139 - INFO - [diffusion][Epoch 8056] diffusion learning rate: 0.001
2024-11-05 02:43:01,141 - INFO - [diffusion][Epoch 8056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:01,142 - INFO - [diffusion][Epoch 8057] Epoch 8058/12000
2024-11-05 02:43:05,234 - INFO - [diffusion][Epoch 8057] diffusion training Loss: 0.061448088847100735
2024-11-05 02:43:05,237 - INFO - [diffusion][Epoch 8057] diffusion learning rate: 0.001
2024-11-05 02:43:05,239 - INFO - [diffusion][Epoch 8057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:05,240 - INFO - [diffusion][Epoch 8058] Epoch 8059/12000
2024-11-05 02:43:09,245 - INFO - [diffusion][Epoch 8058] diffusion training Loss: 0.04990605637431145
2024-11-05 02:43:09,248 - INFO - [diffusion][Epoch 8058] diffusion learning rate: 0.001
2024-11-05 02:43:09,250 - INFO - [diffusion][Epoch 8058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:09,251 - INFO - [diffusion][Epoch 8059] Epoch 8060/12000
2024-11-05 02:43:13,347 - INFO - [diffusion][Epoch 8059] diffusion training Loss: 0.05668242182582617
2024-11-05 02:43:13,349 - INFO - [diffusion][Epoch 8059] diffusion learning rate: 0.001
2024-11-05 02:43:13,377 - INFO - [diffusion][Epoch 8059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:13,379 - INFO - [diffusion][Epoch 8060] Epoch 8061/12000
2024-11-05 02:43:17,477 - INFO - [diffusion][Epoch 8060] diffusion training Loss: 0.05619933269917965
2024-11-05 02:43:17,479 - INFO - [diffusion][Epoch 8060] diffusion learning rate: 0.001
2024-11-05 02:43:17,481 - INFO - [diffusion][Epoch 8060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:17,482 - INFO - [diffusion][Epoch 8061] Epoch 8062/12000
2024-11-05 02:43:21,484 - INFO - [diffusion][Epoch 8061] diffusion training Loss: 0.056831720285117626
2024-11-05 02:43:21,486 - INFO - [diffusion][Epoch 8061] diffusion learning rate: 0.001
2024-11-05 02:43:21,488 - INFO - [diffusion][Epoch 8061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:21,489 - INFO - [diffusion][Epoch 8062] Epoch 8063/12000
2024-11-05 02:43:25,584 - INFO - [diffusion][Epoch 8062] diffusion training Loss: 0.05767102912068367
2024-11-05 02:43:25,586 - INFO - [diffusion][Epoch 8062] diffusion learning rate: 0.001
2024-11-05 02:43:25,587 - INFO - [diffusion][Epoch 8062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:25,589 - INFO - [diffusion][Epoch 8063] Epoch 8064/12000
2024-11-05 02:43:29,605 - INFO - [diffusion][Epoch 8063] diffusion training Loss: 0.053634561598300934
2024-11-05 02:43:29,607 - INFO - [diffusion][Epoch 8063] diffusion learning rate: 0.001
2024-11-05 02:43:29,608 - INFO - [diffusion][Epoch 8063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:29,609 - INFO - [diffusion][Epoch 8064] Epoch 8065/12000
2024-11-05 02:43:34,196 - INFO - [diffusion][Epoch 8064] diffusion training Loss: 0.051358435302972794
2024-11-05 02:43:34,197 - INFO - [diffusion][Epoch 8064] diffusion learning rate: 0.001
2024-11-05 02:43:34,199 - INFO - [diffusion][Epoch 8064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:34,201 - INFO - [diffusion][Epoch 8065] Epoch 8066/12000
2024-11-05 02:43:38,340 - INFO - [diffusion][Epoch 8065] diffusion training Loss: 0.054715401493012905
2024-11-05 02:43:38,342 - INFO - [diffusion][Epoch 8065] diffusion learning rate: 0.001
2024-11-05 02:43:38,344 - INFO - [diffusion][Epoch 8065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:38,345 - INFO - [diffusion][Epoch 8066] Epoch 8067/12000
2024-11-05 02:43:42,405 - INFO - [diffusion][Epoch 8066] diffusion training Loss: 0.054570951499044895
2024-11-05 02:43:42,406 - INFO - [diffusion][Epoch 8066] diffusion learning rate: 0.001
2024-11-05 02:43:42,408 - INFO - [diffusion][Epoch 8066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:42,409 - INFO - [diffusion][Epoch 8067] Epoch 8068/12000
2024-11-05 02:43:46,539 - INFO - [diffusion][Epoch 8067] diffusion training Loss: 0.05391387641429901
2024-11-05 02:43:46,542 - INFO - [diffusion][Epoch 8067] diffusion learning rate: 0.001
2024-11-05 02:43:46,544 - INFO - [diffusion][Epoch 8067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:46,546 - INFO - [diffusion][Epoch 8068] Epoch 8069/12000
2024-11-05 02:43:50,627 - INFO - [diffusion][Epoch 8068] diffusion training Loss: 0.04942323826253414
2024-11-05 02:43:50,629 - INFO - [diffusion][Epoch 8068] diffusion learning rate: 0.001
2024-11-05 02:43:50,631 - INFO - [diffusion][Epoch 8068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:50,632 - INFO - [diffusion][Epoch 8069] Epoch 8070/12000
2024-11-05 02:43:54,728 - INFO - [diffusion][Epoch 8069] diffusion training Loss: 0.05424334667623043
2024-11-05 02:43:54,731 - INFO - [diffusion][Epoch 8069] diffusion learning rate: 0.001
2024-11-05 02:43:54,733 - INFO - [diffusion][Epoch 8069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:54,734 - INFO - [diffusion][Epoch 8070] Epoch 8071/12000
2024-11-05 02:43:58,867 - INFO - [diffusion][Epoch 8070] diffusion training Loss: 0.05310698878020048
2024-11-05 02:43:58,870 - INFO - [diffusion][Epoch 8070] diffusion learning rate: 0.001
2024-11-05 02:43:58,872 - INFO - [diffusion][Epoch 8070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:43:58,874 - INFO - [diffusion][Epoch 8071] Epoch 8072/12000
2024-11-05 02:44:02,800 - INFO - [diffusion][Epoch 8071] diffusion training Loss: 0.05566288251429796
2024-11-05 02:44:02,802 - INFO - [diffusion][Epoch 8071] diffusion learning rate: 0.001
2024-11-05 02:44:02,804 - INFO - [diffusion][Epoch 8071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:02,805 - INFO - [diffusion][Epoch 8072] Epoch 8073/12000
2024-11-05 02:44:06,924 - INFO - [diffusion][Epoch 8072] diffusion training Loss: 0.05538579262793064
2024-11-05 02:44:06,926 - INFO - [diffusion][Epoch 8072] diffusion learning rate: 0.001
2024-11-05 02:44:06,928 - INFO - [diffusion][Epoch 8072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:06,929 - INFO - [diffusion][Epoch 8073] Epoch 8074/12000
2024-11-05 02:44:11,040 - INFO - [diffusion][Epoch 8073] diffusion training Loss: 0.05249153450131416
2024-11-05 02:44:11,042 - INFO - [diffusion][Epoch 8073] diffusion learning rate: 0.001
2024-11-05 02:44:11,044 - INFO - [diffusion][Epoch 8073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:11,045 - INFO - [diffusion][Epoch 8074] Epoch 8075/12000
2024-11-05 02:44:15,161 - INFO - [diffusion][Epoch 8074] diffusion training Loss: 0.05160334147512913
2024-11-05 02:44:15,162 - INFO - [diffusion][Epoch 8074] diffusion learning rate: 0.001
2024-11-05 02:44:15,164 - INFO - [diffusion][Epoch 8074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:15,166 - INFO - [diffusion][Epoch 8075] Epoch 8076/12000
2024-11-05 02:44:19,283 - INFO - [diffusion][Epoch 8075] diffusion training Loss: 0.06070124637335539
2024-11-05 02:44:19,286 - INFO - [diffusion][Epoch 8075] diffusion learning rate: 0.001
2024-11-05 02:44:19,314 - INFO - [diffusion][Epoch 8075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:19,316 - INFO - [diffusion][Epoch 8076] Epoch 8077/12000
2024-11-05 02:44:23,457 - INFO - [diffusion][Epoch 8076] diffusion training Loss: 0.05439817998558283
2024-11-05 02:44:23,460 - INFO - [diffusion][Epoch 8076] diffusion learning rate: 0.001
2024-11-05 02:44:23,462 - INFO - [diffusion][Epoch 8076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:23,463 - INFO - [diffusion][Epoch 8077] Epoch 8078/12000
2024-11-05 02:44:27,521 - INFO - [diffusion][Epoch 8077] diffusion training Loss: 0.049821753054857254
2024-11-05 02:44:27,524 - INFO - [diffusion][Epoch 8077] diffusion learning rate: 0.001
2024-11-05 02:44:27,526 - INFO - [diffusion][Epoch 8077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:27,527 - INFO - [diffusion][Epoch 8078] Epoch 8079/12000
2024-11-05 02:44:31,436 - INFO - [diffusion][Epoch 8078] diffusion training Loss: 0.05384101253002882
2024-11-05 02:44:31,438 - INFO - [diffusion][Epoch 8078] diffusion learning rate: 0.001
2024-11-05 02:44:31,440 - INFO - [diffusion][Epoch 8078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:31,441 - INFO - [diffusion][Epoch 8079] Epoch 8080/12000
2024-11-05 02:44:35,632 - INFO - [diffusion][Epoch 8079] diffusion training Loss: 0.0550395492464304
2024-11-05 02:44:35,635 - INFO - [diffusion][Epoch 8079] diffusion learning rate: 0.001
2024-11-05 02:44:35,656 - INFO - [diffusion][Epoch 8079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:35,658 - INFO - [diffusion][Epoch 8080] Epoch 8081/12000
2024-11-05 02:44:39,796 - INFO - [diffusion][Epoch 8080] diffusion training Loss: 0.05361700430512428
2024-11-05 02:44:39,798 - INFO - [diffusion][Epoch 8080] diffusion learning rate: 0.001
2024-11-05 02:44:39,800 - INFO - [diffusion][Epoch 8080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:39,801 - INFO - [diffusion][Epoch 8081] Epoch 8082/12000
2024-11-05 02:44:43,791 - INFO - [diffusion][Epoch 8081] diffusion training Loss: 0.05007829889655113
2024-11-05 02:44:43,793 - INFO - [diffusion][Epoch 8081] diffusion learning rate: 0.001
2024-11-05 02:44:43,795 - INFO - [diffusion][Epoch 8081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:43,797 - INFO - [diffusion][Epoch 8082] Epoch 8083/12000
2024-11-05 02:44:47,861 - INFO - [diffusion][Epoch 8082] diffusion training Loss: 0.05432306695729494
2024-11-05 02:44:47,864 - INFO - [diffusion][Epoch 8082] diffusion learning rate: 0.001
2024-11-05 02:44:47,866 - INFO - [diffusion][Epoch 8082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:47,867 - INFO - [diffusion][Epoch 8083] Epoch 8084/12000
2024-11-05 02:44:52,296 - INFO - [diffusion][Epoch 8083] diffusion training Loss: 0.05261926166713238
2024-11-05 02:44:52,299 - INFO - [diffusion][Epoch 8083] diffusion learning rate: 0.001
2024-11-05 02:44:52,300 - INFO - [diffusion][Epoch 8083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:52,302 - INFO - [diffusion][Epoch 8084] Epoch 8085/12000
2024-11-05 02:44:56,382 - INFO - [diffusion][Epoch 8084] diffusion training Loss: 0.054707217030227184
2024-11-05 02:44:56,384 - INFO - [diffusion][Epoch 8084] diffusion learning rate: 0.001
2024-11-05 02:44:56,385 - INFO - [diffusion][Epoch 8084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:44:56,387 - INFO - [diffusion][Epoch 8085] Epoch 8086/12000
2024-11-05 02:45:00,521 - INFO - [diffusion][Epoch 8085] diffusion training Loss: 0.05792958661913872
2024-11-05 02:45:00,523 - INFO - [diffusion][Epoch 8085] diffusion learning rate: 0.001
2024-11-05 02:45:00,525 - INFO - [diffusion][Epoch 8085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:00,526 - INFO - [diffusion][Epoch 8086] Epoch 8087/12000
2024-11-05 02:45:04,759 - INFO - [diffusion][Epoch 8086] diffusion training Loss: 0.05669920891523361
2024-11-05 02:45:04,761 - INFO - [diffusion][Epoch 8086] diffusion learning rate: 0.001
2024-11-05 02:45:04,763 - INFO - [diffusion][Epoch 8086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:04,764 - INFO - [diffusion][Epoch 8087] Epoch 8088/12000
2024-11-05 02:45:08,797 - INFO - [diffusion][Epoch 8087] diffusion training Loss: 0.05115516483783722
2024-11-05 02:45:08,799 - INFO - [diffusion][Epoch 8087] diffusion learning rate: 0.001
2024-11-05 02:45:08,801 - INFO - [diffusion][Epoch 8087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:08,802 - INFO - [diffusion][Epoch 8088] Epoch 8089/12000
2024-11-05 02:45:12,818 - INFO - [diffusion][Epoch 8088] diffusion training Loss: 0.05107961967587471
2024-11-05 02:45:12,820 - INFO - [diffusion][Epoch 8088] diffusion learning rate: 0.001
2024-11-05 02:45:12,860 - INFO - [diffusion][Epoch 8088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:12,862 - INFO - [diffusion][Epoch 8089] Epoch 8090/12000
2024-11-05 02:45:16,910 - INFO - [diffusion][Epoch 8089] diffusion training Loss: 0.04940230678766966
2024-11-05 02:45:16,912 - INFO - [diffusion][Epoch 8089] diffusion learning rate: 0.001
2024-11-05 02:45:16,915 - INFO - [diffusion][Epoch 8089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:16,916 - INFO - [diffusion][Epoch 8090] Epoch 8091/12000
2024-11-05 02:45:20,971 - INFO - [diffusion][Epoch 8090] diffusion training Loss: 0.05368472170084715
2024-11-05 02:45:20,973 - INFO - [diffusion][Epoch 8090] diffusion learning rate: 0.001
2024-11-05 02:45:20,975 - INFO - [diffusion][Epoch 8090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:20,976 - INFO - [diffusion][Epoch 8091] Epoch 8092/12000
2024-11-05 02:45:24,987 - INFO - [diffusion][Epoch 8091] diffusion training Loss: 0.050406768918037415
2024-11-05 02:45:24,989 - INFO - [diffusion][Epoch 8091] diffusion learning rate: 0.001
2024-11-05 02:45:24,991 - INFO - [diffusion][Epoch 8091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:24,992 - INFO - [diffusion][Epoch 8092] Epoch 8093/12000
2024-11-05 02:45:29,113 - INFO - [diffusion][Epoch 8092] diffusion training Loss: 0.04937326442450285
2024-11-05 02:45:29,115 - INFO - [diffusion][Epoch 8092] diffusion learning rate: 0.001
2024-11-05 02:45:29,117 - INFO - [diffusion][Epoch 8092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:29,118 - INFO - [diffusion][Epoch 8093] Epoch 8094/12000
2024-11-05 02:45:33,218 - INFO - [diffusion][Epoch 8093] diffusion training Loss: 0.056127999909222126
2024-11-05 02:45:33,220 - INFO - [diffusion][Epoch 8093] diffusion learning rate: 0.001
2024-11-05 02:45:33,222 - INFO - [diffusion][Epoch 8093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:33,223 - INFO - [diffusion][Epoch 8094] Epoch 8095/12000
2024-11-05 02:45:37,391 - INFO - [diffusion][Epoch 8094] diffusion training Loss: 0.04952177777886391
2024-11-05 02:45:37,393 - INFO - [diffusion][Epoch 8094] diffusion learning rate: 0.001
2024-11-05 02:45:37,395 - INFO - [diffusion][Epoch 8094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:37,396 - INFO - [diffusion][Epoch 8095] Epoch 8096/12000
2024-11-05 02:45:41,451 - INFO - [diffusion][Epoch 8095] diffusion training Loss: 0.05746817123144865
2024-11-05 02:45:41,453 - INFO - [diffusion][Epoch 8095] diffusion learning rate: 0.001
2024-11-05 02:45:41,455 - INFO - [diffusion][Epoch 8095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:41,456 - INFO - [diffusion][Epoch 8096] Epoch 8097/12000
2024-11-05 02:45:45,458 - INFO - [diffusion][Epoch 8096] diffusion training Loss: 0.051527852192521095
2024-11-05 02:45:45,460 - INFO - [diffusion][Epoch 8096] diffusion learning rate: 0.001
2024-11-05 02:45:45,468 - INFO - [diffusion][Epoch 8096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:45,469 - INFO - [diffusion][Epoch 8097] Epoch 8098/12000
2024-11-05 02:45:49,614 - INFO - [diffusion][Epoch 8097] diffusion training Loss: 0.051763118244707584
2024-11-05 02:45:49,617 - INFO - [diffusion][Epoch 8097] diffusion learning rate: 0.001
2024-11-05 02:45:49,619 - INFO - [diffusion][Epoch 8097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:49,621 - INFO - [diffusion][Epoch 8098] Epoch 8099/12000
2024-11-05 02:45:53,767 - INFO - [diffusion][Epoch 8098] diffusion training Loss: 0.05517017748206854
2024-11-05 02:45:53,769 - INFO - [diffusion][Epoch 8098] diffusion learning rate: 0.001
2024-11-05 02:45:53,771 - INFO - [diffusion][Epoch 8098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:53,772 - INFO - [diffusion][Epoch 8099] Epoch 8100/12000
2024-11-05 02:45:57,912 - INFO - [diffusion][Epoch 8099] diffusion training Loss: 0.052933904342353344
2024-11-05 02:45:57,915 - INFO - [diffusion][Epoch 8099] diffusion learning rate: 0.001
2024-11-05 02:45:57,916 - INFO - [diffusion][Epoch 8099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:45:57,918 - INFO - [diffusion][Epoch 8100] Epoch 8101/12000
2024-11-05 02:46:02,100 - INFO - [diffusion][Epoch 8100] diffusion training Loss: 0.06015688739717007
2024-11-05 02:46:02,102 - INFO - [diffusion][Epoch 8100] diffusion learning rate: 0.001
2024-11-05 02:46:02,104 - INFO - [diffusion][Epoch 8100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:02,105 - INFO - [diffusion][Epoch 8101] Epoch 8102/12000
2024-11-05 02:46:06,133 - INFO - [diffusion][Epoch 8101] diffusion training Loss: 0.055080607533454895
2024-11-05 02:46:06,136 - INFO - [diffusion][Epoch 8101] diffusion learning rate: 0.001
2024-11-05 02:46:06,138 - INFO - [diffusion][Epoch 8101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:06,139 - INFO - [diffusion][Epoch 8102] Epoch 8103/12000
2024-11-05 02:46:10,297 - INFO - [diffusion][Epoch 8102] diffusion training Loss: 0.05213125701993704
2024-11-05 02:46:10,299 - INFO - [diffusion][Epoch 8102] diffusion learning rate: 0.001
2024-11-05 02:46:10,301 - INFO - [diffusion][Epoch 8102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:10,302 - INFO - [diffusion][Epoch 8103] Epoch 8104/12000
2024-11-05 02:46:14,219 - INFO - [diffusion][Epoch 8103] diffusion training Loss: 0.05647752806544304
2024-11-05 02:46:14,222 - INFO - [diffusion][Epoch 8103] diffusion learning rate: 0.001
2024-11-05 02:46:14,224 - INFO - [diffusion][Epoch 8103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:14,225 - INFO - [diffusion][Epoch 8104] Epoch 8105/12000
2024-11-05 02:46:18,275 - INFO - [diffusion][Epoch 8104] diffusion training Loss: 0.05878221895545721
2024-11-05 02:46:18,278 - INFO - [diffusion][Epoch 8104] diffusion learning rate: 0.001
2024-11-05 02:46:18,280 - INFO - [diffusion][Epoch 8104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:18,281 - INFO - [diffusion][Epoch 8105] Epoch 8106/12000
2024-11-05 02:46:23,005 - INFO - [diffusion][Epoch 8105] diffusion training Loss: 0.05788456927984953
2024-11-05 02:46:23,007 - INFO - [diffusion][Epoch 8105] diffusion learning rate: 0.001
2024-11-05 02:46:23,009 - INFO - [diffusion][Epoch 8105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:23,010 - INFO - [diffusion][Epoch 8106] Epoch 8107/12000
2024-11-05 02:46:27,143 - INFO - [diffusion][Epoch 8106] diffusion training Loss: 0.04909925162792206
2024-11-05 02:46:27,145 - INFO - [diffusion][Epoch 8106] diffusion learning rate: 0.001
2024-11-05 02:46:27,147 - INFO - [diffusion][Epoch 8106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:27,148 - INFO - [diffusion][Epoch 8107] Epoch 8108/12000
2024-11-05 02:46:31,292 - INFO - [diffusion][Epoch 8107] diffusion training Loss: 0.05202821176499128
2024-11-05 02:46:31,294 - INFO - [diffusion][Epoch 8107] diffusion learning rate: 0.001
2024-11-05 02:46:31,296 - INFO - [diffusion][Epoch 8107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:31,297 - INFO - [diffusion][Epoch 8108] Epoch 8109/12000
2024-11-05 02:46:35,425 - INFO - [diffusion][Epoch 8108] diffusion training Loss: 0.0497118653729558
2024-11-05 02:46:35,427 - INFO - [diffusion][Epoch 8108] diffusion learning rate: 0.001
2024-11-05 02:46:35,429 - INFO - [diffusion][Epoch 8108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:35,430 - INFO - [diffusion][Epoch 8109] Epoch 8110/12000
2024-11-05 02:46:39,588 - INFO - [diffusion][Epoch 8109] diffusion training Loss: 0.055645246990025043
2024-11-05 02:46:39,590 - INFO - [diffusion][Epoch 8109] diffusion learning rate: 0.001
2024-11-05 02:46:39,592 - INFO - [diffusion][Epoch 8109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:39,593 - INFO - [diffusion][Epoch 8110] Epoch 8111/12000
2024-11-05 02:46:43,641 - INFO - [diffusion][Epoch 8110] diffusion training Loss: 0.05038424767553806
2024-11-05 02:46:43,642 - INFO - [diffusion][Epoch 8110] diffusion learning rate: 0.001
2024-11-05 02:46:43,644 - INFO - [diffusion][Epoch 8110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:43,645 - INFO - [diffusion][Epoch 8111] Epoch 8112/12000
2024-11-05 02:46:47,751 - INFO - [diffusion][Epoch 8111] diffusion training Loss: 0.05395291745662689
2024-11-05 02:46:47,753 - INFO - [diffusion][Epoch 8111] diffusion learning rate: 0.001
2024-11-05 02:46:47,755 - INFO - [diffusion][Epoch 8111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:47,756 - INFO - [diffusion][Epoch 8112] Epoch 8113/12000
2024-11-05 02:46:51,923 - INFO - [diffusion][Epoch 8112] diffusion training Loss: 0.051103707402944565
2024-11-05 02:46:51,926 - INFO - [diffusion][Epoch 8112] diffusion learning rate: 0.001
2024-11-05 02:46:51,927 - INFO - [diffusion][Epoch 8112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:51,929 - INFO - [diffusion][Epoch 8113] Epoch 8114/12000
2024-11-05 02:46:56,041 - INFO - [diffusion][Epoch 8113] diffusion training Loss: 0.055588165298104286
2024-11-05 02:46:56,043 - INFO - [diffusion][Epoch 8113] diffusion learning rate: 0.001
2024-11-05 02:46:56,045 - INFO - [diffusion][Epoch 8113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:46:56,047 - INFO - [diffusion][Epoch 8114] Epoch 8115/12000
2024-11-05 02:47:00,088 - INFO - [diffusion][Epoch 8114] diffusion training Loss: 0.05074017960578203
2024-11-05 02:47:00,091 - INFO - [diffusion][Epoch 8114] diffusion learning rate: 0.001
2024-11-05 02:47:00,093 - INFO - [diffusion][Epoch 8114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:00,094 - INFO - [diffusion][Epoch 8115] Epoch 8116/12000
2024-11-05 02:47:04,229 - INFO - [diffusion][Epoch 8115] diffusion training Loss: 0.05692677665501833
2024-11-05 02:47:04,232 - INFO - [diffusion][Epoch 8115] diffusion learning rate: 0.001
2024-11-05 02:47:04,234 - INFO - [diffusion][Epoch 8115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:04,235 - INFO - [diffusion][Epoch 8116] Epoch 8117/12000
2024-11-05 02:47:08,423 - INFO - [diffusion][Epoch 8116] diffusion training Loss: 0.05600213631987572
2024-11-05 02:47:08,425 - INFO - [diffusion][Epoch 8116] diffusion learning rate: 0.001
2024-11-05 02:47:08,427 - INFO - [diffusion][Epoch 8116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:08,428 - INFO - [diffusion][Epoch 8117] Epoch 8118/12000
2024-11-05 02:47:12,419 - INFO - [diffusion][Epoch 8117] diffusion training Loss: 0.052910247817635536
2024-11-05 02:47:12,591 - INFO - [diffusion][Epoch 8117] diffusion learning rate: 0.001
2024-11-05 02:47:12,593 - INFO - [diffusion][Epoch 8117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:12,595 - INFO - [diffusion][Epoch 8118] Epoch 8119/12000
2024-11-05 02:47:16,755 - INFO - [diffusion][Epoch 8118] diffusion training Loss: 0.056368885561823845
2024-11-05 02:47:16,757 - INFO - [diffusion][Epoch 8118] diffusion learning rate: 0.001
2024-11-05 02:47:16,759 - INFO - [diffusion][Epoch 8118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:16,760 - INFO - [diffusion][Epoch 8119] Epoch 8120/12000
2024-11-05 02:47:20,898 - INFO - [diffusion][Epoch 8119] diffusion training Loss: 0.053954548202455044
2024-11-05 02:47:20,900 - INFO - [diffusion][Epoch 8119] diffusion learning rate: 0.001
2024-11-05 02:47:20,902 - INFO - [diffusion][Epoch 8119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:20,903 - INFO - [diffusion][Epoch 8120] Epoch 8121/12000
2024-11-05 02:47:25,022 - INFO - [diffusion][Epoch 8120] diffusion training Loss: 0.05467955023050308
2024-11-05 02:47:25,024 - INFO - [diffusion][Epoch 8120] diffusion learning rate: 0.001
2024-11-05 02:47:25,026 - INFO - [diffusion][Epoch 8120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:25,027 - INFO - [diffusion][Epoch 8121] Epoch 8122/12000
2024-11-05 02:47:29,146 - INFO - [diffusion][Epoch 8121] diffusion training Loss: 0.04906452260911465
2024-11-05 02:47:29,148 - INFO - [diffusion][Epoch 8121] diffusion learning rate: 0.001
2024-11-05 02:47:29,150 - INFO - [diffusion][Epoch 8121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:29,151 - INFO - [diffusion][Epoch 8122] Epoch 8123/12000
2024-11-05 02:47:33,290 - INFO - [diffusion][Epoch 8122] diffusion training Loss: 0.057730309665203094
2024-11-05 02:47:33,292 - INFO - [diffusion][Epoch 8122] diffusion learning rate: 0.001
2024-11-05 02:47:33,294 - INFO - [diffusion][Epoch 8122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:33,295 - INFO - [diffusion][Epoch 8123] Epoch 8124/12000
2024-11-05 02:47:37,413 - INFO - [diffusion][Epoch 8123] diffusion training Loss: 0.05457665305584669
2024-11-05 02:47:37,415 - INFO - [diffusion][Epoch 8123] diffusion learning rate: 0.001
2024-11-05 02:47:37,417 - INFO - [diffusion][Epoch 8123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:37,418 - INFO - [diffusion][Epoch 8124] Epoch 8125/12000
2024-11-05 02:47:41,578 - INFO - [diffusion][Epoch 8124] diffusion training Loss: 0.05564571265131235
2024-11-05 02:47:41,580 - INFO - [diffusion][Epoch 8124] diffusion learning rate: 0.001
2024-11-05 02:47:41,582 - INFO - [diffusion][Epoch 8124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:41,583 - INFO - [diffusion][Epoch 8125] Epoch 8126/12000
2024-11-05 02:47:45,767 - INFO - [diffusion][Epoch 8125] diffusion training Loss: 0.05364854447543621
2024-11-05 02:47:45,769 - INFO - [diffusion][Epoch 8125] diffusion learning rate: 0.001
2024-11-05 02:47:45,771 - INFO - [diffusion][Epoch 8125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:45,772 - INFO - [diffusion][Epoch 8126] Epoch 8127/12000
2024-11-05 02:47:50,094 - INFO - [diffusion][Epoch 8126] diffusion training Loss: 0.05606001988053322
2024-11-05 02:47:50,096 - INFO - [diffusion][Epoch 8126] diffusion learning rate: 0.001
2024-11-05 02:47:50,098 - INFO - [diffusion][Epoch 8126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:50,099 - INFO - [diffusion][Epoch 8127] Epoch 8128/12000
2024-11-05 02:47:54,027 - INFO - [diffusion][Epoch 8127] diffusion training Loss: 0.053363083861768246
2024-11-05 02:47:54,030 - INFO - [diffusion][Epoch 8127] diffusion learning rate: 0.001
2024-11-05 02:47:54,032 - INFO - [diffusion][Epoch 8127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:54,033 - INFO - [diffusion][Epoch 8128] Epoch 8129/12000
2024-11-05 02:47:58,134 - INFO - [diffusion][Epoch 8128] diffusion training Loss: 0.054658979177474976
2024-11-05 02:47:58,136 - INFO - [diffusion][Epoch 8128] diffusion learning rate: 0.001
2024-11-05 02:47:58,138 - INFO - [diffusion][Epoch 8128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:47:58,139 - INFO - [diffusion][Epoch 8129] Epoch 8130/12000
2024-11-05 02:48:02,257 - INFO - [diffusion][Epoch 8129] diffusion training Loss: 0.052333672530949116
2024-11-05 02:48:02,260 - INFO - [diffusion][Epoch 8129] diffusion learning rate: 0.001
2024-11-05 02:48:02,262 - INFO - [diffusion][Epoch 8129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:02,263 - INFO - [diffusion][Epoch 8130] Epoch 8131/12000
2024-11-05 02:48:06,424 - INFO - [diffusion][Epoch 8130] diffusion training Loss: 0.05703307595103979
2024-11-05 02:48:06,426 - INFO - [diffusion][Epoch 8130] diffusion learning rate: 0.001
2024-11-05 02:48:06,428 - INFO - [diffusion][Epoch 8130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:06,429 - INFO - [diffusion][Epoch 8131] Epoch 8132/12000
2024-11-05 02:48:10,530 - INFO - [diffusion][Epoch 8131] diffusion training Loss: 0.049097927287220955
2024-11-05 02:48:10,532 - INFO - [diffusion][Epoch 8131] diffusion learning rate: 0.001
2024-11-05 02:48:10,534 - INFO - [diffusion][Epoch 8131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:10,536 - INFO - [diffusion][Epoch 8132] Epoch 8133/12000
2024-11-05 02:48:14,641 - INFO - [diffusion][Epoch 8132] diffusion training Loss: 0.056083704344928265
2024-11-05 02:48:14,643 - INFO - [diffusion][Epoch 8132] diffusion learning rate: 0.001
2024-11-05 02:48:14,645 - INFO - [diffusion][Epoch 8132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:14,646 - INFO - [diffusion][Epoch 8133] Epoch 8134/12000
2024-11-05 02:48:18,798 - INFO - [diffusion][Epoch 8133] diffusion training Loss: 0.05522293131798506
2024-11-05 02:48:18,800 - INFO - [diffusion][Epoch 8133] diffusion learning rate: 0.001
2024-11-05 02:48:18,802 - INFO - [diffusion][Epoch 8133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:18,803 - INFO - [diffusion][Epoch 8134] Epoch 8135/12000
2024-11-05 02:48:22,862 - INFO - [diffusion][Epoch 8134] diffusion training Loss: 0.0491003692150116
2024-11-05 02:48:22,864 - INFO - [diffusion][Epoch 8134] diffusion learning rate: 0.001
2024-11-05 02:48:22,866 - INFO - [diffusion][Epoch 8134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:22,867 - INFO - [diffusion][Epoch 8135] Epoch 8136/12000
2024-11-05 02:48:26,899 - INFO - [diffusion][Epoch 8135] diffusion training Loss: 0.05288590956479311
2024-11-05 02:48:26,901 - INFO - [diffusion][Epoch 8135] diffusion learning rate: 0.001
2024-11-05 02:48:26,903 - INFO - [diffusion][Epoch 8135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:26,904 - INFO - [diffusion][Epoch 8136] Epoch 8137/12000
2024-11-05 02:48:31,048 - INFO - [diffusion][Epoch 8136] diffusion training Loss: 0.05571323353797197
2024-11-05 02:48:31,050 - INFO - [diffusion][Epoch 8136] diffusion learning rate: 0.001
2024-11-05 02:48:31,052 - INFO - [diffusion][Epoch 8136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:31,053 - INFO - [diffusion][Epoch 8137] Epoch 8138/12000
2024-11-05 02:48:35,159 - INFO - [diffusion][Epoch 8137] diffusion training Loss: 0.05233252141624689
2024-11-05 02:48:35,162 - INFO - [diffusion][Epoch 8137] diffusion learning rate: 0.001
2024-11-05 02:48:35,164 - INFO - [diffusion][Epoch 8137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:35,165 - INFO - [diffusion][Epoch 8138] Epoch 8139/12000
2024-11-05 02:48:39,200 - INFO - [diffusion][Epoch 8138] diffusion training Loss: 0.05420887004584074
2024-11-05 02:48:39,202 - INFO - [diffusion][Epoch 8138] diffusion learning rate: 0.001
2024-11-05 02:48:39,203 - INFO - [diffusion][Epoch 8138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:39,205 - INFO - [diffusion][Epoch 8139] Epoch 8140/12000
2024-11-05 02:48:43,379 - INFO - [diffusion][Epoch 8139] diffusion training Loss: 0.05327989161014557
2024-11-05 02:48:43,381 - INFO - [diffusion][Epoch 8139] diffusion learning rate: 0.001
2024-11-05 02:48:43,382 - INFO - [diffusion][Epoch 8139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:43,384 - INFO - [diffusion][Epoch 8140] Epoch 8141/12000
2024-11-05 02:48:47,387 - INFO - [diffusion][Epoch 8140] diffusion training Loss: 0.055178698152303696
2024-11-05 02:48:47,389 - INFO - [diffusion][Epoch 8140] diffusion learning rate: 0.001
2024-11-05 02:48:47,391 - INFO - [diffusion][Epoch 8140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:47,392 - INFO - [diffusion][Epoch 8141] Epoch 8142/12000
2024-11-05 02:48:51,388 - INFO - [diffusion][Epoch 8141] diffusion training Loss: 0.0560914259403944
2024-11-05 02:48:51,390 - INFO - [diffusion][Epoch 8141] diffusion learning rate: 0.001
2024-11-05 02:48:51,392 - INFO - [diffusion][Epoch 8141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:51,393 - INFO - [diffusion][Epoch 8142] Epoch 8143/12000
2024-11-05 02:48:55,277 - INFO - [diffusion][Epoch 8142] diffusion training Loss: 0.05367614887654781
2024-11-05 02:48:55,280 - INFO - [diffusion][Epoch 8142] diffusion learning rate: 0.001
2024-11-05 02:48:55,282 - INFO - [diffusion][Epoch 8142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:55,283 - INFO - [diffusion][Epoch 8143] Epoch 8144/12000
2024-11-05 02:48:59,339 - INFO - [diffusion][Epoch 8143] diffusion training Loss: 0.05581068433821201
2024-11-05 02:48:59,342 - INFO - [diffusion][Epoch 8143] diffusion learning rate: 0.001
2024-11-05 02:48:59,344 - INFO - [diffusion][Epoch 8143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:48:59,346 - INFO - [diffusion][Epoch 8144] Epoch 8145/12000
2024-11-05 02:49:03,405 - INFO - [diffusion][Epoch 8144] diffusion training Loss: 0.05049876030534506
2024-11-05 02:49:03,407 - INFO - [diffusion][Epoch 8144] diffusion learning rate: 0.001
2024-11-05 02:49:03,409 - INFO - [diffusion][Epoch 8144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:03,411 - INFO - [diffusion][Epoch 8145] Epoch 8146/12000
2024-11-05 02:49:07,341 - INFO - [diffusion][Epoch 8145] diffusion training Loss: 0.05145977344363928
2024-11-05 02:49:07,343 - INFO - [diffusion][Epoch 8145] diffusion learning rate: 0.001
2024-11-05 02:49:07,345 - INFO - [diffusion][Epoch 8145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:07,346 - INFO - [diffusion][Epoch 8146] Epoch 8147/12000
2024-11-05 02:49:11,547 - INFO - [diffusion][Epoch 8146] diffusion training Loss: 0.05093731917440891
2024-11-05 02:49:11,549 - INFO - [diffusion][Epoch 8146] diffusion learning rate: 0.001
2024-11-05 02:49:11,551 - INFO - [diffusion][Epoch 8146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:11,552 - INFO - [diffusion][Epoch 8147] Epoch 8148/12000
2024-11-05 02:49:15,677 - INFO - [diffusion][Epoch 8147] diffusion training Loss: 0.05534729175269604
2024-11-05 02:49:15,679 - INFO - [diffusion][Epoch 8147] diffusion learning rate: 0.001
2024-11-05 02:49:15,680 - INFO - [diffusion][Epoch 8147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:15,682 - INFO - [diffusion][Epoch 8148] Epoch 8149/12000
2024-11-05 02:49:19,778 - INFO - [diffusion][Epoch 8148] diffusion training Loss: 0.05522180814296007
2024-11-05 02:49:19,779 - INFO - [diffusion][Epoch 8148] diffusion learning rate: 0.001
2024-11-05 02:49:19,781 - INFO - [diffusion][Epoch 8148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:19,783 - INFO - [diffusion][Epoch 8149] Epoch 8150/12000
2024-11-05 02:49:23,908 - INFO - [diffusion][Epoch 8149] diffusion training Loss: 0.058504545129835606
2024-11-05 02:49:23,910 - INFO - [diffusion][Epoch 8149] diffusion learning rate: 0.001
2024-11-05 02:49:23,912 - INFO - [diffusion][Epoch 8149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:23,913 - INFO - [diffusion][Epoch 8150] Epoch 8151/12000
2024-11-05 02:49:28,011 - INFO - [diffusion][Epoch 8150] diffusion training Loss: 0.052113003097474575
2024-11-05 02:49:28,014 - INFO - [diffusion][Epoch 8150] diffusion learning rate: 0.001
2024-11-05 02:49:28,015 - INFO - [diffusion][Epoch 8150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:28,017 - INFO - [diffusion][Epoch 8151] Epoch 8152/12000
2024-11-05 02:49:32,058 - INFO - [diffusion][Epoch 8151] diffusion training Loss: 0.05653498414903879
2024-11-05 02:49:32,060 - INFO - [diffusion][Epoch 8151] diffusion learning rate: 0.001
2024-11-05 02:49:32,062 - INFO - [diffusion][Epoch 8151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:32,063 - INFO - [diffusion][Epoch 8152] Epoch 8153/12000
2024-11-05 02:49:36,154 - INFO - [diffusion][Epoch 8152] diffusion training Loss: 0.05283770803362131
2024-11-05 02:49:36,157 - INFO - [diffusion][Epoch 8152] diffusion learning rate: 0.001
2024-11-05 02:49:36,158 - INFO - [diffusion][Epoch 8152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:36,160 - INFO - [diffusion][Epoch 8153] Epoch 8154/12000
2024-11-05 02:49:40,219 - INFO - [diffusion][Epoch 8153] diffusion training Loss: 0.05732468143105507
2024-11-05 02:49:40,221 - INFO - [diffusion][Epoch 8153] diffusion learning rate: 0.001
2024-11-05 02:49:40,223 - INFO - [diffusion][Epoch 8153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:40,225 - INFO - [diffusion][Epoch 8154] Epoch 8155/12000
2024-11-05 02:49:44,386 - INFO - [diffusion][Epoch 8154] diffusion training Loss: 0.05443034507334232
2024-11-05 02:49:44,388 - INFO - [diffusion][Epoch 8154] diffusion learning rate: 0.001
2024-11-05 02:49:44,390 - INFO - [diffusion][Epoch 8154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:44,391 - INFO - [diffusion][Epoch 8155] Epoch 8156/12000
2024-11-05 02:49:48,528 - INFO - [diffusion][Epoch 8155] diffusion training Loss: 0.050338675267994404
2024-11-05 02:49:48,530 - INFO - [diffusion][Epoch 8155] diffusion learning rate: 0.001
2024-11-05 02:49:48,531 - INFO - [diffusion][Epoch 8155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:48,533 - INFO - [diffusion][Epoch 8156] Epoch 8157/12000
2024-11-05 02:49:52,647 - INFO - [diffusion][Epoch 8156] diffusion training Loss: 0.04947800189256668
2024-11-05 02:49:52,649 - INFO - [diffusion][Epoch 8156] diffusion learning rate: 0.001
2024-11-05 02:49:52,651 - INFO - [diffusion][Epoch 8156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:52,652 - INFO - [diffusion][Epoch 8157] Epoch 8158/12000
2024-11-05 02:49:56,682 - INFO - [diffusion][Epoch 8157] diffusion training Loss: 0.05404862388968468
2024-11-05 02:49:56,685 - INFO - [diffusion][Epoch 8157] diffusion learning rate: 0.001
2024-11-05 02:49:56,687 - INFO - [diffusion][Epoch 8157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:49:56,688 - INFO - [diffusion][Epoch 8158] Epoch 8159/12000
2024-11-05 02:50:00,662 - INFO - [diffusion][Epoch 8158] diffusion training Loss: 0.053927864879369736
2024-11-05 02:50:00,664 - INFO - [diffusion][Epoch 8158] diffusion learning rate: 0.001
2024-11-05 02:50:00,666 - INFO - [diffusion][Epoch 8158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:00,667 - INFO - [diffusion][Epoch 8159] Epoch 8160/12000
2024-11-05 02:50:04,684 - INFO - [diffusion][Epoch 8159] diffusion training Loss: 0.05229681171476841
2024-11-05 02:50:04,686 - INFO - [diffusion][Epoch 8159] diffusion learning rate: 0.001
2024-11-05 02:50:04,688 - INFO - [diffusion][Epoch 8159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:04,689 - INFO - [diffusion][Epoch 8160] Epoch 8161/12000
2024-11-05 02:50:08,649 - INFO - [diffusion][Epoch 8160] diffusion training Loss: 0.05281067732721567
2024-11-05 02:50:08,651 - INFO - [diffusion][Epoch 8160] diffusion learning rate: 0.001
2024-11-05 02:50:08,653 - INFO - [diffusion][Epoch 8160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:08,654 - INFO - [diffusion][Epoch 8161] Epoch 8162/12000
2024-11-05 02:50:12,621 - INFO - [diffusion][Epoch 8161] diffusion training Loss: 0.051294405944645405
2024-11-05 02:50:12,722 - INFO - [diffusion][Epoch 8161] diffusion learning rate: 0.001
2024-11-05 02:50:12,724 - INFO - [diffusion][Epoch 8161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:12,725 - INFO - [diffusion][Epoch 8162] Epoch 8163/12000
2024-11-05 02:50:16,853 - INFO - [diffusion][Epoch 8162] diffusion training Loss: 0.05118452571332455
2024-11-05 02:50:16,855 - INFO - [diffusion][Epoch 8162] diffusion learning rate: 0.001
2024-11-05 02:50:16,857 - INFO - [diffusion][Epoch 8162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:16,858 - INFO - [diffusion][Epoch 8163] Epoch 8164/12000
2024-11-05 02:50:21,032 - INFO - [diffusion][Epoch 8163] diffusion training Loss: 0.05245428066700697
2024-11-05 02:50:21,034 - INFO - [diffusion][Epoch 8163] diffusion learning rate: 0.001
2024-11-05 02:50:21,036 - INFO - [diffusion][Epoch 8163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:21,037 - INFO - [diffusion][Epoch 8164] Epoch 8165/12000
2024-11-05 02:50:25,086 - INFO - [diffusion][Epoch 8164] diffusion training Loss: 0.05822905711829662
2024-11-05 02:50:25,088 - INFO - [diffusion][Epoch 8164] diffusion learning rate: 0.001
2024-11-05 02:50:25,090 - INFO - [diffusion][Epoch 8164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:25,091 - INFO - [diffusion][Epoch 8165] Epoch 8166/12000
2024-11-05 02:50:29,252 - INFO - [diffusion][Epoch 8165] diffusion training Loss: 0.0517672523856163
2024-11-05 02:50:29,254 - INFO - [diffusion][Epoch 8165] diffusion learning rate: 0.001
2024-11-05 02:50:29,255 - INFO - [diffusion][Epoch 8165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:29,257 - INFO - [diffusion][Epoch 8166] Epoch 8167/12000
2024-11-05 02:50:33,592 - INFO - [diffusion][Epoch 8166] diffusion training Loss: 0.05857707839459181
2024-11-05 02:50:33,594 - INFO - [diffusion][Epoch 8166] diffusion learning rate: 0.001
2024-11-05 02:50:33,596 - INFO - [diffusion][Epoch 8166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:33,597 - INFO - [diffusion][Epoch 8167] Epoch 8168/12000
2024-11-05 02:50:37,761 - INFO - [diffusion][Epoch 8167] diffusion training Loss: 0.055193424224853516
2024-11-05 02:50:37,763 - INFO - [diffusion][Epoch 8167] diffusion learning rate: 0.001
2024-11-05 02:50:37,764 - INFO - [diffusion][Epoch 8167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:37,766 - INFO - [diffusion][Epoch 8168] Epoch 8169/12000
2024-11-05 02:50:41,901 - INFO - [diffusion][Epoch 8168] diffusion training Loss: 0.05992109328508377
2024-11-05 02:50:41,903 - INFO - [diffusion][Epoch 8168] diffusion learning rate: 0.001
2024-11-05 02:50:41,905 - INFO - [diffusion][Epoch 8168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:41,906 - INFO - [diffusion][Epoch 8169] Epoch 8170/12000
2024-11-05 02:50:46,040 - INFO - [diffusion][Epoch 8169] diffusion training Loss: 0.05436377786099911
2024-11-05 02:50:46,042 - INFO - [diffusion][Epoch 8169] diffusion learning rate: 0.001
2024-11-05 02:50:46,043 - INFO - [diffusion][Epoch 8169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:46,045 - INFO - [diffusion][Epoch 8170] Epoch 8171/12000
2024-11-05 02:50:50,143 - INFO - [diffusion][Epoch 8170] diffusion training Loss: 0.048361741937696934
2024-11-05 02:50:50,145 - INFO - [diffusion][Epoch 8170] diffusion learning rate: 0.001
2024-11-05 02:50:50,147 - INFO - [diffusion][Epoch 8170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:50,149 - INFO - [diffusion][Epoch 8171] Epoch 8172/12000
2024-11-05 02:50:54,297 - INFO - [diffusion][Epoch 8171] diffusion training Loss: 0.05395954567939043
2024-11-05 02:50:54,299 - INFO - [diffusion][Epoch 8171] diffusion learning rate: 0.001
2024-11-05 02:50:54,301 - INFO - [diffusion][Epoch 8171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:54,302 - INFO - [diffusion][Epoch 8172] Epoch 8173/12000
2024-11-05 02:50:58,444 - INFO - [diffusion][Epoch 8172] diffusion training Loss: 0.05140952207148075
2024-11-05 02:50:58,447 - INFO - [diffusion][Epoch 8172] diffusion learning rate: 0.001
2024-11-05 02:50:58,449 - INFO - [diffusion][Epoch 8172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:50:58,450 - INFO - [diffusion][Epoch 8173] Epoch 8174/12000
2024-11-05 02:51:02,616 - INFO - [diffusion][Epoch 8173] diffusion training Loss: 0.05561942979693413
2024-11-05 02:51:02,618 - INFO - [diffusion][Epoch 8173] diffusion learning rate: 0.001
2024-11-05 02:51:02,619 - INFO - [diffusion][Epoch 8173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:02,621 - INFO - [diffusion][Epoch 8174] Epoch 8175/12000
2024-11-05 02:51:06,799 - INFO - [diffusion][Epoch 8174] diffusion training Loss: 0.05543351545929909
2024-11-05 02:51:06,801 - INFO - [diffusion][Epoch 8174] diffusion learning rate: 0.001
2024-11-05 02:51:06,803 - INFO - [diffusion][Epoch 8174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:06,804 - INFO - [diffusion][Epoch 8175] Epoch 8176/12000
2024-11-05 02:51:10,989 - INFO - [diffusion][Epoch 8175] diffusion training Loss: 0.049495285376906395
2024-11-05 02:51:10,991 - INFO - [diffusion][Epoch 8175] diffusion learning rate: 0.001
2024-11-05 02:51:10,993 - INFO - [diffusion][Epoch 8175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:10,994 - INFO - [diffusion][Epoch 8176] Epoch 8177/12000
2024-11-05 02:51:15,098 - INFO - [diffusion][Epoch 8176] diffusion training Loss: 0.051347517408430576
2024-11-05 02:51:15,100 - INFO - [diffusion][Epoch 8176] diffusion learning rate: 0.001
2024-11-05 02:51:15,102 - INFO - [diffusion][Epoch 8176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:15,103 - INFO - [diffusion][Epoch 8177] Epoch 8178/12000
2024-11-05 02:51:19,157 - INFO - [diffusion][Epoch 8177] diffusion training Loss: 0.05507834255695343
2024-11-05 02:51:19,159 - INFO - [diffusion][Epoch 8177] diffusion learning rate: 0.001
2024-11-05 02:51:19,161 - INFO - [diffusion][Epoch 8177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:19,162 - INFO - [diffusion][Epoch 8178] Epoch 8179/12000
2024-11-05 02:51:23,173 - INFO - [diffusion][Epoch 8178] diffusion training Loss: 0.05296712927520275
2024-11-05 02:51:23,175 - INFO - [diffusion][Epoch 8178] diffusion learning rate: 0.001
2024-11-05 02:51:23,177 - INFO - [diffusion][Epoch 8178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:23,178 - INFO - [diffusion][Epoch 8179] Epoch 8180/12000
2024-11-05 02:51:27,134 - INFO - [diffusion][Epoch 8179] diffusion training Loss: 0.05691420380026102
2024-11-05 02:51:27,136 - INFO - [diffusion][Epoch 8179] diffusion learning rate: 0.001
2024-11-05 02:51:27,138 - INFO - [diffusion][Epoch 8179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:27,139 - INFO - [diffusion][Epoch 8180] Epoch 8181/12000
2024-11-05 02:51:31,226 - INFO - [diffusion][Epoch 8180] diffusion training Loss: 0.055131291039288044
2024-11-05 02:51:31,229 - INFO - [diffusion][Epoch 8180] diffusion learning rate: 0.001
2024-11-05 02:51:31,230 - INFO - [diffusion][Epoch 8180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:31,232 - INFO - [diffusion][Epoch 8181] Epoch 8182/12000
2024-11-05 02:51:35,360 - INFO - [diffusion][Epoch 8181] diffusion training Loss: 0.04985052067786455
2024-11-05 02:51:35,362 - INFO - [diffusion][Epoch 8181] diffusion learning rate: 0.001
2024-11-05 02:51:35,364 - INFO - [diffusion][Epoch 8181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:35,365 - INFO - [diffusion][Epoch 8182] Epoch 8183/12000
2024-11-05 02:51:39,510 - INFO - [diffusion][Epoch 8182] diffusion training Loss: 0.053638188168406487
2024-11-05 02:51:39,512 - INFO - [diffusion][Epoch 8182] diffusion learning rate: 0.001
2024-11-05 02:51:39,514 - INFO - [diffusion][Epoch 8182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:39,515 - INFO - [diffusion][Epoch 8183] Epoch 8184/12000
2024-11-05 02:51:43,615 - INFO - [diffusion][Epoch 8183] diffusion training Loss: 0.05665775388479233
2024-11-05 02:51:43,617 - INFO - [diffusion][Epoch 8183] diffusion learning rate: 0.001
2024-11-05 02:51:43,619 - INFO - [diffusion][Epoch 8183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:43,621 - INFO - [diffusion][Epoch 8184] Epoch 8185/12000
2024-11-05 02:51:47,726 - INFO - [diffusion][Epoch 8184] diffusion training Loss: 0.051538544707000256
2024-11-05 02:51:47,728 - INFO - [diffusion][Epoch 8184] diffusion learning rate: 0.001
2024-11-05 02:51:47,730 - INFO - [diffusion][Epoch 8184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:47,732 - INFO - [diffusion][Epoch 8185] Epoch 8186/12000
2024-11-05 02:51:51,857 - INFO - [diffusion][Epoch 8185] diffusion training Loss: 0.059287432581186295
2024-11-05 02:51:51,859 - INFO - [diffusion][Epoch 8185] diffusion learning rate: 0.001
2024-11-05 02:51:51,877 - INFO - [diffusion][Epoch 8185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:51,879 - INFO - [diffusion][Epoch 8186] Epoch 8187/12000
2024-11-05 02:51:55,975 - INFO - [diffusion][Epoch 8186] diffusion training Loss: 0.05207555927336216
2024-11-05 02:51:55,977 - INFO - [diffusion][Epoch 8186] diffusion learning rate: 0.001
2024-11-05 02:51:55,979 - INFO - [diffusion][Epoch 8186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:51:55,980 - INFO - [diffusion][Epoch 8187] Epoch 8188/12000
2024-11-05 02:52:00,602 - INFO - [diffusion][Epoch 8187] diffusion training Loss: 0.052497051656246185
2024-11-05 02:52:00,605 - INFO - [diffusion][Epoch 8187] diffusion learning rate: 0.001
2024-11-05 02:52:00,607 - INFO - [diffusion][Epoch 8187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:00,608 - INFO - [diffusion][Epoch 8188] Epoch 8189/12000
2024-11-05 02:52:04,757 - INFO - [diffusion][Epoch 8188] diffusion training Loss: 0.052807947620749474
2024-11-05 02:52:04,759 - INFO - [diffusion][Epoch 8188] diffusion learning rate: 0.001
2024-11-05 02:52:04,761 - INFO - [diffusion][Epoch 8188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:04,762 - INFO - [diffusion][Epoch 8189] Epoch 8190/12000
2024-11-05 02:52:08,900 - INFO - [diffusion][Epoch 8189] diffusion training Loss: 0.05287700705230236
2024-11-05 02:52:08,902 - INFO - [diffusion][Epoch 8189] diffusion learning rate: 0.001
2024-11-05 02:52:08,903 - INFO - [diffusion][Epoch 8189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:08,904 - INFO - [diffusion][Epoch 8190] Epoch 8191/12000
2024-11-05 02:52:13,039 - INFO - [diffusion][Epoch 8190] diffusion training Loss: 0.0516915125772357
2024-11-05 02:52:13,042 - INFO - [diffusion][Epoch 8190] diffusion learning rate: 0.001
2024-11-05 02:52:13,043 - INFO - [diffusion][Epoch 8190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:13,045 - INFO - [diffusion][Epoch 8191] Epoch 8192/12000
2024-11-05 02:52:17,147 - INFO - [diffusion][Epoch 8191] diffusion training Loss: 0.05293508432805538
2024-11-05 02:52:17,149 - INFO - [diffusion][Epoch 8191] diffusion learning rate: 0.001
2024-11-05 02:52:17,151 - INFO - [diffusion][Epoch 8191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:17,152 - INFO - [diffusion][Epoch 8192] Epoch 8193/12000
2024-11-05 02:52:21,220 - INFO - [diffusion][Epoch 8192] diffusion training Loss: 0.0599199254065752
2024-11-05 02:52:21,223 - INFO - [diffusion][Epoch 8192] diffusion learning rate: 0.001
2024-11-05 02:52:21,225 - INFO - [diffusion][Epoch 8192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:21,226 - INFO - [diffusion][Epoch 8193] Epoch 8194/12000
2024-11-05 02:52:25,313 - INFO - [diffusion][Epoch 8193] diffusion training Loss: 0.0479110311716795
2024-11-05 02:52:25,315 - INFO - [diffusion][Epoch 8193] diffusion learning rate: 0.001
2024-11-05 02:52:25,317 - INFO - [diffusion][Epoch 8193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:25,318 - INFO - [diffusion][Epoch 8194] Epoch 8195/12000
2024-11-05 02:52:29,246 - INFO - [diffusion][Epoch 8194] diffusion training Loss: 0.05273029860109091
2024-11-05 02:52:29,249 - INFO - [diffusion][Epoch 8194] diffusion learning rate: 0.001
2024-11-05 02:52:29,251 - INFO - [diffusion][Epoch 8194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:29,253 - INFO - [diffusion][Epoch 8195] Epoch 8196/12000
2024-11-05 02:52:33,220 - INFO - [diffusion][Epoch 8195] diffusion training Loss: 0.04909401573240757
2024-11-05 02:52:33,222 - INFO - [diffusion][Epoch 8195] diffusion learning rate: 0.001
2024-11-05 02:52:33,223 - INFO - [diffusion][Epoch 8195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:33,225 - INFO - [diffusion][Epoch 8196] Epoch 8197/12000
2024-11-05 02:52:37,273 - INFO - [diffusion][Epoch 8196] diffusion training Loss: 0.058063333854079247
2024-11-05 02:52:37,275 - INFO - [diffusion][Epoch 8196] diffusion learning rate: 0.001
2024-11-05 02:52:37,277 - INFO - [diffusion][Epoch 8196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:37,279 - INFO - [diffusion][Epoch 8197] Epoch 8198/12000
2024-11-05 02:52:41,342 - INFO - [diffusion][Epoch 8197] diffusion training Loss: 0.052767918445169926
2024-11-05 02:52:41,346 - INFO - [diffusion][Epoch 8197] diffusion learning rate: 0.001
2024-11-05 02:52:41,347 - INFO - [diffusion][Epoch 8197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:41,349 - INFO - [diffusion][Epoch 8198] Epoch 8199/12000
2024-11-05 02:52:45,424 - INFO - [diffusion][Epoch 8198] diffusion training Loss: 0.05132713168859482
2024-11-05 02:52:45,426 - INFO - [diffusion][Epoch 8198] diffusion learning rate: 0.001
2024-11-05 02:52:45,456 - INFO - [diffusion][Epoch 8198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:45,458 - INFO - [diffusion][Epoch 8199] Epoch 8200/12000
2024-11-05 02:52:49,563 - INFO - [diffusion][Epoch 8199] diffusion training Loss: 0.050808168947696686
2024-11-05 02:52:49,565 - INFO - [diffusion][Epoch 8199] diffusion learning rate: 0.001
2024-11-05 02:52:49,567 - INFO - [diffusion][Epoch 8199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:49,568 - INFO - [diffusion][Epoch 8200] Epoch 8201/12000
2024-11-05 02:52:53,541 - INFO - [diffusion][Epoch 8200] diffusion training Loss: 0.05346886161714792
2024-11-05 02:52:53,543 - INFO - [diffusion][Epoch 8200] diffusion learning rate: 0.001
2024-11-05 02:52:53,545 - INFO - [diffusion][Epoch 8200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:53,546 - INFO - [diffusion][Epoch 8201] Epoch 8202/12000
2024-11-05 02:52:57,494 - INFO - [diffusion][Epoch 8201] diffusion training Loss: 0.05113627575337887
2024-11-05 02:52:57,496 - INFO - [diffusion][Epoch 8201] diffusion learning rate: 0.001
2024-11-05 02:52:57,498 - INFO - [diffusion][Epoch 8201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:52:57,500 - INFO - [diffusion][Epoch 8202] Epoch 8203/12000
2024-11-05 02:53:01,634 - INFO - [diffusion][Epoch 8202] diffusion training Loss: 0.05562647897750139
2024-11-05 02:53:01,637 - INFO - [diffusion][Epoch 8202] diffusion learning rate: 0.001
2024-11-05 02:53:01,639 - INFO - [diffusion][Epoch 8202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:01,640 - INFO - [diffusion][Epoch 8203] Epoch 8204/12000
2024-11-05 02:53:05,715 - INFO - [diffusion][Epoch 8203] diffusion training Loss: 0.05036900378763676
2024-11-05 02:53:05,717 - INFO - [diffusion][Epoch 8203] diffusion learning rate: 0.001
2024-11-05 02:53:05,757 - INFO - [diffusion][Epoch 8203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:05,759 - INFO - [diffusion][Epoch 8204] Epoch 8205/12000
2024-11-05 02:53:09,886 - INFO - [diffusion][Epoch 8204] diffusion training Loss: 0.05565654020756483
2024-11-05 02:53:09,888 - INFO - [diffusion][Epoch 8204] diffusion learning rate: 0.001
2024-11-05 02:53:09,890 - INFO - [diffusion][Epoch 8204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:09,891 - INFO - [diffusion][Epoch 8205] Epoch 8206/12000
2024-11-05 02:53:14,006 - INFO - [diffusion][Epoch 8205] diffusion training Loss: 0.05170034244656563
2024-11-05 02:53:14,008 - INFO - [diffusion][Epoch 8205] diffusion learning rate: 0.001
2024-11-05 02:53:14,010 - INFO - [diffusion][Epoch 8205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:14,011 - INFO - [diffusion][Epoch 8206] Epoch 8207/12000
2024-11-05 02:53:18,160 - INFO - [diffusion][Epoch 8206] diffusion training Loss: 0.05724376533180475
2024-11-05 02:53:18,163 - INFO - [diffusion][Epoch 8206] diffusion learning rate: 0.001
2024-11-05 02:53:18,165 - INFO - [diffusion][Epoch 8206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:18,167 - INFO - [diffusion][Epoch 8207] Epoch 8208/12000
2024-11-05 02:53:22,422 - INFO - [diffusion][Epoch 8207] diffusion training Loss: 0.05467993579804897
2024-11-05 02:53:22,424 - INFO - [diffusion][Epoch 8207] diffusion learning rate: 0.001
2024-11-05 02:53:22,426 - INFO - [diffusion][Epoch 8207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:22,427 - INFO - [diffusion][Epoch 8208] Epoch 8209/12000
2024-11-05 02:53:26,253 - INFO - [diffusion][Epoch 8208] diffusion training Loss: 0.056625633500516415
2024-11-05 02:53:26,255 - INFO - [diffusion][Epoch 8208] diffusion learning rate: 0.001
2024-11-05 02:53:26,297 - INFO - [diffusion][Epoch 8208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:26,299 - INFO - [diffusion][Epoch 8209] Epoch 8210/12000
2024-11-05 02:53:30,326 - INFO - [diffusion][Epoch 8209] diffusion training Loss: 0.05385005380958319
2024-11-05 02:53:30,328 - INFO - [diffusion][Epoch 8209] diffusion learning rate: 0.001
2024-11-05 02:53:30,330 - INFO - [diffusion][Epoch 8209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:30,331 - INFO - [diffusion][Epoch 8210] Epoch 8211/12000
2024-11-05 02:53:34,319 - INFO - [diffusion][Epoch 8210] diffusion training Loss: 0.051917703822255135
2024-11-05 02:53:34,321 - INFO - [diffusion][Epoch 8210] diffusion learning rate: 0.001
2024-11-05 02:53:34,323 - INFO - [diffusion][Epoch 8210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:34,325 - INFO - [diffusion][Epoch 8211] Epoch 8212/12000
2024-11-05 02:53:38,229 - INFO - [diffusion][Epoch 8211] diffusion training Loss: 0.04921284317970276
2024-11-05 02:53:38,231 - INFO - [diffusion][Epoch 8211] diffusion learning rate: 0.001
2024-11-05 02:53:38,232 - INFO - [diffusion][Epoch 8211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:38,234 - INFO - [diffusion][Epoch 8212] Epoch 8213/12000
2024-11-05 02:53:42,029 - INFO - [diffusion][Epoch 8212] diffusion training Loss: 0.05595266632735729
2024-11-05 02:53:42,031 - INFO - [diffusion][Epoch 8212] diffusion learning rate: 0.001
2024-11-05 02:53:42,033 - INFO - [diffusion][Epoch 8212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:42,035 - INFO - [diffusion][Epoch 8213] Epoch 8214/12000
2024-11-05 02:53:45,961 - INFO - [diffusion][Epoch 8213] diffusion training Loss: 0.04919410962611437
2024-11-05 02:53:45,963 - INFO - [diffusion][Epoch 8213] diffusion learning rate: 0.001
2024-11-05 02:53:45,964 - INFO - [diffusion][Epoch 8213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:45,966 - INFO - [diffusion][Epoch 8214] Epoch 8215/12000
2024-11-05 02:53:49,968 - INFO - [diffusion][Epoch 8214] diffusion training Loss: 0.05539694707840681
2024-11-05 02:53:49,970 - INFO - [diffusion][Epoch 8214] diffusion learning rate: 0.001
2024-11-05 02:53:49,972 - INFO - [diffusion][Epoch 8214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:49,973 - INFO - [diffusion][Epoch 8215] Epoch 8216/12000
2024-11-05 02:53:54,010 - INFO - [diffusion][Epoch 8215] diffusion training Loss: 0.05155334062874317
2024-11-05 02:53:54,012 - INFO - [diffusion][Epoch 8215] diffusion learning rate: 0.001
2024-11-05 02:53:54,014 - INFO - [diffusion][Epoch 8215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:54,015 - INFO - [diffusion][Epoch 8216] Epoch 8217/12000
2024-11-05 02:53:58,074 - INFO - [diffusion][Epoch 8216] diffusion training Loss: 0.05233038403093815
2024-11-05 02:53:58,076 - INFO - [diffusion][Epoch 8216] diffusion learning rate: 0.001
2024-11-05 02:53:58,078 - INFO - [diffusion][Epoch 8216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:53:58,080 - INFO - [diffusion][Epoch 8217] Epoch 8218/12000
2024-11-05 02:54:02,176 - INFO - [diffusion][Epoch 8217] diffusion training Loss: 0.056214625015854836
2024-11-05 02:54:02,179 - INFO - [diffusion][Epoch 8217] diffusion learning rate: 0.001
2024-11-05 02:54:02,181 - INFO - [diffusion][Epoch 8217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:02,183 - INFO - [diffusion][Epoch 8218] Epoch 8219/12000
2024-11-05 02:54:06,080 - INFO - [diffusion][Epoch 8218] diffusion training Loss: 0.05326084326952696
2024-11-05 02:54:06,082 - INFO - [diffusion][Epoch 8218] diffusion learning rate: 0.001
2024-11-05 02:54:06,084 - INFO - [diffusion][Epoch 8218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:06,085 - INFO - [diffusion][Epoch 8219] Epoch 8220/12000
2024-11-05 02:54:10,123 - INFO - [diffusion][Epoch 8219] diffusion training Loss: 0.05534878186881542
2024-11-05 02:54:10,125 - INFO - [diffusion][Epoch 8219] diffusion learning rate: 0.001
2024-11-05 02:54:10,126 - INFO - [diffusion][Epoch 8219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:10,128 - INFO - [diffusion][Epoch 8220] Epoch 8221/12000
2024-11-05 02:54:14,214 - INFO - [diffusion][Epoch 8220] diffusion training Loss: 0.05356556363403797
2024-11-05 02:54:14,216 - INFO - [diffusion][Epoch 8220] diffusion learning rate: 0.001
2024-11-05 02:54:14,218 - INFO - [diffusion][Epoch 8220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:14,219 - INFO - [diffusion][Epoch 8221] Epoch 8222/12000
2024-11-05 02:54:18,295 - INFO - [diffusion][Epoch 8221] diffusion training Loss: 0.05301958415657282
2024-11-05 02:54:18,298 - INFO - [diffusion][Epoch 8221] diffusion learning rate: 0.001
2024-11-05 02:54:18,300 - INFO - [diffusion][Epoch 8221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:18,302 - INFO - [diffusion][Epoch 8222] Epoch 8223/12000
2024-11-05 02:54:22,380 - INFO - [diffusion][Epoch 8222] diffusion training Loss: 0.05280266888439655
2024-11-05 02:54:22,382 - INFO - [diffusion][Epoch 8222] diffusion learning rate: 0.001
2024-11-05 02:54:22,384 - INFO - [diffusion][Epoch 8222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:22,385 - INFO - [diffusion][Epoch 8223] Epoch 8224/12000
2024-11-05 02:54:26,408 - INFO - [diffusion][Epoch 8223] diffusion training Loss: 0.05353168956935406
2024-11-05 02:54:26,410 - INFO - [diffusion][Epoch 8223] diffusion learning rate: 0.001
2024-11-05 02:54:26,413 - INFO - [diffusion][Epoch 8223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:26,416 - INFO - [diffusion][Epoch 8224] Epoch 8225/12000
2024-11-05 02:54:30,510 - INFO - [diffusion][Epoch 8224] diffusion training Loss: 0.056018550880253315
2024-11-05 02:54:30,512 - INFO - [diffusion][Epoch 8224] diffusion learning rate: 0.001
2024-11-05 02:54:30,514 - INFO - [diffusion][Epoch 8224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:30,515 - INFO - [diffusion][Epoch 8225] Epoch 8226/12000
2024-11-05 02:54:34,564 - INFO - [diffusion][Epoch 8225] diffusion training Loss: 0.049978296272456646
2024-11-05 02:54:34,566 - INFO - [diffusion][Epoch 8225] diffusion learning rate: 0.001
2024-11-05 02:54:34,594 - INFO - [diffusion][Epoch 8225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:34,596 - INFO - [diffusion][Epoch 8226] Epoch 8227/12000
2024-11-05 02:54:38,608 - INFO - [diffusion][Epoch 8226] diffusion training Loss: 0.0535911601036787
2024-11-05 02:54:38,610 - INFO - [diffusion][Epoch 8226] diffusion learning rate: 0.001
2024-11-05 02:54:38,612 - INFO - [diffusion][Epoch 8226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:38,613 - INFO - [diffusion][Epoch 8227] Epoch 8228/12000
2024-11-05 02:54:42,773 - INFO - [diffusion][Epoch 8227] diffusion training Loss: 0.04786044452339411
2024-11-05 02:54:42,776 - INFO - [diffusion][Epoch 8227] diffusion learning rate: 0.001
2024-11-05 02:54:42,778 - INFO - [diffusion][Epoch 8227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:42,779 - INFO - [diffusion][Epoch 8228] Epoch 8229/12000
2024-11-05 02:54:47,426 - INFO - [diffusion][Epoch 8228] diffusion training Loss: 0.058169673196971416
2024-11-05 02:54:47,428 - INFO - [diffusion][Epoch 8228] diffusion learning rate: 0.001
2024-11-05 02:54:47,430 - INFO - [diffusion][Epoch 8228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:47,432 - INFO - [diffusion][Epoch 8229] Epoch 8230/12000
2024-11-05 02:54:51,575 - INFO - [diffusion][Epoch 8229] diffusion training Loss: 0.05773978307843208
2024-11-05 02:54:51,577 - INFO - [diffusion][Epoch 8229] diffusion learning rate: 0.001
2024-11-05 02:54:51,579 - INFO - [diffusion][Epoch 8229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:51,580 - INFO - [diffusion][Epoch 8230] Epoch 8231/12000
2024-11-05 02:54:55,697 - INFO - [diffusion][Epoch 8230] diffusion training Loss: 0.05202206131070852
2024-11-05 02:54:55,699 - INFO - [diffusion][Epoch 8230] diffusion learning rate: 0.001
2024-11-05 02:54:55,701 - INFO - [diffusion][Epoch 8230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:55,703 - INFO - [diffusion][Epoch 8231] Epoch 8232/12000
2024-11-05 02:54:59,689 - INFO - [diffusion][Epoch 8231] diffusion training Loss: 0.05807832069694996
2024-11-05 02:54:59,691 - INFO - [diffusion][Epoch 8231] diffusion learning rate: 0.001
2024-11-05 02:54:59,693 - INFO - [diffusion][Epoch 8231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:54:59,694 - INFO - [diffusion][Epoch 8232] Epoch 8233/12000
2024-11-05 02:55:03,690 - INFO - [diffusion][Epoch 8232] diffusion training Loss: 0.05816314369440079
2024-11-05 02:55:03,693 - INFO - [diffusion][Epoch 8232] diffusion learning rate: 0.001
2024-11-05 02:55:03,695 - INFO - [diffusion][Epoch 8232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:03,696 - INFO - [diffusion][Epoch 8233] Epoch 8234/12000
2024-11-05 02:55:07,762 - INFO - [diffusion][Epoch 8233] diffusion training Loss: 0.05624028109014034
2024-11-05 02:55:07,764 - INFO - [diffusion][Epoch 8233] diffusion learning rate: 0.001
2024-11-05 02:55:07,767 - INFO - [diffusion][Epoch 8233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:07,769 - INFO - [diffusion][Epoch 8234] Epoch 8235/12000
2024-11-05 02:55:11,804 - INFO - [diffusion][Epoch 8234] diffusion training Loss: 0.053469596430659294
2024-11-05 02:55:11,806 - INFO - [diffusion][Epoch 8234] diffusion learning rate: 0.001
2024-11-05 02:55:11,808 - INFO - [diffusion][Epoch 8234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:11,809 - INFO - [diffusion][Epoch 8235] Epoch 8236/12000
2024-11-05 02:55:15,890 - INFO - [diffusion][Epoch 8235] diffusion training Loss: 0.05576953198760748
2024-11-05 02:55:15,893 - INFO - [diffusion][Epoch 8235] diffusion learning rate: 0.001
2024-11-05 02:55:15,894 - INFO - [diffusion][Epoch 8235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:15,896 - INFO - [diffusion][Epoch 8236] Epoch 8237/12000
2024-11-05 02:55:19,996 - INFO - [diffusion][Epoch 8236] diffusion training Loss: 0.04683931637555361
2024-11-05 02:55:19,999 - INFO - [diffusion][Epoch 8236] diffusion learning rate: 0.001
2024-11-05 02:55:20,001 - INFO - [diffusion][Epoch 8236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:20,002 - INFO - [diffusion][Epoch 8237] Epoch 8238/12000
2024-11-05 02:55:24,111 - INFO - [diffusion][Epoch 8237] diffusion training Loss: 0.05619816388934851
2024-11-05 02:55:24,113 - INFO - [diffusion][Epoch 8237] diffusion learning rate: 0.001
2024-11-05 02:55:24,115 - INFO - [diffusion][Epoch 8237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:24,116 - INFO - [diffusion][Epoch 8238] Epoch 8239/12000
2024-11-05 02:55:28,168 - INFO - [diffusion][Epoch 8238] diffusion training Loss: 0.05862845201045275
2024-11-05 02:55:28,170 - INFO - [diffusion][Epoch 8238] diffusion learning rate: 0.001
2024-11-05 02:55:28,172 - INFO - [diffusion][Epoch 8238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:28,173 - INFO - [diffusion][Epoch 8239] Epoch 8240/12000
2024-11-05 02:55:32,304 - INFO - [diffusion][Epoch 8239] diffusion training Loss: 0.056603725999593735
2024-11-05 02:55:32,307 - INFO - [diffusion][Epoch 8239] diffusion learning rate: 0.001
2024-11-05 02:55:32,309 - INFO - [diffusion][Epoch 8239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:32,310 - INFO - [diffusion][Epoch 8240] Epoch 8241/12000
2024-11-05 02:55:36,343 - INFO - [diffusion][Epoch 8240] diffusion training Loss: 0.057064338587224483
2024-11-05 02:55:36,346 - INFO - [diffusion][Epoch 8240] diffusion learning rate: 0.001
2024-11-05 02:55:36,348 - INFO - [diffusion][Epoch 8240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:36,349 - INFO - [diffusion][Epoch 8241] Epoch 8242/12000
2024-11-05 02:55:40,477 - INFO - [diffusion][Epoch 8241] diffusion training Loss: 0.05610283277928829
2024-11-05 02:55:40,479 - INFO - [diffusion][Epoch 8241] diffusion learning rate: 0.001
2024-11-05 02:55:40,481 - INFO - [diffusion][Epoch 8241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:40,482 - INFO - [diffusion][Epoch 8242] Epoch 8243/12000
2024-11-05 02:55:44,596 - INFO - [diffusion][Epoch 8242] diffusion training Loss: 0.05572137329727411
2024-11-05 02:55:44,598 - INFO - [diffusion][Epoch 8242] diffusion learning rate: 0.001
2024-11-05 02:55:44,600 - INFO - [diffusion][Epoch 8242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:44,602 - INFO - [diffusion][Epoch 8243] Epoch 8244/12000
2024-11-05 02:55:48,744 - INFO - [diffusion][Epoch 8243] diffusion training Loss: 0.05719490721821785
2024-11-05 02:55:48,746 - INFO - [diffusion][Epoch 8243] diffusion learning rate: 0.001
2024-11-05 02:55:48,748 - INFO - [diffusion][Epoch 8243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:48,749 - INFO - [diffusion][Epoch 8244] Epoch 8245/12000
2024-11-05 02:55:52,782 - INFO - [diffusion][Epoch 8244] diffusion training Loss: 0.05457736924290657
2024-11-05 02:55:52,784 - INFO - [diffusion][Epoch 8244] diffusion learning rate: 0.001
2024-11-05 02:55:52,786 - INFO - [diffusion][Epoch 8244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:52,787 - INFO - [diffusion][Epoch 8245] Epoch 8246/12000
2024-11-05 02:55:56,941 - INFO - [diffusion][Epoch 8245] diffusion training Loss: 0.05191817879676819
2024-11-05 02:55:56,943 - INFO - [diffusion][Epoch 8245] diffusion learning rate: 0.001
2024-11-05 02:55:56,945 - INFO - [diffusion][Epoch 8245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:55:56,946 - INFO - [diffusion][Epoch 8246] Epoch 8247/12000
2024-11-05 02:56:00,971 - INFO - [diffusion][Epoch 8246] diffusion training Loss: 0.05534632131457329
2024-11-05 02:56:00,973 - INFO - [diffusion][Epoch 8246] diffusion learning rate: 0.001
2024-11-05 02:56:00,975 - INFO - [diffusion][Epoch 8246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:00,976 - INFO - [diffusion][Epoch 8247] Epoch 8248/12000
2024-11-05 02:56:05,096 - INFO - [diffusion][Epoch 8247] diffusion training Loss: 0.05495178047567606
2024-11-05 02:56:05,099 - INFO - [diffusion][Epoch 8247] diffusion learning rate: 0.001
2024-11-05 02:56:05,100 - INFO - [diffusion][Epoch 8247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:05,102 - INFO - [diffusion][Epoch 8248] Epoch 8249/12000
2024-11-05 02:56:09,334 - INFO - [diffusion][Epoch 8248] diffusion training Loss: 0.052920980378985405
2024-11-05 02:56:09,337 - INFO - [diffusion][Epoch 8248] diffusion learning rate: 0.001
2024-11-05 02:56:09,339 - INFO - [diffusion][Epoch 8248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:09,340 - INFO - [diffusion][Epoch 8249] Epoch 8250/12000
2024-11-05 02:56:13,332 - INFO - [diffusion][Epoch 8249] diffusion training Loss: 0.05393636878579855
2024-11-05 02:56:13,334 - INFO - [diffusion][Epoch 8249] diffusion learning rate: 0.001
2024-11-05 02:56:13,336 - INFO - [diffusion][Epoch 8249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:13,337 - INFO - [diffusion][Epoch 8250] Epoch 8251/12000
2024-11-05 02:56:17,389 - INFO - [diffusion][Epoch 8250] diffusion training Loss: 0.05483973305672407
2024-11-05 02:56:17,392 - INFO - [diffusion][Epoch 8250] diffusion learning rate: 0.001
2024-11-05 02:56:17,394 - INFO - [diffusion][Epoch 8250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:17,395 - INFO - [diffusion][Epoch 8251] Epoch 8252/12000
2024-11-05 02:56:21,495 - INFO - [diffusion][Epoch 8251] diffusion training Loss: 0.0626106234267354
2024-11-05 02:56:21,497 - INFO - [diffusion][Epoch 8251] diffusion learning rate: 0.001
2024-11-05 02:56:21,498 - INFO - [diffusion][Epoch 8251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:21,499 - INFO - [diffusion][Epoch 8252] Epoch 8253/12000
2024-11-05 02:56:25,606 - INFO - [diffusion][Epoch 8252] diffusion training Loss: 0.052762131206691265
2024-11-05 02:56:25,608 - INFO - [diffusion][Epoch 8252] diffusion learning rate: 0.001
2024-11-05 02:56:25,610 - INFO - [diffusion][Epoch 8252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:25,611 - INFO - [diffusion][Epoch 8253] Epoch 8254/12000
2024-11-05 02:56:29,747 - INFO - [diffusion][Epoch 8253] diffusion training Loss: 0.0529981916770339
2024-11-05 02:56:29,749 - INFO - [diffusion][Epoch 8253] diffusion learning rate: 0.001
2024-11-05 02:56:29,751 - INFO - [diffusion][Epoch 8253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:29,752 - INFO - [diffusion][Epoch 8254] Epoch 8255/12000
2024-11-05 02:56:33,849 - INFO - [diffusion][Epoch 8254] diffusion training Loss: 0.05602673254907131
2024-11-05 02:56:33,851 - INFO - [diffusion][Epoch 8254] diffusion learning rate: 0.001
2024-11-05 02:56:33,852 - INFO - [diffusion][Epoch 8254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:33,854 - INFO - [diffusion][Epoch 8255] Epoch 8256/12000
2024-11-05 02:56:37,907 - INFO - [diffusion][Epoch 8255] diffusion training Loss: 0.05032313521951437
2024-11-05 02:56:37,909 - INFO - [diffusion][Epoch 8255] diffusion learning rate: 0.001
2024-11-05 02:56:37,911 - INFO - [diffusion][Epoch 8255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:37,912 - INFO - [diffusion][Epoch 8256] Epoch 8257/12000
2024-11-05 02:56:41,927 - INFO - [diffusion][Epoch 8256] diffusion training Loss: 0.05157520342618227
2024-11-05 02:56:41,929 - INFO - [diffusion][Epoch 8256] diffusion learning rate: 0.001
2024-11-05 02:56:41,931 - INFO - [diffusion][Epoch 8256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:41,932 - INFO - [diffusion][Epoch 8257] Epoch 8258/12000
2024-11-05 02:56:46,044 - INFO - [diffusion][Epoch 8257] diffusion training Loss: 0.05147655215114355
2024-11-05 02:56:46,047 - INFO - [diffusion][Epoch 8257] diffusion learning rate: 0.001
2024-11-05 02:56:46,049 - INFO - [diffusion][Epoch 8257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:46,050 - INFO - [diffusion][Epoch 8258] Epoch 8259/12000
2024-11-05 02:56:50,027 - INFO - [diffusion][Epoch 8258] diffusion training Loss: 0.04728156886994839
2024-11-05 02:56:50,029 - INFO - [diffusion][Epoch 8258] diffusion learning rate: 0.001
2024-11-05 02:56:50,031 - INFO - [diffusion][Epoch 8258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:50,032 - INFO - [diffusion][Epoch 8259] Epoch 8260/12000
2024-11-05 02:56:54,045 - INFO - [diffusion][Epoch 8259] diffusion training Loss: 0.05058823898434639
2024-11-05 02:56:54,047 - INFO - [diffusion][Epoch 8259] diffusion learning rate: 0.001
2024-11-05 02:56:54,090 - INFO - [diffusion][Epoch 8259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:54,091 - INFO - [diffusion][Epoch 8260] Epoch 8261/12000
2024-11-05 02:56:58,111 - INFO - [diffusion][Epoch 8260] diffusion training Loss: 0.050974391400814056
2024-11-05 02:56:58,113 - INFO - [diffusion][Epoch 8260] diffusion learning rate: 0.001
2024-11-05 02:56:58,115 - INFO - [diffusion][Epoch 8260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:56:58,116 - INFO - [diffusion][Epoch 8261] Epoch 8262/12000
2024-11-05 02:57:02,201 - INFO - [diffusion][Epoch 8261] diffusion training Loss: 0.05193411000072956
2024-11-05 02:57:02,203 - INFO - [diffusion][Epoch 8261] diffusion learning rate: 0.001
2024-11-05 02:57:02,205 - INFO - [diffusion][Epoch 8261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:02,206 - INFO - [diffusion][Epoch 8262] Epoch 8263/12000
2024-11-05 02:57:06,322 - INFO - [diffusion][Epoch 8262] diffusion training Loss: 0.05345813371241093
2024-11-05 02:57:06,324 - INFO - [diffusion][Epoch 8262] diffusion learning rate: 0.001
2024-11-05 02:57:06,326 - INFO - [diffusion][Epoch 8262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:06,327 - INFO - [diffusion][Epoch 8263] Epoch 8264/12000
2024-11-05 02:57:10,412 - INFO - [diffusion][Epoch 8263] diffusion training Loss: 0.057671116665005684
2024-11-05 02:57:10,414 - INFO - [diffusion][Epoch 8263] diffusion learning rate: 0.001
2024-11-05 02:57:10,416 - INFO - [diffusion][Epoch 8263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:10,417 - INFO - [diffusion][Epoch 8264] Epoch 8265/12000
2024-11-05 02:57:14,468 - INFO - [diffusion][Epoch 8264] diffusion training Loss: 0.05607823934406042
2024-11-05 02:57:14,470 - INFO - [diffusion][Epoch 8264] diffusion learning rate: 0.001
2024-11-05 02:57:14,471 - INFO - [diffusion][Epoch 8264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:14,473 - INFO - [diffusion][Epoch 8265] Epoch 8266/12000
2024-11-05 02:57:18,438 - INFO - [diffusion][Epoch 8265] diffusion training Loss: 0.053929101675748825
2024-11-05 02:57:18,440 - INFO - [diffusion][Epoch 8265] diffusion learning rate: 0.001
2024-11-05 02:57:18,441 - INFO - [diffusion][Epoch 8265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:18,443 - INFO - [diffusion][Epoch 8266] Epoch 8267/12000
2024-11-05 02:57:22,539 - INFO - [diffusion][Epoch 8266] diffusion training Loss: 0.05255866330116987
2024-11-05 02:57:22,541 - INFO - [diffusion][Epoch 8266] diffusion learning rate: 0.001
2024-11-05 02:57:22,543 - INFO - [diffusion][Epoch 8266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:22,544 - INFO - [diffusion][Epoch 8267] Epoch 8268/12000
2024-11-05 02:57:26,523 - INFO - [diffusion][Epoch 8267] diffusion training Loss: 0.0581178143620491
2024-11-05 02:57:26,525 - INFO - [diffusion][Epoch 8267] diffusion learning rate: 0.001
2024-11-05 02:57:26,527 - INFO - [diffusion][Epoch 8267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:26,528 - INFO - [diffusion][Epoch 8268] Epoch 8269/12000
2024-11-05 02:57:30,674 - INFO - [diffusion][Epoch 8268] diffusion training Loss: 0.04919719975441694
2024-11-05 02:57:30,676 - INFO - [diffusion][Epoch 8268] diffusion learning rate: 0.001
2024-11-05 02:57:30,678 - INFO - [diffusion][Epoch 8268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:30,679 - INFO - [diffusion][Epoch 8269] Epoch 8270/12000
2024-11-05 02:57:35,087 - INFO - [diffusion][Epoch 8269] diffusion training Loss: 0.05479681398719549
2024-11-05 02:57:35,126 - INFO - [diffusion][Epoch 8269] diffusion learning rate: 0.001
2024-11-05 02:57:35,131 - INFO - [diffusion][Epoch 8269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:35,132 - INFO - [diffusion][Epoch 8270] Epoch 8271/12000
2024-11-05 02:57:39,192 - INFO - [diffusion][Epoch 8270] diffusion training Loss: 0.05384054034948349
2024-11-05 02:57:39,194 - INFO - [diffusion][Epoch 8270] diffusion learning rate: 0.001
2024-11-05 02:57:39,196 - INFO - [diffusion][Epoch 8270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:39,197 - INFO - [diffusion][Epoch 8271] Epoch 8272/12000
2024-11-05 02:57:43,224 - INFO - [diffusion][Epoch 8271] diffusion training Loss: 0.050500549376010895
2024-11-05 02:57:43,228 - INFO - [diffusion][Epoch 8271] diffusion learning rate: 0.001
2024-11-05 02:57:43,230 - INFO - [diffusion][Epoch 8271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:43,231 - INFO - [diffusion][Epoch 8272] Epoch 8273/12000
2024-11-05 02:57:47,344 - INFO - [diffusion][Epoch 8272] diffusion training Loss: 0.05156539287418127
2024-11-05 02:57:47,345 - INFO - [diffusion][Epoch 8272] diffusion learning rate: 0.001
2024-11-05 02:57:47,347 - INFO - [diffusion][Epoch 8272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:47,348 - INFO - [diffusion][Epoch 8273] Epoch 8274/12000
2024-11-05 02:57:51,505 - INFO - [diffusion][Epoch 8273] diffusion training Loss: 0.05548600573092699
2024-11-05 02:57:51,507 - INFO - [diffusion][Epoch 8273] diffusion learning rate: 0.001
2024-11-05 02:57:51,509 - INFO - [diffusion][Epoch 8273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:51,510 - INFO - [diffusion][Epoch 8274] Epoch 8275/12000
2024-11-05 02:57:55,631 - INFO - [diffusion][Epoch 8274] diffusion training Loss: 0.053907730616629124
2024-11-05 02:57:55,633 - INFO - [diffusion][Epoch 8274] diffusion learning rate: 0.001
2024-11-05 02:57:55,635 - INFO - [diffusion][Epoch 8274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:55,636 - INFO - [diffusion][Epoch 8275] Epoch 8276/12000
2024-11-05 02:57:59,790 - INFO - [diffusion][Epoch 8275] diffusion training Loss: 0.047236885875463486
2024-11-05 02:57:59,792 - INFO - [diffusion][Epoch 8275] diffusion learning rate: 0.001
2024-11-05 02:57:59,794 - INFO - [diffusion][Epoch 8275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:57:59,795 - INFO - [diffusion][Epoch 8276] Epoch 8277/12000
2024-11-05 02:58:03,912 - INFO - [diffusion][Epoch 8276] diffusion training Loss: 0.05152208358049393
2024-11-05 02:58:03,914 - INFO - [diffusion][Epoch 8276] diffusion learning rate: 0.001
2024-11-05 02:58:03,915 - INFO - [diffusion][Epoch 8276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:03,917 - INFO - [diffusion][Epoch 8277] Epoch 8278/12000
2024-11-05 02:58:07,943 - INFO - [diffusion][Epoch 8277] diffusion training Loss: 0.050877938978374004
2024-11-05 02:58:07,952 - INFO - [diffusion][Epoch 8277] diffusion learning rate: 0.001
2024-11-05 02:58:07,954 - INFO - [diffusion][Epoch 8277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:07,955 - INFO - [diffusion][Epoch 8278] Epoch 8279/12000
2024-11-05 02:58:12,075 - INFO - [diffusion][Epoch 8278] diffusion training Loss: 0.05275490786880255
2024-11-05 02:58:12,078 - INFO - [diffusion][Epoch 8278] diffusion learning rate: 0.001
2024-11-05 02:58:12,080 - INFO - [diffusion][Epoch 8278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:12,081 - INFO - [diffusion][Epoch 8279] Epoch 8280/12000
2024-11-05 02:58:16,174 - INFO - [diffusion][Epoch 8279] diffusion training Loss: 0.04901055432856083
2024-11-05 02:58:16,176 - INFO - [diffusion][Epoch 8279] diffusion learning rate: 0.001
2024-11-05 02:58:16,178 - INFO - [diffusion][Epoch 8279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:16,180 - INFO - [diffusion][Epoch 8280] Epoch 8281/12000
2024-11-05 02:58:20,330 - INFO - [diffusion][Epoch 8280] diffusion training Loss: 0.055071091279387474
2024-11-05 02:58:20,332 - INFO - [diffusion][Epoch 8280] diffusion learning rate: 0.001
2024-11-05 02:58:20,334 - INFO - [diffusion][Epoch 8280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:20,335 - INFO - [diffusion][Epoch 8281] Epoch 8282/12000
2024-11-05 02:58:24,282 - INFO - [diffusion][Epoch 8281] diffusion training Loss: 0.059323386289179325
2024-11-05 02:58:24,284 - INFO - [diffusion][Epoch 8281] diffusion learning rate: 0.001
2024-11-05 02:58:24,325 - INFO - [diffusion][Epoch 8281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:24,326 - INFO - [diffusion][Epoch 8282] Epoch 8283/12000
2024-11-05 02:58:28,243 - INFO - [diffusion][Epoch 8282] diffusion training Loss: 0.05153009947389364
2024-11-05 02:58:28,245 - INFO - [diffusion][Epoch 8282] diffusion learning rate: 0.001
2024-11-05 02:58:28,247 - INFO - [diffusion][Epoch 8282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:28,249 - INFO - [diffusion][Epoch 8283] Epoch 8284/12000
2024-11-05 02:58:32,254 - INFO - [diffusion][Epoch 8283] diffusion training Loss: 0.05494143720716238
2024-11-05 02:58:32,256 - INFO - [diffusion][Epoch 8283] diffusion learning rate: 0.001
2024-11-05 02:58:32,257 - INFO - [diffusion][Epoch 8283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:32,259 - INFO - [diffusion][Epoch 8284] Epoch 8285/12000
2024-11-05 02:58:36,373 - INFO - [diffusion][Epoch 8284] diffusion training Loss: 0.052900880575180054
2024-11-05 02:58:36,375 - INFO - [diffusion][Epoch 8284] diffusion learning rate: 0.001
2024-11-05 02:58:36,377 - INFO - [diffusion][Epoch 8284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:36,378 - INFO - [diffusion][Epoch 8285] Epoch 8286/12000
2024-11-05 02:58:40,545 - INFO - [diffusion][Epoch 8285] diffusion training Loss: 0.056241647340357304
2024-11-05 02:58:40,548 - INFO - [diffusion][Epoch 8285] diffusion learning rate: 0.001
2024-11-05 02:58:40,549 - INFO - [diffusion][Epoch 8285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:40,551 - INFO - [diffusion][Epoch 8286] Epoch 8287/12000
2024-11-05 02:58:44,638 - INFO - [diffusion][Epoch 8286] diffusion training Loss: 0.054154254496097565
2024-11-05 02:58:44,640 - INFO - [diffusion][Epoch 8286] diffusion learning rate: 0.001
2024-11-05 02:58:44,642 - INFO - [diffusion][Epoch 8286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:44,643 - INFO - [diffusion][Epoch 8287] Epoch 8288/12000
2024-11-05 02:58:48,779 - INFO - [diffusion][Epoch 8287] diffusion training Loss: 0.04977664817124605
2024-11-05 02:58:48,781 - INFO - [diffusion][Epoch 8287] diffusion learning rate: 0.001
2024-11-05 02:58:48,782 - INFO - [diffusion][Epoch 8287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:48,784 - INFO - [diffusion][Epoch 8288] Epoch 8289/12000
2024-11-05 02:58:52,813 - INFO - [diffusion][Epoch 8288] diffusion training Loss: 0.056437176652252674
2024-11-05 02:58:52,816 - INFO - [diffusion][Epoch 8288] diffusion learning rate: 0.001
2024-11-05 02:58:52,819 - INFO - [diffusion][Epoch 8288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:52,820 - INFO - [diffusion][Epoch 8289] Epoch 8290/12000
2024-11-05 02:58:56,919 - INFO - [diffusion][Epoch 8289] diffusion training Loss: 0.057242450304329395
2024-11-05 02:58:56,921 - INFO - [diffusion][Epoch 8289] diffusion learning rate: 0.001
2024-11-05 02:58:56,923 - INFO - [diffusion][Epoch 8289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:58:56,924 - INFO - [diffusion][Epoch 8290] Epoch 8291/12000
2024-11-05 02:59:01,263 - INFO - [diffusion][Epoch 8290] diffusion training Loss: 0.05237562023103237
2024-11-05 02:59:01,265 - INFO - [diffusion][Epoch 8290] diffusion learning rate: 0.001
2024-11-05 02:59:01,267 - INFO - [diffusion][Epoch 8290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:01,269 - INFO - [diffusion][Epoch 8291] Epoch 8292/12000
2024-11-05 02:59:05,315 - INFO - [diffusion][Epoch 8291] diffusion training Loss: 0.05400657746940851
2024-11-05 02:59:05,317 - INFO - [diffusion][Epoch 8291] diffusion learning rate: 0.001
2024-11-05 02:59:05,320 - INFO - [diffusion][Epoch 8291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:05,322 - INFO - [diffusion][Epoch 8292] Epoch 8293/12000
2024-11-05 02:59:09,383 - INFO - [diffusion][Epoch 8292] diffusion training Loss: 0.054839491844177246
2024-11-05 02:59:09,386 - INFO - [diffusion][Epoch 8292] diffusion learning rate: 0.001
2024-11-05 02:59:09,388 - INFO - [diffusion][Epoch 8292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:09,389 - INFO - [diffusion][Epoch 8293] Epoch 8294/12000
2024-11-05 02:59:13,419 - INFO - [diffusion][Epoch 8293] diffusion training Loss: 0.061436602845788
2024-11-05 02:59:13,467 - INFO - [diffusion][Epoch 8293] diffusion learning rate: 0.001
2024-11-05 02:59:13,469 - INFO - [diffusion][Epoch 8293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:13,470 - INFO - [diffusion][Epoch 8294] Epoch 8295/12000
2024-11-05 02:59:17,572 - INFO - [diffusion][Epoch 8294] diffusion training Loss: 0.05719929654151201
2024-11-05 02:59:17,574 - INFO - [diffusion][Epoch 8294] diffusion learning rate: 0.001
2024-11-05 02:59:17,576 - INFO - [diffusion][Epoch 8294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:17,577 - INFO - [diffusion][Epoch 8295] Epoch 8296/12000
2024-11-05 02:59:21,680 - INFO - [diffusion][Epoch 8295] diffusion training Loss: 0.05624391231685877
2024-11-05 02:59:21,683 - INFO - [diffusion][Epoch 8295] diffusion learning rate: 0.001
2024-11-05 02:59:21,685 - INFO - [diffusion][Epoch 8295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:21,686 - INFO - [diffusion][Epoch 8296] Epoch 8297/12000
2024-11-05 02:59:25,648 - INFO - [diffusion][Epoch 8296] diffusion training Loss: 0.04972674511373043
2024-11-05 02:59:25,650 - INFO - [diffusion][Epoch 8296] diffusion learning rate: 0.001
2024-11-05 02:59:25,652 - INFO - [diffusion][Epoch 8296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:25,653 - INFO - [diffusion][Epoch 8297] Epoch 8298/12000
2024-11-05 02:59:29,777 - INFO - [diffusion][Epoch 8297] diffusion training Loss: 0.054564861580729485
2024-11-05 02:59:29,779 - INFO - [diffusion][Epoch 8297] diffusion learning rate: 0.001
2024-11-05 02:59:29,849 - INFO - [diffusion][Epoch 8297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:29,851 - INFO - [diffusion][Epoch 8298] Epoch 8299/12000
2024-11-05 02:59:34,022 - INFO - [diffusion][Epoch 8298] diffusion training Loss: 0.05471090879291296
2024-11-05 02:59:34,023 - INFO - [diffusion][Epoch 8298] diffusion learning rate: 0.001
2024-11-05 02:59:34,025 - INFO - [diffusion][Epoch 8298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:34,027 - INFO - [diffusion][Epoch 8299] Epoch 8300/12000
2024-11-05 02:59:38,145 - INFO - [diffusion][Epoch 8299] diffusion training Loss: 0.051393795758485794
2024-11-05 02:59:38,147 - INFO - [diffusion][Epoch 8299] diffusion learning rate: 0.001
2024-11-05 02:59:38,148 - INFO - [diffusion][Epoch 8299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:38,150 - INFO - [diffusion][Epoch 8300] Epoch 8301/12000
2024-11-05 02:59:42,263 - INFO - [diffusion][Epoch 8300] diffusion training Loss: 0.05611410178244114
2024-11-05 02:59:42,265 - INFO - [diffusion][Epoch 8300] diffusion learning rate: 0.001
2024-11-05 02:59:42,266 - INFO - [diffusion][Epoch 8300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:42,268 - INFO - [diffusion][Epoch 8301] Epoch 8302/12000
2024-11-05 02:59:46,257 - INFO - [diffusion][Epoch 8301] diffusion training Loss: 0.05435861926525831
2024-11-05 02:59:46,261 - INFO - [diffusion][Epoch 8301] diffusion learning rate: 0.001
2024-11-05 02:59:46,263 - INFO - [diffusion][Epoch 8301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:46,266 - INFO - [diffusion][Epoch 8302] Epoch 8303/12000
2024-11-05 02:59:50,278 - INFO - [diffusion][Epoch 8302] diffusion training Loss: 0.0546204149723053
2024-11-05 02:59:50,280 - INFO - [diffusion][Epoch 8302] diffusion learning rate: 0.001
2024-11-05 02:59:50,282 - INFO - [diffusion][Epoch 8302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:50,284 - INFO - [diffusion][Epoch 8303] Epoch 8304/12000
2024-11-05 02:59:54,372 - INFO - [diffusion][Epoch 8303] diffusion training Loss: 0.05160633195191622
2024-11-05 02:59:54,374 - INFO - [diffusion][Epoch 8303] diffusion learning rate: 0.001
2024-11-05 02:59:54,376 - INFO - [diffusion][Epoch 8303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:54,377 - INFO - [diffusion][Epoch 8304] Epoch 8305/12000
2024-11-05 02:59:58,533 - INFO - [diffusion][Epoch 8304] diffusion training Loss: 0.05070447828620672
2024-11-05 02:59:58,535 - INFO - [diffusion][Epoch 8304] diffusion learning rate: 0.001
2024-11-05 02:59:58,537 - INFO - [diffusion][Epoch 8304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 02:59:58,538 - INFO - [diffusion][Epoch 8305] Epoch 8306/12000
2024-11-05 03:00:02,552 - INFO - [diffusion][Epoch 8305] diffusion training Loss: 0.05126972962170839
2024-11-05 03:00:02,554 - INFO - [diffusion][Epoch 8305] diffusion learning rate: 0.001
2024-11-05 03:00:02,556 - INFO - [diffusion][Epoch 8305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:02,557 - INFO - [diffusion][Epoch 8306] Epoch 8307/12000
2024-11-05 03:00:06,487 - INFO - [diffusion][Epoch 8306] diffusion training Loss: 0.05332759767770767
2024-11-05 03:00:06,489 - INFO - [diffusion][Epoch 8306] diffusion learning rate: 0.001
2024-11-05 03:00:06,492 - INFO - [diffusion][Epoch 8306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:06,493 - INFO - [diffusion][Epoch 8307] Epoch 8308/12000
2024-11-05 03:00:10,646 - INFO - [diffusion][Epoch 8307] diffusion training Loss: 0.04693201929330826
2024-11-05 03:00:10,648 - INFO - [diffusion][Epoch 8307] diffusion learning rate: 0.001
2024-11-05 03:00:10,650 - INFO - [diffusion][Epoch 8307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:10,651 - INFO - [diffusion][Epoch 8308] Epoch 8309/12000
2024-11-05 03:00:14,668 - INFO - [diffusion][Epoch 8308] diffusion training Loss: 0.059698356315493584
2024-11-05 03:00:14,670 - INFO - [diffusion][Epoch 8308] diffusion learning rate: 0.001
2024-11-05 03:00:14,712 - INFO - [diffusion][Epoch 8308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:14,713 - INFO - [diffusion][Epoch 8309] Epoch 8310/12000
2024-11-05 03:00:18,933 - INFO - [diffusion][Epoch 8309] diffusion training Loss: 0.05339450854808092
2024-11-05 03:00:18,935 - INFO - [diffusion][Epoch 8309] diffusion learning rate: 0.001
2024-11-05 03:00:18,937 - INFO - [diffusion][Epoch 8309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:18,938 - INFO - [diffusion][Epoch 8310] Epoch 8311/12000
2024-11-05 03:00:23,049 - INFO - [diffusion][Epoch 8310] diffusion training Loss: 0.05188289750367403
2024-11-05 03:00:23,052 - INFO - [diffusion][Epoch 8310] diffusion learning rate: 0.001
2024-11-05 03:00:23,053 - INFO - [diffusion][Epoch 8310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:23,055 - INFO - [diffusion][Epoch 8311] Epoch 8312/12000
2024-11-05 03:00:27,186 - INFO - [diffusion][Epoch 8311] diffusion training Loss: 0.05271438043564558
2024-11-05 03:00:27,188 - INFO - [diffusion][Epoch 8311] diffusion learning rate: 0.001
2024-11-05 03:00:27,190 - INFO - [diffusion][Epoch 8311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:27,192 - INFO - [diffusion][Epoch 8312] Epoch 8313/12000
2024-11-05 03:00:31,319 - INFO - [diffusion][Epoch 8312] diffusion training Loss: 0.052397916093468666
2024-11-05 03:00:31,321 - INFO - [diffusion][Epoch 8312] diffusion learning rate: 0.001
2024-11-05 03:00:31,323 - INFO - [diffusion][Epoch 8312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:31,325 - INFO - [diffusion][Epoch 8313] Epoch 8314/12000
2024-11-05 03:00:35,425 - INFO - [diffusion][Epoch 8313] diffusion training Loss: 0.05949005391448736
2024-11-05 03:00:35,427 - INFO - [diffusion][Epoch 8313] diffusion learning rate: 0.001
2024-11-05 03:00:35,429 - INFO - [diffusion][Epoch 8313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:35,430 - INFO - [diffusion][Epoch 8314] Epoch 8315/12000
2024-11-05 03:00:39,575 - INFO - [diffusion][Epoch 8314] diffusion training Loss: 0.05052830372005701
2024-11-05 03:00:39,577 - INFO - [diffusion][Epoch 8314] diffusion learning rate: 0.001
2024-11-05 03:00:39,579 - INFO - [diffusion][Epoch 8314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:39,580 - INFO - [diffusion][Epoch 8315] Epoch 8316/12000
2024-11-05 03:00:43,552 - INFO - [diffusion][Epoch 8315] diffusion training Loss: 0.050998381339013577
2024-11-05 03:00:43,554 - INFO - [diffusion][Epoch 8315] diffusion learning rate: 0.001
2024-11-05 03:00:43,557 - INFO - [diffusion][Epoch 8315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:43,558 - INFO - [diffusion][Epoch 8316] Epoch 8317/12000
2024-11-05 03:00:47,556 - INFO - [diffusion][Epoch 8316] diffusion training Loss: 0.048364182934165
2024-11-05 03:00:47,580 - INFO - [diffusion][Epoch 8316] diffusion learning rate: 0.001
2024-11-05 03:00:47,582 - INFO - [diffusion][Epoch 8316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:47,583 - INFO - [diffusion][Epoch 8317] Epoch 8318/12000
2024-11-05 03:00:51,613 - INFO - [diffusion][Epoch 8317] diffusion training Loss: 0.05474558565765619
2024-11-05 03:00:51,615 - INFO - [diffusion][Epoch 8317] diffusion learning rate: 0.001
2024-11-05 03:00:51,617 - INFO - [diffusion][Epoch 8317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:51,618 - INFO - [diffusion][Epoch 8318] Epoch 8319/12000
2024-11-05 03:00:55,666 - INFO - [diffusion][Epoch 8318] diffusion training Loss: 0.05211844481527805
2024-11-05 03:00:55,668 - INFO - [diffusion][Epoch 8318] diffusion learning rate: 0.001
2024-11-05 03:00:55,670 - INFO - [diffusion][Epoch 8318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:55,671 - INFO - [diffusion][Epoch 8319] Epoch 8320/12000
2024-11-05 03:00:59,699 - INFO - [diffusion][Epoch 8319] diffusion training Loss: 0.05851942766457796
2024-11-05 03:00:59,701 - INFO - [diffusion][Epoch 8319] diffusion learning rate: 0.001
2024-11-05 03:00:59,703 - INFO - [diffusion][Epoch 8319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:00:59,704 - INFO - [diffusion][Epoch 8320] Epoch 8321/12000
2024-11-05 03:01:03,860 - INFO - [diffusion][Epoch 8320] diffusion training Loss: 0.058581006713211536
2024-11-05 03:01:03,862 - INFO - [diffusion][Epoch 8320] diffusion learning rate: 0.001
2024-11-05 03:01:03,864 - INFO - [diffusion][Epoch 8320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:03,865 - INFO - [diffusion][Epoch 8321] Epoch 8322/12000
2024-11-05 03:01:07,946 - INFO - [diffusion][Epoch 8321] diffusion training Loss: 0.05455922428518534
2024-11-05 03:01:07,949 - INFO - [diffusion][Epoch 8321] diffusion learning rate: 0.001
2024-11-05 03:01:07,951 - INFO - [diffusion][Epoch 8321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:07,952 - INFO - [diffusion][Epoch 8322] Epoch 8323/12000
2024-11-05 03:01:12,031 - INFO - [diffusion][Epoch 8322] diffusion training Loss: 0.04872214142233133
2024-11-05 03:01:12,034 - INFO - [diffusion][Epoch 8322] diffusion learning rate: 0.001
2024-11-05 03:01:12,035 - INFO - [diffusion][Epoch 8322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:12,036 - INFO - [diffusion][Epoch 8323] Epoch 8324/12000
2024-11-05 03:01:16,183 - INFO - [diffusion][Epoch 8323] diffusion training Loss: 0.049672843888401985
2024-11-05 03:01:16,185 - INFO - [diffusion][Epoch 8323] diffusion learning rate: 0.001
2024-11-05 03:01:16,187 - INFO - [diffusion][Epoch 8323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:16,188 - INFO - [diffusion][Epoch 8324] Epoch 8325/12000
2024-11-05 03:01:20,354 - INFO - [diffusion][Epoch 8324] diffusion training Loss: 0.05848534032702446
2024-11-05 03:01:20,356 - INFO - [diffusion][Epoch 8324] diffusion learning rate: 0.001
2024-11-05 03:01:20,358 - INFO - [diffusion][Epoch 8324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:20,359 - INFO - [diffusion][Epoch 8325] Epoch 8326/12000
2024-11-05 03:01:24,470 - INFO - [diffusion][Epoch 8325] diffusion training Loss: 0.04762442968785763
2024-11-05 03:01:24,472 - INFO - [diffusion][Epoch 8325] diffusion learning rate: 0.001
2024-11-05 03:01:24,474 - INFO - [diffusion][Epoch 8325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:24,475 - INFO - [diffusion][Epoch 8326] Epoch 8327/12000
2024-11-05 03:01:28,647 - INFO - [diffusion][Epoch 8326] diffusion training Loss: 0.046699135564267635
2024-11-05 03:01:28,650 - INFO - [diffusion][Epoch 8326] diffusion learning rate: 0.001
2024-11-05 03:01:28,652 - INFO - [diffusion][Epoch 8326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:28,653 - INFO - [diffusion][Epoch 8327] Epoch 8328/12000
2024-11-05 03:01:32,791 - INFO - [diffusion][Epoch 8327] diffusion training Loss: 0.0525799123570323
2024-11-05 03:01:32,793 - INFO - [diffusion][Epoch 8327] diffusion learning rate: 0.001
2024-11-05 03:01:32,795 - INFO - [diffusion][Epoch 8327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:32,796 - INFO - [diffusion][Epoch 8328] Epoch 8329/12000
2024-11-05 03:01:36,728 - INFO - [diffusion][Epoch 8328] diffusion training Loss: 0.05204673483967781
2024-11-05 03:01:36,730 - INFO - [diffusion][Epoch 8328] diffusion learning rate: 0.001
2024-11-05 03:01:36,732 - INFO - [diffusion][Epoch 8328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:36,733 - INFO - [diffusion][Epoch 8329] Epoch 8330/12000
2024-11-05 03:01:40,747 - INFO - [diffusion][Epoch 8329] diffusion training Loss: 0.048154737800359726
2024-11-05 03:01:40,749 - INFO - [diffusion][Epoch 8329] diffusion learning rate: 0.001
2024-11-05 03:01:40,750 - INFO - [diffusion][Epoch 8329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:40,752 - INFO - [diffusion][Epoch 8330] Epoch 8331/12000
2024-11-05 03:01:44,789 - INFO - [diffusion][Epoch 8330] diffusion training Loss: 0.05077100731432438
2024-11-05 03:01:44,791 - INFO - [diffusion][Epoch 8330] diffusion learning rate: 0.001
2024-11-05 03:01:44,793 - INFO - [diffusion][Epoch 8330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:44,794 - INFO - [diffusion][Epoch 8331] Epoch 8332/12000
2024-11-05 03:01:49,291 - INFO - [diffusion][Epoch 8331] diffusion training Loss: 0.05618403945118189
2024-11-05 03:01:49,293 - INFO - [diffusion][Epoch 8331] diffusion learning rate: 0.001
2024-11-05 03:01:49,294 - INFO - [diffusion][Epoch 8331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:49,296 - INFO - [diffusion][Epoch 8332] Epoch 8333/12000
2024-11-05 03:01:53,427 - INFO - [diffusion][Epoch 8332] diffusion training Loss: 0.052825658582150936
2024-11-05 03:01:53,429 - INFO - [diffusion][Epoch 8332] diffusion learning rate: 0.001
2024-11-05 03:01:53,431 - INFO - [diffusion][Epoch 8332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:53,432 - INFO - [diffusion][Epoch 8333] Epoch 8334/12000
2024-11-05 03:01:57,443 - INFO - [diffusion][Epoch 8333] diffusion training Loss: 0.05308110360056162
2024-11-05 03:01:57,445 - INFO - [diffusion][Epoch 8333] diffusion learning rate: 0.001
2024-11-05 03:01:57,447 - INFO - [diffusion][Epoch 8333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:01:57,448 - INFO - [diffusion][Epoch 8334] Epoch 8335/12000
2024-11-05 03:02:01,583 - INFO - [diffusion][Epoch 8334] diffusion training Loss: 0.05554515775293112
2024-11-05 03:02:01,585 - INFO - [diffusion][Epoch 8334] diffusion learning rate: 0.001
2024-11-05 03:02:01,612 - INFO - [diffusion][Epoch 8334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:01,613 - INFO - [diffusion][Epoch 8335] Epoch 8336/12000
2024-11-05 03:02:05,765 - INFO - [diffusion][Epoch 8335] diffusion training Loss: 0.05057691037654877
2024-11-05 03:02:05,767 - INFO - [diffusion][Epoch 8335] diffusion learning rate: 0.001
2024-11-05 03:02:05,768 - INFO - [diffusion][Epoch 8335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:05,770 - INFO - [diffusion][Epoch 8336] Epoch 8337/12000
2024-11-05 03:02:09,858 - INFO - [diffusion][Epoch 8336] diffusion training Loss: 0.06189805921167135
2024-11-05 03:02:09,860 - INFO - [diffusion][Epoch 8336] diffusion learning rate: 0.001
2024-11-05 03:02:09,862 - INFO - [diffusion][Epoch 8336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:09,863 - INFO - [diffusion][Epoch 8337] Epoch 8338/12000
2024-11-05 03:02:13,817 - INFO - [diffusion][Epoch 8337] diffusion training Loss: 0.051055281423032284
2024-11-05 03:02:13,820 - INFO - [diffusion][Epoch 8337] diffusion learning rate: 0.001
2024-11-05 03:02:13,822 - INFO - [diffusion][Epoch 8337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:13,823 - INFO - [diffusion][Epoch 8338] Epoch 8339/12000
2024-11-05 03:02:17,898 - INFO - [diffusion][Epoch 8338] diffusion training Loss: 0.05306913610547781
2024-11-05 03:02:17,901 - INFO - [diffusion][Epoch 8338] diffusion learning rate: 0.001
2024-11-05 03:02:17,903 - INFO - [diffusion][Epoch 8338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:17,904 - INFO - [diffusion][Epoch 8339] Epoch 8340/12000
2024-11-05 03:02:22,036 - INFO - [diffusion][Epoch 8339] diffusion training Loss: 0.04506952781230211
2024-11-05 03:02:22,037 - INFO - [diffusion][Epoch 8339] diffusion learning rate: 0.001
2024-11-05 03:02:22,039 - INFO - [diffusion][Epoch 8339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:22,041 - INFO - [diffusion][Epoch 8340] Epoch 8341/12000
2024-11-05 03:02:26,143 - INFO - [diffusion][Epoch 8340] diffusion training Loss: 0.04957094229757786
2024-11-05 03:02:26,145 - INFO - [diffusion][Epoch 8340] diffusion learning rate: 0.001
2024-11-05 03:02:26,146 - INFO - [diffusion][Epoch 8340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:26,148 - INFO - [diffusion][Epoch 8341] Epoch 8342/12000
2024-11-05 03:02:30,213 - INFO - [diffusion][Epoch 8341] diffusion training Loss: 0.0556200360879302
2024-11-05 03:02:30,215 - INFO - [diffusion][Epoch 8341] diffusion learning rate: 0.001
2024-11-05 03:02:30,216 - INFO - [diffusion][Epoch 8341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:30,218 - INFO - [diffusion][Epoch 8342] Epoch 8343/12000
2024-11-05 03:02:34,298 - INFO - [diffusion][Epoch 8342] diffusion training Loss: 0.05276975408196449
2024-11-05 03:02:34,300 - INFO - [diffusion][Epoch 8342] diffusion learning rate: 0.001
2024-11-05 03:02:34,302 - INFO - [diffusion][Epoch 8342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:34,303 - INFO - [diffusion][Epoch 8343] Epoch 8344/12000
2024-11-05 03:02:38,372 - INFO - [diffusion][Epoch 8343] diffusion training Loss: 0.051035353913903236
2024-11-05 03:02:38,374 - INFO - [diffusion][Epoch 8343] diffusion learning rate: 0.001
2024-11-05 03:02:38,376 - INFO - [diffusion][Epoch 8343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:38,377 - INFO - [diffusion][Epoch 8344] Epoch 8345/12000
2024-11-05 03:02:42,466 - INFO - [diffusion][Epoch 8344] diffusion training Loss: 0.0514286644756794
2024-11-05 03:02:42,468 - INFO - [diffusion][Epoch 8344] diffusion learning rate: 0.001
2024-11-05 03:02:42,470 - INFO - [diffusion][Epoch 8344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:42,471 - INFO - [diffusion][Epoch 8345] Epoch 8346/12000
2024-11-05 03:02:46,623 - INFO - [diffusion][Epoch 8345] diffusion training Loss: 0.05038944445550442
2024-11-05 03:02:46,625 - INFO - [diffusion][Epoch 8345] diffusion learning rate: 0.001
2024-11-05 03:02:46,627 - INFO - [diffusion][Epoch 8345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:46,628 - INFO - [diffusion][Epoch 8346] Epoch 8347/12000
2024-11-05 03:02:50,709 - INFO - [diffusion][Epoch 8346] diffusion training Loss: 0.05317966919392347
2024-11-05 03:02:50,711 - INFO - [diffusion][Epoch 8346] diffusion learning rate: 0.001
2024-11-05 03:02:50,713 - INFO - [diffusion][Epoch 8346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:50,714 - INFO - [diffusion][Epoch 8347] Epoch 8348/12000
2024-11-05 03:02:54,831 - INFO - [diffusion][Epoch 8347] diffusion training Loss: 0.04823235888034105
2024-11-05 03:02:54,833 - INFO - [diffusion][Epoch 8347] diffusion learning rate: 0.001
2024-11-05 03:02:54,835 - INFO - [diffusion][Epoch 8347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:54,836 - INFO - [diffusion][Epoch 8348] Epoch 8349/12000
2024-11-05 03:02:58,975 - INFO - [diffusion][Epoch 8348] diffusion training Loss: 0.052468239329755306
2024-11-05 03:02:58,978 - INFO - [diffusion][Epoch 8348] diffusion learning rate: 0.001
2024-11-05 03:02:58,980 - INFO - [diffusion][Epoch 8348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:02:58,982 - INFO - [diffusion][Epoch 8349] Epoch 8350/12000
2024-11-05 03:03:03,029 - INFO - [diffusion][Epoch 8349] diffusion training Loss: 0.058486671186983585
2024-11-05 03:03:03,031 - INFO - [diffusion][Epoch 8349] diffusion learning rate: 0.001
2024-11-05 03:03:03,033 - INFO - [diffusion][Epoch 8349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:03,035 - INFO - [diffusion][Epoch 8350] Epoch 8351/12000
2024-11-05 03:03:07,188 - INFO - [diffusion][Epoch 8350] diffusion training Loss: 0.055376866832375526
2024-11-05 03:03:07,191 - INFO - [diffusion][Epoch 8350] diffusion learning rate: 0.001
2024-11-05 03:03:07,192 - INFO - [diffusion][Epoch 8350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:07,194 - INFO - [diffusion][Epoch 8351] Epoch 8352/12000
2024-11-05 03:03:11,300 - INFO - [diffusion][Epoch 8351] diffusion training Loss: 0.0591520806774497
2024-11-05 03:03:11,302 - INFO - [diffusion][Epoch 8351] diffusion learning rate: 0.001
2024-11-05 03:03:11,304 - INFO - [diffusion][Epoch 8351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:11,305 - INFO - [diffusion][Epoch 8352] Epoch 8353/12000
2024-11-05 03:03:16,044 - INFO - [diffusion][Epoch 8352] diffusion training Loss: 0.05559206660836935
2024-11-05 03:03:16,046 - INFO - [diffusion][Epoch 8352] diffusion learning rate: 0.001
2024-11-05 03:03:16,048 - INFO - [diffusion][Epoch 8352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:16,049 - INFO - [diffusion][Epoch 8353] Epoch 8354/12000
2024-11-05 03:03:20,019 - INFO - [diffusion][Epoch 8353] diffusion training Loss: 0.05436766613274813
2024-11-05 03:03:20,021 - INFO - [diffusion][Epoch 8353] diffusion learning rate: 0.001
2024-11-05 03:03:20,024 - INFO - [diffusion][Epoch 8353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:20,025 - INFO - [diffusion][Epoch 8354] Epoch 8355/12000
2024-11-05 03:03:24,039 - INFO - [diffusion][Epoch 8354] diffusion training Loss: 0.04895817115902901
2024-11-05 03:03:24,041 - INFO - [diffusion][Epoch 8354] diffusion learning rate: 0.001
2024-11-05 03:03:24,043 - INFO - [diffusion][Epoch 8354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:24,044 - INFO - [diffusion][Epoch 8355] Epoch 8356/12000
2024-11-05 03:03:28,142 - INFO - [diffusion][Epoch 8355] diffusion training Loss: 0.05151798948645592
2024-11-05 03:03:28,144 - INFO - [diffusion][Epoch 8355] diffusion learning rate: 0.001
2024-11-05 03:03:28,150 - INFO - [diffusion][Epoch 8355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:28,152 - INFO - [diffusion][Epoch 8356] Epoch 8357/12000
2024-11-05 03:03:32,296 - INFO - [diffusion][Epoch 8356] diffusion training Loss: 0.050178127363324165
2024-11-05 03:03:32,297 - INFO - [diffusion][Epoch 8356] diffusion learning rate: 0.001
2024-11-05 03:03:32,299 - INFO - [diffusion][Epoch 8356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:32,300 - INFO - [diffusion][Epoch 8357] Epoch 8358/12000
2024-11-05 03:03:36,436 - INFO - [diffusion][Epoch 8357] diffusion training Loss: 0.049940756522119045
2024-11-05 03:03:36,439 - INFO - [diffusion][Epoch 8357] diffusion learning rate: 0.001
2024-11-05 03:03:36,440 - INFO - [diffusion][Epoch 8357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:36,442 - INFO - [diffusion][Epoch 8358] Epoch 8359/12000
2024-11-05 03:03:40,572 - INFO - [diffusion][Epoch 8358] diffusion training Loss: 0.04891714733093977
2024-11-05 03:03:40,574 - INFO - [diffusion][Epoch 8358] diffusion learning rate: 0.001
2024-11-05 03:03:40,577 - INFO - [diffusion][Epoch 8358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:40,578 - INFO - [diffusion][Epoch 8359] Epoch 8360/12000
2024-11-05 03:03:44,660 - INFO - [diffusion][Epoch 8359] diffusion training Loss: 0.052099487744271755
2024-11-05 03:03:44,663 - INFO - [diffusion][Epoch 8359] diffusion learning rate: 0.001
2024-11-05 03:03:44,665 - INFO - [diffusion][Epoch 8359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:44,666 - INFO - [diffusion][Epoch 8360] Epoch 8361/12000
2024-11-05 03:03:48,842 - INFO - [diffusion][Epoch 8360] diffusion training Loss: 0.051842059940099716
2024-11-05 03:03:48,843 - INFO - [diffusion][Epoch 8360] diffusion learning rate: 0.001
2024-11-05 03:03:48,845 - INFO - [diffusion][Epoch 8360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:48,847 - INFO - [diffusion][Epoch 8361] Epoch 8362/12000
2024-11-05 03:03:53,017 - INFO - [diffusion][Epoch 8361] diffusion training Loss: 0.044180599972605705
2024-11-05 03:03:53,019 - INFO - [diffusion][Epoch 8361] diffusion learning rate: 0.001
2024-11-05 03:03:53,020 - INFO - [diffusion][Epoch 8361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:53,022 - INFO - [diffusion][Epoch 8362] Epoch 8363/12000
2024-11-05 03:03:57,195 - INFO - [diffusion][Epoch 8362] diffusion training Loss: 0.05498029198497534
2024-11-05 03:03:57,197 - INFO - [diffusion][Epoch 8362] diffusion learning rate: 0.001
2024-11-05 03:03:57,199 - INFO - [diffusion][Epoch 8362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:03:57,200 - INFO - [diffusion][Epoch 8363] Epoch 8364/12000
2024-11-05 03:04:01,310 - INFO - [diffusion][Epoch 8363] diffusion training Loss: 0.0504764448851347
2024-11-05 03:04:01,312 - INFO - [diffusion][Epoch 8363] diffusion learning rate: 0.001
2024-11-05 03:04:01,314 - INFO - [diffusion][Epoch 8363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:01,316 - INFO - [diffusion][Epoch 8364] Epoch 8365/12000
2024-11-05 03:04:05,248 - INFO - [diffusion][Epoch 8364] diffusion training Loss: 0.054633861407637596
2024-11-05 03:04:05,250 - INFO - [diffusion][Epoch 8364] diffusion learning rate: 0.001
2024-11-05 03:04:05,252 - INFO - [diffusion][Epoch 8364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:05,254 - INFO - [diffusion][Epoch 8365] Epoch 8366/12000
2024-11-05 03:04:09,233 - INFO - [diffusion][Epoch 8365] diffusion training Loss: 0.05270167160779238
2024-11-05 03:04:09,235 - INFO - [diffusion][Epoch 8365] diffusion learning rate: 0.001
2024-11-05 03:04:09,237 - INFO - [diffusion][Epoch 8365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:09,238 - INFO - [diffusion][Epoch 8366] Epoch 8367/12000
2024-11-05 03:04:13,277 - INFO - [diffusion][Epoch 8366] diffusion training Loss: 0.051771980710327625
2024-11-05 03:04:13,279 - INFO - [diffusion][Epoch 8366] diffusion learning rate: 0.001
2024-11-05 03:04:13,280 - INFO - [diffusion][Epoch 8366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:13,281 - INFO - [diffusion][Epoch 8367] Epoch 8368/12000
2024-11-05 03:04:17,325 - INFO - [diffusion][Epoch 8367] diffusion training Loss: 0.05470369104295969
2024-11-05 03:04:17,328 - INFO - [diffusion][Epoch 8367] diffusion learning rate: 0.001
2024-11-05 03:04:17,330 - INFO - [diffusion][Epoch 8367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:17,331 - INFO - [diffusion][Epoch 8368] Epoch 8369/12000
2024-11-05 03:04:21,361 - INFO - [diffusion][Epoch 8368] diffusion training Loss: 0.059508624486625195
2024-11-05 03:04:21,363 - INFO - [diffusion][Epoch 8368] diffusion learning rate: 0.001
2024-11-05 03:04:21,365 - INFO - [diffusion][Epoch 8368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:21,367 - INFO - [diffusion][Epoch 8369] Epoch 8370/12000
2024-11-05 03:04:25,469 - INFO - [diffusion][Epoch 8369] diffusion training Loss: 0.06006504688411951
2024-11-05 03:04:25,471 - INFO - [diffusion][Epoch 8369] diffusion learning rate: 0.001
2024-11-05 03:04:25,500 - INFO - [diffusion][Epoch 8369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:25,501 - INFO - [diffusion][Epoch 8370] Epoch 8371/12000
2024-11-05 03:04:29,629 - INFO - [diffusion][Epoch 8370] diffusion training Loss: 0.05797053314745426
2024-11-05 03:04:29,631 - INFO - [diffusion][Epoch 8370] diffusion learning rate: 0.001
2024-11-05 03:04:29,633 - INFO - [diffusion][Epoch 8370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:29,634 - INFO - [diffusion][Epoch 8371] Epoch 8372/12000
2024-11-05 03:04:34,056 - INFO - [diffusion][Epoch 8371] diffusion training Loss: 0.05908110458403826
2024-11-05 03:04:34,058 - INFO - [diffusion][Epoch 8371] diffusion learning rate: 0.001
2024-11-05 03:04:34,060 - INFO - [diffusion][Epoch 8371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:34,061 - INFO - [diffusion][Epoch 8372] Epoch 8373/12000
2024-11-05 03:04:38,074 - INFO - [diffusion][Epoch 8372] diffusion training Loss: 0.05538521520793438
2024-11-05 03:04:38,077 - INFO - [diffusion][Epoch 8372] diffusion learning rate: 0.001
2024-11-05 03:04:38,079 - INFO - [diffusion][Epoch 8372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:38,080 - INFO - [diffusion][Epoch 8373] Epoch 8374/12000
2024-11-05 03:04:42,147 - INFO - [diffusion][Epoch 8373] diffusion training Loss: 0.0576210580766201
2024-11-05 03:04:42,149 - INFO - [diffusion][Epoch 8373] diffusion learning rate: 0.001
2024-11-05 03:04:42,150 - INFO - [diffusion][Epoch 8373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:42,152 - INFO - [diffusion][Epoch 8374] Epoch 8375/12000
2024-11-05 03:04:46,214 - INFO - [diffusion][Epoch 8374] diffusion training Loss: 0.05558505654335022
2024-11-05 03:04:46,216 - INFO - [diffusion][Epoch 8374] diffusion learning rate: 0.001
2024-11-05 03:04:46,218 - INFO - [diffusion][Epoch 8374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:46,220 - INFO - [diffusion][Epoch 8375] Epoch 8376/12000
2024-11-05 03:04:50,244 - INFO - [diffusion][Epoch 8375] diffusion training Loss: 0.051494804210960865
2024-11-05 03:04:50,247 - INFO - [diffusion][Epoch 8375] diffusion learning rate: 0.001
2024-11-05 03:04:50,248 - INFO - [diffusion][Epoch 8375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:50,250 - INFO - [diffusion][Epoch 8376] Epoch 8377/12000
2024-11-05 03:04:54,186 - INFO - [diffusion][Epoch 8376] diffusion training Loss: 0.05419448111206293
2024-11-05 03:04:54,188 - INFO - [diffusion][Epoch 8376] diffusion learning rate: 0.001
2024-11-05 03:04:54,190 - INFO - [diffusion][Epoch 8376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:54,192 - INFO - [diffusion][Epoch 8377] Epoch 8378/12000
2024-11-05 03:04:58,290 - INFO - [diffusion][Epoch 8377] diffusion training Loss: 0.05649140942841768
2024-11-05 03:04:58,293 - INFO - [diffusion][Epoch 8377] diffusion learning rate: 0.001
2024-11-05 03:04:58,294 - INFO - [diffusion][Epoch 8377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:04:58,296 - INFO - [diffusion][Epoch 8378] Epoch 8379/12000
2024-11-05 03:05:02,338 - INFO - [diffusion][Epoch 8378] diffusion training Loss: 0.06252538878470659
2024-11-05 03:05:02,340 - INFO - [diffusion][Epoch 8378] diffusion learning rate: 0.001
2024-11-05 03:05:02,343 - INFO - [diffusion][Epoch 8378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:02,344 - INFO - [diffusion][Epoch 8379] Epoch 8380/12000
2024-11-05 03:05:06,469 - INFO - [diffusion][Epoch 8379] diffusion training Loss: 0.053175874054431915
2024-11-05 03:05:06,471 - INFO - [diffusion][Epoch 8379] diffusion learning rate: 0.001
2024-11-05 03:05:06,473 - INFO - [diffusion][Epoch 8379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:06,475 - INFO - [diffusion][Epoch 8380] Epoch 8381/12000
2024-11-05 03:05:10,555 - INFO - [diffusion][Epoch 8380] diffusion training Loss: 0.062088592909276485
2024-11-05 03:05:10,557 - INFO - [diffusion][Epoch 8380] diffusion learning rate: 0.001
2024-11-05 03:05:10,559 - INFO - [diffusion][Epoch 8380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:10,560 - INFO - [diffusion][Epoch 8381] Epoch 8382/12000
2024-11-05 03:05:14,557 - INFO - [diffusion][Epoch 8381] diffusion training Loss: 0.055913026444613934
2024-11-05 03:05:14,559 - INFO - [diffusion][Epoch 8381] diffusion learning rate: 0.001
2024-11-05 03:05:14,561 - INFO - [diffusion][Epoch 8381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:14,563 - INFO - [diffusion][Epoch 8382] Epoch 8383/12000
2024-11-05 03:05:18,429 - INFO - [diffusion][Epoch 8382] diffusion training Loss: 0.053193388506770134
2024-11-05 03:05:18,432 - INFO - [diffusion][Epoch 8382] diffusion learning rate: 0.001
2024-11-05 03:05:18,435 - INFO - [diffusion][Epoch 8382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:18,436 - INFO - [diffusion][Epoch 8383] Epoch 8384/12000
2024-11-05 03:05:22,484 - INFO - [diffusion][Epoch 8383] diffusion training Loss: 0.053241717629134655
2024-11-05 03:05:22,487 - INFO - [diffusion][Epoch 8383] diffusion learning rate: 0.001
2024-11-05 03:05:22,489 - INFO - [diffusion][Epoch 8383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:22,491 - INFO - [diffusion][Epoch 8384] Epoch 8385/12000
2024-11-05 03:05:26,527 - INFO - [diffusion][Epoch 8384] diffusion training Loss: 0.05184420011937618
2024-11-05 03:05:26,529 - INFO - [diffusion][Epoch 8384] diffusion learning rate: 0.001
2024-11-05 03:05:26,531 - INFO - [diffusion][Epoch 8384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:26,532 - INFO - [diffusion][Epoch 8385] Epoch 8386/12000
2024-11-05 03:05:30,635 - INFO - [diffusion][Epoch 8385] diffusion training Loss: 0.05349194258451462
2024-11-05 03:05:30,637 - INFO - [diffusion][Epoch 8385] diffusion learning rate: 0.001
2024-11-05 03:05:30,639 - INFO - [diffusion][Epoch 8385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:30,640 - INFO - [diffusion][Epoch 8386] Epoch 8387/12000
2024-11-05 03:05:34,609 - INFO - [diffusion][Epoch 8386] diffusion training Loss: 0.04680114146322012
2024-11-05 03:05:34,611 - INFO - [diffusion][Epoch 8386] diffusion learning rate: 0.001
2024-11-05 03:05:34,613 - INFO - [diffusion][Epoch 8386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:34,614 - INFO - [diffusion][Epoch 8387] Epoch 8388/12000
2024-11-05 03:05:38,691 - INFO - [diffusion][Epoch 8387] diffusion training Loss: 0.04722719453275204
2024-11-05 03:05:38,693 - INFO - [diffusion][Epoch 8387] diffusion learning rate: 0.001
2024-11-05 03:05:38,695 - INFO - [diffusion][Epoch 8387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:38,696 - INFO - [diffusion][Epoch 8388] Epoch 8389/12000
2024-11-05 03:05:42,788 - INFO - [diffusion][Epoch 8388] diffusion training Loss: 0.050275194458663464
2024-11-05 03:05:42,790 - INFO - [diffusion][Epoch 8388] diffusion learning rate: 0.001
2024-11-05 03:05:42,792 - INFO - [diffusion][Epoch 8388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:42,793 - INFO - [diffusion][Epoch 8389] Epoch 8390/12000
2024-11-05 03:05:46,955 - INFO - [diffusion][Epoch 8389] diffusion training Loss: 0.056005897000432014
2024-11-05 03:05:46,958 - INFO - [diffusion][Epoch 8389] diffusion learning rate: 0.001
2024-11-05 03:05:46,959 - INFO - [diffusion][Epoch 8389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:46,961 - INFO - [diffusion][Epoch 8390] Epoch 8391/12000
2024-11-05 03:05:51,094 - INFO - [diffusion][Epoch 8390] diffusion training Loss: 0.05526668019592762
2024-11-05 03:05:51,096 - INFO - [diffusion][Epoch 8390] diffusion learning rate: 0.001
2024-11-05 03:05:51,145 - INFO - [diffusion][Epoch 8390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:51,146 - INFO - [diffusion][Epoch 8391] Epoch 8392/12000
2024-11-05 03:05:55,096 - INFO - [diffusion][Epoch 8391] diffusion training Loss: 0.0504189096391201
2024-11-05 03:05:55,098 - INFO - [diffusion][Epoch 8391] diffusion learning rate: 0.001
2024-11-05 03:05:55,100 - INFO - [diffusion][Epoch 8391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:55,101 - INFO - [diffusion][Epoch 8392] Epoch 8393/12000
2024-11-05 03:05:59,257 - INFO - [diffusion][Epoch 8392] diffusion training Loss: 0.05127881933003664
2024-11-05 03:05:59,259 - INFO - [diffusion][Epoch 8392] diffusion learning rate: 0.001
2024-11-05 03:05:59,261 - INFO - [diffusion][Epoch 8392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:05:59,262 - INFO - [diffusion][Epoch 8393] Epoch 8394/12000
2024-11-05 03:06:03,456 - INFO - [diffusion][Epoch 8393] diffusion training Loss: 0.058723035268485546
2024-11-05 03:06:03,458 - INFO - [diffusion][Epoch 8393] diffusion learning rate: 0.001
2024-11-05 03:06:03,460 - INFO - [diffusion][Epoch 8393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:03,462 - INFO - [diffusion][Epoch 8394] Epoch 8395/12000
2024-11-05 03:06:07,568 - INFO - [diffusion][Epoch 8394] diffusion training Loss: 0.050255605950951576
2024-11-05 03:06:07,570 - INFO - [diffusion][Epoch 8394] diffusion learning rate: 0.001
2024-11-05 03:06:07,598 - INFO - [diffusion][Epoch 8394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:07,599 - INFO - [diffusion][Epoch 8395] Epoch 8396/12000
2024-11-05 03:06:11,649 - INFO - [diffusion][Epoch 8395] diffusion training Loss: 0.052928549237549305
2024-11-05 03:06:11,651 - INFO - [diffusion][Epoch 8395] diffusion learning rate: 0.001
2024-11-05 03:06:11,653 - INFO - [diffusion][Epoch 8395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:11,654 - INFO - [diffusion][Epoch 8396] Epoch 8397/12000
2024-11-05 03:06:15,784 - INFO - [diffusion][Epoch 8396] diffusion training Loss: 0.05407543946057558
2024-11-05 03:06:15,786 - INFO - [diffusion][Epoch 8396] diffusion learning rate: 0.001
2024-11-05 03:06:15,788 - INFO - [diffusion][Epoch 8396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:15,789 - INFO - [diffusion][Epoch 8397] Epoch 8398/12000
2024-11-05 03:06:19,912 - INFO - [diffusion][Epoch 8397] diffusion training Loss: 0.052044580690562725
2024-11-05 03:06:19,914 - INFO - [diffusion][Epoch 8397] diffusion learning rate: 0.001
2024-11-05 03:06:19,916 - INFO - [diffusion][Epoch 8397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:19,918 - INFO - [diffusion][Epoch 8398] Epoch 8399/12000
2024-11-05 03:06:24,074 - INFO - [diffusion][Epoch 8398] diffusion training Loss: 0.050461662001907825
2024-11-05 03:06:24,077 - INFO - [diffusion][Epoch 8398] diffusion learning rate: 0.001
2024-11-05 03:06:24,079 - INFO - [diffusion][Epoch 8398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:24,080 - INFO - [diffusion][Epoch 8399] Epoch 8400/12000
2024-11-05 03:06:28,188 - INFO - [diffusion][Epoch 8399] diffusion training Loss: 0.05827550310641527
2024-11-05 03:06:28,220 - INFO - [diffusion][Epoch 8399] diffusion learning rate: 0.001
2024-11-05 03:06:28,222 - INFO - [diffusion][Epoch 8399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:28,223 - INFO - [diffusion][Epoch 8400] Epoch 8401/12000
2024-11-05 03:06:32,284 - INFO - [diffusion][Epoch 8400] diffusion training Loss: 0.04923824593424797
2024-11-05 03:06:32,286 - INFO - [diffusion][Epoch 8400] diffusion learning rate: 0.001
2024-11-05 03:06:32,288 - INFO - [diffusion][Epoch 8400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:32,290 - INFO - [diffusion][Epoch 8401] Epoch 8402/12000
2024-11-05 03:06:36,430 - INFO - [diffusion][Epoch 8401] diffusion training Loss: 0.05101925693452358
2024-11-05 03:06:36,432 - INFO - [diffusion][Epoch 8401] diffusion learning rate: 0.001
2024-11-05 03:06:36,434 - INFO - [diffusion][Epoch 8401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:36,436 - INFO - [diffusion][Epoch 8402] Epoch 8403/12000
2024-11-05 03:06:40,520 - INFO - [diffusion][Epoch 8402] diffusion training Loss: 0.048685149289667606
2024-11-05 03:06:40,522 - INFO - [diffusion][Epoch 8402] diffusion learning rate: 0.001
2024-11-05 03:06:40,524 - INFO - [diffusion][Epoch 8402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:40,525 - INFO - [diffusion][Epoch 8403] Epoch 8404/12000
2024-11-05 03:06:44,651 - INFO - [diffusion][Epoch 8403] diffusion training Loss: 0.052287518978118896
2024-11-05 03:06:44,653 - INFO - [diffusion][Epoch 8403] diffusion learning rate: 0.001
2024-11-05 03:06:44,655 - INFO - [diffusion][Epoch 8403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:44,656 - INFO - [diffusion][Epoch 8404] Epoch 8405/12000
2024-11-05 03:06:48,760 - INFO - [diffusion][Epoch 8404] diffusion training Loss: 0.05089722480624914
2024-11-05 03:06:48,762 - INFO - [diffusion][Epoch 8404] diffusion learning rate: 0.001
2024-11-05 03:06:48,764 - INFO - [diffusion][Epoch 8404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:48,765 - INFO - [diffusion][Epoch 8405] Epoch 8406/12000
2024-11-05 03:06:52,861 - INFO - [diffusion][Epoch 8405] diffusion training Loss: 0.05142485722899437
2024-11-05 03:06:52,863 - INFO - [diffusion][Epoch 8405] diffusion learning rate: 0.001
2024-11-05 03:06:52,864 - INFO - [diffusion][Epoch 8405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:52,865 - INFO - [diffusion][Epoch 8406] Epoch 8407/12000
2024-11-05 03:06:57,013 - INFO - [diffusion][Epoch 8406] diffusion training Loss: 0.05673614703118801
2024-11-05 03:06:57,015 - INFO - [diffusion][Epoch 8406] diffusion learning rate: 0.001
2024-11-05 03:06:57,017 - INFO - [diffusion][Epoch 8406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:06:57,019 - INFO - [diffusion][Epoch 8407] Epoch 8408/12000
2024-11-05 03:07:01,057 - INFO - [diffusion][Epoch 8407] diffusion training Loss: 0.05529135838150978
2024-11-05 03:07:01,059 - INFO - [diffusion][Epoch 8407] diffusion learning rate: 0.001
2024-11-05 03:07:01,061 - INFO - [diffusion][Epoch 8407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:01,063 - INFO - [diffusion][Epoch 8408] Epoch 8409/12000
2024-11-05 03:07:05,140 - INFO - [diffusion][Epoch 8408] diffusion training Loss: 0.05164197739213705
2024-11-05 03:07:05,143 - INFO - [diffusion][Epoch 8408] diffusion learning rate: 0.001
2024-11-05 03:07:05,145 - INFO - [diffusion][Epoch 8408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:05,146 - INFO - [diffusion][Epoch 8409] Epoch 8410/12000
2024-11-05 03:07:09,198 - INFO - [diffusion][Epoch 8409] diffusion training Loss: 0.0519285062327981
2024-11-05 03:07:09,201 - INFO - [diffusion][Epoch 8409] diffusion learning rate: 0.001
2024-11-05 03:07:09,203 - INFO - [diffusion][Epoch 8409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:09,204 - INFO - [diffusion][Epoch 8410] Epoch 8411/12000
2024-11-05 03:07:13,297 - INFO - [diffusion][Epoch 8410] diffusion training Loss: 0.05476605612784624
2024-11-05 03:07:13,300 - INFO - [diffusion][Epoch 8410] diffusion learning rate: 0.001
2024-11-05 03:07:13,302 - INFO - [diffusion][Epoch 8410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:13,303 - INFO - [diffusion][Epoch 8411] Epoch 8412/12000
2024-11-05 03:07:17,995 - INFO - [diffusion][Epoch 8411] diffusion training Loss: 0.05207762215286493
2024-11-05 03:07:17,999 - INFO - [diffusion][Epoch 8411] diffusion learning rate: 0.001
2024-11-05 03:07:18,001 - INFO - [diffusion][Epoch 8411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:18,002 - INFO - [diffusion][Epoch 8412] Epoch 8413/12000
2024-11-05 03:07:21,922 - INFO - [diffusion][Epoch 8412] diffusion training Loss: 0.049805354326963425
2024-11-05 03:07:21,925 - INFO - [diffusion][Epoch 8412] diffusion learning rate: 0.001
2024-11-05 03:07:21,926 - INFO - [diffusion][Epoch 8412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:21,928 - INFO - [diffusion][Epoch 8413] Epoch 8414/12000
2024-11-05 03:07:26,121 - INFO - [diffusion][Epoch 8413] diffusion training Loss: 0.055338747799396515
2024-11-05 03:07:26,123 - INFO - [diffusion][Epoch 8413] diffusion learning rate: 0.001
2024-11-05 03:07:26,125 - INFO - [diffusion][Epoch 8413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:26,127 - INFO - [diffusion][Epoch 8414] Epoch 8415/12000
2024-11-05 03:07:30,153 - INFO - [diffusion][Epoch 8414] diffusion training Loss: 0.05684482306241989
2024-11-05 03:07:30,155 - INFO - [diffusion][Epoch 8414] diffusion learning rate: 0.001
2024-11-05 03:07:30,157 - INFO - [diffusion][Epoch 8414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:30,158 - INFO - [diffusion][Epoch 8415] Epoch 8416/12000
2024-11-05 03:07:34,293 - INFO - [diffusion][Epoch 8415] diffusion training Loss: 0.055548849515616894
2024-11-05 03:07:34,295 - INFO - [diffusion][Epoch 8415] diffusion learning rate: 0.001
2024-11-05 03:07:34,297 - INFO - [diffusion][Epoch 8415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:34,298 - INFO - [diffusion][Epoch 8416] Epoch 8417/12000
2024-11-05 03:07:38,433 - INFO - [diffusion][Epoch 8416] diffusion training Loss: 0.049860008992254734
2024-11-05 03:07:38,436 - INFO - [diffusion][Epoch 8416] diffusion learning rate: 0.001
2024-11-05 03:07:38,438 - INFO - [diffusion][Epoch 8416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:38,439 - INFO - [diffusion][Epoch 8417] Epoch 8418/12000
2024-11-05 03:07:42,559 - INFO - [diffusion][Epoch 8417] diffusion training Loss: 0.05699527822434902
2024-11-05 03:07:42,561 - INFO - [diffusion][Epoch 8417] diffusion learning rate: 0.001
2024-11-05 03:07:42,563 - INFO - [diffusion][Epoch 8417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:42,564 - INFO - [diffusion][Epoch 8418] Epoch 8419/12000
2024-11-05 03:07:46,598 - INFO - [diffusion][Epoch 8418] diffusion training Loss: 0.05905175115913153
2024-11-05 03:07:46,600 - INFO - [diffusion][Epoch 8418] diffusion learning rate: 0.001
2024-11-05 03:07:46,602 - INFO - [diffusion][Epoch 8418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:46,603 - INFO - [diffusion][Epoch 8419] Epoch 8420/12000
2024-11-05 03:07:50,555 - INFO - [diffusion][Epoch 8419] diffusion training Loss: 0.060093846172094345
2024-11-05 03:07:50,557 - INFO - [diffusion][Epoch 8419] diffusion learning rate: 0.001
2024-11-05 03:07:50,559 - INFO - [diffusion][Epoch 8419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:50,560 - INFO - [diffusion][Epoch 8420] Epoch 8421/12000
2024-11-05 03:07:54,599 - INFO - [diffusion][Epoch 8420] diffusion training Loss: 0.05599483195692301
2024-11-05 03:07:54,601 - INFO - [diffusion][Epoch 8420] diffusion learning rate: 0.001
2024-11-05 03:07:54,603 - INFO - [diffusion][Epoch 8420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:54,604 - INFO - [diffusion][Epoch 8421] Epoch 8422/12000
2024-11-05 03:07:58,654 - INFO - [diffusion][Epoch 8421] diffusion training Loss: 0.058007427491247654
2024-11-05 03:07:58,657 - INFO - [diffusion][Epoch 8421] diffusion learning rate: 0.001
2024-11-05 03:07:58,659 - INFO - [diffusion][Epoch 8421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:07:58,660 - INFO - [diffusion][Epoch 8422] Epoch 8423/12000
2024-11-05 03:08:02,604 - INFO - [diffusion][Epoch 8422] diffusion training Loss: 0.05707085505127907
2024-11-05 03:08:02,607 - INFO - [diffusion][Epoch 8422] diffusion learning rate: 0.001
2024-11-05 03:08:02,608 - INFO - [diffusion][Epoch 8422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:02,610 - INFO - [diffusion][Epoch 8423] Epoch 8424/12000
2024-11-05 03:08:06,636 - INFO - [diffusion][Epoch 8423] diffusion training Loss: 0.05371794104576111
2024-11-05 03:08:06,638 - INFO - [diffusion][Epoch 8423] diffusion learning rate: 0.001
2024-11-05 03:08:06,640 - INFO - [diffusion][Epoch 8423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:06,641 - INFO - [diffusion][Epoch 8424] Epoch 8425/12000
2024-11-05 03:08:10,725 - INFO - [diffusion][Epoch 8424] diffusion training Loss: 0.05457977578043938
2024-11-05 03:08:10,727 - INFO - [diffusion][Epoch 8424] diffusion learning rate: 0.001
2024-11-05 03:08:10,729 - INFO - [diffusion][Epoch 8424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:10,730 - INFO - [diffusion][Epoch 8425] Epoch 8426/12000
2024-11-05 03:08:14,735 - INFO - [diffusion][Epoch 8425] diffusion training Loss: 0.05093152076005936
2024-11-05 03:08:14,737 - INFO - [diffusion][Epoch 8425] diffusion learning rate: 0.001
2024-11-05 03:08:14,738 - INFO - [diffusion][Epoch 8425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:14,740 - INFO - [diffusion][Epoch 8426] Epoch 8427/12000
2024-11-05 03:08:18,860 - INFO - [diffusion][Epoch 8426] diffusion training Loss: 0.04968889709562063
2024-11-05 03:08:18,862 - INFO - [diffusion][Epoch 8426] diffusion learning rate: 0.001
2024-11-05 03:08:18,864 - INFO - [diffusion][Epoch 8426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:18,865 - INFO - [diffusion][Epoch 8427] Epoch 8428/12000
2024-11-05 03:08:23,056 - INFO - [diffusion][Epoch 8427] diffusion training Loss: 0.0535066332668066
2024-11-05 03:08:23,059 - INFO - [diffusion][Epoch 8427] diffusion learning rate: 0.001
2024-11-05 03:08:23,061 - INFO - [diffusion][Epoch 8427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:23,062 - INFO - [diffusion][Epoch 8428] Epoch 8429/12000
2024-11-05 03:08:27,180 - INFO - [diffusion][Epoch 8428] diffusion training Loss: 0.05185678135603666
2024-11-05 03:08:27,183 - INFO - [diffusion][Epoch 8428] diffusion learning rate: 0.001
2024-11-05 03:08:27,185 - INFO - [diffusion][Epoch 8428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:27,186 - INFO - [diffusion][Epoch 8429] Epoch 8430/12000
2024-11-05 03:08:31,330 - INFO - [diffusion][Epoch 8429] diffusion training Loss: 0.05344234872609377
2024-11-05 03:08:31,332 - INFO - [diffusion][Epoch 8429] diffusion learning rate: 0.001
2024-11-05 03:08:31,334 - INFO - [diffusion][Epoch 8429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:31,335 - INFO - [diffusion][Epoch 8430] Epoch 8431/12000
2024-11-05 03:08:35,366 - INFO - [diffusion][Epoch 8430] diffusion training Loss: 0.056116292253136635
2024-11-05 03:08:35,368 - INFO - [diffusion][Epoch 8430] diffusion learning rate: 0.001
2024-11-05 03:08:35,370 - INFO - [diffusion][Epoch 8430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:35,371 - INFO - [diffusion][Epoch 8431] Epoch 8432/12000
2024-11-05 03:08:39,371 - INFO - [diffusion][Epoch 8431] diffusion training Loss: 0.05558234639465809
2024-11-05 03:08:39,373 - INFO - [diffusion][Epoch 8431] diffusion learning rate: 0.001
2024-11-05 03:08:39,375 - INFO - [diffusion][Epoch 8431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:39,376 - INFO - [diffusion][Epoch 8432] Epoch 8433/12000
2024-11-05 03:08:43,904 - INFO - [diffusion][Epoch 8432] diffusion training Loss: 0.05189044773578644
2024-11-05 03:08:43,905 - INFO - [diffusion][Epoch 8432] diffusion learning rate: 0.001
2024-11-05 03:08:43,907 - INFO - [diffusion][Epoch 8432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:43,908 - INFO - [diffusion][Epoch 8433] Epoch 8434/12000
2024-11-05 03:08:48,000 - INFO - [diffusion][Epoch 8433] diffusion training Loss: 0.04998920112848282
2024-11-05 03:08:48,002 - INFO - [diffusion][Epoch 8433] diffusion learning rate: 0.001
2024-11-05 03:08:48,003 - INFO - [diffusion][Epoch 8433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:48,005 - INFO - [diffusion][Epoch 8434] Epoch 8435/12000
2024-11-05 03:08:52,039 - INFO - [diffusion][Epoch 8434] diffusion training Loss: 0.05412582401186228
2024-11-05 03:08:52,041 - INFO - [diffusion][Epoch 8434] diffusion learning rate: 0.001
2024-11-05 03:08:52,043 - INFO - [diffusion][Epoch 8434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:52,045 - INFO - [diffusion][Epoch 8435] Epoch 8436/12000
2024-11-05 03:08:56,128 - INFO - [diffusion][Epoch 8435] diffusion training Loss: 0.05886684078723192
2024-11-05 03:08:56,130 - INFO - [diffusion][Epoch 8435] diffusion learning rate: 0.001
2024-11-05 03:08:56,132 - INFO - [diffusion][Epoch 8435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:08:56,134 - INFO - [diffusion][Epoch 8436] Epoch 8437/12000
2024-11-05 03:09:00,276 - INFO - [diffusion][Epoch 8436] diffusion training Loss: 0.05783713422715664
2024-11-05 03:09:00,278 - INFO - [diffusion][Epoch 8436] diffusion learning rate: 0.001
2024-11-05 03:09:00,280 - INFO - [diffusion][Epoch 8436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:00,281 - INFO - [diffusion][Epoch 8437] Epoch 8438/12000
2024-11-05 03:09:04,325 - INFO - [diffusion][Epoch 8437] diffusion training Loss: 0.05507765989750624
2024-11-05 03:09:04,327 - INFO - [diffusion][Epoch 8437] diffusion learning rate: 0.001
2024-11-05 03:09:04,329 - INFO - [diffusion][Epoch 8437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:04,330 - INFO - [diffusion][Epoch 8438] Epoch 8439/12000
2024-11-05 03:09:08,399 - INFO - [diffusion][Epoch 8438] diffusion training Loss: 0.055317667312920094
2024-11-05 03:09:08,401 - INFO - [diffusion][Epoch 8438] diffusion learning rate: 0.001
2024-11-05 03:09:08,403 - INFO - [diffusion][Epoch 8438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:08,405 - INFO - [diffusion][Epoch 8439] Epoch 8440/12000
2024-11-05 03:09:12,530 - INFO - [diffusion][Epoch 8439] diffusion training Loss: 0.05412551108747721
2024-11-05 03:09:12,609 - INFO - [diffusion][Epoch 8439] diffusion learning rate: 0.001
2024-11-05 03:09:12,612 - INFO - [diffusion][Epoch 8439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:12,613 - INFO - [diffusion][Epoch 8440] Epoch 8441/12000
2024-11-05 03:09:16,722 - INFO - [diffusion][Epoch 8440] diffusion training Loss: 0.04840327613055706
2024-11-05 03:09:16,724 - INFO - [diffusion][Epoch 8440] diffusion learning rate: 0.001
2024-11-05 03:09:16,726 - INFO - [diffusion][Epoch 8440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:16,727 - INFO - [diffusion][Epoch 8441] Epoch 8442/12000
2024-11-05 03:09:20,824 - INFO - [diffusion][Epoch 8441] diffusion training Loss: 0.05302110966295004
2024-11-05 03:09:20,826 - INFO - [diffusion][Epoch 8441] diffusion learning rate: 0.001
2024-11-05 03:09:20,828 - INFO - [diffusion][Epoch 8441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:20,829 - INFO - [diffusion][Epoch 8442] Epoch 8443/12000
2024-11-05 03:09:24,866 - INFO - [diffusion][Epoch 8442] diffusion training Loss: 0.05289511941373348
2024-11-05 03:09:24,869 - INFO - [diffusion][Epoch 8442] diffusion learning rate: 0.001
2024-11-05 03:09:24,871 - INFO - [diffusion][Epoch 8442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:24,873 - INFO - [diffusion][Epoch 8443] Epoch 8444/12000
2024-11-05 03:09:29,051 - INFO - [diffusion][Epoch 8443] diffusion training Loss: 0.05360658001154661
2024-11-05 03:09:29,053 - INFO - [diffusion][Epoch 8443] diffusion learning rate: 0.001
2024-11-05 03:09:29,093 - INFO - [diffusion][Epoch 8443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:29,095 - INFO - [diffusion][Epoch 8444] Epoch 8445/12000
2024-11-05 03:09:33,193 - INFO - [diffusion][Epoch 8444] diffusion training Loss: 0.04954487457871437
2024-11-05 03:09:33,195 - INFO - [diffusion][Epoch 8444] diffusion learning rate: 0.001
2024-11-05 03:09:33,197 - INFO - [diffusion][Epoch 8444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:33,198 - INFO - [diffusion][Epoch 8445] Epoch 8446/12000
2024-11-05 03:09:37,341 - INFO - [diffusion][Epoch 8445] diffusion training Loss: 0.05212798435240984
2024-11-05 03:09:37,344 - INFO - [diffusion][Epoch 8445] diffusion learning rate: 0.001
2024-11-05 03:09:37,346 - INFO - [diffusion][Epoch 8445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:37,347 - INFO - [diffusion][Epoch 8446] Epoch 8447/12000
2024-11-05 03:09:41,445 - INFO - [diffusion][Epoch 8446] diffusion training Loss: 0.05410610791295767
2024-11-05 03:09:41,448 - INFO - [diffusion][Epoch 8446] diffusion learning rate: 0.001
2024-11-05 03:09:41,449 - INFO - [diffusion][Epoch 8446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:41,451 - INFO - [diffusion][Epoch 8447] Epoch 8448/12000
2024-11-05 03:09:45,500 - INFO - [diffusion][Epoch 8447] diffusion training Loss: 0.05460054613649845
2024-11-05 03:09:45,502 - INFO - [diffusion][Epoch 8447] diffusion learning rate: 0.001
2024-11-05 03:09:45,505 - INFO - [diffusion][Epoch 8447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:45,506 - INFO - [diffusion][Epoch 8448] Epoch 8449/12000
2024-11-05 03:09:49,656 - INFO - [diffusion][Epoch 8448] diffusion training Loss: 0.050184354186058044
2024-11-05 03:09:49,658 - INFO - [diffusion][Epoch 8448] diffusion learning rate: 0.001
2024-11-05 03:09:49,660 - INFO - [diffusion][Epoch 8448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:49,661 - INFO - [diffusion][Epoch 8449] Epoch 8450/12000
2024-11-05 03:09:53,775 - INFO - [diffusion][Epoch 8449] diffusion training Loss: 0.06015975773334503
2024-11-05 03:09:53,777 - INFO - [diffusion][Epoch 8449] diffusion learning rate: 0.001
2024-11-05 03:09:53,779 - INFO - [diffusion][Epoch 8449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:53,781 - INFO - [diffusion][Epoch 8450] Epoch 8451/12000
2024-11-05 03:09:57,948 - INFO - [diffusion][Epoch 8450] diffusion training Loss: 0.053841883316636086
2024-11-05 03:09:57,949 - INFO - [diffusion][Epoch 8450] diffusion learning rate: 0.001
2024-11-05 03:09:57,951 - INFO - [diffusion][Epoch 8450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:09:57,952 - INFO - [diffusion][Epoch 8451] Epoch 8452/12000
2024-11-05 03:10:01,916 - INFO - [diffusion][Epoch 8451] diffusion training Loss: 0.05313835013657808
2024-11-05 03:10:01,918 - INFO - [diffusion][Epoch 8451] diffusion learning rate: 0.001
2024-11-05 03:10:01,920 - INFO - [diffusion][Epoch 8451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:01,921 - INFO - [diffusion][Epoch 8452] Epoch 8453/12000
2024-11-05 03:10:06,052 - INFO - [diffusion][Epoch 8452] diffusion training Loss: 0.05019015446305275
2024-11-05 03:10:06,054 - INFO - [diffusion][Epoch 8452] diffusion learning rate: 0.001
2024-11-05 03:10:06,057 - INFO - [diffusion][Epoch 8452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:06,058 - INFO - [diffusion][Epoch 8453] Epoch 8454/12000
2024-11-05 03:10:10,169 - INFO - [diffusion][Epoch 8453] diffusion training Loss: 0.05063262116163969
2024-11-05 03:10:10,172 - INFO - [diffusion][Epoch 8453] diffusion learning rate: 0.001
2024-11-05 03:10:10,174 - INFO - [diffusion][Epoch 8453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:10,175 - INFO - [diffusion][Epoch 8454] Epoch 8455/12000
2024-11-05 03:10:14,327 - INFO - [diffusion][Epoch 8454] diffusion training Loss: 0.049676296301186085
2024-11-05 03:10:14,329 - INFO - [diffusion][Epoch 8454] diffusion learning rate: 0.001
2024-11-05 03:10:14,331 - INFO - [diffusion][Epoch 8454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:14,333 - INFO - [diffusion][Epoch 8455] Epoch 8456/12000
2024-11-05 03:10:18,530 - INFO - [diffusion][Epoch 8455] diffusion training Loss: 0.057360561564564705
2024-11-05 03:10:18,532 - INFO - [diffusion][Epoch 8455] diffusion learning rate: 0.001
2024-11-05 03:10:18,534 - INFO - [diffusion][Epoch 8455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:18,535 - INFO - [diffusion][Epoch 8456] Epoch 8457/12000
2024-11-05 03:10:22,620 - INFO - [diffusion][Epoch 8456] diffusion training Loss: 0.04770415462553501
2024-11-05 03:10:22,622 - INFO - [diffusion][Epoch 8456] diffusion learning rate: 0.001
2024-11-05 03:10:22,623 - INFO - [diffusion][Epoch 8456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:22,625 - INFO - [diffusion][Epoch 8457] Epoch 8458/12000
2024-11-05 03:10:26,749 - INFO - [diffusion][Epoch 8457] diffusion training Loss: 0.05734463036060333
2024-11-05 03:10:26,752 - INFO - [diffusion][Epoch 8457] diffusion learning rate: 0.001
2024-11-05 03:10:26,753 - INFO - [diffusion][Epoch 8457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:26,755 - INFO - [diffusion][Epoch 8458] Epoch 8459/12000
2024-11-05 03:10:30,878 - INFO - [diffusion][Epoch 8458] diffusion training Loss: 0.053860584273934364
2024-11-05 03:10:30,880 - INFO - [diffusion][Epoch 8458] diffusion learning rate: 0.001
2024-11-05 03:10:30,881 - INFO - [diffusion][Epoch 8458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:30,883 - INFO - [diffusion][Epoch 8459] Epoch 8460/12000
2024-11-05 03:10:34,707 - INFO - [diffusion][Epoch 8459] diffusion training Loss: 0.05212835781276226
2024-11-05 03:10:34,709 - INFO - [diffusion][Epoch 8459] diffusion learning rate: 0.001
2024-11-05 03:10:34,711 - INFO - [diffusion][Epoch 8459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:34,712 - INFO - [diffusion][Epoch 8460] Epoch 8461/12000
2024-11-05 03:10:38,713 - INFO - [diffusion][Epoch 8460] diffusion training Loss: 0.05446846690028906
2024-11-05 03:10:38,715 - INFO - [diffusion][Epoch 8460] diffusion learning rate: 0.001
2024-11-05 03:10:38,717 - INFO - [diffusion][Epoch 8460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:38,718 - INFO - [diffusion][Epoch 8461] Epoch 8462/12000
2024-11-05 03:10:42,738 - INFO - [diffusion][Epoch 8461] diffusion training Loss: 0.05382588319480419
2024-11-05 03:10:42,740 - INFO - [diffusion][Epoch 8461] diffusion learning rate: 0.001
2024-11-05 03:10:42,742 - INFO - [diffusion][Epoch 8461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:42,744 - INFO - [diffusion][Epoch 8462] Epoch 8463/12000
2024-11-05 03:10:46,849 - INFO - [diffusion][Epoch 8462] diffusion training Loss: 0.05131016671657562
2024-11-05 03:10:46,851 - INFO - [diffusion][Epoch 8462] diffusion learning rate: 0.001
2024-11-05 03:10:46,853 - INFO - [diffusion][Epoch 8462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:46,854 - INFO - [diffusion][Epoch 8463] Epoch 8464/12000
2024-11-05 03:10:50,820 - INFO - [diffusion][Epoch 8463] diffusion training Loss: 0.05102792102843523
2024-11-05 03:10:50,822 - INFO - [diffusion][Epoch 8463] diffusion learning rate: 0.001
2024-11-05 03:10:50,823 - INFO - [diffusion][Epoch 8463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:50,825 - INFO - [diffusion][Epoch 8464] Epoch 8465/12000
2024-11-05 03:10:54,983 - INFO - [diffusion][Epoch 8464] diffusion training Loss: 0.05373112205415964
2024-11-05 03:10:54,985 - INFO - [diffusion][Epoch 8464] diffusion learning rate: 0.001
2024-11-05 03:10:54,987 - INFO - [diffusion][Epoch 8464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:54,988 - INFO - [diffusion][Epoch 8465] Epoch 8466/12000
2024-11-05 03:10:59,100 - INFO - [diffusion][Epoch 8465] diffusion training Loss: 0.04985871631652117
2024-11-05 03:10:59,102 - INFO - [diffusion][Epoch 8465] diffusion learning rate: 0.001
2024-11-05 03:10:59,104 - INFO - [diffusion][Epoch 8465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:10:59,105 - INFO - [diffusion][Epoch 8466] Epoch 8467/12000
2024-11-05 03:11:03,103 - INFO - [diffusion][Epoch 8466] diffusion training Loss: 0.054425133392214775
2024-11-05 03:11:03,106 - INFO - [diffusion][Epoch 8466] diffusion learning rate: 0.001
2024-11-05 03:11:03,140 - INFO - [diffusion][Epoch 8466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:03,142 - INFO - [diffusion][Epoch 8467] Epoch 8468/12000
2024-11-05 03:11:07,083 - INFO - [diffusion][Epoch 8467] diffusion training Loss: 0.05175359360873699
2024-11-05 03:11:07,085 - INFO - [diffusion][Epoch 8467] diffusion learning rate: 0.001
2024-11-05 03:11:07,086 - INFO - [diffusion][Epoch 8467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:07,087 - INFO - [diffusion][Epoch 8468] Epoch 8469/12000
2024-11-05 03:11:10,976 - INFO - [diffusion][Epoch 8468] diffusion training Loss: 0.047543453983962536
2024-11-05 03:11:10,978 - INFO - [diffusion][Epoch 8468] diffusion learning rate: 0.001
2024-11-05 03:11:10,980 - INFO - [diffusion][Epoch 8468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:10,981 - INFO - [diffusion][Epoch 8469] Epoch 8470/12000
2024-11-05 03:11:15,000 - INFO - [diffusion][Epoch 8469] diffusion training Loss: 0.05465348809957504
2024-11-05 03:11:15,002 - INFO - [diffusion][Epoch 8469] diffusion learning rate: 0.001
2024-11-05 03:11:15,004 - INFO - [diffusion][Epoch 8469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:15,005 - INFO - [diffusion][Epoch 8470] Epoch 8471/12000
2024-11-05 03:11:19,175 - INFO - [diffusion][Epoch 8470] diffusion training Loss: 0.05595597717911005
2024-11-05 03:11:19,178 - INFO - [diffusion][Epoch 8470] diffusion learning rate: 0.001
2024-11-05 03:11:19,180 - INFO - [diffusion][Epoch 8470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:19,182 - INFO - [diffusion][Epoch 8471] Epoch 8472/12000
2024-11-05 03:11:23,305 - INFO - [diffusion][Epoch 8471] diffusion training Loss: 0.049671370536088943
2024-11-05 03:11:23,307 - INFO - [diffusion][Epoch 8471] diffusion learning rate: 0.001
2024-11-05 03:11:23,309 - INFO - [diffusion][Epoch 8471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:23,310 - INFO - [diffusion][Epoch 8472] Epoch 8473/12000
2024-11-05 03:11:27,487 - INFO - [diffusion][Epoch 8472] diffusion training Loss: 0.0528031038120389
2024-11-05 03:11:27,490 - INFO - [diffusion][Epoch 8472] diffusion learning rate: 0.001
2024-11-05 03:11:27,492 - INFO - [diffusion][Epoch 8472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:27,493 - INFO - [diffusion][Epoch 8473] Epoch 8474/12000
2024-11-05 03:11:31,900 - INFO - [diffusion][Epoch 8473] diffusion training Loss: 0.054138023406267166
2024-11-05 03:11:31,902 - INFO - [diffusion][Epoch 8473] diffusion learning rate: 0.001
2024-11-05 03:11:31,904 - INFO - [diffusion][Epoch 8473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:31,905 - INFO - [diffusion][Epoch 8474] Epoch 8475/12000
2024-11-05 03:11:35,872 - INFO - [diffusion][Epoch 8474] diffusion training Loss: 0.05826606974005699
2024-11-05 03:11:35,874 - INFO - [diffusion][Epoch 8474] diffusion learning rate: 0.001
2024-11-05 03:11:35,876 - INFO - [diffusion][Epoch 8474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:35,877 - INFO - [diffusion][Epoch 8475] Epoch 8476/12000
2024-11-05 03:11:39,859 - INFO - [diffusion][Epoch 8475] diffusion training Loss: 0.05910480488091707
2024-11-05 03:11:39,861 - INFO - [diffusion][Epoch 8475] diffusion learning rate: 0.001
2024-11-05 03:11:39,862 - INFO - [diffusion][Epoch 8475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:39,864 - INFO - [diffusion][Epoch 8476] Epoch 8477/12000
2024-11-05 03:11:43,956 - INFO - [diffusion][Epoch 8476] diffusion training Loss: 0.050395455211400986
2024-11-05 03:11:43,958 - INFO - [diffusion][Epoch 8476] diffusion learning rate: 0.001
2024-11-05 03:11:43,960 - INFO - [diffusion][Epoch 8476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:43,961 - INFO - [diffusion][Epoch 8477] Epoch 8478/12000
2024-11-05 03:11:48,069 - INFO - [diffusion][Epoch 8477] diffusion training Loss: 0.05472494941204786
2024-11-05 03:11:48,071 - INFO - [diffusion][Epoch 8477] diffusion learning rate: 0.001
2024-11-05 03:11:48,073 - INFO - [diffusion][Epoch 8477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:48,074 - INFO - [diffusion][Epoch 8478] Epoch 8479/12000
2024-11-05 03:11:52,192 - INFO - [diffusion][Epoch 8478] diffusion training Loss: 0.04696307796984911
2024-11-05 03:11:52,194 - INFO - [diffusion][Epoch 8478] diffusion learning rate: 0.001
2024-11-05 03:11:52,196 - INFO - [diffusion][Epoch 8478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:52,197 - INFO - [diffusion][Epoch 8479] Epoch 8480/12000
2024-11-05 03:11:56,306 - INFO - [diffusion][Epoch 8479] diffusion training Loss: 0.05220711138099432
2024-11-05 03:11:56,308 - INFO - [diffusion][Epoch 8479] diffusion learning rate: 0.001
2024-11-05 03:11:56,311 - INFO - [diffusion][Epoch 8479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:11:56,312 - INFO - [diffusion][Epoch 8480] Epoch 8481/12000
2024-11-05 03:12:00,326 - INFO - [diffusion][Epoch 8480] diffusion training Loss: 0.05267857015132904
2024-11-05 03:12:00,329 - INFO - [diffusion][Epoch 8480] diffusion learning rate: 0.001
2024-11-05 03:12:00,331 - INFO - [diffusion][Epoch 8480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:00,333 - INFO - [diffusion][Epoch 8481] Epoch 8482/12000
2024-11-05 03:12:04,395 - INFO - [diffusion][Epoch 8481] diffusion training Loss: 0.05146094784140587
2024-11-05 03:12:04,397 - INFO - [diffusion][Epoch 8481] diffusion learning rate: 0.001
2024-11-05 03:12:04,399 - INFO - [diffusion][Epoch 8481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:04,400 - INFO - [diffusion][Epoch 8482] Epoch 8483/12000
2024-11-05 03:12:08,506 - INFO - [diffusion][Epoch 8482] diffusion training Loss: 0.04933489952236414
2024-11-05 03:12:08,508 - INFO - [diffusion][Epoch 8482] diffusion learning rate: 0.001
2024-11-05 03:12:08,510 - INFO - [diffusion][Epoch 8482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:08,511 - INFO - [diffusion][Epoch 8483] Epoch 8484/12000
2024-11-05 03:12:12,645 - INFO - [diffusion][Epoch 8483] diffusion training Loss: 0.05367162264883518
2024-11-05 03:12:12,647 - INFO - [diffusion][Epoch 8483] diffusion learning rate: 0.001
2024-11-05 03:12:12,648 - INFO - [diffusion][Epoch 8483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:12,649 - INFO - [diffusion][Epoch 8484] Epoch 8485/12000
2024-11-05 03:12:16,709 - INFO - [diffusion][Epoch 8484] diffusion training Loss: 0.05431993491947651
2024-11-05 03:12:16,712 - INFO - [diffusion][Epoch 8484] diffusion learning rate: 0.001
2024-11-05 03:12:16,713 - INFO - [diffusion][Epoch 8484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:16,715 - INFO - [diffusion][Epoch 8485] Epoch 8486/12000
2024-11-05 03:12:20,730 - INFO - [diffusion][Epoch 8485] diffusion training Loss: 0.05328631121665239
2024-11-05 03:12:20,732 - INFO - [diffusion][Epoch 8485] diffusion learning rate: 0.001
2024-11-05 03:12:20,734 - INFO - [diffusion][Epoch 8485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:20,735 - INFO - [diffusion][Epoch 8486] Epoch 8487/12000
2024-11-05 03:12:24,861 - INFO - [diffusion][Epoch 8486] diffusion training Loss: 0.05764405615627766
2024-11-05 03:12:24,863 - INFO - [diffusion][Epoch 8486] diffusion learning rate: 0.001
2024-11-05 03:12:24,865 - INFO - [diffusion][Epoch 8486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:24,866 - INFO - [diffusion][Epoch 8487] Epoch 8488/12000
2024-11-05 03:12:28,985 - INFO - [diffusion][Epoch 8487] diffusion training Loss: 0.05645580589771271
2024-11-05 03:12:28,988 - INFO - [diffusion][Epoch 8487] diffusion learning rate: 0.001
2024-11-05 03:12:28,990 - INFO - [diffusion][Epoch 8487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:28,991 - INFO - [diffusion][Epoch 8488] Epoch 8489/12000
2024-11-05 03:12:33,008 - INFO - [diffusion][Epoch 8488] diffusion training Loss: 0.05355321988463402
2024-11-05 03:12:33,010 - INFO - [diffusion][Epoch 8488] diffusion learning rate: 0.001
2024-11-05 03:12:33,012 - INFO - [diffusion][Epoch 8488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:33,013 - INFO - [diffusion][Epoch 8489] Epoch 8490/12000
2024-11-05 03:12:37,147 - INFO - [diffusion][Epoch 8489] diffusion training Loss: 0.04949558340013027
2024-11-05 03:12:37,150 - INFO - [diffusion][Epoch 8489] diffusion learning rate: 0.001
2024-11-05 03:12:37,152 - INFO - [diffusion][Epoch 8489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:37,153 - INFO - [diffusion][Epoch 8490] Epoch 8491/12000
2024-11-05 03:12:41,033 - INFO - [diffusion][Epoch 8490] diffusion training Loss: 0.05359428562223911
2024-11-05 03:12:41,035 - INFO - [diffusion][Epoch 8490] diffusion learning rate: 0.001
2024-11-05 03:12:41,037 - INFO - [diffusion][Epoch 8490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:41,038 - INFO - [diffusion][Epoch 8491] Epoch 8492/12000
2024-11-05 03:12:45,034 - INFO - [diffusion][Epoch 8491] diffusion training Loss: 0.05042302329093218
2024-11-05 03:12:45,036 - INFO - [diffusion][Epoch 8491] diffusion learning rate: 0.001
2024-11-05 03:12:45,038 - INFO - [diffusion][Epoch 8491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:45,039 - INFO - [diffusion][Epoch 8492] Epoch 8493/12000
2024-11-05 03:12:49,116 - INFO - [diffusion][Epoch 8492] diffusion training Loss: 0.05266875121742487
2024-11-05 03:12:49,118 - INFO - [diffusion][Epoch 8492] diffusion learning rate: 0.001
2024-11-05 03:12:49,120 - INFO - [diffusion][Epoch 8492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:49,121 - INFO - [diffusion][Epoch 8493] Epoch 8494/12000
2024-11-05 03:12:53,100 - INFO - [diffusion][Epoch 8493] diffusion training Loss: 0.05176843889057636
2024-11-05 03:12:53,102 - INFO - [diffusion][Epoch 8493] diffusion learning rate: 0.001
2024-11-05 03:12:53,105 - INFO - [diffusion][Epoch 8493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:53,107 - INFO - [diffusion][Epoch 8494] Epoch 8495/12000
2024-11-05 03:12:57,224 - INFO - [diffusion][Epoch 8494] diffusion training Loss: 0.04897853545844555
2024-11-05 03:12:57,226 - INFO - [diffusion][Epoch 8494] diffusion learning rate: 0.001
2024-11-05 03:12:57,228 - INFO - [diffusion][Epoch 8494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:12:57,229 - INFO - [diffusion][Epoch 8495] Epoch 8496/12000
2024-11-05 03:13:01,489 - INFO - [diffusion][Epoch 8495] diffusion training Loss: 0.054511635564267635
2024-11-05 03:13:01,491 - INFO - [diffusion][Epoch 8495] diffusion learning rate: 0.001
2024-11-05 03:13:01,493 - INFO - [diffusion][Epoch 8495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:01,494 - INFO - [diffusion][Epoch 8496] Epoch 8497/12000
2024-11-05 03:13:05,615 - INFO - [diffusion][Epoch 8496] diffusion training Loss: 0.048441906459629536
2024-11-05 03:13:05,617 - INFO - [diffusion][Epoch 8496] diffusion learning rate: 0.001
2024-11-05 03:13:05,619 - INFO - [diffusion][Epoch 8496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:05,621 - INFO - [diffusion][Epoch 8497] Epoch 8498/12000
2024-11-05 03:13:09,668 - INFO - [diffusion][Epoch 8497] diffusion training Loss: 0.0486350329592824
2024-11-05 03:13:09,671 - INFO - [diffusion][Epoch 8497] diffusion learning rate: 0.001
2024-11-05 03:13:09,673 - INFO - [diffusion][Epoch 8497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:09,675 - INFO - [diffusion][Epoch 8498] Epoch 8499/12000
2024-11-05 03:13:13,778 - INFO - [diffusion][Epoch 8498] diffusion training Loss: 0.05073781404644251
2024-11-05 03:13:13,780 - INFO - [diffusion][Epoch 8498] diffusion learning rate: 0.001
2024-11-05 03:13:13,782 - INFO - [diffusion][Epoch 8498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:13,783 - INFO - [diffusion][Epoch 8499] Epoch 8500/12000
2024-11-05 03:13:17,926 - INFO - [diffusion][Epoch 8499] diffusion training Loss: 0.051618389785289764
2024-11-05 03:13:17,928 - INFO - [diffusion][Epoch 8499] diffusion learning rate: 0.001
2024-11-05 03:13:17,930 - INFO - [diffusion][Epoch 8499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:17,931 - INFO - [diffusion][Epoch 8500] Epoch 8501/12000
2024-11-05 03:13:22,029 - INFO - [diffusion][Epoch 8500] diffusion training Loss: 0.05127285420894623
2024-11-05 03:13:22,031 - INFO - [diffusion][Epoch 8500] diffusion learning rate: 0.001
2024-11-05 03:13:22,033 - INFO - [diffusion][Epoch 8500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:22,034 - INFO - [diffusion][Epoch 8501] Epoch 8502/12000
2024-11-05 03:13:26,042 - INFO - [diffusion][Epoch 8501] diffusion training Loss: 0.04805235471576452
2024-11-05 03:13:26,044 - INFO - [diffusion][Epoch 8501] diffusion learning rate: 0.001
2024-11-05 03:13:26,046 - INFO - [diffusion][Epoch 8501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:26,047 - INFO - [diffusion][Epoch 8502] Epoch 8503/12000
2024-11-05 03:13:30,208 - INFO - [diffusion][Epoch 8502] diffusion training Loss: 0.056425255723297596
2024-11-05 03:13:30,211 - INFO - [diffusion][Epoch 8502] diffusion learning rate: 0.001
2024-11-05 03:13:30,213 - INFO - [diffusion][Epoch 8502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:30,214 - INFO - [diffusion][Epoch 8503] Epoch 8504/12000
2024-11-05 03:13:34,330 - INFO - [diffusion][Epoch 8503] diffusion training Loss: 0.05047942418605089
2024-11-05 03:13:34,332 - INFO - [diffusion][Epoch 8503] diffusion learning rate: 0.001
2024-11-05 03:13:34,334 - INFO - [diffusion][Epoch 8503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:34,335 - INFO - [diffusion][Epoch 8504] Epoch 8505/12000
2024-11-05 03:13:38,352 - INFO - [diffusion][Epoch 8504] diffusion training Loss: 0.048621995374560356
2024-11-05 03:13:38,354 - INFO - [diffusion][Epoch 8504] diffusion learning rate: 0.001
2024-11-05 03:13:38,356 - INFO - [diffusion][Epoch 8504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:38,357 - INFO - [diffusion][Epoch 8505] Epoch 8506/12000
2024-11-05 03:13:42,375 - INFO - [diffusion][Epoch 8505] diffusion training Loss: 0.05078867822885513
2024-11-05 03:13:42,377 - INFO - [diffusion][Epoch 8505] diffusion learning rate: 0.001
2024-11-05 03:13:42,379 - INFO - [diffusion][Epoch 8505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:42,380 - INFO - [diffusion][Epoch 8506] Epoch 8507/12000
2024-11-05 03:13:46,383 - INFO - [diffusion][Epoch 8506] diffusion training Loss: 0.05458915792405605
2024-11-05 03:13:46,385 - INFO - [diffusion][Epoch 8506] diffusion learning rate: 0.001
2024-11-05 03:13:46,387 - INFO - [diffusion][Epoch 8506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:46,389 - INFO - [diffusion][Epoch 8507] Epoch 8508/12000
2024-11-05 03:13:50,422 - INFO - [diffusion][Epoch 8507] diffusion training Loss: 0.05235184542834759
2024-11-05 03:13:50,424 - INFO - [diffusion][Epoch 8507] diffusion learning rate: 0.001
2024-11-05 03:13:50,426 - INFO - [diffusion][Epoch 8507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:50,427 - INFO - [diffusion][Epoch 8508] Epoch 8509/12000
2024-11-05 03:13:54,546 - INFO - [diffusion][Epoch 8508] diffusion training Loss: 0.056587256491184235
2024-11-05 03:13:54,548 - INFO - [diffusion][Epoch 8508] diffusion learning rate: 0.001
2024-11-05 03:13:54,555 - INFO - [diffusion][Epoch 8508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:54,557 - INFO - [diffusion][Epoch 8509] Epoch 8510/12000
2024-11-05 03:13:58,641 - INFO - [diffusion][Epoch 8509] diffusion training Loss: 0.05203465465456247
2024-11-05 03:13:58,643 - INFO - [diffusion][Epoch 8509] diffusion learning rate: 0.001
2024-11-05 03:13:58,645 - INFO - [diffusion][Epoch 8509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:13:58,647 - INFO - [diffusion][Epoch 8510] Epoch 8511/12000
2024-11-05 03:14:02,668 - INFO - [diffusion][Epoch 8510] diffusion training Loss: 0.054126907140016556
2024-11-05 03:14:02,670 - INFO - [diffusion][Epoch 8510] diffusion learning rate: 0.001
2024-11-05 03:14:02,672 - INFO - [diffusion][Epoch 8510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:02,674 - INFO - [diffusion][Epoch 8511] Epoch 8512/12000
2024-11-05 03:14:06,841 - INFO - [diffusion][Epoch 8511] diffusion training Loss: 0.048270915634930134
2024-11-05 03:14:06,843 - INFO - [diffusion][Epoch 8511] diffusion learning rate: 0.001
2024-11-05 03:14:06,845 - INFO - [diffusion][Epoch 8511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:06,846 - INFO - [diffusion][Epoch 8512] Epoch 8513/12000
2024-11-05 03:14:10,889 - INFO - [diffusion][Epoch 8512] diffusion training Loss: 0.059347737580537796
2024-11-05 03:14:10,891 - INFO - [diffusion][Epoch 8512] diffusion learning rate: 0.001
2024-11-05 03:14:10,893 - INFO - [diffusion][Epoch 8512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:10,894 - INFO - [diffusion][Epoch 8513] Epoch 8514/12000
2024-11-05 03:14:14,991 - INFO - [diffusion][Epoch 8513] diffusion training Loss: 0.05103262234479189
2024-11-05 03:14:14,993 - INFO - [diffusion][Epoch 8513] diffusion learning rate: 0.001
2024-11-05 03:14:14,994 - INFO - [diffusion][Epoch 8513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:14,996 - INFO - [diffusion][Epoch 8514] Epoch 8515/12000
2024-11-05 03:14:19,112 - INFO - [diffusion][Epoch 8514] diffusion training Loss: 0.055345564149320126
2024-11-05 03:14:19,113 - INFO - [diffusion][Epoch 8514] diffusion learning rate: 0.001
2024-11-05 03:14:19,116 - INFO - [diffusion][Epoch 8514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:19,117 - INFO - [diffusion][Epoch 8515] Epoch 8516/12000
2024-11-05 03:14:23,207 - INFO - [diffusion][Epoch 8515] diffusion training Loss: 0.052198296412825584
2024-11-05 03:14:23,209 - INFO - [diffusion][Epoch 8515] diffusion learning rate: 0.001
2024-11-05 03:14:23,211 - INFO - [diffusion][Epoch 8515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:23,212 - INFO - [diffusion][Epoch 8516] Epoch 8517/12000
2024-11-05 03:14:27,657 - INFO - [diffusion][Epoch 8516] diffusion training Loss: 0.05404167342931032
2024-11-05 03:14:27,659 - INFO - [diffusion][Epoch 8516] diffusion learning rate: 0.001
2024-11-05 03:14:27,660 - INFO - [diffusion][Epoch 8516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:27,662 - INFO - [diffusion][Epoch 8517] Epoch 8518/12000
2024-11-05 03:14:31,667 - INFO - [diffusion][Epoch 8517] diffusion training Loss: 0.053229669108986855
2024-11-05 03:14:31,671 - INFO - [diffusion][Epoch 8517] diffusion learning rate: 0.001
2024-11-05 03:14:31,672 - INFO - [diffusion][Epoch 8517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:31,674 - INFO - [diffusion][Epoch 8518] Epoch 8519/12000
2024-11-05 03:14:35,639 - INFO - [diffusion][Epoch 8518] diffusion training Loss: 0.04946627374738455
2024-11-05 03:14:35,642 - INFO - [diffusion][Epoch 8518] diffusion learning rate: 0.001
2024-11-05 03:14:35,645 - INFO - [diffusion][Epoch 8518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:35,649 - INFO - [diffusion][Epoch 8519] Epoch 8520/12000
2024-11-05 03:14:39,746 - INFO - [diffusion][Epoch 8519] diffusion training Loss: 0.05227279756218195
2024-11-05 03:14:39,748 - INFO - [diffusion][Epoch 8519] diffusion learning rate: 0.001
2024-11-05 03:14:39,749 - INFO - [diffusion][Epoch 8519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:39,751 - INFO - [diffusion][Epoch 8520] Epoch 8521/12000
2024-11-05 03:14:43,867 - INFO - [diffusion][Epoch 8520] diffusion training Loss: 0.04867999628186226
2024-11-05 03:14:43,869 - INFO - [diffusion][Epoch 8520] diffusion learning rate: 0.001
2024-11-05 03:14:43,871 - INFO - [diffusion][Epoch 8520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:43,872 - INFO - [diffusion][Epoch 8521] Epoch 8522/12000
2024-11-05 03:14:47,854 - INFO - [diffusion][Epoch 8521] diffusion training Loss: 0.05308128986507654
2024-11-05 03:14:47,856 - INFO - [diffusion][Epoch 8521] diffusion learning rate: 0.001
2024-11-05 03:14:47,858 - INFO - [diffusion][Epoch 8521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:47,859 - INFO - [diffusion][Epoch 8522] Epoch 8523/12000
2024-11-05 03:14:51,992 - INFO - [diffusion][Epoch 8522] diffusion training Loss: 0.05456200335174799
2024-11-05 03:14:51,994 - INFO - [diffusion][Epoch 8522] diffusion learning rate: 0.001
2024-11-05 03:14:52,022 - INFO - [diffusion][Epoch 8522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:52,023 - INFO - [diffusion][Epoch 8523] Epoch 8524/12000
2024-11-05 03:14:56,121 - INFO - [diffusion][Epoch 8523] diffusion training Loss: 0.05310351122170687
2024-11-05 03:14:56,123 - INFO - [diffusion][Epoch 8523] diffusion learning rate: 0.001
2024-11-05 03:14:56,124 - INFO - [diffusion][Epoch 8523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:14:56,126 - INFO - [diffusion][Epoch 8524] Epoch 8525/12000
2024-11-05 03:15:00,281 - INFO - [diffusion][Epoch 8524] diffusion training Loss: 0.054442089051008224
2024-11-05 03:15:00,283 - INFO - [diffusion][Epoch 8524] diffusion learning rate: 0.001
2024-11-05 03:15:00,285 - INFO - [diffusion][Epoch 8524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:00,286 - INFO - [diffusion][Epoch 8525] Epoch 8526/12000
2024-11-05 03:15:04,419 - INFO - [diffusion][Epoch 8525] diffusion training Loss: 0.05750374682247639
2024-11-05 03:15:04,421 - INFO - [diffusion][Epoch 8525] diffusion learning rate: 0.001
2024-11-05 03:15:04,423 - INFO - [diffusion][Epoch 8525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:04,424 - INFO - [diffusion][Epoch 8526] Epoch 8527/12000
2024-11-05 03:15:08,475 - INFO - [diffusion][Epoch 8526] diffusion training Loss: 0.05275841802358627
2024-11-05 03:15:08,477 - INFO - [diffusion][Epoch 8526] diffusion learning rate: 0.001
2024-11-05 03:15:08,478 - INFO - [diffusion][Epoch 8526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:08,480 - INFO - [diffusion][Epoch 8527] Epoch 8528/12000
2024-11-05 03:15:12,402 - INFO - [diffusion][Epoch 8527] diffusion training Loss: 0.05275505967438221
2024-11-05 03:15:12,518 - INFO - [diffusion][Epoch 8527] diffusion learning rate: 0.001
2024-11-05 03:15:12,520 - INFO - [diffusion][Epoch 8527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:12,522 - INFO - [diffusion][Epoch 8528] Epoch 8529/12000
2024-11-05 03:15:16,563 - INFO - [diffusion][Epoch 8528] diffusion training Loss: 0.05096758063882589
2024-11-05 03:15:16,565 - INFO - [diffusion][Epoch 8528] diffusion learning rate: 0.001
2024-11-05 03:15:16,567 - INFO - [diffusion][Epoch 8528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:16,569 - INFO - [diffusion][Epoch 8529] Epoch 8530/12000
2024-11-05 03:15:20,725 - INFO - [diffusion][Epoch 8529] diffusion training Loss: 0.0577606400474906
2024-11-05 03:15:20,727 - INFO - [diffusion][Epoch 8529] diffusion learning rate: 0.001
2024-11-05 03:15:20,729 - INFO - [diffusion][Epoch 8529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:20,730 - INFO - [diffusion][Epoch 8530] Epoch 8531/12000
2024-11-05 03:15:24,692 - INFO - [diffusion][Epoch 8530] diffusion training Loss: 0.0538649233058095
2024-11-05 03:15:24,694 - INFO - [diffusion][Epoch 8530] diffusion learning rate: 0.001
2024-11-05 03:15:24,696 - INFO - [diffusion][Epoch 8530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:24,697 - INFO - [diffusion][Epoch 8531] Epoch 8532/12000
2024-11-05 03:15:28,817 - INFO - [diffusion][Epoch 8531] diffusion training Loss: 0.057010519318282604
2024-11-05 03:15:28,819 - INFO - [diffusion][Epoch 8531] diffusion learning rate: 0.001
2024-11-05 03:15:28,820 - INFO - [diffusion][Epoch 8531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:28,822 - INFO - [diffusion][Epoch 8532] Epoch 8533/12000
2024-11-05 03:15:32,946 - INFO - [diffusion][Epoch 8532] diffusion training Loss: 0.05508909747004509
2024-11-05 03:15:32,949 - INFO - [diffusion][Epoch 8532] diffusion learning rate: 0.001
2024-11-05 03:15:32,951 - INFO - [diffusion][Epoch 8532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:32,952 - INFO - [diffusion][Epoch 8533] Epoch 8534/12000
2024-11-05 03:15:37,131 - INFO - [diffusion][Epoch 8533] diffusion training Loss: 0.05380321014672518
2024-11-05 03:15:37,133 - INFO - [diffusion][Epoch 8533] diffusion learning rate: 0.001
2024-11-05 03:15:37,135 - INFO - [diffusion][Epoch 8533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:37,136 - INFO - [diffusion][Epoch 8534] Epoch 8535/12000
2024-11-05 03:15:41,267 - INFO - [diffusion][Epoch 8534] diffusion training Loss: 0.05186271108686924
2024-11-05 03:15:41,270 - INFO - [diffusion][Epoch 8534] diffusion learning rate: 0.001
2024-11-05 03:15:41,271 - INFO - [diffusion][Epoch 8534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:41,273 - INFO - [diffusion][Epoch 8535] Epoch 8536/12000
2024-11-05 03:15:45,777 - INFO - [diffusion][Epoch 8535] diffusion training Loss: 0.05666518770158291
2024-11-05 03:15:45,779 - INFO - [diffusion][Epoch 8535] diffusion learning rate: 0.001
2024-11-05 03:15:45,780 - INFO - [diffusion][Epoch 8535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:45,782 - INFO - [diffusion][Epoch 8536] Epoch 8537/12000
2024-11-05 03:15:49,882 - INFO - [diffusion][Epoch 8536] diffusion training Loss: 0.05565119348466396
2024-11-05 03:15:49,884 - INFO - [diffusion][Epoch 8536] diffusion learning rate: 0.001
2024-11-05 03:15:49,886 - INFO - [diffusion][Epoch 8536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:49,887 - INFO - [diffusion][Epoch 8537] Epoch 8538/12000
2024-11-05 03:15:53,900 - INFO - [diffusion][Epoch 8537] diffusion training Loss: 0.05158752389252186
2024-11-05 03:15:53,902 - INFO - [diffusion][Epoch 8537] diffusion learning rate: 0.001
2024-11-05 03:15:53,903 - INFO - [diffusion][Epoch 8537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:53,905 - INFO - [diffusion][Epoch 8538] Epoch 8539/12000
2024-11-05 03:15:57,988 - INFO - [diffusion][Epoch 8538] diffusion training Loss: 0.050996119156479836
2024-11-05 03:15:57,991 - INFO - [diffusion][Epoch 8538] diffusion learning rate: 0.001
2024-11-05 03:15:57,993 - INFO - [diffusion][Epoch 8538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:15:57,996 - INFO - [diffusion][Epoch 8539] Epoch 8540/12000
2024-11-05 03:16:02,102 - INFO - [diffusion][Epoch 8539] diffusion training Loss: 0.05373764596879482
2024-11-05 03:16:02,104 - INFO - [diffusion][Epoch 8539] diffusion learning rate: 0.001
2024-11-05 03:16:02,105 - INFO - [diffusion][Epoch 8539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:02,107 - INFO - [diffusion][Epoch 8540] Epoch 8541/12000
2024-11-05 03:16:06,224 - INFO - [diffusion][Epoch 8540] diffusion training Loss: 0.05288499128073454
2024-11-05 03:16:06,226 - INFO - [diffusion][Epoch 8540] diffusion learning rate: 0.001
2024-11-05 03:16:06,228 - INFO - [diffusion][Epoch 8540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:06,230 - INFO - [diffusion][Epoch 8541] Epoch 8542/12000
2024-11-05 03:16:10,356 - INFO - [diffusion][Epoch 8541] diffusion training Loss: 0.05013519525527954
2024-11-05 03:16:10,357 - INFO - [diffusion][Epoch 8541] diffusion learning rate: 0.001
2024-11-05 03:16:10,359 - INFO - [diffusion][Epoch 8541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:10,361 - INFO - [diffusion][Epoch 8542] Epoch 8543/12000
2024-11-05 03:16:14,495 - INFO - [diffusion][Epoch 8542] diffusion training Loss: 0.05333685874938965
2024-11-05 03:16:14,497 - INFO - [diffusion][Epoch 8542] diffusion learning rate: 0.001
2024-11-05 03:16:14,500 - INFO - [diffusion][Epoch 8542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:14,501 - INFO - [diffusion][Epoch 8543] Epoch 8544/12000
2024-11-05 03:16:18,559 - INFO - [diffusion][Epoch 8543] diffusion training Loss: 0.05436092149466276
2024-11-05 03:16:18,560 - INFO - [diffusion][Epoch 8543] diffusion learning rate: 0.001
2024-11-05 03:16:18,562 - INFO - [diffusion][Epoch 8543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:18,564 - INFO - [diffusion][Epoch 8544] Epoch 8545/12000
2024-11-05 03:16:22,572 - INFO - [diffusion][Epoch 8544] diffusion training Loss: 0.04976116679608822
2024-11-05 03:16:22,574 - INFO - [diffusion][Epoch 8544] diffusion learning rate: 0.001
2024-11-05 03:16:22,576 - INFO - [diffusion][Epoch 8544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:22,577 - INFO - [diffusion][Epoch 8545] Epoch 8546/12000
2024-11-05 03:16:26,576 - INFO - [diffusion][Epoch 8545] diffusion training Loss: 0.06302380654960871
2024-11-05 03:16:26,578 - INFO - [diffusion][Epoch 8545] diffusion learning rate: 0.001
2024-11-05 03:16:26,580 - INFO - [diffusion][Epoch 8545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:26,581 - INFO - [diffusion][Epoch 8546] Epoch 8547/12000
2024-11-05 03:16:30,713 - INFO - [diffusion][Epoch 8546] diffusion training Loss: 0.055105866864323616
2024-11-05 03:16:30,715 - INFO - [diffusion][Epoch 8546] diffusion learning rate: 0.001
2024-11-05 03:16:30,754 - INFO - [diffusion][Epoch 8546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:30,755 - INFO - [diffusion][Epoch 8547] Epoch 8548/12000
2024-11-05 03:16:34,887 - INFO - [diffusion][Epoch 8547] diffusion training Loss: 0.05691182427108288
2024-11-05 03:16:34,890 - INFO - [diffusion][Epoch 8547] diffusion learning rate: 0.001
2024-11-05 03:16:34,892 - INFO - [diffusion][Epoch 8547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:34,893 - INFO - [diffusion][Epoch 8548] Epoch 8549/12000
2024-11-05 03:16:38,952 - INFO - [diffusion][Epoch 8548] diffusion training Loss: 0.05530844256281853
2024-11-05 03:16:38,954 - INFO - [diffusion][Epoch 8548] diffusion learning rate: 0.001
2024-11-05 03:16:38,955 - INFO - [diffusion][Epoch 8548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:38,957 - INFO - [diffusion][Epoch 8549] Epoch 8550/12000
2024-11-05 03:16:43,128 - INFO - [diffusion][Epoch 8549] diffusion training Loss: 0.05151298362761736
2024-11-05 03:16:43,130 - INFO - [diffusion][Epoch 8549] diffusion learning rate: 0.001
2024-11-05 03:16:43,132 - INFO - [diffusion][Epoch 8549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:43,133 - INFO - [diffusion][Epoch 8550] Epoch 8551/12000
2024-11-05 03:16:47,306 - INFO - [diffusion][Epoch 8550] diffusion training Loss: 0.05114685371518135
2024-11-05 03:16:47,308 - INFO - [diffusion][Epoch 8550] diffusion learning rate: 0.001
2024-11-05 03:16:47,309 - INFO - [diffusion][Epoch 8550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:47,311 - INFO - [diffusion][Epoch 8551] Epoch 8552/12000
2024-11-05 03:16:51,391 - INFO - [diffusion][Epoch 8551] diffusion training Loss: 0.05213426146656275
2024-11-05 03:16:51,393 - INFO - [diffusion][Epoch 8551] diffusion learning rate: 0.001
2024-11-05 03:16:51,395 - INFO - [diffusion][Epoch 8551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:51,396 - INFO - [diffusion][Epoch 8552] Epoch 8553/12000
2024-11-05 03:16:55,487 - INFO - [diffusion][Epoch 8552] diffusion training Loss: 0.058845351450145245
2024-11-05 03:16:55,489 - INFO - [diffusion][Epoch 8552] diffusion learning rate: 0.001
2024-11-05 03:16:55,491 - INFO - [diffusion][Epoch 8552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:55,492 - INFO - [diffusion][Epoch 8553] Epoch 8554/12000
2024-11-05 03:16:59,678 - INFO - [diffusion][Epoch 8553] diffusion training Loss: 0.05259001534432173
2024-11-05 03:16:59,680 - INFO - [diffusion][Epoch 8553] diffusion learning rate: 0.001
2024-11-05 03:16:59,682 - INFO - [diffusion][Epoch 8553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:16:59,683 - INFO - [diffusion][Epoch 8554] Epoch 8555/12000
2024-11-05 03:17:03,833 - INFO - [diffusion][Epoch 8554] diffusion training Loss: 0.05757489055395126
2024-11-05 03:17:03,835 - INFO - [diffusion][Epoch 8554] diffusion learning rate: 0.001
2024-11-05 03:17:03,837 - INFO - [diffusion][Epoch 8554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:03,838 - INFO - [diffusion][Epoch 8555] Epoch 8556/12000
2024-11-05 03:17:07,950 - INFO - [diffusion][Epoch 8555] diffusion training Loss: 0.05352492164820433
2024-11-05 03:17:07,951 - INFO - [diffusion][Epoch 8555] diffusion learning rate: 0.001
2024-11-05 03:17:07,953 - INFO - [diffusion][Epoch 8555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:07,954 - INFO - [diffusion][Epoch 8556] Epoch 8557/12000
2024-11-05 03:17:12,032 - INFO - [diffusion][Epoch 8556] diffusion training Loss: 0.062235310673713684
2024-11-05 03:17:12,034 - INFO - [diffusion][Epoch 8556] diffusion learning rate: 0.001
2024-11-05 03:17:12,036 - INFO - [diffusion][Epoch 8556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:12,037 - INFO - [diffusion][Epoch 8557] Epoch 8558/12000
2024-11-05 03:17:16,293 - INFO - [diffusion][Epoch 8557] diffusion training Loss: 0.053229885175824165
2024-11-05 03:17:16,295 - INFO - [diffusion][Epoch 8557] diffusion learning rate: 0.001
2024-11-05 03:17:16,297 - INFO - [diffusion][Epoch 8557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:16,298 - INFO - [diffusion][Epoch 8558] Epoch 8559/12000
2024-11-05 03:17:20,345 - INFO - [diffusion][Epoch 8558] diffusion training Loss: 0.0494082672521472
2024-11-05 03:17:20,347 - INFO - [diffusion][Epoch 8558] diffusion learning rate: 0.001
2024-11-05 03:17:20,349 - INFO - [diffusion][Epoch 8558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:20,350 - INFO - [diffusion][Epoch 8559] Epoch 8560/12000
2024-11-05 03:17:24,454 - INFO - [diffusion][Epoch 8559] diffusion training Loss: 0.052027562633156776
2024-11-05 03:17:24,456 - INFO - [diffusion][Epoch 8559] diffusion learning rate: 0.001
2024-11-05 03:17:24,458 - INFO - [diffusion][Epoch 8559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:24,459 - INFO - [diffusion][Epoch 8560] Epoch 8561/12000
2024-11-05 03:17:28,502 - INFO - [diffusion][Epoch 8560] diffusion training Loss: 0.05464761704206467
2024-11-05 03:17:28,504 - INFO - [diffusion][Epoch 8560] diffusion learning rate: 0.001
2024-11-05 03:17:28,505 - INFO - [diffusion][Epoch 8560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:28,506 - INFO - [diffusion][Epoch 8561] Epoch 8562/12000
2024-11-05 03:17:32,566 - INFO - [diffusion][Epoch 8561] diffusion training Loss: 0.05014162790030241
2024-11-05 03:17:32,568 - INFO - [diffusion][Epoch 8561] diffusion learning rate: 0.001
2024-11-05 03:17:32,570 - INFO - [diffusion][Epoch 8561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:32,571 - INFO - [diffusion][Epoch 8562] Epoch 8563/12000
2024-11-05 03:17:36,644 - INFO - [diffusion][Epoch 8562] diffusion training Loss: 0.05368944723159075
2024-11-05 03:17:36,648 - INFO - [diffusion][Epoch 8562] diffusion learning rate: 0.001
2024-11-05 03:17:36,650 - INFO - [diffusion][Epoch 8562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:36,651 - INFO - [diffusion][Epoch 8563] Epoch 8564/12000
2024-11-05 03:17:40,754 - INFO - [diffusion][Epoch 8563] diffusion training Loss: 0.05099309701472521
2024-11-05 03:17:40,759 - INFO - [diffusion][Epoch 8563] diffusion learning rate: 0.001
2024-11-05 03:17:40,761 - INFO - [diffusion][Epoch 8563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:40,763 - INFO - [diffusion][Epoch 8564] Epoch 8565/12000
2024-11-05 03:17:44,785 - INFO - [diffusion][Epoch 8564] diffusion training Loss: 0.0572367487475276
2024-11-05 03:17:44,787 - INFO - [diffusion][Epoch 8564] diffusion learning rate: 0.001
2024-11-05 03:17:44,789 - INFO - [diffusion][Epoch 8564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:44,791 - INFO - [diffusion][Epoch 8565] Epoch 8566/12000
2024-11-05 03:17:48,930 - INFO - [diffusion][Epoch 8565] diffusion training Loss: 0.05679512023925781
2024-11-05 03:17:48,932 - INFO - [diffusion][Epoch 8565] diffusion learning rate: 0.001
2024-11-05 03:17:48,934 - INFO - [diffusion][Epoch 8565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:48,936 - INFO - [diffusion][Epoch 8566] Epoch 8567/12000
2024-11-05 03:17:53,092 - INFO - [diffusion][Epoch 8566] diffusion training Loss: 0.05083882436156273
2024-11-05 03:17:53,094 - INFO - [diffusion][Epoch 8566] diffusion learning rate: 0.001
2024-11-05 03:17:53,096 - INFO - [diffusion][Epoch 8566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:53,098 - INFO - [diffusion][Epoch 8567] Epoch 8568/12000
2024-11-05 03:17:57,197 - INFO - [diffusion][Epoch 8567] diffusion training Loss: 0.05161557998508215
2024-11-05 03:17:57,199 - INFO - [diffusion][Epoch 8567] diffusion learning rate: 0.001
2024-11-05 03:17:57,201 - INFO - [diffusion][Epoch 8567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:17:57,202 - INFO - [diffusion][Epoch 8568] Epoch 8569/12000
2024-11-05 03:18:01,124 - INFO - [diffusion][Epoch 8568] diffusion training Loss: 0.05006144288927317
2024-11-05 03:18:01,127 - INFO - [diffusion][Epoch 8568] diffusion learning rate: 0.001
2024-11-05 03:18:01,129 - INFO - [diffusion][Epoch 8568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:01,130 - INFO - [diffusion][Epoch 8569] Epoch 8570/12000
2024-11-05 03:18:05,207 - INFO - [diffusion][Epoch 8569] diffusion training Loss: 0.047842834144830704
2024-11-05 03:18:05,209 - INFO - [diffusion][Epoch 8569] diffusion learning rate: 0.001
2024-11-05 03:18:05,211 - INFO - [diffusion][Epoch 8569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:05,212 - INFO - [diffusion][Epoch 8570] Epoch 8571/12000
2024-11-05 03:18:09,276 - INFO - [diffusion][Epoch 8570] diffusion training Loss: 0.051071276888251305
2024-11-05 03:18:09,278 - INFO - [diffusion][Epoch 8570] diffusion learning rate: 0.001
2024-11-05 03:18:09,280 - INFO - [diffusion][Epoch 8570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:09,281 - INFO - [diffusion][Epoch 8571] Epoch 8572/12000
2024-11-05 03:18:13,425 - INFO - [diffusion][Epoch 8571] diffusion training Loss: 0.048838659189641476
2024-11-05 03:18:13,427 - INFO - [diffusion][Epoch 8571] diffusion learning rate: 0.001
2024-11-05 03:18:13,429 - INFO - [diffusion][Epoch 8571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:13,430 - INFO - [diffusion][Epoch 8572] Epoch 8573/12000
2024-11-05 03:18:17,468 - INFO - [diffusion][Epoch 8572] diffusion training Loss: 0.05270519573241472
2024-11-05 03:18:17,470 - INFO - [diffusion][Epoch 8572] diffusion learning rate: 0.001
2024-11-05 03:18:17,473 - INFO - [diffusion][Epoch 8572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:17,474 - INFO - [diffusion][Epoch 8573] Epoch 8574/12000
2024-11-05 03:18:21,400 - INFO - [diffusion][Epoch 8573] diffusion training Loss: 0.048025269992649555
2024-11-05 03:18:21,402 - INFO - [diffusion][Epoch 8573] diffusion learning rate: 0.001
2024-11-05 03:18:21,404 - INFO - [diffusion][Epoch 8573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:21,405 - INFO - [diffusion][Epoch 8574] Epoch 8575/12000
2024-11-05 03:18:25,306 - INFO - [diffusion][Epoch 8574] diffusion training Loss: 0.05123679433017969
2024-11-05 03:18:25,308 - INFO - [diffusion][Epoch 8574] diffusion learning rate: 0.001
2024-11-05 03:18:25,310 - INFO - [diffusion][Epoch 8574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:25,312 - INFO - [diffusion][Epoch 8575] Epoch 8576/12000
2024-11-05 03:18:29,342 - INFO - [diffusion][Epoch 8575] diffusion training Loss: 0.05346609093248844
2024-11-05 03:18:29,344 - INFO - [diffusion][Epoch 8575] diffusion learning rate: 0.001
2024-11-05 03:18:29,346 - INFO - [diffusion][Epoch 8575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:29,348 - INFO - [diffusion][Epoch 8576] Epoch 8577/12000
2024-11-05 03:18:33,334 - INFO - [diffusion][Epoch 8576] diffusion training Loss: 0.048793865367770195
2024-11-05 03:18:33,336 - INFO - [diffusion][Epoch 8576] diffusion learning rate: 0.001
2024-11-05 03:18:33,339 - INFO - [diffusion][Epoch 8576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:33,341 - INFO - [diffusion][Epoch 8577] Epoch 8578/12000
2024-11-05 03:18:37,321 - INFO - [diffusion][Epoch 8577] diffusion training Loss: 0.056086636148393154
2024-11-05 03:18:37,324 - INFO - [diffusion][Epoch 8577] diffusion learning rate: 0.001
2024-11-05 03:18:37,326 - INFO - [diffusion][Epoch 8577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:37,328 - INFO - [diffusion][Epoch 8578] Epoch 8579/12000
2024-11-05 03:18:41,626 - INFO - [diffusion][Epoch 8578] diffusion training Loss: 0.05343694984912872
2024-11-05 03:18:41,628 - INFO - [diffusion][Epoch 8578] diffusion learning rate: 0.001
2024-11-05 03:18:41,630 - INFO - [diffusion][Epoch 8578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:41,631 - INFO - [diffusion][Epoch 8579] Epoch 8580/12000
2024-11-05 03:18:45,670 - INFO - [diffusion][Epoch 8579] diffusion training Loss: 0.05628676246851683
2024-11-05 03:18:45,672 - INFO - [diffusion][Epoch 8579] diffusion learning rate: 0.001
2024-11-05 03:18:45,674 - INFO - [diffusion][Epoch 8579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:45,675 - INFO - [diffusion][Epoch 8580] Epoch 8581/12000
2024-11-05 03:18:49,793 - INFO - [diffusion][Epoch 8580] diffusion training Loss: 0.055747684091329575
2024-11-05 03:18:49,795 - INFO - [diffusion][Epoch 8580] diffusion learning rate: 0.001
2024-11-05 03:18:49,797 - INFO - [diffusion][Epoch 8580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:49,798 - INFO - [diffusion][Epoch 8581] Epoch 8582/12000
2024-11-05 03:18:53,813 - INFO - [diffusion][Epoch 8581] diffusion training Loss: 0.04875761643052101
2024-11-05 03:18:53,815 - INFO - [diffusion][Epoch 8581] diffusion learning rate: 0.001
2024-11-05 03:18:53,816 - INFO - [diffusion][Epoch 8581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:53,818 - INFO - [diffusion][Epoch 8582] Epoch 8583/12000
2024-11-05 03:18:57,968 - INFO - [diffusion][Epoch 8582] diffusion training Loss: 0.053660652600228786
2024-11-05 03:18:57,970 - INFO - [diffusion][Epoch 8582] diffusion learning rate: 0.001
2024-11-05 03:18:57,972 - INFO - [diffusion][Epoch 8582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:18:57,973 - INFO - [diffusion][Epoch 8583] Epoch 8584/12000
2024-11-05 03:19:02,073 - INFO - [diffusion][Epoch 8583] diffusion training Loss: 0.05270883999764919
2024-11-05 03:19:02,075 - INFO - [diffusion][Epoch 8583] diffusion learning rate: 0.001
2024-11-05 03:19:02,077 - INFO - [diffusion][Epoch 8583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:02,078 - INFO - [diffusion][Epoch 8584] Epoch 8585/12000
2024-11-05 03:19:06,182 - INFO - [diffusion][Epoch 8584] diffusion training Loss: 0.05344048794358969
2024-11-05 03:19:06,185 - INFO - [diffusion][Epoch 8584] diffusion learning rate: 0.001
2024-11-05 03:19:06,187 - INFO - [diffusion][Epoch 8584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:06,188 - INFO - [diffusion][Epoch 8585] Epoch 8586/12000
2024-11-05 03:19:10,337 - INFO - [diffusion][Epoch 8585] diffusion training Loss: 0.05464718025177717
2024-11-05 03:19:10,339 - INFO - [diffusion][Epoch 8585] diffusion learning rate: 0.001
2024-11-05 03:19:10,340 - INFO - [diffusion][Epoch 8585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:10,342 - INFO - [diffusion][Epoch 8586] Epoch 8587/12000
2024-11-05 03:19:14,357 - INFO - [diffusion][Epoch 8586] diffusion training Loss: 0.05148984584957361
2024-11-05 03:19:14,359 - INFO - [diffusion][Epoch 8586] diffusion learning rate: 0.001
2024-11-05 03:19:14,361 - INFO - [diffusion][Epoch 8586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:14,362 - INFO - [diffusion][Epoch 8587] Epoch 8588/12000
2024-11-05 03:19:18,409 - INFO - [diffusion][Epoch 8587] diffusion training Loss: 0.047624568454921246
2024-11-05 03:19:18,411 - INFO - [diffusion][Epoch 8587] diffusion learning rate: 0.001
2024-11-05 03:19:18,413 - INFO - [diffusion][Epoch 8587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:18,414 - INFO - [diffusion][Epoch 8588] Epoch 8589/12000
2024-11-05 03:19:22,550 - INFO - [diffusion][Epoch 8588] diffusion training Loss: 0.04795723408460617
2024-11-05 03:19:22,552 - INFO - [diffusion][Epoch 8588] diffusion learning rate: 0.001
2024-11-05 03:19:22,554 - INFO - [diffusion][Epoch 8588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:22,556 - INFO - [diffusion][Epoch 8589] Epoch 8590/12000
2024-11-05 03:19:26,669 - INFO - [diffusion][Epoch 8589] diffusion training Loss: 0.054382070899009705
2024-11-05 03:19:26,671 - INFO - [diffusion][Epoch 8589] diffusion learning rate: 0.001
2024-11-05 03:19:26,673 - INFO - [diffusion][Epoch 8589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:26,674 - INFO - [diffusion][Epoch 8590] Epoch 8591/12000
2024-11-05 03:19:30,771 - INFO - [diffusion][Epoch 8590] diffusion training Loss: 0.05236854497343302
2024-11-05 03:19:30,773 - INFO - [diffusion][Epoch 8590] diffusion learning rate: 0.001
2024-11-05 03:19:30,775 - INFO - [diffusion][Epoch 8590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:30,776 - INFO - [diffusion][Epoch 8591] Epoch 8592/12000
2024-11-05 03:19:34,821 - INFO - [diffusion][Epoch 8591] diffusion training Loss: 0.051584068685770035
2024-11-05 03:19:34,824 - INFO - [diffusion][Epoch 8591] diffusion learning rate: 0.001
2024-11-05 03:19:34,826 - INFO - [diffusion][Epoch 8591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:34,827 - INFO - [diffusion][Epoch 8592] Epoch 8593/12000
2024-11-05 03:19:38,953 - INFO - [diffusion][Epoch 8592] diffusion training Loss: 0.05152559746056795
2024-11-05 03:19:38,956 - INFO - [diffusion][Epoch 8592] diffusion learning rate: 0.001
2024-11-05 03:19:38,958 - INFO - [diffusion][Epoch 8592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:38,959 - INFO - [diffusion][Epoch 8593] Epoch 8594/12000
2024-11-05 03:19:43,005 - INFO - [diffusion][Epoch 8593] diffusion training Loss: 0.05330989696085453
2024-11-05 03:19:43,007 - INFO - [diffusion][Epoch 8593] diffusion learning rate: 0.001
2024-11-05 03:19:43,008 - INFO - [diffusion][Epoch 8593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:43,010 - INFO - [diffusion][Epoch 8594] Epoch 8595/12000
2024-11-05 03:19:47,120 - INFO - [diffusion][Epoch 8594] diffusion training Loss: 0.052709585055708885
2024-11-05 03:19:47,123 - INFO - [diffusion][Epoch 8594] diffusion learning rate: 0.001
2024-11-05 03:19:47,124 - INFO - [diffusion][Epoch 8594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:47,125 - INFO - [diffusion][Epoch 8595] Epoch 8596/12000
2024-11-05 03:19:51,254 - INFO - [diffusion][Epoch 8595] diffusion training Loss: 0.058460372500121593
2024-11-05 03:19:51,256 - INFO - [diffusion][Epoch 8595] diffusion learning rate: 0.001
2024-11-05 03:19:51,258 - INFO - [diffusion][Epoch 8595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:51,259 - INFO - [diffusion][Epoch 8596] Epoch 8597/12000
2024-11-05 03:19:55,391 - INFO - [diffusion][Epoch 8596] diffusion training Loss: 0.0516390698030591
2024-11-05 03:19:55,393 - INFO - [diffusion][Epoch 8596] diffusion learning rate: 0.001
2024-11-05 03:19:55,395 - INFO - [diffusion][Epoch 8596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:55,397 - INFO - [diffusion][Epoch 8597] Epoch 8598/12000
2024-11-05 03:19:59,540 - INFO - [diffusion][Epoch 8597] diffusion training Loss: 0.05603348556905985
2024-11-05 03:19:59,542 - INFO - [diffusion][Epoch 8597] diffusion learning rate: 0.001
2024-11-05 03:19:59,544 - INFO - [diffusion][Epoch 8597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:19:59,545 - INFO - [diffusion][Epoch 8598] Epoch 8599/12000
2024-11-05 03:20:03,619 - INFO - [diffusion][Epoch 8598] diffusion training Loss: 0.05652855336666107
2024-11-05 03:20:03,621 - INFO - [diffusion][Epoch 8598] diffusion learning rate: 0.001
2024-11-05 03:20:03,623 - INFO - [diffusion][Epoch 8598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:03,625 - INFO - [diffusion][Epoch 8599] Epoch 8600/12000
2024-11-05 03:20:07,841 - INFO - [diffusion][Epoch 8599] diffusion training Loss: 0.053831130266189575
2024-11-05 03:20:07,844 - INFO - [diffusion][Epoch 8599] diffusion learning rate: 0.001
2024-11-05 03:20:07,846 - INFO - [diffusion][Epoch 8599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:07,847 - INFO - [diffusion][Epoch 8600] Epoch 8601/12000
2024-11-05 03:20:11,782 - INFO - [diffusion][Epoch 8600] diffusion training Loss: 0.05545058660209179
2024-11-05 03:20:11,783 - INFO - [diffusion][Epoch 8600] diffusion learning rate: 0.001
2024-11-05 03:20:11,785 - INFO - [diffusion][Epoch 8600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:11,786 - INFO - [diffusion][Epoch 8601] Epoch 8602/12000
2024-11-05 03:20:15,781 - INFO - [diffusion][Epoch 8601] diffusion training Loss: 0.05471110064536333
2024-11-05 03:20:15,783 - INFO - [diffusion][Epoch 8601] diffusion learning rate: 0.001
2024-11-05 03:20:15,785 - INFO - [diffusion][Epoch 8601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:15,786 - INFO - [diffusion][Epoch 8602] Epoch 8603/12000
2024-11-05 03:20:19,875 - INFO - [diffusion][Epoch 8602] diffusion training Loss: 0.05190252512693405
2024-11-05 03:20:19,877 - INFO - [diffusion][Epoch 8602] diffusion learning rate: 0.001
2024-11-05 03:20:19,879 - INFO - [diffusion][Epoch 8602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:19,880 - INFO - [diffusion][Epoch 8603] Epoch 8604/12000
2024-11-05 03:20:24,025 - INFO - [diffusion][Epoch 8603] diffusion training Loss: 0.04822211805731058
2024-11-05 03:20:24,027 - INFO - [diffusion][Epoch 8603] diffusion learning rate: 0.001
2024-11-05 03:20:24,030 - INFO - [diffusion][Epoch 8603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:24,031 - INFO - [diffusion][Epoch 8604] Epoch 8605/12000
2024-11-05 03:20:28,158 - INFO - [diffusion][Epoch 8604] diffusion training Loss: 0.050402721390128136
2024-11-05 03:20:28,160 - INFO - [diffusion][Epoch 8604] diffusion learning rate: 0.001
2024-11-05 03:20:28,162 - INFO - [diffusion][Epoch 8604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:28,163 - INFO - [diffusion][Epoch 8605] Epoch 8606/12000
2024-11-05 03:20:32,224 - INFO - [diffusion][Epoch 8605] diffusion training Loss: 0.04651734884828329
2024-11-05 03:20:32,226 - INFO - [diffusion][Epoch 8605] diffusion learning rate: 0.001
2024-11-05 03:20:32,228 - INFO - [diffusion][Epoch 8605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:32,229 - INFO - [diffusion][Epoch 8606] Epoch 8607/12000
2024-11-05 03:20:36,299 - INFO - [diffusion][Epoch 8606] diffusion training Loss: 0.051226384937763214
2024-11-05 03:20:36,301 - INFO - [diffusion][Epoch 8606] diffusion learning rate: 0.001
2024-11-05 03:20:36,302 - INFO - [diffusion][Epoch 8606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:36,304 - INFO - [diffusion][Epoch 8607] Epoch 8608/12000
2024-11-05 03:20:40,249 - INFO - [diffusion][Epoch 8607] diffusion training Loss: 0.04790299944579601
2024-11-05 03:20:40,252 - INFO - [diffusion][Epoch 8607] diffusion learning rate: 0.001
2024-11-05 03:20:40,254 - INFO - [diffusion][Epoch 8607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:40,255 - INFO - [diffusion][Epoch 8608] Epoch 8609/12000
2024-11-05 03:20:44,382 - INFO - [diffusion][Epoch 8608] diffusion training Loss: 0.056272074580192566
2024-11-05 03:20:44,384 - INFO - [diffusion][Epoch 8608] diffusion learning rate: 0.001
2024-11-05 03:20:44,385 - INFO - [diffusion][Epoch 8608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:44,387 - INFO - [diffusion][Epoch 8609] Epoch 8610/12000
2024-11-05 03:20:48,450 - INFO - [diffusion][Epoch 8609] diffusion training Loss: 0.05252181552350521
2024-11-05 03:20:48,452 - INFO - [diffusion][Epoch 8609] diffusion learning rate: 0.001
2024-11-05 03:20:48,454 - INFO - [diffusion][Epoch 8609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:48,455 - INFO - [diffusion][Epoch 8610] Epoch 8611/12000
2024-11-05 03:20:52,637 - INFO - [diffusion][Epoch 8610] diffusion training Loss: 0.05306782480329275
2024-11-05 03:20:52,640 - INFO - [diffusion][Epoch 8610] diffusion learning rate: 0.001
2024-11-05 03:20:52,642 - INFO - [diffusion][Epoch 8610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:52,643 - INFO - [diffusion][Epoch 8611] Epoch 8612/12000
2024-11-05 03:20:56,787 - INFO - [diffusion][Epoch 8611] diffusion training Loss: 0.05135144479572773
2024-11-05 03:20:56,789 - INFO - [diffusion][Epoch 8611] diffusion learning rate: 0.001
2024-11-05 03:20:56,790 - INFO - [diffusion][Epoch 8611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:20:56,792 - INFO - [diffusion][Epoch 8612] Epoch 8613/12000
2024-11-05 03:21:00,847 - INFO - [diffusion][Epoch 8612] diffusion training Loss: 0.049332582391798496
2024-11-05 03:21:00,849 - INFO - [diffusion][Epoch 8612] diffusion learning rate: 0.001
2024-11-05 03:21:00,850 - INFO - [diffusion][Epoch 8612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:00,851 - INFO - [diffusion][Epoch 8613] Epoch 8614/12000
2024-11-05 03:21:04,921 - INFO - [diffusion][Epoch 8613] diffusion training Loss: 0.05322262458503246
2024-11-05 03:21:04,923 - INFO - [diffusion][Epoch 8613] diffusion learning rate: 0.001
2024-11-05 03:21:04,925 - INFO - [diffusion][Epoch 8613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:04,926 - INFO - [diffusion][Epoch 8614] Epoch 8615/12000
2024-11-05 03:21:08,861 - INFO - [diffusion][Epoch 8614] diffusion training Loss: 0.058011448942124844
2024-11-05 03:21:08,863 - INFO - [diffusion][Epoch 8614] diffusion learning rate: 0.001
2024-11-05 03:21:08,865 - INFO - [diffusion][Epoch 8614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:08,866 - INFO - [diffusion][Epoch 8615] Epoch 8616/12000
2024-11-05 03:21:12,781 - INFO - [diffusion][Epoch 8615] diffusion training Loss: 0.05882761161774397
2024-11-05 03:21:12,783 - INFO - [diffusion][Epoch 8615] diffusion learning rate: 0.001
2024-11-05 03:21:12,785 - INFO - [diffusion][Epoch 8615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:12,786 - INFO - [diffusion][Epoch 8616] Epoch 8617/12000
2024-11-05 03:21:16,957 - INFO - [diffusion][Epoch 8616] diffusion training Loss: 0.053229681216180325
2024-11-05 03:21:16,959 - INFO - [diffusion][Epoch 8616] diffusion learning rate: 0.001
2024-11-05 03:21:16,961 - INFO - [diffusion][Epoch 8616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:16,962 - INFO - [diffusion][Epoch 8617] Epoch 8618/12000
2024-11-05 03:21:21,147 - INFO - [diffusion][Epoch 8617] diffusion training Loss: 0.053715349175035954
2024-11-05 03:21:21,149 - INFO - [diffusion][Epoch 8617] diffusion learning rate: 0.001
2024-11-05 03:21:21,151 - INFO - [diffusion][Epoch 8617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:21,152 - INFO - [diffusion][Epoch 8618] Epoch 8619/12000
2024-11-05 03:21:25,297 - INFO - [diffusion][Epoch 8618] diffusion training Loss: 0.05352915357798338
2024-11-05 03:21:25,299 - INFO - [diffusion][Epoch 8618] diffusion learning rate: 0.001
2024-11-05 03:21:25,301 - INFO - [diffusion][Epoch 8618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:25,302 - INFO - [diffusion][Epoch 8619] Epoch 8620/12000
2024-11-05 03:21:29,437 - INFO - [diffusion][Epoch 8619] diffusion training Loss: 0.05108143761754036
2024-11-05 03:21:29,439 - INFO - [diffusion][Epoch 8619] diffusion learning rate: 0.001
2024-11-05 03:21:29,441 - INFO - [diffusion][Epoch 8619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:29,443 - INFO - [diffusion][Epoch 8620] Epoch 8621/12000
2024-11-05 03:21:33,768 - INFO - [diffusion][Epoch 8620] diffusion training Loss: 0.05349912215024233
2024-11-05 03:21:33,770 - INFO - [diffusion][Epoch 8620] diffusion learning rate: 0.001
2024-11-05 03:21:33,772 - INFO - [diffusion][Epoch 8620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:33,773 - INFO - [diffusion][Epoch 8621] Epoch 8622/12000
2024-11-05 03:21:37,897 - INFO - [diffusion][Epoch 8621] diffusion training Loss: 0.05049227736890316
2024-11-05 03:21:37,899 - INFO - [diffusion][Epoch 8621] diffusion learning rate: 0.001
2024-11-05 03:21:37,901 - INFO - [diffusion][Epoch 8621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:37,902 - INFO - [diffusion][Epoch 8622] Epoch 8623/12000
2024-11-05 03:21:41,864 - INFO - [diffusion][Epoch 8622] diffusion training Loss: 0.05022791586816311
2024-11-05 03:21:41,870 - INFO - [diffusion][Epoch 8622] diffusion learning rate: 0.001
2024-11-05 03:21:41,872 - INFO - [diffusion][Epoch 8622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:41,874 - INFO - [diffusion][Epoch 8623] Epoch 8624/12000
2024-11-05 03:21:45,971 - INFO - [diffusion][Epoch 8623] diffusion training Loss: 0.053933789022266865
2024-11-05 03:21:45,973 - INFO - [diffusion][Epoch 8623] diffusion learning rate: 0.001
2024-11-05 03:21:45,975 - INFO - [diffusion][Epoch 8623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:45,976 - INFO - [diffusion][Epoch 8624] Epoch 8625/12000
2024-11-05 03:21:50,062 - INFO - [diffusion][Epoch 8624] diffusion training Loss: 0.05063364841043949
2024-11-05 03:21:50,064 - INFO - [diffusion][Epoch 8624] diffusion learning rate: 0.001
2024-11-05 03:21:50,066 - INFO - [diffusion][Epoch 8624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:50,067 - INFO - [diffusion][Epoch 8625] Epoch 8626/12000
2024-11-05 03:21:54,070 - INFO - [diffusion][Epoch 8625] diffusion training Loss: 0.053300194442272186
2024-11-05 03:21:54,072 - INFO - [diffusion][Epoch 8625] diffusion learning rate: 0.001
2024-11-05 03:21:54,074 - INFO - [diffusion][Epoch 8625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:54,075 - INFO - [diffusion][Epoch 8626] Epoch 8627/12000
2024-11-05 03:21:58,144 - INFO - [diffusion][Epoch 8626] diffusion training Loss: 0.04990098997950554
2024-11-05 03:21:58,146 - INFO - [diffusion][Epoch 8626] diffusion learning rate: 0.001
2024-11-05 03:21:58,148 - INFO - [diffusion][Epoch 8626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:21:58,149 - INFO - [diffusion][Epoch 8627] Epoch 8628/12000
2024-11-05 03:22:02,214 - INFO - [diffusion][Epoch 8627] diffusion training Loss: 0.052782151848077774
2024-11-05 03:22:02,216 - INFO - [diffusion][Epoch 8627] diffusion learning rate: 0.001
2024-11-05 03:22:02,217 - INFO - [diffusion][Epoch 8627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:02,219 - INFO - [diffusion][Epoch 8628] Epoch 8629/12000
2024-11-05 03:22:06,338 - INFO - [diffusion][Epoch 8628] diffusion training Loss: 0.05389189999550581
2024-11-05 03:22:06,340 - INFO - [diffusion][Epoch 8628] diffusion learning rate: 0.001
2024-11-05 03:22:06,342 - INFO - [diffusion][Epoch 8628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:06,343 - INFO - [diffusion][Epoch 8629] Epoch 8630/12000
2024-11-05 03:22:10,485 - INFO - [diffusion][Epoch 8629] diffusion training Loss: 0.05063272546976805
2024-11-05 03:22:10,487 - INFO - [diffusion][Epoch 8629] diffusion learning rate: 0.001
2024-11-05 03:22:10,488 - INFO - [diffusion][Epoch 8629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:10,490 - INFO - [diffusion][Epoch 8630] Epoch 8631/12000
2024-11-05 03:22:14,432 - INFO - [diffusion][Epoch 8630] diffusion training Loss: 0.053472187370061874
2024-11-05 03:22:14,435 - INFO - [diffusion][Epoch 8630] diffusion learning rate: 0.001
2024-11-05 03:22:14,437 - INFO - [diffusion][Epoch 8630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:14,439 - INFO - [diffusion][Epoch 8631] Epoch 8632/12000
2024-11-05 03:22:18,458 - INFO - [diffusion][Epoch 8631] diffusion training Loss: 0.04974746145308018
2024-11-05 03:22:18,460 - INFO - [diffusion][Epoch 8631] diffusion learning rate: 0.001
2024-11-05 03:22:18,462 - INFO - [diffusion][Epoch 8631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:18,463 - INFO - [diffusion][Epoch 8632] Epoch 8633/12000
2024-11-05 03:22:22,532 - INFO - [diffusion][Epoch 8632] diffusion training Loss: 0.052309148013591766
2024-11-05 03:22:22,534 - INFO - [diffusion][Epoch 8632] diffusion learning rate: 0.001
2024-11-05 03:22:22,536 - INFO - [diffusion][Epoch 8632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:22,537 - INFO - [diffusion][Epoch 8633] Epoch 8634/12000
2024-11-05 03:22:26,496 - INFO - [diffusion][Epoch 8633] diffusion training Loss: 0.05645747575908899
2024-11-05 03:22:26,498 - INFO - [diffusion][Epoch 8633] diffusion learning rate: 0.001
2024-11-05 03:22:26,499 - INFO - [diffusion][Epoch 8633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:26,501 - INFO - [diffusion][Epoch 8634] Epoch 8635/12000
2024-11-05 03:22:30,509 - INFO - [diffusion][Epoch 8634] diffusion training Loss: 0.05225591082125902
2024-11-05 03:22:30,511 - INFO - [diffusion][Epoch 8634] diffusion learning rate: 0.001
2024-11-05 03:22:30,512 - INFO - [diffusion][Epoch 8634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:30,514 - INFO - [diffusion][Epoch 8635] Epoch 8636/12000
2024-11-05 03:22:34,506 - INFO - [diffusion][Epoch 8635] diffusion training Loss: 0.05296104773879051
2024-11-05 03:22:34,508 - INFO - [diffusion][Epoch 8635] diffusion learning rate: 0.001
2024-11-05 03:22:34,510 - INFO - [diffusion][Epoch 8635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:34,511 - INFO - [diffusion][Epoch 8636] Epoch 8637/12000
2024-11-05 03:22:38,486 - INFO - [diffusion][Epoch 8636] diffusion training Loss: 0.05248730443418026
2024-11-05 03:22:38,489 - INFO - [diffusion][Epoch 8636] diffusion learning rate: 0.001
2024-11-05 03:22:38,491 - INFO - [diffusion][Epoch 8636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:38,493 - INFO - [diffusion][Epoch 8637] Epoch 8638/12000
2024-11-05 03:22:42,526 - INFO - [diffusion][Epoch 8637] diffusion training Loss: 0.05149262957274914
2024-11-05 03:22:42,528 - INFO - [diffusion][Epoch 8637] diffusion learning rate: 0.001
2024-11-05 03:22:42,530 - INFO - [diffusion][Epoch 8637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:42,531 - INFO - [diffusion][Epoch 8638] Epoch 8639/12000
2024-11-05 03:22:46,691 - INFO - [diffusion][Epoch 8638] diffusion training Loss: 0.0540683688595891
2024-11-05 03:22:46,693 - INFO - [diffusion][Epoch 8638] diffusion learning rate: 0.001
2024-11-05 03:22:46,695 - INFO - [diffusion][Epoch 8638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:46,696 - INFO - [diffusion][Epoch 8639] Epoch 8640/12000
2024-11-05 03:22:50,855 - INFO - [diffusion][Epoch 8639] diffusion training Loss: 0.05737132765352726
2024-11-05 03:22:50,857 - INFO - [diffusion][Epoch 8639] diffusion learning rate: 0.001
2024-11-05 03:22:50,859 - INFO - [diffusion][Epoch 8639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:50,860 - INFO - [diffusion][Epoch 8640] Epoch 8641/12000
2024-11-05 03:22:55,147 - INFO - [diffusion][Epoch 8640] diffusion training Loss: 0.0520592276006937
2024-11-05 03:22:55,149 - INFO - [diffusion][Epoch 8640] diffusion learning rate: 0.001
2024-11-05 03:22:55,150 - INFO - [diffusion][Epoch 8640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:55,151 - INFO - [diffusion][Epoch 8641] Epoch 8642/12000
2024-11-05 03:22:59,456 - INFO - [diffusion][Epoch 8641] diffusion training Loss: 0.05167859327048063
2024-11-05 03:22:59,458 - INFO - [diffusion][Epoch 8641] diffusion learning rate: 0.001
2024-11-05 03:22:59,459 - INFO - [diffusion][Epoch 8641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:22:59,461 - INFO - [diffusion][Epoch 8642] Epoch 8643/12000
2024-11-05 03:23:03,583 - INFO - [diffusion][Epoch 8642] diffusion training Loss: 0.0500132804736495
2024-11-05 03:23:03,584 - INFO - [diffusion][Epoch 8642] diffusion learning rate: 0.001
2024-11-05 03:23:03,586 - INFO - [diffusion][Epoch 8642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:03,587 - INFO - [diffusion][Epoch 8643] Epoch 8644/12000
2024-11-05 03:23:07,571 - INFO - [diffusion][Epoch 8643] diffusion training Loss: 0.052240125834941864
2024-11-05 03:23:07,574 - INFO - [diffusion][Epoch 8643] diffusion learning rate: 0.001
2024-11-05 03:23:07,601 - INFO - [diffusion][Epoch 8643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:07,602 - INFO - [diffusion][Epoch 8644] Epoch 8645/12000
2024-11-05 03:23:11,587 - INFO - [diffusion][Epoch 8644] diffusion training Loss: 0.05487367603927851
2024-11-05 03:23:11,589 - INFO - [diffusion][Epoch 8644] diffusion learning rate: 0.001
2024-11-05 03:23:11,591 - INFO - [diffusion][Epoch 8644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:11,592 - INFO - [diffusion][Epoch 8645] Epoch 8646/12000
2024-11-05 03:23:15,662 - INFO - [diffusion][Epoch 8645] diffusion training Loss: 0.05044178385287523
2024-11-05 03:23:15,664 - INFO - [diffusion][Epoch 8645] diffusion learning rate: 0.001
2024-11-05 03:23:15,667 - INFO - [diffusion][Epoch 8645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:15,668 - INFO - [diffusion][Epoch 8646] Epoch 8647/12000
2024-11-05 03:23:19,705 - INFO - [diffusion][Epoch 8646] diffusion training Loss: 0.046205081045627594
2024-11-05 03:23:19,708 - INFO - [diffusion][Epoch 8646] diffusion learning rate: 0.001
2024-11-05 03:23:19,709 - INFO - [diffusion][Epoch 8646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:19,711 - INFO - [diffusion][Epoch 8647] Epoch 8648/12000
2024-11-05 03:23:23,735 - INFO - [diffusion][Epoch 8647] diffusion training Loss: 0.04814428091049194
2024-11-05 03:23:23,737 - INFO - [diffusion][Epoch 8647] diffusion learning rate: 0.001
2024-11-05 03:23:23,739 - INFO - [diffusion][Epoch 8647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:23,740 - INFO - [diffusion][Epoch 8648] Epoch 8649/12000
2024-11-05 03:23:27,819 - INFO - [diffusion][Epoch 8648] diffusion training Loss: 0.04596459120512009
2024-11-05 03:23:27,821 - INFO - [diffusion][Epoch 8648] diffusion learning rate: 0.001
2024-11-05 03:23:27,823 - INFO - [diffusion][Epoch 8648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:27,824 - INFO - [diffusion][Epoch 8649] Epoch 8650/12000
2024-11-05 03:23:31,936 - INFO - [diffusion][Epoch 8649] diffusion training Loss: 0.05287407245486975
2024-11-05 03:23:31,938 - INFO - [diffusion][Epoch 8649] diffusion learning rate: 0.001
2024-11-05 03:23:31,939 - INFO - [diffusion][Epoch 8649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:31,941 - INFO - [diffusion][Epoch 8650] Epoch 8651/12000
2024-11-05 03:23:36,030 - INFO - [diffusion][Epoch 8650] diffusion training Loss: 0.04724578745663166
2024-11-05 03:23:36,038 - INFO - [diffusion][Epoch 8650] diffusion learning rate: 0.001
2024-11-05 03:23:36,040 - INFO - [diffusion][Epoch 8650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:36,041 - INFO - [diffusion][Epoch 8651] Epoch 8652/12000
2024-11-05 03:23:40,195 - INFO - [diffusion][Epoch 8651] diffusion training Loss: 0.050202020443975925
2024-11-05 03:23:40,198 - INFO - [diffusion][Epoch 8651] diffusion learning rate: 0.001
2024-11-05 03:23:40,200 - INFO - [diffusion][Epoch 8651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:40,201 - INFO - [diffusion][Epoch 8652] Epoch 8653/12000
2024-11-05 03:23:44,388 - INFO - [diffusion][Epoch 8652] diffusion training Loss: 0.05815378576517105
2024-11-05 03:23:44,391 - INFO - [diffusion][Epoch 8652] diffusion learning rate: 0.001
2024-11-05 03:23:44,393 - INFO - [diffusion][Epoch 8652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:44,394 - INFO - [diffusion][Epoch 8653] Epoch 8654/12000
2024-11-05 03:23:48,552 - INFO - [diffusion][Epoch 8653] diffusion training Loss: 0.04960224684327841
2024-11-05 03:23:48,554 - INFO - [diffusion][Epoch 8653] diffusion learning rate: 0.001
2024-11-05 03:23:48,556 - INFO - [diffusion][Epoch 8653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:48,557 - INFO - [diffusion][Epoch 8654] Epoch 8655/12000
2024-11-05 03:23:52,699 - INFO - [diffusion][Epoch 8654] diffusion training Loss: 0.05110273975878954
2024-11-05 03:23:52,701 - INFO - [diffusion][Epoch 8654] diffusion learning rate: 0.001
2024-11-05 03:23:52,708 - INFO - [diffusion][Epoch 8654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:52,709 - INFO - [diffusion][Epoch 8655] Epoch 8656/12000
2024-11-05 03:23:56,908 - INFO - [diffusion][Epoch 8655] diffusion training Loss: 0.0528672207146883
2024-11-05 03:23:56,911 - INFO - [diffusion][Epoch 8655] diffusion learning rate: 0.001
2024-11-05 03:23:56,950 - INFO - [diffusion][Epoch 8655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:23:56,952 - INFO - [diffusion][Epoch 8656] Epoch 8657/12000
2024-11-05 03:24:00,909 - INFO - [diffusion][Epoch 8656] diffusion training Loss: 0.051710693165659904
2024-11-05 03:24:00,911 - INFO - [diffusion][Epoch 8656] diffusion learning rate: 0.001
2024-11-05 03:24:00,913 - INFO - [diffusion][Epoch 8656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:00,915 - INFO - [diffusion][Epoch 8657] Epoch 8658/12000
2024-11-05 03:24:05,019 - INFO - [diffusion][Epoch 8657] diffusion training Loss: 0.051555363461375237
2024-11-05 03:24:05,021 - INFO - [diffusion][Epoch 8657] diffusion learning rate: 0.001
2024-11-05 03:24:05,023 - INFO - [diffusion][Epoch 8657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:05,025 - INFO - [diffusion][Epoch 8658] Epoch 8659/12000
2024-11-05 03:24:08,996 - INFO - [diffusion][Epoch 8658] diffusion training Loss: 0.05550230201333761
2024-11-05 03:24:08,998 - INFO - [diffusion][Epoch 8658] diffusion learning rate: 0.001
2024-11-05 03:24:09,000 - INFO - [diffusion][Epoch 8658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:09,001 - INFO - [diffusion][Epoch 8659] Epoch 8660/12000
2024-11-05 03:24:13,009 - INFO - [diffusion][Epoch 8659] diffusion training Loss: 0.051028914749622345
2024-11-05 03:24:13,159 - INFO - [diffusion][Epoch 8659] diffusion learning rate: 0.001
2024-11-05 03:24:13,161 - INFO - [diffusion][Epoch 8659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:13,163 - INFO - [diffusion][Epoch 8660] Epoch 8661/12000
2024-11-05 03:24:17,313 - INFO - [diffusion][Epoch 8660] diffusion training Loss: 0.05631991755217314
2024-11-05 03:24:17,315 - INFO - [diffusion][Epoch 8660] diffusion learning rate: 0.001
2024-11-05 03:24:17,317 - INFO - [diffusion][Epoch 8660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:17,318 - INFO - [diffusion][Epoch 8661] Epoch 8662/12000
2024-11-05 03:24:21,412 - INFO - [diffusion][Epoch 8661] diffusion training Loss: 0.05377481784671545
2024-11-05 03:24:21,414 - INFO - [diffusion][Epoch 8661] diffusion learning rate: 0.001
2024-11-05 03:24:21,416 - INFO - [diffusion][Epoch 8661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:21,417 - INFO - [diffusion][Epoch 8662] Epoch 8663/12000
2024-11-05 03:24:25,700 - INFO - [diffusion][Epoch 8662] diffusion training Loss: 0.04811558220535517
2024-11-05 03:24:25,701 - INFO - [diffusion][Epoch 8662] diffusion learning rate: 0.001
2024-11-05 03:24:25,703 - INFO - [diffusion][Epoch 8662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:25,705 - INFO - [diffusion][Epoch 8663] Epoch 8664/12000
2024-11-05 03:24:29,735 - INFO - [diffusion][Epoch 8663] diffusion training Loss: 0.0541761564090848
2024-11-05 03:24:29,737 - INFO - [diffusion][Epoch 8663] diffusion learning rate: 0.001
2024-11-05 03:24:29,739 - INFO - [diffusion][Epoch 8663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:29,740 - INFO - [diffusion][Epoch 8664] Epoch 8665/12000
2024-11-05 03:24:33,888 - INFO - [diffusion][Epoch 8664] diffusion training Loss: 0.05375508777797222
2024-11-05 03:24:33,890 - INFO - [diffusion][Epoch 8664] diffusion learning rate: 0.001
2024-11-05 03:24:33,892 - INFO - [diffusion][Epoch 8664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:33,893 - INFO - [diffusion][Epoch 8665] Epoch 8666/12000
2024-11-05 03:24:37,966 - INFO - [diffusion][Epoch 8665] diffusion training Loss: 0.0547984829172492
2024-11-05 03:24:37,968 - INFO - [diffusion][Epoch 8665] diffusion learning rate: 0.001
2024-11-05 03:24:37,969 - INFO - [diffusion][Epoch 8665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:37,970 - INFO - [diffusion][Epoch 8666] Epoch 8667/12000
2024-11-05 03:24:42,134 - INFO - [diffusion][Epoch 8666] diffusion training Loss: 0.049616940319538116
2024-11-05 03:24:42,137 - INFO - [diffusion][Epoch 8666] diffusion learning rate: 0.001
2024-11-05 03:24:42,139 - INFO - [diffusion][Epoch 8666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:42,140 - INFO - [diffusion][Epoch 8667] Epoch 8668/12000
2024-11-05 03:24:46,175 - INFO - [diffusion][Epoch 8667] diffusion training Loss: 0.05377060826867819
2024-11-05 03:24:46,190 - INFO - [diffusion][Epoch 8667] diffusion learning rate: 0.001
2024-11-05 03:24:46,192 - INFO - [diffusion][Epoch 8667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:46,193 - INFO - [diffusion][Epoch 8668] Epoch 8669/12000
2024-11-05 03:24:50,300 - INFO - [diffusion][Epoch 8668] diffusion training Loss: 0.05449069291353226
2024-11-05 03:24:50,302 - INFO - [diffusion][Epoch 8668] diffusion learning rate: 0.001
2024-11-05 03:24:50,304 - INFO - [diffusion][Epoch 8668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:50,306 - INFO - [diffusion][Epoch 8669] Epoch 8670/12000
2024-11-05 03:24:54,306 - INFO - [diffusion][Epoch 8669] diffusion training Loss: 0.04960700962692499
2024-11-05 03:24:54,308 - INFO - [diffusion][Epoch 8669] diffusion learning rate: 0.001
2024-11-05 03:24:54,309 - INFO - [diffusion][Epoch 8669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:54,311 - INFO - [diffusion][Epoch 8670] Epoch 8671/12000
2024-11-05 03:24:58,331 - INFO - [diffusion][Epoch 8670] diffusion training Loss: 0.05763827636837959
2024-11-05 03:24:58,333 - INFO - [diffusion][Epoch 8670] diffusion learning rate: 0.001
2024-11-05 03:24:58,335 - INFO - [diffusion][Epoch 8670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:24:58,336 - INFO - [diffusion][Epoch 8671] Epoch 8672/12000
2024-11-05 03:25:02,390 - INFO - [diffusion][Epoch 8671] diffusion training Loss: 0.049483160488307476
2024-11-05 03:25:02,392 - INFO - [diffusion][Epoch 8671] diffusion learning rate: 0.001
2024-11-05 03:25:02,394 - INFO - [diffusion][Epoch 8671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:02,395 - INFO - [diffusion][Epoch 8672] Epoch 8673/12000
2024-11-05 03:25:06,432 - INFO - [diffusion][Epoch 8672] diffusion training Loss: 0.04652960505336523
2024-11-05 03:25:06,434 - INFO - [diffusion][Epoch 8672] diffusion learning rate: 0.001
2024-11-05 03:25:06,436 - INFO - [diffusion][Epoch 8672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:06,437 - INFO - [diffusion][Epoch 8673] Epoch 8674/12000
2024-11-05 03:25:10,479 - INFO - [diffusion][Epoch 8673] diffusion training Loss: 0.04871163424104452
2024-11-05 03:25:10,481 - INFO - [diffusion][Epoch 8673] diffusion learning rate: 0.001
2024-11-05 03:25:10,483 - INFO - [diffusion][Epoch 8673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:10,484 - INFO - [diffusion][Epoch 8674] Epoch 8675/12000
2024-11-05 03:25:14,552 - INFO - [diffusion][Epoch 8674] diffusion training Loss: 0.05248696077615023
2024-11-05 03:25:14,554 - INFO - [diffusion][Epoch 8674] diffusion learning rate: 0.001
2024-11-05 03:25:14,557 - INFO - [diffusion][Epoch 8674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:14,558 - INFO - [diffusion][Epoch 8675] Epoch 8676/12000
2024-11-05 03:25:18,633 - INFO - [diffusion][Epoch 8675] diffusion training Loss: 0.0569234499707818
2024-11-05 03:25:18,635 - INFO - [diffusion][Epoch 8675] diffusion learning rate: 0.001
2024-11-05 03:25:18,637 - INFO - [diffusion][Epoch 8675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:18,638 - INFO - [diffusion][Epoch 8676] Epoch 8677/12000
2024-11-05 03:25:22,768 - INFO - [diffusion][Epoch 8676] diffusion training Loss: 0.04816030245274305
2024-11-05 03:25:22,771 - INFO - [diffusion][Epoch 8676] diffusion learning rate: 0.001
2024-11-05 03:25:22,772 - INFO - [diffusion][Epoch 8676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:22,774 - INFO - [diffusion][Epoch 8677] Epoch 8678/12000
2024-11-05 03:25:26,821 - INFO - [diffusion][Epoch 8677] diffusion training Loss: 0.05367839150130749
2024-11-05 03:25:26,823 - INFO - [diffusion][Epoch 8677] diffusion learning rate: 0.001
2024-11-05 03:25:26,825 - INFO - [diffusion][Epoch 8677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:26,826 - INFO - [diffusion][Epoch 8678] Epoch 8679/12000
2024-11-05 03:25:30,951 - INFO - [diffusion][Epoch 8678] diffusion training Loss: 0.04755066987127066
2024-11-05 03:25:30,953 - INFO - [diffusion][Epoch 8678] diffusion learning rate: 0.001
2024-11-05 03:25:30,955 - INFO - [diffusion][Epoch 8678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:30,956 - INFO - [diffusion][Epoch 8679] Epoch 8680/12000
2024-11-05 03:25:34,921 - INFO - [diffusion][Epoch 8679] diffusion training Loss: 0.05286402814090252
2024-11-05 03:25:34,923 - INFO - [diffusion][Epoch 8679] diffusion learning rate: 0.001
2024-11-05 03:25:34,943 - INFO - [diffusion][Epoch 8679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:34,945 - INFO - [diffusion][Epoch 8680] Epoch 8681/12000
2024-11-05 03:25:38,979 - INFO - [diffusion][Epoch 8680] diffusion training Loss: 0.04646375682204962
2024-11-05 03:25:38,981 - INFO - [diffusion][Epoch 8680] diffusion learning rate: 0.001
2024-11-05 03:25:38,983 - INFO - [diffusion][Epoch 8680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:38,984 - INFO - [diffusion][Epoch 8681] Epoch 8682/12000
2024-11-05 03:25:43,040 - INFO - [diffusion][Epoch 8681] diffusion training Loss: 0.05124020483344793
2024-11-05 03:25:43,042 - INFO - [diffusion][Epoch 8681] diffusion learning rate: 0.001
2024-11-05 03:25:43,044 - INFO - [diffusion][Epoch 8681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:43,045 - INFO - [diffusion][Epoch 8682] Epoch 8683/12000
2024-11-05 03:25:47,060 - INFO - [diffusion][Epoch 8682] diffusion training Loss: 0.050142381340265274
2024-11-05 03:25:47,063 - INFO - [diffusion][Epoch 8682] diffusion learning rate: 0.001
2024-11-05 03:25:47,065 - INFO - [diffusion][Epoch 8682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:47,066 - INFO - [diffusion][Epoch 8683] Epoch 8684/12000
2024-11-05 03:25:51,430 - INFO - [diffusion][Epoch 8683] diffusion training Loss: 0.053981827571988106
2024-11-05 03:25:51,432 - INFO - [diffusion][Epoch 8683] diffusion learning rate: 0.001
2024-11-05 03:25:51,434 - INFO - [diffusion][Epoch 8683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:51,436 - INFO - [diffusion][Epoch 8684] Epoch 8685/12000
2024-11-05 03:25:55,445 - INFO - [diffusion][Epoch 8684] diffusion training Loss: 0.047364311292767525
2024-11-05 03:25:55,447 - INFO - [diffusion][Epoch 8684] diffusion learning rate: 0.001
2024-11-05 03:25:55,449 - INFO - [diffusion][Epoch 8684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:55,451 - INFO - [diffusion][Epoch 8685] Epoch 8686/12000
2024-11-05 03:25:59,535 - INFO - [diffusion][Epoch 8685] diffusion training Loss: 0.050675646401941776
2024-11-05 03:25:59,537 - INFO - [diffusion][Epoch 8685] diffusion learning rate: 0.001
2024-11-05 03:25:59,539 - INFO - [diffusion][Epoch 8685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:25:59,540 - INFO - [diffusion][Epoch 8686] Epoch 8687/12000
2024-11-05 03:26:03,660 - INFO - [diffusion][Epoch 8686] diffusion training Loss: 0.04962647147476673
2024-11-05 03:26:03,663 - INFO - [diffusion][Epoch 8686] diffusion learning rate: 0.001
2024-11-05 03:26:03,664 - INFO - [diffusion][Epoch 8686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:03,666 - INFO - [diffusion][Epoch 8687] Epoch 8688/12000
2024-11-05 03:26:07,778 - INFO - [diffusion][Epoch 8687] diffusion training Loss: 0.05063668638467789
2024-11-05 03:26:07,780 - INFO - [diffusion][Epoch 8687] diffusion learning rate: 0.001
2024-11-05 03:26:07,783 - INFO - [diffusion][Epoch 8687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:07,785 - INFO - [diffusion][Epoch 8688] Epoch 8689/12000
2024-11-05 03:26:11,715 - INFO - [diffusion][Epoch 8688] diffusion training Loss: 0.056658429093658924
2024-11-05 03:26:11,717 - INFO - [diffusion][Epoch 8688] diffusion learning rate: 0.001
2024-11-05 03:26:11,719 - INFO - [diffusion][Epoch 8688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:11,720 - INFO - [diffusion][Epoch 8689] Epoch 8690/12000
2024-11-05 03:26:15,843 - INFO - [diffusion][Epoch 8689] diffusion training Loss: 0.052971544675529
2024-11-05 03:26:15,845 - INFO - [diffusion][Epoch 8689] diffusion learning rate: 0.001
2024-11-05 03:26:15,847 - INFO - [diffusion][Epoch 8689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:15,848 - INFO - [diffusion][Epoch 8690] Epoch 8691/12000
2024-11-05 03:26:19,926 - INFO - [diffusion][Epoch 8690] diffusion training Loss: 0.052962472662329674
2024-11-05 03:26:19,928 - INFO - [diffusion][Epoch 8690] diffusion learning rate: 0.001
2024-11-05 03:26:19,930 - INFO - [diffusion][Epoch 8690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:19,932 - INFO - [diffusion][Epoch 8691] Epoch 8692/12000
2024-11-05 03:26:23,916 - INFO - [diffusion][Epoch 8691] diffusion training Loss: 0.05023985542356968
2024-11-05 03:26:23,918 - INFO - [diffusion][Epoch 8691] diffusion learning rate: 0.001
2024-11-05 03:26:23,920 - INFO - [diffusion][Epoch 8691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:23,921 - INFO - [diffusion][Epoch 8692] Epoch 8693/12000
2024-11-05 03:26:27,920 - INFO - [diffusion][Epoch 8692] diffusion training Loss: 0.05275542102754116
2024-11-05 03:26:27,924 - INFO - [diffusion][Epoch 8692] diffusion learning rate: 0.001
2024-11-05 03:26:27,927 - INFO - [diffusion][Epoch 8692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:27,929 - INFO - [diffusion][Epoch 8693] Epoch 8694/12000
2024-11-05 03:26:32,057 - INFO - [diffusion][Epoch 8693] diffusion training Loss: 0.05407586973160505
2024-11-05 03:26:32,059 - INFO - [diffusion][Epoch 8693] diffusion learning rate: 0.001
2024-11-05 03:26:32,060 - INFO - [diffusion][Epoch 8693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:32,062 - INFO - [diffusion][Epoch 8694] Epoch 8695/12000
2024-11-05 03:26:36,191 - INFO - [diffusion][Epoch 8694] diffusion training Loss: 0.0526821231469512
2024-11-05 03:26:36,193 - INFO - [diffusion][Epoch 8694] diffusion learning rate: 0.001
2024-11-05 03:26:36,195 - INFO - [diffusion][Epoch 8694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:36,197 - INFO - [diffusion][Epoch 8695] Epoch 8696/12000
2024-11-05 03:26:40,420 - INFO - [diffusion][Epoch 8695] diffusion training Loss: 0.051491870544850826
2024-11-05 03:26:40,422 - INFO - [diffusion][Epoch 8695] diffusion learning rate: 0.001
2024-11-05 03:26:40,423 - INFO - [diffusion][Epoch 8695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:40,425 - INFO - [diffusion][Epoch 8696] Epoch 8697/12000
2024-11-05 03:26:44,513 - INFO - [diffusion][Epoch 8696] diffusion training Loss: 0.05806938465684652
2024-11-05 03:26:44,515 - INFO - [diffusion][Epoch 8696] diffusion learning rate: 0.001
2024-11-05 03:26:44,517 - INFO - [diffusion][Epoch 8696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:44,518 - INFO - [diffusion][Epoch 8697] Epoch 8698/12000
2024-11-05 03:26:48,644 - INFO - [diffusion][Epoch 8697] diffusion training Loss: 0.052419387735426426
2024-11-05 03:26:48,647 - INFO - [diffusion][Epoch 8697] diffusion learning rate: 0.001
2024-11-05 03:26:48,650 - INFO - [diffusion][Epoch 8697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:48,652 - INFO - [diffusion][Epoch 8698] Epoch 8699/12000
2024-11-05 03:26:52,796 - INFO - [diffusion][Epoch 8698] diffusion training Loss: 0.05488986801356077
2024-11-05 03:26:52,799 - INFO - [diffusion][Epoch 8698] diffusion learning rate: 0.001
2024-11-05 03:26:52,827 - INFO - [diffusion][Epoch 8698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:52,828 - INFO - [diffusion][Epoch 8699] Epoch 8700/12000
2024-11-05 03:26:56,935 - INFO - [diffusion][Epoch 8699] diffusion training Loss: 0.04972563497722149
2024-11-05 03:26:56,937 - INFO - [diffusion][Epoch 8699] diffusion learning rate: 0.001
2024-11-05 03:26:56,939 - INFO - [diffusion][Epoch 8699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:26:56,941 - INFO - [diffusion][Epoch 8700] Epoch 8701/12000
2024-11-05 03:27:00,953 - INFO - [diffusion][Epoch 8700] diffusion training Loss: 0.05365042854100466
2024-11-05 03:27:00,955 - INFO - [diffusion][Epoch 8700] diffusion learning rate: 0.001
2024-11-05 03:27:00,957 - INFO - [diffusion][Epoch 8700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:00,958 - INFO - [diffusion][Epoch 8701] Epoch 8702/12000
2024-11-05 03:27:05,094 - INFO - [diffusion][Epoch 8701] diffusion training Loss: 0.04755431693047285
2024-11-05 03:27:05,096 - INFO - [diffusion][Epoch 8701] diffusion learning rate: 0.001
2024-11-05 03:27:05,097 - INFO - [diffusion][Epoch 8701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:05,099 - INFO - [diffusion][Epoch 8702] Epoch 8703/12000
2024-11-05 03:27:09,101 - INFO - [diffusion][Epoch 8702] diffusion training Loss: 0.0547781465575099
2024-11-05 03:27:09,103 - INFO - [diffusion][Epoch 8702] diffusion learning rate: 0.001
2024-11-05 03:27:09,105 - INFO - [diffusion][Epoch 8702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:09,106 - INFO - [diffusion][Epoch 8703] Epoch 8704/12000
2024-11-05 03:27:13,154 - INFO - [diffusion][Epoch 8703] diffusion training Loss: 0.04790496453642845
2024-11-05 03:27:13,156 - INFO - [diffusion][Epoch 8703] diffusion learning rate: 0.001
2024-11-05 03:27:13,157 - INFO - [diffusion][Epoch 8703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:13,159 - INFO - [diffusion][Epoch 8704] Epoch 8705/12000
2024-11-05 03:27:17,289 - INFO - [diffusion][Epoch 8704] diffusion training Loss: 0.050172300077974796
2024-11-05 03:27:17,291 - INFO - [diffusion][Epoch 8704] diffusion learning rate: 0.001
2024-11-05 03:27:17,292 - INFO - [diffusion][Epoch 8704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:17,294 - INFO - [diffusion][Epoch 8705] Epoch 8706/12000
2024-11-05 03:27:21,309 - INFO - [diffusion][Epoch 8705] diffusion training Loss: 0.055589611642062664
2024-11-05 03:27:21,311 - INFO - [diffusion][Epoch 8705] diffusion learning rate: 0.001
2024-11-05 03:27:21,313 - INFO - [diffusion][Epoch 8705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:21,314 - INFO - [diffusion][Epoch 8706] Epoch 8707/12000
2024-11-05 03:27:25,344 - INFO - [diffusion][Epoch 8706] diffusion training Loss: 0.05522541329264641
2024-11-05 03:27:25,346 - INFO - [diffusion][Epoch 8706] diffusion learning rate: 0.001
2024-11-05 03:27:25,348 - INFO - [diffusion][Epoch 8706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:25,349 - INFO - [diffusion][Epoch 8707] Epoch 8708/12000
2024-11-05 03:27:29,497 - INFO - [diffusion][Epoch 8707] diffusion training Loss: 0.05178511142730713
2024-11-05 03:27:29,499 - INFO - [diffusion][Epoch 8707] diffusion learning rate: 0.001
2024-11-05 03:27:29,501 - INFO - [diffusion][Epoch 8707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:29,502 - INFO - [diffusion][Epoch 8708] Epoch 8709/12000
2024-11-05 03:27:33,572 - INFO - [diffusion][Epoch 8708] diffusion training Loss: 0.054695495404303074
2024-11-05 03:27:33,574 - INFO - [diffusion][Epoch 8708] diffusion learning rate: 0.001
2024-11-05 03:27:33,576 - INFO - [diffusion][Epoch 8708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:33,577 - INFO - [diffusion][Epoch 8709] Epoch 8710/12000
2024-11-05 03:27:37,619 - INFO - [diffusion][Epoch 8709] diffusion training Loss: 0.05161333363503218
2024-11-05 03:27:37,621 - INFO - [diffusion][Epoch 8709] diffusion learning rate: 0.001
2024-11-05 03:27:37,624 - INFO - [diffusion][Epoch 8709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:37,625 - INFO - [diffusion][Epoch 8710] Epoch 8711/12000
2024-11-05 03:27:41,744 - INFO - [diffusion][Epoch 8710] diffusion training Loss: 0.05239027366042137
2024-11-05 03:27:41,746 - INFO - [diffusion][Epoch 8710] diffusion learning rate: 0.001
2024-11-05 03:27:41,749 - INFO - [diffusion][Epoch 8710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:41,751 - INFO - [diffusion][Epoch 8711] Epoch 8712/12000
2024-11-05 03:27:45,775 - INFO - [diffusion][Epoch 8711] diffusion training Loss: 0.04918573051691055
2024-11-05 03:27:45,777 - INFO - [diffusion][Epoch 8711] diffusion learning rate: 0.001
2024-11-05 03:27:45,804 - INFO - [diffusion][Epoch 8711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:45,806 - INFO - [diffusion][Epoch 8712] Epoch 8713/12000
2024-11-05 03:27:49,904 - INFO - [diffusion][Epoch 8712] diffusion training Loss: 0.05153229180723429
2024-11-05 03:27:49,907 - INFO - [diffusion][Epoch 8712] diffusion learning rate: 0.001
2024-11-05 03:27:49,909 - INFO - [diffusion][Epoch 8712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:49,910 - INFO - [diffusion][Epoch 8713] Epoch 8714/12000
2024-11-05 03:27:53,920 - INFO - [diffusion][Epoch 8713] diffusion training Loss: 0.053507003001868725
2024-11-05 03:27:53,922 - INFO - [diffusion][Epoch 8713] diffusion learning rate: 0.001
2024-11-05 03:27:53,924 - INFO - [diffusion][Epoch 8713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:53,925 - INFO - [diffusion][Epoch 8714] Epoch 8715/12000
2024-11-05 03:27:57,957 - INFO - [diffusion][Epoch 8714] diffusion training Loss: 0.0523702846840024
2024-11-05 03:27:57,959 - INFO - [diffusion][Epoch 8714] diffusion learning rate: 0.001
2024-11-05 03:27:57,961 - INFO - [diffusion][Epoch 8714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:27:57,962 - INFO - [diffusion][Epoch 8715] Epoch 8716/12000
2024-11-05 03:28:01,874 - INFO - [diffusion][Epoch 8715] diffusion training Loss: 0.05250978749245405
2024-11-05 03:28:01,898 - INFO - [diffusion][Epoch 8715] diffusion learning rate: 0.001
2024-11-05 03:28:01,899 - INFO - [diffusion][Epoch 8715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:01,901 - INFO - [diffusion][Epoch 8716] Epoch 8717/12000
2024-11-05 03:28:05,896 - INFO - [diffusion][Epoch 8716] diffusion training Loss: 0.05528239905834198
2024-11-05 03:28:05,898 - INFO - [diffusion][Epoch 8716] diffusion learning rate: 0.001
2024-11-05 03:28:05,900 - INFO - [diffusion][Epoch 8716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:05,902 - INFO - [diffusion][Epoch 8717] Epoch 8718/12000
2024-11-05 03:28:09,976 - INFO - [diffusion][Epoch 8717] diffusion training Loss: 0.05702077504247427
2024-11-05 03:28:09,979 - INFO - [diffusion][Epoch 8717] diffusion learning rate: 0.001
2024-11-05 03:28:09,981 - INFO - [diffusion][Epoch 8717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:09,982 - INFO - [diffusion][Epoch 8718] Epoch 8719/12000
2024-11-05 03:28:14,123 - INFO - [diffusion][Epoch 8718] diffusion training Loss: 0.05476076155900955
2024-11-05 03:28:14,125 - INFO - [diffusion][Epoch 8718] diffusion learning rate: 0.001
2024-11-05 03:28:14,127 - INFO - [diffusion][Epoch 8718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:14,129 - INFO - [diffusion][Epoch 8719] Epoch 8720/12000
2024-11-05 03:28:18,217 - INFO - [diffusion][Epoch 8719] diffusion training Loss: 0.05124781932681799
2024-11-05 03:28:18,219 - INFO - [diffusion][Epoch 8719] diffusion learning rate: 0.001
2024-11-05 03:28:18,222 - INFO - [diffusion][Epoch 8719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:18,224 - INFO - [diffusion][Epoch 8720] Epoch 8721/12000
2024-11-05 03:28:22,345 - INFO - [diffusion][Epoch 8720] diffusion training Loss: 0.053172096610069275
2024-11-05 03:28:22,347 - INFO - [diffusion][Epoch 8720] diffusion learning rate: 0.001
2024-11-05 03:28:22,379 - INFO - [diffusion][Epoch 8720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:22,381 - INFO - [diffusion][Epoch 8721] Epoch 8722/12000
2024-11-05 03:28:26,336 - INFO - [diffusion][Epoch 8721] diffusion training Loss: 0.04762861039489508
2024-11-05 03:28:26,338 - INFO - [diffusion][Epoch 8721] diffusion learning rate: 0.001
2024-11-05 03:28:26,340 - INFO - [diffusion][Epoch 8721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:26,342 - INFO - [diffusion][Epoch 8722] Epoch 8723/12000
2024-11-05 03:28:30,372 - INFO - [diffusion][Epoch 8722] diffusion training Loss: 0.05181723274290562
2024-11-05 03:28:30,374 - INFO - [diffusion][Epoch 8722] diffusion learning rate: 0.001
2024-11-05 03:28:30,376 - INFO - [diffusion][Epoch 8722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:30,377 - INFO - [diffusion][Epoch 8723] Epoch 8724/12000
2024-11-05 03:28:34,502 - INFO - [diffusion][Epoch 8723] diffusion training Loss: 0.05109670013189316
2024-11-05 03:28:34,504 - INFO - [diffusion][Epoch 8723] diffusion learning rate: 0.001
2024-11-05 03:28:34,506 - INFO - [diffusion][Epoch 8723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:34,507 - INFO - [diffusion][Epoch 8724] Epoch 8725/12000
2024-11-05 03:28:38,611 - INFO - [diffusion][Epoch 8724] diffusion training Loss: 0.050717431120574474
2024-11-05 03:28:38,613 - INFO - [diffusion][Epoch 8724] diffusion learning rate: 0.001
2024-11-05 03:28:38,614 - INFO - [diffusion][Epoch 8724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:38,616 - INFO - [diffusion][Epoch 8725] Epoch 8726/12000
2024-11-05 03:28:42,763 - INFO - [diffusion][Epoch 8725] diffusion training Loss: 0.05160955619066954
2024-11-05 03:28:42,765 - INFO - [diffusion][Epoch 8725] diffusion learning rate: 0.001
2024-11-05 03:28:42,808 - INFO - [diffusion][Epoch 8725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:42,810 - INFO - [diffusion][Epoch 8726] Epoch 8727/12000
2024-11-05 03:28:46,850 - INFO - [diffusion][Epoch 8726] diffusion training Loss: 0.05656548123806715
2024-11-05 03:28:46,853 - INFO - [diffusion][Epoch 8726] diffusion learning rate: 0.001
2024-11-05 03:28:46,855 - INFO - [diffusion][Epoch 8726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:46,856 - INFO - [diffusion][Epoch 8727] Epoch 8728/12000
2024-11-05 03:28:50,969 - INFO - [diffusion][Epoch 8727] diffusion training Loss: 0.045921639539301395
2024-11-05 03:28:50,972 - INFO - [diffusion][Epoch 8727] diffusion learning rate: 0.001
2024-11-05 03:28:50,974 - INFO - [diffusion][Epoch 8727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:50,975 - INFO - [diffusion][Epoch 8728] Epoch 8729/12000
2024-11-05 03:28:55,110 - INFO - [diffusion][Epoch 8728] diffusion training Loss: 0.05351948644965887
2024-11-05 03:28:55,112 - INFO - [diffusion][Epoch 8728] diffusion learning rate: 0.001
2024-11-05 03:28:55,114 - INFO - [diffusion][Epoch 8728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:55,115 - INFO - [diffusion][Epoch 8729] Epoch 8730/12000
2024-11-05 03:28:59,175 - INFO - [diffusion][Epoch 8729] diffusion training Loss: 0.04906257055699825
2024-11-05 03:28:59,177 - INFO - [diffusion][Epoch 8729] diffusion learning rate: 0.001
2024-11-05 03:28:59,178 - INFO - [diffusion][Epoch 8729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:28:59,180 - INFO - [diffusion][Epoch 8730] Epoch 8731/12000
2024-11-05 03:29:03,245 - INFO - [diffusion][Epoch 8730] diffusion training Loss: 0.059098987840116024
2024-11-05 03:29:03,247 - INFO - [diffusion][Epoch 8730] diffusion learning rate: 0.001
2024-11-05 03:29:03,249 - INFO - [diffusion][Epoch 8730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:03,250 - INFO - [diffusion][Epoch 8731] Epoch 8732/12000
2024-11-05 03:29:07,299 - INFO - [diffusion][Epoch 8731] diffusion training Loss: 0.04961522854864597
2024-11-05 03:29:07,301 - INFO - [diffusion][Epoch 8731] diffusion learning rate: 0.001
2024-11-05 03:29:07,303 - INFO - [diffusion][Epoch 8731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:07,304 - INFO - [diffusion][Epoch 8732] Epoch 8733/12000
2024-11-05 03:29:11,380 - INFO - [diffusion][Epoch 8732] diffusion training Loss: 0.05402341764420271
2024-11-05 03:29:11,382 - INFO - [diffusion][Epoch 8732] diffusion learning rate: 0.001
2024-11-05 03:29:11,384 - INFO - [diffusion][Epoch 8732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:11,385 - INFO - [diffusion][Epoch 8733] Epoch 8734/12000
2024-11-05 03:29:15,511 - INFO - [diffusion][Epoch 8733] diffusion training Loss: 0.05466218013316393
2024-11-05 03:29:15,514 - INFO - [diffusion][Epoch 8733] diffusion learning rate: 0.001
2024-11-05 03:29:15,542 - INFO - [diffusion][Epoch 8733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:15,543 - INFO - [diffusion][Epoch 8734] Epoch 8735/12000
2024-11-05 03:29:19,682 - INFO - [diffusion][Epoch 8734] diffusion training Loss: 0.058121747337281704
2024-11-05 03:29:19,684 - INFO - [diffusion][Epoch 8734] diffusion learning rate: 0.001
2024-11-05 03:29:19,686 - INFO - [diffusion][Epoch 8734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:19,687 - INFO - [diffusion][Epoch 8735] Epoch 8736/12000
2024-11-05 03:29:23,744 - INFO - [diffusion][Epoch 8735] diffusion training Loss: 0.05142505653202534
2024-11-05 03:29:23,745 - INFO - [diffusion][Epoch 8735] diffusion learning rate: 0.001
2024-11-05 03:29:23,747 - INFO - [diffusion][Epoch 8735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:23,748 - INFO - [diffusion][Epoch 8736] Epoch 8737/12000
2024-11-05 03:29:27,867 - INFO - [diffusion][Epoch 8736] diffusion training Loss: 0.05643479246646166
2024-11-05 03:29:27,869 - INFO - [diffusion][Epoch 8736] diffusion learning rate: 0.001
2024-11-05 03:29:27,871 - INFO - [diffusion][Epoch 8736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:27,872 - INFO - [diffusion][Epoch 8737] Epoch 8738/12000
2024-11-05 03:29:31,958 - INFO - [diffusion][Epoch 8737] diffusion training Loss: 0.05471839942038059
2024-11-05 03:29:31,960 - INFO - [diffusion][Epoch 8737] diffusion learning rate: 0.001
2024-11-05 03:29:31,962 - INFO - [diffusion][Epoch 8737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:31,963 - INFO - [diffusion][Epoch 8738] Epoch 8739/12000
2024-11-05 03:29:35,854 - INFO - [diffusion][Epoch 8738] diffusion training Loss: 0.056012081913650036
2024-11-05 03:29:35,856 - INFO - [diffusion][Epoch 8738] diffusion learning rate: 0.001
2024-11-05 03:29:35,858 - INFO - [diffusion][Epoch 8738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:35,859 - INFO - [diffusion][Epoch 8739] Epoch 8740/12000
2024-11-05 03:29:39,782 - INFO - [diffusion][Epoch 8739] diffusion training Loss: 0.058498864993453026
2024-11-05 03:29:39,784 - INFO - [diffusion][Epoch 8739] diffusion learning rate: 0.001
2024-11-05 03:29:39,786 - INFO - [diffusion][Epoch 8739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:39,787 - INFO - [diffusion][Epoch 8740] Epoch 8741/12000
2024-11-05 03:29:43,858 - INFO - [diffusion][Epoch 8740] diffusion training Loss: 0.05008349008858204
2024-11-05 03:29:43,860 - INFO - [diffusion][Epoch 8740] diffusion learning rate: 0.001
2024-11-05 03:29:43,862 - INFO - [diffusion][Epoch 8740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:43,863 - INFO - [diffusion][Epoch 8741] Epoch 8742/12000
2024-11-05 03:29:47,969 - INFO - [diffusion][Epoch 8741] diffusion training Loss: 0.057258387096226215
2024-11-05 03:29:47,971 - INFO - [diffusion][Epoch 8741] diffusion learning rate: 0.001
2024-11-05 03:29:47,973 - INFO - [diffusion][Epoch 8741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:47,975 - INFO - [diffusion][Epoch 8742] Epoch 8743/12000
2024-11-05 03:29:52,075 - INFO - [diffusion][Epoch 8742] diffusion training Loss: 0.053866297006607056
2024-11-05 03:29:52,078 - INFO - [diffusion][Epoch 8742] diffusion learning rate: 0.001
2024-11-05 03:29:52,081 - INFO - [diffusion][Epoch 8742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:52,082 - INFO - [diffusion][Epoch 8743] Epoch 8744/12000
2024-11-05 03:29:56,109 - INFO - [diffusion][Epoch 8743] diffusion training Loss: 0.05424401257187128
2024-11-05 03:29:56,112 - INFO - [diffusion][Epoch 8743] diffusion learning rate: 0.001
2024-11-05 03:29:56,114 - INFO - [diffusion][Epoch 8743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:29:56,115 - INFO - [diffusion][Epoch 8744] Epoch 8745/12000
2024-11-05 03:30:00,125 - INFO - [diffusion][Epoch 8744] diffusion training Loss: 0.052460139617323875
2024-11-05 03:30:00,127 - INFO - [diffusion][Epoch 8744] diffusion learning rate: 0.001
2024-11-05 03:30:00,128 - INFO - [diffusion][Epoch 8744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:00,129 - INFO - [diffusion][Epoch 8745] Epoch 8746/12000
2024-11-05 03:30:04,308 - INFO - [diffusion][Epoch 8745] diffusion training Loss: 0.05306056048721075
2024-11-05 03:30:04,310 - INFO - [diffusion][Epoch 8745] diffusion learning rate: 0.001
2024-11-05 03:30:04,312 - INFO - [diffusion][Epoch 8745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:04,313 - INFO - [diffusion][Epoch 8746] Epoch 8747/12000
2024-11-05 03:30:08,250 - INFO - [diffusion][Epoch 8746] diffusion training Loss: 0.05182378459721804
2024-11-05 03:30:08,252 - INFO - [diffusion][Epoch 8746] diffusion learning rate: 0.001
2024-11-05 03:30:08,254 - INFO - [diffusion][Epoch 8746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:08,255 - INFO - [diffusion][Epoch 8747] Epoch 8748/12000
2024-11-05 03:30:12,166 - INFO - [diffusion][Epoch 8747] diffusion training Loss: 0.054106454364955425
2024-11-05 03:30:12,501 - INFO - [diffusion][Epoch 8747] diffusion learning rate: 0.001
2024-11-05 03:30:12,503 - INFO - [diffusion][Epoch 8747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:12,504 - INFO - [diffusion][Epoch 8748] Epoch 8749/12000
2024-11-05 03:30:16,605 - INFO - [diffusion][Epoch 8748] diffusion training Loss: 0.052004300989210606
2024-11-05 03:30:16,608 - INFO - [diffusion][Epoch 8748] diffusion learning rate: 0.001
2024-11-05 03:30:16,610 - INFO - [diffusion][Epoch 8748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:16,611 - INFO - [diffusion][Epoch 8749] Epoch 8750/12000
2024-11-05 03:30:20,615 - INFO - [diffusion][Epoch 8749] diffusion training Loss: 0.04995551239699125
2024-11-05 03:30:20,617 - INFO - [diffusion][Epoch 8749] diffusion learning rate: 0.001
2024-11-05 03:30:20,619 - INFO - [diffusion][Epoch 8749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:20,620 - INFO - [diffusion][Epoch 8750] Epoch 8751/12000
2024-11-05 03:30:24,565 - INFO - [diffusion][Epoch 8750] diffusion training Loss: 0.05379960499703884
2024-11-05 03:30:24,567 - INFO - [diffusion][Epoch 8750] diffusion learning rate: 0.001
2024-11-05 03:30:24,569 - INFO - [diffusion][Epoch 8750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:24,571 - INFO - [diffusion][Epoch 8751] Epoch 8752/12000
2024-11-05 03:30:28,624 - INFO - [diffusion][Epoch 8751] diffusion training Loss: 0.06115738861262798
2024-11-05 03:30:28,626 - INFO - [diffusion][Epoch 8751] diffusion learning rate: 0.001
2024-11-05 03:30:28,628 - INFO - [diffusion][Epoch 8751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:28,629 - INFO - [diffusion][Epoch 8752] Epoch 8753/12000
2024-11-05 03:30:32,846 - INFO - [diffusion][Epoch 8752] diffusion training Loss: 0.05168497748672962
2024-11-05 03:30:32,848 - INFO - [diffusion][Epoch 8752] diffusion learning rate: 0.001
2024-11-05 03:30:32,850 - INFO - [diffusion][Epoch 8752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:32,851 - INFO - [diffusion][Epoch 8753] Epoch 8754/12000
2024-11-05 03:30:36,992 - INFO - [diffusion][Epoch 8753] diffusion training Loss: 0.056012773886322975
2024-11-05 03:30:36,994 - INFO - [diffusion][Epoch 8753] diffusion learning rate: 0.001
2024-11-05 03:30:36,996 - INFO - [diffusion][Epoch 8753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:36,997 - INFO - [diffusion][Epoch 8754] Epoch 8755/12000
2024-11-05 03:30:41,171 - INFO - [diffusion][Epoch 8754] diffusion training Loss: 0.054684373550117016
2024-11-05 03:30:41,173 - INFO - [diffusion][Epoch 8754] diffusion learning rate: 0.001
2024-11-05 03:30:41,175 - INFO - [diffusion][Epoch 8754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:41,176 - INFO - [diffusion][Epoch 8755] Epoch 8756/12000
2024-11-05 03:30:45,263 - INFO - [diffusion][Epoch 8755] diffusion training Loss: 0.04960005823522806
2024-11-05 03:30:45,265 - INFO - [diffusion][Epoch 8755] diffusion learning rate: 0.001
2024-11-05 03:30:45,267 - INFO - [diffusion][Epoch 8755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:45,268 - INFO - [diffusion][Epoch 8756] Epoch 8757/12000
2024-11-05 03:30:49,407 - INFO - [diffusion][Epoch 8756] diffusion training Loss: 0.05361186619848013
2024-11-05 03:30:49,410 - INFO - [diffusion][Epoch 8756] diffusion learning rate: 0.001
2024-11-05 03:30:49,412 - INFO - [diffusion][Epoch 8756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:49,413 - INFO - [diffusion][Epoch 8757] Epoch 8758/12000
2024-11-05 03:30:53,488 - INFO - [diffusion][Epoch 8757] diffusion training Loss: 0.047768584452569485
2024-11-05 03:30:53,490 - INFO - [diffusion][Epoch 8757] diffusion learning rate: 0.001
2024-11-05 03:30:53,492 - INFO - [diffusion][Epoch 8757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:53,493 - INFO - [diffusion][Epoch 8758] Epoch 8759/12000
2024-11-05 03:30:57,563 - INFO - [diffusion][Epoch 8758] diffusion training Loss: 0.057036018930375576
2024-11-05 03:30:57,565 - INFO - [diffusion][Epoch 8758] diffusion learning rate: 0.001
2024-11-05 03:30:57,566 - INFO - [diffusion][Epoch 8758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:30:57,567 - INFO - [diffusion][Epoch 8759] Epoch 8760/12000
2024-11-05 03:31:01,673 - INFO - [diffusion][Epoch 8759] diffusion training Loss: 0.052792515605688095
2024-11-05 03:31:01,675 - INFO - [diffusion][Epoch 8759] diffusion learning rate: 0.001
2024-11-05 03:31:01,677 - INFO - [diffusion][Epoch 8759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:01,678 - INFO - [diffusion][Epoch 8760] Epoch 8761/12000
2024-11-05 03:31:05,735 - INFO - [diffusion][Epoch 8760] diffusion training Loss: 0.059377264231443405
2024-11-05 03:31:05,737 - INFO - [diffusion][Epoch 8760] diffusion learning rate: 0.001
2024-11-05 03:31:05,738 - INFO - [diffusion][Epoch 8760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:05,740 - INFO - [diffusion][Epoch 8761] Epoch 8762/12000
2024-11-05 03:31:09,874 - INFO - [diffusion][Epoch 8761] diffusion training Loss: 0.05258581880480051
2024-11-05 03:31:09,876 - INFO - [diffusion][Epoch 8761] diffusion learning rate: 0.001
2024-11-05 03:31:09,877 - INFO - [diffusion][Epoch 8761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:09,879 - INFO - [diffusion][Epoch 8762] Epoch 8763/12000
2024-11-05 03:31:13,990 - INFO - [diffusion][Epoch 8762] diffusion training Loss: 0.05217396840453148
2024-11-05 03:31:13,992 - INFO - [diffusion][Epoch 8762] diffusion learning rate: 0.001
2024-11-05 03:31:13,994 - INFO - [diffusion][Epoch 8762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:13,996 - INFO - [diffusion][Epoch 8763] Epoch 8764/12000
2024-11-05 03:31:18,040 - INFO - [diffusion][Epoch 8763] diffusion training Loss: 0.05068555474281311
2024-11-05 03:31:18,042 - INFO - [diffusion][Epoch 8763] diffusion learning rate: 0.001
2024-11-05 03:31:18,044 - INFO - [diffusion][Epoch 8763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:18,045 - INFO - [diffusion][Epoch 8764] Epoch 8765/12000
2024-11-05 03:31:22,169 - INFO - [diffusion][Epoch 8764] diffusion training Loss: 0.054665226489305496
2024-11-05 03:31:22,171 - INFO - [diffusion][Epoch 8764] diffusion learning rate: 0.001
2024-11-05 03:31:22,198 - INFO - [diffusion][Epoch 8764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:22,200 - INFO - [diffusion][Epoch 8765] Epoch 8766/12000
2024-11-05 03:31:26,135 - INFO - [diffusion][Epoch 8765] diffusion training Loss: 0.051850817166268826
2024-11-05 03:31:26,137 - INFO - [diffusion][Epoch 8765] diffusion learning rate: 0.001
2024-11-05 03:31:26,139 - INFO - [diffusion][Epoch 8765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:26,140 - INFO - [diffusion][Epoch 8766] Epoch 8767/12000
2024-11-05 03:31:30,356 - INFO - [diffusion][Epoch 8766] diffusion training Loss: 0.052073078230023384
2024-11-05 03:31:30,507 - INFO - [diffusion][Epoch 8766] diffusion learning rate: 0.001
2024-11-05 03:31:30,508 - INFO - [diffusion][Epoch 8766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:30,510 - INFO - [diffusion][Epoch 8767] Epoch 8768/12000
2024-11-05 03:31:34,566 - INFO - [diffusion][Epoch 8767] diffusion training Loss: 0.061312370002269745
2024-11-05 03:31:34,568 - INFO - [diffusion][Epoch 8767] diffusion learning rate: 0.001
2024-11-05 03:31:34,570 - INFO - [diffusion][Epoch 8767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:34,571 - INFO - [diffusion][Epoch 8768] Epoch 8769/12000
2024-11-05 03:31:38,639 - INFO - [diffusion][Epoch 8768] diffusion training Loss: 0.05386215355247259
2024-11-05 03:31:38,641 - INFO - [diffusion][Epoch 8768] diffusion learning rate: 0.001
2024-11-05 03:31:38,642 - INFO - [diffusion][Epoch 8768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:38,644 - INFO - [diffusion][Epoch 8769] Epoch 8770/12000
2024-11-05 03:31:42,779 - INFO - [diffusion][Epoch 8769] diffusion training Loss: 0.05503898859024048
2024-11-05 03:31:42,781 - INFO - [diffusion][Epoch 8769] diffusion learning rate: 0.001
2024-11-05 03:31:42,783 - INFO - [diffusion][Epoch 8769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:42,784 - INFO - [diffusion][Epoch 8770] Epoch 8771/12000
2024-11-05 03:31:46,822 - INFO - [diffusion][Epoch 8770] diffusion training Loss: 0.05113643780350685
2024-11-05 03:31:46,824 - INFO - [diffusion][Epoch 8770] diffusion learning rate: 0.001
2024-11-05 03:31:46,826 - INFO - [diffusion][Epoch 8770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:46,828 - INFO - [diffusion][Epoch 8771] Epoch 8772/12000
2024-11-05 03:31:50,787 - INFO - [diffusion][Epoch 8771] diffusion training Loss: 0.0522674610838294
2024-11-05 03:31:50,790 - INFO - [diffusion][Epoch 8771] diffusion learning rate: 0.001
2024-11-05 03:31:50,792 - INFO - [diffusion][Epoch 8771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:50,793 - INFO - [diffusion][Epoch 8772] Epoch 8773/12000
2024-11-05 03:31:54,753 - INFO - [diffusion][Epoch 8772] diffusion training Loss: 0.0594699177891016
2024-11-05 03:31:54,756 - INFO - [diffusion][Epoch 8772] diffusion learning rate: 0.001
2024-11-05 03:31:54,758 - INFO - [diffusion][Epoch 8772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:54,759 - INFO - [diffusion][Epoch 8773] Epoch 8774/12000
2024-11-05 03:31:58,726 - INFO - [diffusion][Epoch 8773] diffusion training Loss: 0.05157770216464996
2024-11-05 03:31:58,728 - INFO - [diffusion][Epoch 8773] diffusion learning rate: 0.001
2024-11-05 03:31:58,730 - INFO - [diffusion][Epoch 8773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:31:58,731 - INFO - [diffusion][Epoch 8774] Epoch 8775/12000
2024-11-05 03:32:02,696 - INFO - [diffusion][Epoch 8774] diffusion training Loss: 0.051842265762388706
2024-11-05 03:32:02,699 - INFO - [diffusion][Epoch 8774] diffusion learning rate: 0.001
2024-11-05 03:32:02,701 - INFO - [diffusion][Epoch 8774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:02,702 - INFO - [diffusion][Epoch 8775] Epoch 8776/12000
2024-11-05 03:32:06,582 - INFO - [diffusion][Epoch 8775] diffusion training Loss: 0.055548026226460934
2024-11-05 03:32:06,584 - INFO - [diffusion][Epoch 8775] diffusion learning rate: 0.001
2024-11-05 03:32:06,587 - INFO - [diffusion][Epoch 8775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:06,588 - INFO - [diffusion][Epoch 8776] Epoch 8777/12000
2024-11-05 03:32:10,469 - INFO - [diffusion][Epoch 8776] diffusion training Loss: 0.05745458509773016
2024-11-05 03:32:10,471 - INFO - [diffusion][Epoch 8776] diffusion learning rate: 0.001
2024-11-05 03:32:10,473 - INFO - [diffusion][Epoch 8776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:10,474 - INFO - [diffusion][Epoch 8777] Epoch 8778/12000
2024-11-05 03:32:14,525 - INFO - [diffusion][Epoch 8777] diffusion training Loss: 0.05082486476749182
2024-11-05 03:32:14,528 - INFO - [diffusion][Epoch 8777] diffusion learning rate: 0.001
2024-11-05 03:32:14,530 - INFO - [diffusion][Epoch 8777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:14,532 - INFO - [diffusion][Epoch 8778] Epoch 8779/12000
2024-11-05 03:32:18,383 - INFO - [diffusion][Epoch 8778] diffusion training Loss: 0.053890117444097996
2024-11-05 03:32:18,385 - INFO - [diffusion][Epoch 8778] diffusion learning rate: 0.001
2024-11-05 03:32:18,387 - INFO - [diffusion][Epoch 8778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:18,388 - INFO - [diffusion][Epoch 8779] Epoch 8780/12000
2024-11-05 03:32:22,324 - INFO - [diffusion][Epoch 8779] diffusion training Loss: 0.052574547939002514
2024-11-05 03:32:22,326 - INFO - [diffusion][Epoch 8779] diffusion learning rate: 0.001
2024-11-05 03:32:22,328 - INFO - [diffusion][Epoch 8779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:22,329 - INFO - [diffusion][Epoch 8780] Epoch 8781/12000
2024-11-05 03:32:26,344 - INFO - [diffusion][Epoch 8780] diffusion training Loss: 0.05501964129507542
2024-11-05 03:32:26,346 - INFO - [diffusion][Epoch 8780] diffusion learning rate: 0.001
2024-11-05 03:32:26,348 - INFO - [diffusion][Epoch 8780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:26,349 - INFO - [diffusion][Epoch 8781] Epoch 8782/12000
2024-11-05 03:32:30,294 - INFO - [diffusion][Epoch 8781] diffusion training Loss: 0.047299785539507866
2024-11-05 03:32:30,296 - INFO - [diffusion][Epoch 8781] diffusion learning rate: 0.001
2024-11-05 03:32:30,298 - INFO - [diffusion][Epoch 8781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:30,299 - INFO - [diffusion][Epoch 8782] Epoch 8783/12000
2024-11-05 03:32:34,376 - INFO - [diffusion][Epoch 8782] diffusion training Loss: 0.0519824530929327
2024-11-05 03:32:34,378 - INFO - [diffusion][Epoch 8782] diffusion learning rate: 0.001
2024-11-05 03:32:34,380 - INFO - [diffusion][Epoch 8782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:34,381 - INFO - [diffusion][Epoch 8783] Epoch 8784/12000
2024-11-05 03:32:38,389 - INFO - [diffusion][Epoch 8783] diffusion training Loss: 0.048380584456026554
2024-11-05 03:32:38,391 - INFO - [diffusion][Epoch 8783] diffusion learning rate: 0.001
2024-11-05 03:32:38,392 - INFO - [diffusion][Epoch 8783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:38,393 - INFO - [diffusion][Epoch 8784] Epoch 8785/12000
2024-11-05 03:32:42,427 - INFO - [diffusion][Epoch 8784] diffusion training Loss: 0.05307254381477833
2024-11-05 03:32:42,429 - INFO - [diffusion][Epoch 8784] diffusion learning rate: 0.001
2024-11-05 03:32:42,431 - INFO - [diffusion][Epoch 8784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:42,432 - INFO - [diffusion][Epoch 8785] Epoch 8786/12000
2024-11-05 03:32:46,394 - INFO - [diffusion][Epoch 8785] diffusion training Loss: 0.055159357376396656
2024-11-05 03:32:46,397 - INFO - [diffusion][Epoch 8785] diffusion learning rate: 0.001
2024-11-05 03:32:46,399 - INFO - [diffusion][Epoch 8785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:46,401 - INFO - [diffusion][Epoch 8786] Epoch 8787/12000
2024-11-05 03:32:50,609 - INFO - [diffusion][Epoch 8786] diffusion training Loss: 0.05257319379597902
2024-11-05 03:32:50,611 - INFO - [diffusion][Epoch 8786] diffusion learning rate: 0.001
2024-11-05 03:32:50,649 - INFO - [diffusion][Epoch 8786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:50,650 - INFO - [diffusion][Epoch 8787] Epoch 8788/12000
2024-11-05 03:32:54,683 - INFO - [diffusion][Epoch 8787] diffusion training Loss: 0.05542237125337124
2024-11-05 03:32:54,685 - INFO - [diffusion][Epoch 8787] diffusion learning rate: 0.001
2024-11-05 03:32:54,687 - INFO - [diffusion][Epoch 8787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:54,688 - INFO - [diffusion][Epoch 8788] Epoch 8789/12000
2024-11-05 03:32:58,693 - INFO - [diffusion][Epoch 8788] diffusion training Loss: 0.056005656719207764
2024-11-05 03:32:58,697 - INFO - [diffusion][Epoch 8788] diffusion learning rate: 0.001
2024-11-05 03:32:58,699 - INFO - [diffusion][Epoch 8788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:32:58,701 - INFO - [diffusion][Epoch 8789] Epoch 8790/12000
2024-11-05 03:33:02,681 - INFO - [diffusion][Epoch 8789] diffusion training Loss: 0.05706899892538786
2024-11-05 03:33:02,683 - INFO - [diffusion][Epoch 8789] diffusion learning rate: 0.001
2024-11-05 03:33:02,685 - INFO - [diffusion][Epoch 8789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:02,686 - INFO - [diffusion][Epoch 8790] Epoch 8791/12000
2024-11-05 03:33:06,824 - INFO - [diffusion][Epoch 8790] diffusion training Loss: 0.05227659083902836
2024-11-05 03:33:06,826 - INFO - [diffusion][Epoch 8790] diffusion learning rate: 0.001
2024-11-05 03:33:06,847 - INFO - [diffusion][Epoch 8790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:06,848 - INFO - [diffusion][Epoch 8791] Epoch 8792/12000
2024-11-05 03:33:10,954 - INFO - [diffusion][Epoch 8791] diffusion training Loss: 0.05360069591552019
2024-11-05 03:33:10,957 - INFO - [diffusion][Epoch 8791] diffusion learning rate: 0.001
2024-11-05 03:33:10,959 - INFO - [diffusion][Epoch 8791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:10,960 - INFO - [diffusion][Epoch 8792] Epoch 8793/12000
2024-11-05 03:33:14,867 - INFO - [diffusion][Epoch 8792] diffusion training Loss: 0.05320024583488703
2024-11-05 03:33:14,870 - INFO - [diffusion][Epoch 8792] diffusion learning rate: 0.001
2024-11-05 03:33:14,872 - INFO - [diffusion][Epoch 8792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:14,873 - INFO - [diffusion][Epoch 8793] Epoch 8794/12000
2024-11-05 03:33:18,830 - INFO - [diffusion][Epoch 8793] diffusion training Loss: 0.05961377266794443
2024-11-05 03:33:18,832 - INFO - [diffusion][Epoch 8793] diffusion learning rate: 0.001
2024-11-05 03:33:18,834 - INFO - [diffusion][Epoch 8793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:18,835 - INFO - [diffusion][Epoch 8794] Epoch 8795/12000
2024-11-05 03:33:22,936 - INFO - [diffusion][Epoch 8794] diffusion training Loss: 0.04977373220026493
2024-11-05 03:33:22,938 - INFO - [diffusion][Epoch 8794] diffusion learning rate: 0.001
2024-11-05 03:33:22,940 - INFO - [diffusion][Epoch 8794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:22,942 - INFO - [diffusion][Epoch 8795] Epoch 8796/12000
2024-11-05 03:33:27,073 - INFO - [diffusion][Epoch 8795] diffusion training Loss: 0.05468543712049723
2024-11-05 03:33:27,075 - INFO - [diffusion][Epoch 8795] diffusion learning rate: 0.001
2024-11-05 03:33:27,077 - INFO - [diffusion][Epoch 8795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:27,078 - INFO - [diffusion][Epoch 8796] Epoch 8797/12000
2024-11-05 03:33:31,204 - INFO - [diffusion][Epoch 8796] diffusion training Loss: 0.053724479861557484
2024-11-05 03:33:31,206 - INFO - [diffusion][Epoch 8796] diffusion learning rate: 0.001
2024-11-05 03:33:31,208 - INFO - [diffusion][Epoch 8796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:31,209 - INFO - [diffusion][Epoch 8797] Epoch 8798/12000
2024-11-05 03:33:35,321 - INFO - [diffusion][Epoch 8797] diffusion training Loss: 0.0543926116079092
2024-11-05 03:33:35,323 - INFO - [diffusion][Epoch 8797] diffusion learning rate: 0.001
2024-11-05 03:33:35,325 - INFO - [diffusion][Epoch 8797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:35,326 - INFO - [diffusion][Epoch 8798] Epoch 8799/12000
2024-11-05 03:33:39,474 - INFO - [diffusion][Epoch 8798] diffusion training Loss: 0.0518035963177681
2024-11-05 03:33:39,475 - INFO - [diffusion][Epoch 8798] diffusion learning rate: 0.001
2024-11-05 03:33:39,477 - INFO - [diffusion][Epoch 8798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:39,479 - INFO - [diffusion][Epoch 8799] Epoch 8800/12000
2024-11-05 03:33:43,517 - INFO - [diffusion][Epoch 8799] diffusion training Loss: 0.048839252442121506
2024-11-05 03:33:43,519 - INFO - [diffusion][Epoch 8799] diffusion learning rate: 0.001
2024-11-05 03:33:43,521 - INFO - [diffusion][Epoch 8799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:43,522 - INFO - [diffusion][Epoch 8800] Epoch 8801/12000
2024-11-05 03:33:47,576 - INFO - [diffusion][Epoch 8800] diffusion training Loss: 0.05499657616019249
2024-11-05 03:33:47,578 - INFO - [diffusion][Epoch 8800] diffusion learning rate: 0.001
2024-11-05 03:33:47,580 - INFO - [diffusion][Epoch 8800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:47,582 - INFO - [diffusion][Epoch 8801] Epoch 8802/12000
2024-11-05 03:33:51,677 - INFO - [diffusion][Epoch 8801] diffusion training Loss: 0.04980457667261362
2024-11-05 03:33:51,679 - INFO - [diffusion][Epoch 8801] diffusion learning rate: 0.001
2024-11-05 03:33:51,681 - INFO - [diffusion][Epoch 8801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:51,682 - INFO - [diffusion][Epoch 8802] Epoch 8803/12000
2024-11-05 03:33:55,653 - INFO - [diffusion][Epoch 8802] diffusion training Loss: 0.052815393544733524
2024-11-05 03:33:55,655 - INFO - [diffusion][Epoch 8802] diffusion learning rate: 0.001
2024-11-05 03:33:55,656 - INFO - [diffusion][Epoch 8802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:55,658 - INFO - [diffusion][Epoch 8803] Epoch 8804/12000
2024-11-05 03:33:59,737 - INFO - [diffusion][Epoch 8803] diffusion training Loss: 0.0540109109133482
2024-11-05 03:33:59,740 - INFO - [diffusion][Epoch 8803] diffusion learning rate: 0.001
2024-11-05 03:33:59,741 - INFO - [diffusion][Epoch 8803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:33:59,743 - INFO - [diffusion][Epoch 8804] Epoch 8805/12000
2024-11-05 03:34:03,852 - INFO - [diffusion][Epoch 8804] diffusion training Loss: 0.053027210757136345
2024-11-05 03:34:03,854 - INFO - [diffusion][Epoch 8804] diffusion learning rate: 0.001
2024-11-05 03:34:03,856 - INFO - [diffusion][Epoch 8804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:03,858 - INFO - [diffusion][Epoch 8805] Epoch 8806/12000
2024-11-05 03:34:08,267 - INFO - [diffusion][Epoch 8805] diffusion training Loss: 0.05090418830513954
2024-11-05 03:34:08,268 - INFO - [diffusion][Epoch 8805] diffusion learning rate: 0.001
2024-11-05 03:34:08,271 - INFO - [diffusion][Epoch 8805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:08,272 - INFO - [diffusion][Epoch 8806] Epoch 8807/12000
2024-11-05 03:34:12,383 - INFO - [diffusion][Epoch 8806] diffusion training Loss: 0.05706832371652126
2024-11-05 03:34:12,611 - INFO - [diffusion][Epoch 8806] diffusion learning rate: 0.001
2024-11-05 03:34:12,613 - INFO - [diffusion][Epoch 8806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:12,614 - INFO - [diffusion][Epoch 8807] Epoch 8808/12000
2024-11-05 03:34:16,693 - INFO - [diffusion][Epoch 8807] diffusion training Loss: 0.052980197593569756
2024-11-05 03:34:16,696 - INFO - [diffusion][Epoch 8807] diffusion learning rate: 0.001
2024-11-05 03:34:16,698 - INFO - [diffusion][Epoch 8807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:16,699 - INFO - [diffusion][Epoch 8808] Epoch 8809/12000
2024-11-05 03:34:20,771 - INFO - [diffusion][Epoch 8808] diffusion training Loss: 0.050920478999614716
2024-11-05 03:34:20,773 - INFO - [diffusion][Epoch 8808] diffusion learning rate: 0.001
2024-11-05 03:34:20,774 - INFO - [diffusion][Epoch 8808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:20,776 - INFO - [diffusion][Epoch 8809] Epoch 8810/12000
2024-11-05 03:34:24,825 - INFO - [diffusion][Epoch 8809] diffusion training Loss: 0.05240046512335539
2024-11-05 03:34:24,827 - INFO - [diffusion][Epoch 8809] diffusion learning rate: 0.001
2024-11-05 03:34:24,829 - INFO - [diffusion][Epoch 8809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:24,830 - INFO - [diffusion][Epoch 8810] Epoch 8811/12000
2024-11-05 03:34:28,868 - INFO - [diffusion][Epoch 8810] diffusion training Loss: 0.0508308969438076
2024-11-05 03:34:28,870 - INFO - [diffusion][Epoch 8810] diffusion learning rate: 0.001
2024-11-05 03:34:28,873 - INFO - [diffusion][Epoch 8810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:28,874 - INFO - [diffusion][Epoch 8811] Epoch 8812/12000
2024-11-05 03:34:32,935 - INFO - [diffusion][Epoch 8811] diffusion training Loss: 0.04994106758385897
2024-11-05 03:34:32,937 - INFO - [diffusion][Epoch 8811] diffusion learning rate: 0.001
2024-11-05 03:34:32,939 - INFO - [diffusion][Epoch 8811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:32,940 - INFO - [diffusion][Epoch 8812] Epoch 8813/12000
2024-11-05 03:34:37,095 - INFO - [diffusion][Epoch 8812] diffusion training Loss: 0.04697231110185385
2024-11-05 03:34:37,098 - INFO - [diffusion][Epoch 8812] diffusion learning rate: 0.001
2024-11-05 03:34:37,100 - INFO - [diffusion][Epoch 8812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:37,101 - INFO - [diffusion][Epoch 8813] Epoch 8814/12000
2024-11-05 03:34:41,114 - INFO - [diffusion][Epoch 8813] diffusion training Loss: 0.04742971248924732
2024-11-05 03:34:41,116 - INFO - [diffusion][Epoch 8813] diffusion learning rate: 0.001
2024-11-05 03:34:41,118 - INFO - [diffusion][Epoch 8813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:41,119 - INFO - [diffusion][Epoch 8814] Epoch 8815/12000
2024-11-05 03:34:45,214 - INFO - [diffusion][Epoch 8814] diffusion training Loss: 0.05199932213872671
2024-11-05 03:34:45,216 - INFO - [diffusion][Epoch 8814] diffusion learning rate: 0.001
2024-11-05 03:34:45,218 - INFO - [diffusion][Epoch 8814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:45,220 - INFO - [diffusion][Epoch 8815] Epoch 8816/12000
2024-11-05 03:34:49,226 - INFO - [diffusion][Epoch 8815] diffusion training Loss: 0.05324922129511833
2024-11-05 03:34:49,228 - INFO - [diffusion][Epoch 8815] diffusion learning rate: 0.001
2024-11-05 03:34:49,230 - INFO - [diffusion][Epoch 8815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:49,232 - INFO - [diffusion][Epoch 8816] Epoch 8817/12000
2024-11-05 03:34:53,186 - INFO - [diffusion][Epoch 8816] diffusion training Loss: 0.055550720542669296
2024-11-05 03:34:53,189 - INFO - [diffusion][Epoch 8816] diffusion learning rate: 0.001
2024-11-05 03:34:53,191 - INFO - [diffusion][Epoch 8816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:53,192 - INFO - [diffusion][Epoch 8817] Epoch 8818/12000
2024-11-05 03:34:57,264 - INFO - [diffusion][Epoch 8817] diffusion training Loss: 0.048559519462287426
2024-11-05 03:34:57,266 - INFO - [diffusion][Epoch 8817] diffusion learning rate: 0.001
2024-11-05 03:34:57,268 - INFO - [diffusion][Epoch 8817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:34:57,270 - INFO - [diffusion][Epoch 8818] Epoch 8819/12000
2024-11-05 03:35:01,376 - INFO - [diffusion][Epoch 8818] diffusion training Loss: 0.0574778588488698
2024-11-05 03:35:01,378 - INFO - [diffusion][Epoch 8818] diffusion learning rate: 0.001
2024-11-05 03:35:01,380 - INFO - [diffusion][Epoch 8818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:01,381 - INFO - [diffusion][Epoch 8819] Epoch 8820/12000
2024-11-05 03:35:05,499 - INFO - [diffusion][Epoch 8819] diffusion training Loss: 0.05240645632147789
2024-11-05 03:35:05,501 - INFO - [diffusion][Epoch 8819] diffusion learning rate: 0.001
2024-11-05 03:35:05,502 - INFO - [diffusion][Epoch 8819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:05,504 - INFO - [diffusion][Epoch 8820] Epoch 8821/12000
2024-11-05 03:35:09,587 - INFO - [diffusion][Epoch 8820] diffusion training Loss: 0.05189735721796751
2024-11-05 03:35:09,589 - INFO - [diffusion][Epoch 8820] diffusion learning rate: 0.001
2024-11-05 03:35:09,590 - INFO - [diffusion][Epoch 8820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:09,592 - INFO - [diffusion][Epoch 8821] Epoch 8822/12000
2024-11-05 03:35:13,818 - INFO - [diffusion][Epoch 8821] diffusion training Loss: 0.0510765640065074
2024-11-05 03:35:13,820 - INFO - [diffusion][Epoch 8821] diffusion learning rate: 0.001
2024-11-05 03:35:13,841 - INFO - [diffusion][Epoch 8821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:13,842 - INFO - [diffusion][Epoch 8822] Epoch 8823/12000
2024-11-05 03:35:17,950 - INFO - [diffusion][Epoch 8822] diffusion training Loss: 0.0560391079634428
2024-11-05 03:35:17,953 - INFO - [diffusion][Epoch 8822] diffusion learning rate: 0.001
2024-11-05 03:35:17,955 - INFO - [diffusion][Epoch 8822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:17,956 - INFO - [diffusion][Epoch 8823] Epoch 8824/12000
2024-11-05 03:35:21,906 - INFO - [diffusion][Epoch 8823] diffusion training Loss: 0.05954108573496342
2024-11-05 03:35:21,908 - INFO - [diffusion][Epoch 8823] diffusion learning rate: 0.001
2024-11-05 03:35:21,910 - INFO - [diffusion][Epoch 8823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:21,911 - INFO - [diffusion][Epoch 8824] Epoch 8825/12000
2024-11-05 03:35:25,870 - INFO - [diffusion][Epoch 8824] diffusion training Loss: 0.051498761400580406
2024-11-05 03:35:25,872 - INFO - [diffusion][Epoch 8824] diffusion learning rate: 0.001
2024-11-05 03:35:25,874 - INFO - [diffusion][Epoch 8824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:25,875 - INFO - [diffusion][Epoch 8825] Epoch 8826/12000
2024-11-05 03:35:29,903 - INFO - [diffusion][Epoch 8825] diffusion training Loss: 0.05267483089119196
2024-11-05 03:35:29,905 - INFO - [diffusion][Epoch 8825] diffusion learning rate: 0.001
2024-11-05 03:35:29,907 - INFO - [diffusion][Epoch 8825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:29,908 - INFO - [diffusion][Epoch 8826] Epoch 8827/12000
2024-11-05 03:35:33,830 - INFO - [diffusion][Epoch 8826] diffusion training Loss: 0.052512917667627335
2024-11-05 03:35:33,832 - INFO - [diffusion][Epoch 8826] diffusion learning rate: 0.001
2024-11-05 03:35:33,833 - INFO - [diffusion][Epoch 8826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:33,835 - INFO - [diffusion][Epoch 8827] Epoch 8828/12000
2024-11-05 03:35:37,854 - INFO - [diffusion][Epoch 8827] diffusion training Loss: 0.05506264232099056
2024-11-05 03:35:37,856 - INFO - [diffusion][Epoch 8827] diffusion learning rate: 0.001
2024-11-05 03:35:37,857 - INFO - [diffusion][Epoch 8827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:37,859 - INFO - [diffusion][Epoch 8828] Epoch 8829/12000
2024-11-05 03:35:41,953 - INFO - [diffusion][Epoch 8828] diffusion training Loss: 0.05399008467793465
2024-11-05 03:35:41,956 - INFO - [diffusion][Epoch 8828] diffusion learning rate: 0.001
2024-11-05 03:35:41,957 - INFO - [diffusion][Epoch 8828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:41,959 - INFO - [diffusion][Epoch 8829] Epoch 8830/12000
2024-11-05 03:35:46,083 - INFO - [diffusion][Epoch 8829] diffusion training Loss: 0.05230258125811815
2024-11-05 03:35:46,085 - INFO - [diffusion][Epoch 8829] diffusion learning rate: 0.001
2024-11-05 03:35:46,087 - INFO - [diffusion][Epoch 8829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:46,088 - INFO - [diffusion][Epoch 8830] Epoch 8831/12000
2024-11-05 03:35:50,100 - INFO - [diffusion][Epoch 8830] diffusion training Loss: 0.054165227338671684
2024-11-05 03:35:50,102 - INFO - [diffusion][Epoch 8830] diffusion learning rate: 0.001
2024-11-05 03:35:50,104 - INFO - [diffusion][Epoch 8830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:50,105 - INFO - [diffusion][Epoch 8831] Epoch 8832/12000
2024-11-05 03:35:54,072 - INFO - [diffusion][Epoch 8831] diffusion training Loss: 0.05002929829061031
2024-11-05 03:35:54,075 - INFO - [diffusion][Epoch 8831] diffusion learning rate: 0.001
2024-11-05 03:35:54,076 - INFO - [diffusion][Epoch 8831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:54,078 - INFO - [diffusion][Epoch 8832] Epoch 8833/12000
2024-11-05 03:35:58,107 - INFO - [diffusion][Epoch 8832] diffusion training Loss: 0.04611040651798248
2024-11-05 03:35:58,109 - INFO - [diffusion][Epoch 8832] diffusion learning rate: 0.001
2024-11-05 03:35:58,111 - INFO - [diffusion][Epoch 8832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:35:58,112 - INFO - [diffusion][Epoch 8833] Epoch 8834/12000
2024-11-05 03:36:02,097 - INFO - [diffusion][Epoch 8833] diffusion training Loss: 0.05635993182659149
2024-11-05 03:36:02,100 - INFO - [diffusion][Epoch 8833] diffusion learning rate: 0.001
2024-11-05 03:36:02,101 - INFO - [diffusion][Epoch 8833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:02,102 - INFO - [diffusion][Epoch 8834] Epoch 8835/12000
2024-11-05 03:36:06,176 - INFO - [diffusion][Epoch 8834] diffusion training Loss: 0.04955580551177263
2024-11-05 03:36:06,178 - INFO - [diffusion][Epoch 8834] diffusion learning rate: 0.001
2024-11-05 03:36:06,180 - INFO - [diffusion][Epoch 8834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:06,181 - INFO - [diffusion][Epoch 8835] Epoch 8836/12000
2024-11-05 03:36:10,310 - INFO - [diffusion][Epoch 8835] diffusion training Loss: 0.05367426760494709
2024-11-05 03:36:10,312 - INFO - [diffusion][Epoch 8835] diffusion learning rate: 0.001
2024-11-05 03:36:10,314 - INFO - [diffusion][Epoch 8835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:10,315 - INFO - [diffusion][Epoch 8836] Epoch 8837/12000
2024-11-05 03:36:14,421 - INFO - [diffusion][Epoch 8836] diffusion training Loss: 0.05265034642070532
2024-11-05 03:36:14,423 - INFO - [diffusion][Epoch 8836] diffusion learning rate: 0.001
2024-11-05 03:36:14,425 - INFO - [diffusion][Epoch 8836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:14,426 - INFO - [diffusion][Epoch 8837] Epoch 8838/12000
2024-11-05 03:36:18,524 - INFO - [diffusion][Epoch 8837] diffusion training Loss: 0.05040057189762592
2024-11-05 03:36:18,526 - INFO - [diffusion][Epoch 8837] diffusion learning rate: 0.001
2024-11-05 03:36:18,528 - INFO - [diffusion][Epoch 8837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:18,529 - INFO - [diffusion][Epoch 8838] Epoch 8839/12000
2024-11-05 03:36:22,631 - INFO - [diffusion][Epoch 8838] diffusion training Loss: 0.05112260673195124
2024-11-05 03:36:22,632 - INFO - [diffusion][Epoch 8838] diffusion learning rate: 0.001
2024-11-05 03:36:22,634 - INFO - [diffusion][Epoch 8838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:22,635 - INFO - [diffusion][Epoch 8839] Epoch 8840/12000
2024-11-05 03:36:26,711 - INFO - [diffusion][Epoch 8839] diffusion training Loss: 0.04997014906257391
2024-11-05 03:36:26,713 - INFO - [diffusion][Epoch 8839] diffusion learning rate: 0.001
2024-11-05 03:36:26,715 - INFO - [diffusion][Epoch 8839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:26,716 - INFO - [diffusion][Epoch 8840] Epoch 8841/12000
2024-11-05 03:36:30,767 - INFO - [diffusion][Epoch 8840] diffusion training Loss: 0.051636386662721634
2024-11-05 03:36:30,769 - INFO - [diffusion][Epoch 8840] diffusion learning rate: 0.001
2024-11-05 03:36:30,771 - INFO - [diffusion][Epoch 8840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:30,772 - INFO - [diffusion][Epoch 8841] Epoch 8842/12000
2024-11-05 03:36:34,811 - INFO - [diffusion][Epoch 8841] diffusion training Loss: 0.052012535743415356
2024-11-05 03:36:34,813 - INFO - [diffusion][Epoch 8841] diffusion learning rate: 0.001
2024-11-05 03:36:34,815 - INFO - [diffusion][Epoch 8841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:34,816 - INFO - [diffusion][Epoch 8842] Epoch 8843/12000
2024-11-05 03:36:38,893 - INFO - [diffusion][Epoch 8842] diffusion training Loss: 0.05040091276168823
2024-11-05 03:36:38,895 - INFO - [diffusion][Epoch 8842] diffusion learning rate: 0.001
2024-11-05 03:36:38,897 - INFO - [diffusion][Epoch 8842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:38,898 - INFO - [diffusion][Epoch 8843] Epoch 8844/12000
2024-11-05 03:36:43,000 - INFO - [diffusion][Epoch 8843] diffusion training Loss: 0.0541665144264698
2024-11-05 03:36:43,002 - INFO - [diffusion][Epoch 8843] diffusion learning rate: 0.001
2024-11-05 03:36:43,004 - INFO - [diffusion][Epoch 8843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:43,005 - INFO - [diffusion][Epoch 8844] Epoch 8845/12000
2024-11-05 03:36:47,055 - INFO - [diffusion][Epoch 8844] diffusion training Loss: 0.051414391957223415
2024-11-05 03:36:47,057 - INFO - [diffusion][Epoch 8844] diffusion learning rate: 0.001
2024-11-05 03:36:47,059 - INFO - [diffusion][Epoch 8844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:47,061 - INFO - [diffusion][Epoch 8845] Epoch 8846/12000
2024-11-05 03:36:51,042 - INFO - [diffusion][Epoch 8845] diffusion training Loss: 0.055063750594854355
2024-11-05 03:36:51,044 - INFO - [diffusion][Epoch 8845] diffusion learning rate: 0.001
2024-11-05 03:36:51,046 - INFO - [diffusion][Epoch 8845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:51,047 - INFO - [diffusion][Epoch 8846] Epoch 8847/12000
2024-11-05 03:36:55,073 - INFO - [diffusion][Epoch 8846] diffusion training Loss: 0.04786813352257013
2024-11-05 03:36:55,076 - INFO - [diffusion][Epoch 8846] diffusion learning rate: 0.001
2024-11-05 03:36:55,078 - INFO - [diffusion][Epoch 8846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:55,079 - INFO - [diffusion][Epoch 8847] Epoch 8848/12000
2024-11-05 03:36:59,409 - INFO - [diffusion][Epoch 8847] diffusion training Loss: 0.0499491598457098
2024-11-05 03:36:59,412 - INFO - [diffusion][Epoch 8847] diffusion learning rate: 0.001
2024-11-05 03:36:59,414 - INFO - [diffusion][Epoch 8847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:36:59,415 - INFO - [diffusion][Epoch 8848] Epoch 8849/12000
2024-11-05 03:37:03,522 - INFO - [diffusion][Epoch 8848] diffusion training Loss: 0.04853804875165224
2024-11-05 03:37:03,525 - INFO - [diffusion][Epoch 8848] diffusion learning rate: 0.001
2024-11-05 03:37:03,527 - INFO - [diffusion][Epoch 8848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:03,528 - INFO - [diffusion][Epoch 8849] Epoch 8850/12000
2024-11-05 03:37:07,605 - INFO - [diffusion][Epoch 8849] diffusion training Loss: 0.04710226692259312
2024-11-05 03:37:07,607 - INFO - [diffusion][Epoch 8849] diffusion learning rate: 0.001
2024-11-05 03:37:07,609 - INFO - [diffusion][Epoch 8849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:07,610 - INFO - [diffusion][Epoch 8850] Epoch 8851/12000
2024-11-05 03:37:11,528 - INFO - [diffusion][Epoch 8850] diffusion training Loss: 0.05515751242637634
2024-11-05 03:37:11,530 - INFO - [diffusion][Epoch 8850] diffusion learning rate: 0.001
2024-11-05 03:37:11,532 - INFO - [diffusion][Epoch 8850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:11,534 - INFO - [diffusion][Epoch 8851] Epoch 8852/12000
2024-11-05 03:37:15,667 - INFO - [diffusion][Epoch 8851] diffusion training Loss: 0.05354177765548229
2024-11-05 03:37:15,669 - INFO - [diffusion][Epoch 8851] diffusion learning rate: 0.001
2024-11-05 03:37:15,671 - INFO - [diffusion][Epoch 8851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:15,672 - INFO - [diffusion][Epoch 8852] Epoch 8853/12000
2024-11-05 03:37:19,694 - INFO - [diffusion][Epoch 8852] diffusion training Loss: 0.04895100090652704
2024-11-05 03:37:19,696 - INFO - [diffusion][Epoch 8852] diffusion learning rate: 0.001
2024-11-05 03:37:19,698 - INFO - [diffusion][Epoch 8852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:19,699 - INFO - [diffusion][Epoch 8853] Epoch 8854/12000
2024-11-05 03:37:23,771 - INFO - [diffusion][Epoch 8853] diffusion training Loss: 0.0518853347748518
2024-11-05 03:37:23,774 - INFO - [diffusion][Epoch 8853] diffusion learning rate: 0.001
2024-11-05 03:37:23,776 - INFO - [diffusion][Epoch 8853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:23,777 - INFO - [diffusion][Epoch 8854] Epoch 8855/12000
2024-11-05 03:37:27,864 - INFO - [diffusion][Epoch 8854] diffusion training Loss: 0.05562304798513651
2024-11-05 03:37:27,866 - INFO - [diffusion][Epoch 8854] diffusion learning rate: 0.001
2024-11-05 03:37:27,868 - INFO - [diffusion][Epoch 8854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:27,869 - INFO - [diffusion][Epoch 8855] Epoch 8856/12000
2024-11-05 03:37:31,792 - INFO - [diffusion][Epoch 8855] diffusion training Loss: 0.056634086184203625
2024-11-05 03:37:31,794 - INFO - [diffusion][Epoch 8855] diffusion learning rate: 0.001
2024-11-05 03:37:31,795 - INFO - [diffusion][Epoch 8855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:31,797 - INFO - [diffusion][Epoch 8856] Epoch 8857/12000
2024-11-05 03:37:35,895 - INFO - [diffusion][Epoch 8856] diffusion training Loss: 0.05054109077900648
2024-11-05 03:37:35,897 - INFO - [diffusion][Epoch 8856] diffusion learning rate: 0.001
2024-11-05 03:37:35,946 - INFO - [diffusion][Epoch 8856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:35,947 - INFO - [diffusion][Epoch 8857] Epoch 8858/12000
2024-11-05 03:37:40,031 - INFO - [diffusion][Epoch 8857] diffusion training Loss: 0.05511661805212498
2024-11-05 03:37:40,033 - INFO - [diffusion][Epoch 8857] diffusion learning rate: 0.001
2024-11-05 03:37:40,035 - INFO - [diffusion][Epoch 8857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:40,036 - INFO - [diffusion][Epoch 8858] Epoch 8859/12000
2024-11-05 03:37:44,125 - INFO - [diffusion][Epoch 8858] diffusion training Loss: 0.05022421199828386
2024-11-05 03:37:44,127 - INFO - [diffusion][Epoch 8858] diffusion learning rate: 0.001
2024-11-05 03:37:44,129 - INFO - [diffusion][Epoch 8858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:44,130 - INFO - [diffusion][Epoch 8859] Epoch 8860/12000
2024-11-05 03:37:48,151 - INFO - [diffusion][Epoch 8859] diffusion training Loss: 0.04927609208971262
2024-11-05 03:37:48,153 - INFO - [diffusion][Epoch 8859] diffusion learning rate: 0.001
2024-11-05 03:37:48,155 - INFO - [diffusion][Epoch 8859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:48,156 - INFO - [diffusion][Epoch 8860] Epoch 8861/12000
2024-11-05 03:37:52,180 - INFO - [diffusion][Epoch 8860] diffusion training Loss: 0.05122848693281412
2024-11-05 03:37:52,182 - INFO - [diffusion][Epoch 8860] diffusion learning rate: 0.001
2024-11-05 03:37:52,184 - INFO - [diffusion][Epoch 8860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:52,185 - INFO - [diffusion][Epoch 8861] Epoch 8862/12000
2024-11-05 03:37:56,214 - INFO - [diffusion][Epoch 8861] diffusion training Loss: 0.05370587110519409
2024-11-05 03:37:56,216 - INFO - [diffusion][Epoch 8861] diffusion learning rate: 0.001
2024-11-05 03:37:56,218 - INFO - [diffusion][Epoch 8861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:37:56,219 - INFO - [diffusion][Epoch 8862] Epoch 8863/12000
2024-11-05 03:38:00,318 - INFO - [diffusion][Epoch 8862] diffusion training Loss: 0.05073870811611414
2024-11-05 03:38:00,320 - INFO - [diffusion][Epoch 8862] diffusion learning rate: 0.001
2024-11-05 03:38:00,321 - INFO - [diffusion][Epoch 8862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:00,323 - INFO - [diffusion][Epoch 8863] Epoch 8864/12000
2024-11-05 03:38:04,447 - INFO - [diffusion][Epoch 8863] diffusion training Loss: 0.0512303588911891
2024-11-05 03:38:04,449 - INFO - [diffusion][Epoch 8863] diffusion learning rate: 0.001
2024-11-05 03:38:04,451 - INFO - [diffusion][Epoch 8863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:04,453 - INFO - [diffusion][Epoch 8864] Epoch 8865/12000
2024-11-05 03:38:08,398 - INFO - [diffusion][Epoch 8864] diffusion training Loss: 0.05149994418025017
2024-11-05 03:38:08,400 - INFO - [diffusion][Epoch 8864] diffusion learning rate: 0.001
2024-11-05 03:38:08,402 - INFO - [diffusion][Epoch 8864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:08,403 - INFO - [diffusion][Epoch 8865] Epoch 8866/12000
2024-11-05 03:38:12,427 - INFO - [diffusion][Epoch 8865] diffusion training Loss: 0.05079992115497589
2024-11-05 03:38:12,429 - INFO - [diffusion][Epoch 8865] diffusion learning rate: 0.001
2024-11-05 03:38:12,430 - INFO - [diffusion][Epoch 8865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:12,431 - INFO - [diffusion][Epoch 8866] Epoch 8867/12000
2024-11-05 03:38:16,482 - INFO - [diffusion][Epoch 8866] diffusion training Loss: 0.05211200751364231
2024-11-05 03:38:16,484 - INFO - [diffusion][Epoch 8866] diffusion learning rate: 0.001
2024-11-05 03:38:16,486 - INFO - [diffusion][Epoch 8866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:16,487 - INFO - [diffusion][Epoch 8867] Epoch 8868/12000
2024-11-05 03:38:20,427 - INFO - [diffusion][Epoch 8867] diffusion training Loss: 0.047697803005576134
2024-11-05 03:38:20,429 - INFO - [diffusion][Epoch 8867] diffusion learning rate: 0.001
2024-11-05 03:38:20,457 - INFO - [diffusion][Epoch 8867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:20,459 - INFO - [diffusion][Epoch 8868] Epoch 8869/12000
2024-11-05 03:38:24,570 - INFO - [diffusion][Epoch 8868] diffusion training Loss: 0.05240478087216616
2024-11-05 03:38:24,572 - INFO - [diffusion][Epoch 8868] diffusion learning rate: 0.001
2024-11-05 03:38:24,574 - INFO - [diffusion][Epoch 8868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:24,575 - INFO - [diffusion][Epoch 8869] Epoch 8870/12000
2024-11-05 03:38:29,190 - INFO - [diffusion][Epoch 8869] diffusion training Loss: 0.04837834741920233
2024-11-05 03:38:29,193 - INFO - [diffusion][Epoch 8869] diffusion learning rate: 0.001
2024-11-05 03:38:29,195 - INFO - [diffusion][Epoch 8869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:29,197 - INFO - [diffusion][Epoch 8870] Epoch 8871/12000
2024-11-05 03:38:33,178 - INFO - [diffusion][Epoch 8870] diffusion training Loss: 0.05497569590806961
2024-11-05 03:38:33,180 - INFO - [diffusion][Epoch 8870] diffusion learning rate: 0.001
2024-11-05 03:38:33,182 - INFO - [diffusion][Epoch 8870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:33,183 - INFO - [diffusion][Epoch 8871] Epoch 8872/12000
2024-11-05 03:38:37,037 - INFO - [diffusion][Epoch 8871] diffusion training Loss: 0.05343762692064047
2024-11-05 03:38:37,040 - INFO - [diffusion][Epoch 8871] diffusion learning rate: 0.001
2024-11-05 03:38:37,042 - INFO - [diffusion][Epoch 8871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:37,043 - INFO - [diffusion][Epoch 8872] Epoch 8873/12000
2024-11-05 03:38:41,042 - INFO - [diffusion][Epoch 8872] diffusion training Loss: 0.04773775301873684
2024-11-05 03:38:41,044 - INFO - [diffusion][Epoch 8872] diffusion learning rate: 0.001
2024-11-05 03:38:41,046 - INFO - [diffusion][Epoch 8872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:41,047 - INFO - [diffusion][Epoch 8873] Epoch 8874/12000
2024-11-05 03:38:45,166 - INFO - [diffusion][Epoch 8873] diffusion training Loss: 0.052814639173448086
2024-11-05 03:38:45,168 - INFO - [diffusion][Epoch 8873] diffusion learning rate: 0.001
2024-11-05 03:38:45,170 - INFO - [diffusion][Epoch 8873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:45,172 - INFO - [diffusion][Epoch 8874] Epoch 8875/12000
2024-11-05 03:38:49,066 - INFO - [diffusion][Epoch 8874] diffusion training Loss: 0.046066283248364925
2024-11-05 03:38:49,068 - INFO - [diffusion][Epoch 8874] diffusion learning rate: 0.001
2024-11-05 03:38:49,070 - INFO - [diffusion][Epoch 8874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:49,071 - INFO - [diffusion][Epoch 8875] Epoch 8876/12000
2024-11-05 03:38:53,130 - INFO - [diffusion][Epoch 8875] diffusion training Loss: 0.05392689350992441
2024-11-05 03:38:53,132 - INFO - [diffusion][Epoch 8875] diffusion learning rate: 0.001
2024-11-05 03:38:53,134 - INFO - [diffusion][Epoch 8875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:53,135 - INFO - [diffusion][Epoch 8876] Epoch 8877/12000
2024-11-05 03:38:57,153 - INFO - [diffusion][Epoch 8876] diffusion training Loss: 0.05313394218683243
2024-11-05 03:38:57,155 - INFO - [diffusion][Epoch 8876] diffusion learning rate: 0.001
2024-11-05 03:38:57,157 - INFO - [diffusion][Epoch 8876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:38:57,158 - INFO - [diffusion][Epoch 8877] Epoch 8878/12000
2024-11-05 03:39:01,023 - INFO - [diffusion][Epoch 8877] diffusion training Loss: 0.05630732048302889
2024-11-05 03:39:01,026 - INFO - [diffusion][Epoch 8877] diffusion learning rate: 0.001
2024-11-05 03:39:01,028 - INFO - [diffusion][Epoch 8877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:01,029 - INFO - [diffusion][Epoch 8878] Epoch 8879/12000
2024-11-05 03:39:05,143 - INFO - [diffusion][Epoch 8878] diffusion training Loss: 0.050884054973721504
2024-11-05 03:39:05,146 - INFO - [diffusion][Epoch 8878] diffusion learning rate: 0.001
2024-11-05 03:39:05,148 - INFO - [diffusion][Epoch 8878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:05,149 - INFO - [diffusion][Epoch 8879] Epoch 8880/12000
2024-11-05 03:39:09,169 - INFO - [diffusion][Epoch 8879] diffusion training Loss: 0.05060537997633219
2024-11-05 03:39:09,171 - INFO - [diffusion][Epoch 8879] diffusion learning rate: 0.001
2024-11-05 03:39:09,173 - INFO - [diffusion][Epoch 8879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:09,174 - INFO - [diffusion][Epoch 8880] Epoch 8881/12000
2024-11-05 03:39:13,221 - INFO - [diffusion][Epoch 8880] diffusion training Loss: 0.05488856043666601
2024-11-05 03:39:13,223 - INFO - [diffusion][Epoch 8880] diffusion learning rate: 0.001
2024-11-05 03:39:13,225 - INFO - [diffusion][Epoch 8880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:13,226 - INFO - [diffusion][Epoch 8881] Epoch 8882/12000
2024-11-05 03:39:17,277 - INFO - [diffusion][Epoch 8881] diffusion training Loss: 0.0583763038739562
2024-11-05 03:39:17,279 - INFO - [diffusion][Epoch 8881] diffusion learning rate: 0.001
2024-11-05 03:39:17,281 - INFO - [diffusion][Epoch 8881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:17,282 - INFO - [diffusion][Epoch 8882] Epoch 8883/12000
2024-11-05 03:39:21,365 - INFO - [diffusion][Epoch 8882] diffusion training Loss: 0.05034068878740072
2024-11-05 03:39:21,367 - INFO - [diffusion][Epoch 8882] diffusion learning rate: 0.001
2024-11-05 03:39:21,369 - INFO - [diffusion][Epoch 8882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:21,371 - INFO - [diffusion][Epoch 8883] Epoch 8884/12000
2024-11-05 03:39:25,528 - INFO - [diffusion][Epoch 8883] diffusion training Loss: 0.05542005319148302
2024-11-05 03:39:25,530 - INFO - [diffusion][Epoch 8883] diffusion learning rate: 0.001
2024-11-05 03:39:25,532 - INFO - [diffusion][Epoch 8883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:25,550 - INFO - [diffusion][Epoch 8884] Epoch 8885/12000
2024-11-05 03:39:29,618 - INFO - [diffusion][Epoch 8884] diffusion training Loss: 0.046585917472839355
2024-11-05 03:39:29,620 - INFO - [diffusion][Epoch 8884] diffusion learning rate: 0.001
2024-11-05 03:39:29,622 - INFO - [diffusion][Epoch 8884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:29,623 - INFO - [diffusion][Epoch 8885] Epoch 8886/12000
2024-11-05 03:39:33,650 - INFO - [diffusion][Epoch 8885] diffusion training Loss: 0.05248136259615421
2024-11-05 03:39:33,652 - INFO - [diffusion][Epoch 8885] diffusion learning rate: 0.001
2024-11-05 03:39:33,654 - INFO - [diffusion][Epoch 8885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:33,655 - INFO - [diffusion][Epoch 8886] Epoch 8887/12000
2024-11-05 03:39:37,719 - INFO - [diffusion][Epoch 8886] diffusion training Loss: 0.054000502452254295
2024-11-05 03:39:37,722 - INFO - [diffusion][Epoch 8886] diffusion learning rate: 0.001
2024-11-05 03:39:37,725 - INFO - [diffusion][Epoch 8886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:37,727 - INFO - [diffusion][Epoch 8887] Epoch 8888/12000
2024-11-05 03:39:41,560 - INFO - [diffusion][Epoch 8887] diffusion training Loss: 0.05424943193793297
2024-11-05 03:39:41,562 - INFO - [diffusion][Epoch 8887] diffusion learning rate: 0.001
2024-11-05 03:39:41,563 - INFO - [diffusion][Epoch 8887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:41,564 - INFO - [diffusion][Epoch 8888] Epoch 8889/12000
2024-11-05 03:39:45,562 - INFO - [diffusion][Epoch 8888] diffusion training Loss: 0.05404517613351345
2024-11-05 03:39:45,564 - INFO - [diffusion][Epoch 8888] diffusion learning rate: 0.001
2024-11-05 03:39:45,565 - INFO - [diffusion][Epoch 8888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:45,567 - INFO - [diffusion][Epoch 8889] Epoch 8890/12000
2024-11-05 03:39:49,615 - INFO - [diffusion][Epoch 8889] diffusion training Loss: 0.05958237033337355
2024-11-05 03:39:49,617 - INFO - [diffusion][Epoch 8889] diffusion learning rate: 0.001
2024-11-05 03:39:49,619 - INFO - [diffusion][Epoch 8889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:49,620 - INFO - [diffusion][Epoch 8890] Epoch 8891/12000
2024-11-05 03:39:53,501 - INFO - [diffusion][Epoch 8890] diffusion training Loss: 0.0529022179543972
2024-11-05 03:39:53,503 - INFO - [diffusion][Epoch 8890] diffusion learning rate: 0.001
2024-11-05 03:39:53,504 - INFO - [diffusion][Epoch 8890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:53,506 - INFO - [diffusion][Epoch 8891] Epoch 8892/12000
2024-11-05 03:39:57,536 - INFO - [diffusion][Epoch 8891] diffusion training Loss: 0.049837508238852024
2024-11-05 03:39:57,539 - INFO - [diffusion][Epoch 8891] diffusion learning rate: 0.001
2024-11-05 03:39:57,540 - INFO - [diffusion][Epoch 8891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:39:57,542 - INFO - [diffusion][Epoch 8892] Epoch 8893/12000
2024-11-05 03:40:01,465 - INFO - [diffusion][Epoch 8892] diffusion training Loss: 0.05248038750141859
2024-11-05 03:40:01,467 - INFO - [diffusion][Epoch 8892] diffusion learning rate: 0.001
2024-11-05 03:40:01,469 - INFO - [diffusion][Epoch 8892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:01,471 - INFO - [diffusion][Epoch 8893] Epoch 8894/12000
2024-11-05 03:40:05,355 - INFO - [diffusion][Epoch 8893] diffusion training Loss: 0.051449173130095005
2024-11-05 03:40:05,358 - INFO - [diffusion][Epoch 8893] diffusion learning rate: 0.001
2024-11-05 03:40:05,360 - INFO - [diffusion][Epoch 8893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:05,361 - INFO - [diffusion][Epoch 8894] Epoch 8895/12000
2024-11-05 03:40:09,427 - INFO - [diffusion][Epoch 8894] diffusion training Loss: 0.05326082184910774
2024-11-05 03:40:09,429 - INFO - [diffusion][Epoch 8894] diffusion learning rate: 0.001
2024-11-05 03:40:09,431 - INFO - [diffusion][Epoch 8894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:09,433 - INFO - [diffusion][Epoch 8895] Epoch 8896/12000
2024-11-05 03:40:13,468 - INFO - [diffusion][Epoch 8895] diffusion training Loss: 0.04980046860873699
2024-11-05 03:40:13,470 - INFO - [diffusion][Epoch 8895] diffusion learning rate: 0.001
2024-11-05 03:40:13,471 - INFO - [diffusion][Epoch 8895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:13,472 - INFO - [diffusion][Epoch 8896] Epoch 8897/12000
2024-11-05 03:40:17,571 - INFO - [diffusion][Epoch 8896] diffusion training Loss: 0.054445816203951836
2024-11-05 03:40:17,573 - INFO - [diffusion][Epoch 8896] diffusion learning rate: 0.001
2024-11-05 03:40:17,574 - INFO - [diffusion][Epoch 8896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:17,576 - INFO - [diffusion][Epoch 8897] Epoch 8898/12000
2024-11-05 03:40:21,668 - INFO - [diffusion][Epoch 8897] diffusion training Loss: 0.051802054047584534
2024-11-05 03:40:21,670 - INFO - [diffusion][Epoch 8897] diffusion learning rate: 0.001
2024-11-05 03:40:21,696 - INFO - [diffusion][Epoch 8897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:21,697 - INFO - [diffusion][Epoch 8898] Epoch 8899/12000
2024-11-05 03:40:25,729 - INFO - [diffusion][Epoch 8898] diffusion training Loss: 0.04961707070469856
2024-11-05 03:40:25,732 - INFO - [diffusion][Epoch 8898] diffusion learning rate: 0.001
2024-11-05 03:40:25,733 - INFO - [diffusion][Epoch 8898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:25,735 - INFO - [diffusion][Epoch 8899] Epoch 8900/12000
2024-11-05 03:40:29,668 - INFO - [diffusion][Epoch 8899] diffusion training Loss: 0.053709360770881176
2024-11-05 03:40:29,670 - INFO - [diffusion][Epoch 8899] diffusion learning rate: 0.001
2024-11-05 03:40:29,672 - INFO - [diffusion][Epoch 8899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:29,673 - INFO - [diffusion][Epoch 8900] Epoch 8901/12000
2024-11-05 03:40:33,671 - INFO - [diffusion][Epoch 8900] diffusion training Loss: 0.05303863435983658
2024-11-05 03:40:33,674 - INFO - [diffusion][Epoch 8900] diffusion learning rate: 0.001
2024-11-05 03:40:33,676 - INFO - [diffusion][Epoch 8900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:33,677 - INFO - [diffusion][Epoch 8901] Epoch 8902/12000
2024-11-05 03:40:37,630 - INFO - [diffusion][Epoch 8901] diffusion training Loss: 0.05275579355657101
2024-11-05 03:40:37,632 - INFO - [diffusion][Epoch 8901] diffusion learning rate: 0.001
2024-11-05 03:40:37,633 - INFO - [diffusion][Epoch 8901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:37,635 - INFO - [diffusion][Epoch 8902] Epoch 8903/12000
2024-11-05 03:40:41,714 - INFO - [diffusion][Epoch 8902] diffusion training Loss: 0.05382570903748274
2024-11-05 03:40:41,716 - INFO - [diffusion][Epoch 8902] diffusion learning rate: 0.001
2024-11-05 03:40:41,719 - INFO - [diffusion][Epoch 8902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:41,720 - INFO - [diffusion][Epoch 8903] Epoch 8904/12000
2024-11-05 03:40:45,593 - INFO - [diffusion][Epoch 8903] diffusion training Loss: 0.05408609751611948
2024-11-05 03:40:45,595 - INFO - [diffusion][Epoch 8903] diffusion learning rate: 0.001
2024-11-05 03:40:45,596 - INFO - [diffusion][Epoch 8903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:45,597 - INFO - [diffusion][Epoch 8904] Epoch 8905/12000
2024-11-05 03:40:49,694 - INFO - [diffusion][Epoch 8904] diffusion training Loss: 0.05474577844142914
2024-11-05 03:40:49,696 - INFO - [diffusion][Epoch 8904] diffusion learning rate: 0.001
2024-11-05 03:40:49,697 - INFO - [diffusion][Epoch 8904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:49,698 - INFO - [diffusion][Epoch 8905] Epoch 8906/12000
2024-11-05 03:40:53,819 - INFO - [diffusion][Epoch 8905] diffusion training Loss: 0.053333221934735775
2024-11-05 03:40:53,821 - INFO - [diffusion][Epoch 8905] diffusion learning rate: 0.001
2024-11-05 03:40:53,823 - INFO - [diffusion][Epoch 8905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:53,825 - INFO - [diffusion][Epoch 8906] Epoch 8907/12000
2024-11-05 03:40:57,835 - INFO - [diffusion][Epoch 8906] diffusion training Loss: 0.05123682878911495
2024-11-05 03:40:57,837 - INFO - [diffusion][Epoch 8906] diffusion learning rate: 0.001
2024-11-05 03:40:57,839 - INFO - [diffusion][Epoch 8906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:40:57,840 - INFO - [diffusion][Epoch 8907] Epoch 8908/12000
2024-11-05 03:41:01,868 - INFO - [diffusion][Epoch 8907] diffusion training Loss: 0.04409007076174021
2024-11-05 03:41:01,870 - INFO - [diffusion][Epoch 8907] diffusion learning rate: 0.001
2024-11-05 03:41:01,872 - INFO - [diffusion][Epoch 8907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:01,874 - INFO - [diffusion][Epoch 8908] Epoch 8909/12000
2024-11-05 03:41:05,864 - INFO - [diffusion][Epoch 8908] diffusion training Loss: 0.056127190589904785
2024-11-05 03:41:05,867 - INFO - [diffusion][Epoch 8908] diffusion learning rate: 0.001
2024-11-05 03:41:05,895 - INFO - [diffusion][Epoch 8908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:05,896 - INFO - [diffusion][Epoch 8909] Epoch 8910/12000
2024-11-05 03:41:09,927 - INFO - [diffusion][Epoch 8909] diffusion training Loss: 0.05037629418075085
2024-11-05 03:41:09,929 - INFO - [diffusion][Epoch 8909] diffusion learning rate: 0.001
2024-11-05 03:41:09,930 - INFO - [diffusion][Epoch 8909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:09,932 - INFO - [diffusion][Epoch 8910] Epoch 8911/12000
2024-11-05 03:41:14,235 - INFO - [diffusion][Epoch 8910] diffusion training Loss: 0.051254491321742535
2024-11-05 03:41:14,237 - INFO - [diffusion][Epoch 8910] diffusion learning rate: 0.001
2024-11-05 03:41:14,239 - INFO - [diffusion][Epoch 8910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:14,240 - INFO - [diffusion][Epoch 8911] Epoch 8912/12000
2024-11-05 03:41:18,361 - INFO - [diffusion][Epoch 8911] diffusion training Loss: 0.04754663351923227
2024-11-05 03:41:18,363 - INFO - [diffusion][Epoch 8911] diffusion learning rate: 0.001
2024-11-05 03:41:18,365 - INFO - [diffusion][Epoch 8911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:18,366 - INFO - [diffusion][Epoch 8912] Epoch 8913/12000
2024-11-05 03:41:22,518 - INFO - [diffusion][Epoch 8912] diffusion training Loss: 0.053353420458734035
2024-11-05 03:41:22,520 - INFO - [diffusion][Epoch 8912] diffusion learning rate: 0.001
2024-11-05 03:41:22,522 - INFO - [diffusion][Epoch 8912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:22,523 - INFO - [diffusion][Epoch 8913] Epoch 8914/12000
2024-11-05 03:41:26,610 - INFO - [diffusion][Epoch 8913] diffusion training Loss: 0.05165662802755833
2024-11-05 03:41:26,613 - INFO - [diffusion][Epoch 8913] diffusion learning rate: 0.001
2024-11-05 03:41:26,615 - INFO - [diffusion][Epoch 8913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:26,616 - INFO - [diffusion][Epoch 8914] Epoch 8915/12000
2024-11-05 03:41:30,753 - INFO - [diffusion][Epoch 8914] diffusion training Loss: 0.0570624154061079
2024-11-05 03:41:30,755 - INFO - [diffusion][Epoch 8914] diffusion learning rate: 0.001
2024-11-05 03:41:30,757 - INFO - [diffusion][Epoch 8914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:30,758 - INFO - [diffusion][Epoch 8915] Epoch 8916/12000
2024-11-05 03:41:34,838 - INFO - [diffusion][Epoch 8915] diffusion training Loss: 0.04984682612121105
2024-11-05 03:41:34,840 - INFO - [diffusion][Epoch 8915] diffusion learning rate: 0.001
2024-11-05 03:41:34,841 - INFO - [diffusion][Epoch 8915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:34,843 - INFO - [diffusion][Epoch 8916] Epoch 8917/12000
2024-11-05 03:41:38,898 - INFO - [diffusion][Epoch 8916] diffusion training Loss: 0.046377028338611126
2024-11-05 03:41:38,917 - INFO - [diffusion][Epoch 8916] diffusion learning rate: 0.001
2024-11-05 03:41:38,919 - INFO - [diffusion][Epoch 8916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:38,920 - INFO - [diffusion][Epoch 8917] Epoch 8918/12000
2024-11-05 03:41:42,865 - INFO - [diffusion][Epoch 8917] diffusion training Loss: 0.04858662188053131
2024-11-05 03:41:42,867 - INFO - [diffusion][Epoch 8917] diffusion learning rate: 0.001
2024-11-05 03:41:42,869 - INFO - [diffusion][Epoch 8917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:42,871 - INFO - [diffusion][Epoch 8918] Epoch 8919/12000
2024-11-05 03:41:46,851 - INFO - [diffusion][Epoch 8918] diffusion training Loss: 0.05245873145759106
2024-11-05 03:41:46,853 - INFO - [diffusion][Epoch 8918] diffusion learning rate: 0.001
2024-11-05 03:41:46,854 - INFO - [diffusion][Epoch 8918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:46,855 - INFO - [diffusion][Epoch 8919] Epoch 8920/12000
2024-11-05 03:41:50,790 - INFO - [diffusion][Epoch 8919] diffusion training Loss: 0.04942366015166044
2024-11-05 03:41:50,793 - INFO - [diffusion][Epoch 8919] diffusion learning rate: 0.001
2024-11-05 03:41:50,794 - INFO - [diffusion][Epoch 8919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:50,796 - INFO - [diffusion][Epoch 8920] Epoch 8921/12000
2024-11-05 03:41:54,880 - INFO - [diffusion][Epoch 8920] diffusion training Loss: 0.047973849810659885
2024-11-05 03:41:54,882 - INFO - [diffusion][Epoch 8920] diffusion learning rate: 0.001
2024-11-05 03:41:54,884 - INFO - [diffusion][Epoch 8920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:54,885 - INFO - [diffusion][Epoch 8921] Epoch 8922/12000
2024-11-05 03:41:58,764 - INFO - [diffusion][Epoch 8921] diffusion training Loss: 0.05138824041932821
2024-11-05 03:41:58,766 - INFO - [diffusion][Epoch 8921] diffusion learning rate: 0.001
2024-11-05 03:41:58,768 - INFO - [diffusion][Epoch 8921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:41:58,769 - INFO - [diffusion][Epoch 8922] Epoch 8923/12000
2024-11-05 03:42:02,864 - INFO - [diffusion][Epoch 8922] diffusion training Loss: 0.05243144929409027
2024-11-05 03:42:02,866 - INFO - [diffusion][Epoch 8922] diffusion learning rate: 0.001
2024-11-05 03:42:02,868 - INFO - [diffusion][Epoch 8922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:02,869 - INFO - [diffusion][Epoch 8923] Epoch 8924/12000
2024-11-05 03:42:06,917 - INFO - [diffusion][Epoch 8923] diffusion training Loss: 0.04764729272574186
2024-11-05 03:42:06,920 - INFO - [diffusion][Epoch 8923] diffusion learning rate: 0.001
2024-11-05 03:42:06,923 - INFO - [diffusion][Epoch 8923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:06,924 - INFO - [diffusion][Epoch 8924] Epoch 8925/12000
2024-11-05 03:42:11,024 - INFO - [diffusion][Epoch 8924] diffusion training Loss: 0.05262767057865858
2024-11-05 03:42:11,026 - INFO - [diffusion][Epoch 8924] diffusion learning rate: 0.001
2024-11-05 03:42:11,028 - INFO - [diffusion][Epoch 8924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:11,029 - INFO - [diffusion][Epoch 8925] Epoch 8926/12000
2024-11-05 03:42:15,049 - INFO - [diffusion][Epoch 8925] diffusion training Loss: 0.048188552260398865
2024-11-05 03:42:15,051 - INFO - [diffusion][Epoch 8925] diffusion learning rate: 0.001
2024-11-05 03:42:15,053 - INFO - [diffusion][Epoch 8925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:15,054 - INFO - [diffusion][Epoch 8926] Epoch 8927/12000
2024-11-05 03:42:19,063 - INFO - [diffusion][Epoch 8926] diffusion training Loss: 0.05720122251659632
2024-11-05 03:42:19,065 - INFO - [diffusion][Epoch 8926] diffusion learning rate: 0.001
2024-11-05 03:42:19,067 - INFO - [diffusion][Epoch 8926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:19,068 - INFO - [diffusion][Epoch 8927] Epoch 8928/12000
2024-11-05 03:42:22,992 - INFO - [diffusion][Epoch 8927] diffusion training Loss: 0.05100811831653118
2024-11-05 03:42:22,994 - INFO - [diffusion][Epoch 8927] diffusion learning rate: 0.001
2024-11-05 03:42:22,996 - INFO - [diffusion][Epoch 8927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:22,997 - INFO - [diffusion][Epoch 8928] Epoch 8929/12000
2024-11-05 03:42:27,036 - INFO - [diffusion][Epoch 8928] diffusion training Loss: 0.048902139998972416
2024-11-05 03:42:27,038 - INFO - [diffusion][Epoch 8928] diffusion learning rate: 0.001
2024-11-05 03:42:27,040 - INFO - [diffusion][Epoch 8928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:27,041 - INFO - [diffusion][Epoch 8929] Epoch 8930/12000
2024-11-05 03:42:31,119 - INFO - [diffusion][Epoch 8929] diffusion training Loss: 0.0508127436041832
2024-11-05 03:42:31,121 - INFO - [diffusion][Epoch 8929] diffusion learning rate: 0.001
2024-11-05 03:42:31,123 - INFO - [diffusion][Epoch 8929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:31,124 - INFO - [diffusion][Epoch 8930] Epoch 8931/12000
2024-11-05 03:42:35,218 - INFO - [diffusion][Epoch 8930] diffusion training Loss: 0.05355062987655401
2024-11-05 03:42:35,220 - INFO - [diffusion][Epoch 8930] diffusion learning rate: 0.001
2024-11-05 03:42:35,221 - INFO - [diffusion][Epoch 8930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:35,223 - INFO - [diffusion][Epoch 8931] Epoch 8932/12000
2024-11-05 03:42:39,220 - INFO - [diffusion][Epoch 8931] diffusion training Loss: 0.048806565813720226
2024-11-05 03:42:39,222 - INFO - [diffusion][Epoch 8931] diffusion learning rate: 0.001
2024-11-05 03:42:39,224 - INFO - [diffusion][Epoch 8931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:39,225 - INFO - [diffusion][Epoch 8932] Epoch 8933/12000
2024-11-05 03:42:43,305 - INFO - [diffusion][Epoch 8932] diffusion training Loss: 0.053950075060129166
2024-11-05 03:42:43,309 - INFO - [diffusion][Epoch 8932] diffusion learning rate: 0.001
2024-11-05 03:42:43,312 - INFO - [diffusion][Epoch 8932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:43,313 - INFO - [diffusion][Epoch 8933] Epoch 8934/12000
2024-11-05 03:42:47,361 - INFO - [diffusion][Epoch 8933] diffusion training Loss: 0.05044813081622124
2024-11-05 03:42:47,363 - INFO - [diffusion][Epoch 8933] diffusion learning rate: 0.001
2024-11-05 03:42:47,364 - INFO - [diffusion][Epoch 8933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:47,366 - INFO - [diffusion][Epoch 8934] Epoch 8935/12000
2024-11-05 03:42:51,327 - INFO - [diffusion][Epoch 8934] diffusion training Loss: 0.04632316343486309
2024-11-05 03:42:51,329 - INFO - [diffusion][Epoch 8934] diffusion learning rate: 0.001
2024-11-05 03:42:51,331 - INFO - [diffusion][Epoch 8934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:51,332 - INFO - [diffusion][Epoch 8935] Epoch 8936/12000
2024-11-05 03:42:55,358 - INFO - [diffusion][Epoch 8935] diffusion training Loss: 0.051473019644618034
2024-11-05 03:42:55,360 - INFO - [diffusion][Epoch 8935] diffusion learning rate: 0.001
2024-11-05 03:42:55,362 - INFO - [diffusion][Epoch 8935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:55,363 - INFO - [diffusion][Epoch 8936] Epoch 8937/12000
2024-11-05 03:42:59,413 - INFO - [diffusion][Epoch 8936] diffusion training Loss: 0.050281822681427
2024-11-05 03:42:59,415 - INFO - [diffusion][Epoch 8936] diffusion learning rate: 0.001
2024-11-05 03:42:59,417 - INFO - [diffusion][Epoch 8936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:42:59,418 - INFO - [diffusion][Epoch 8937] Epoch 8938/12000
2024-11-05 03:43:03,460 - INFO - [diffusion][Epoch 8937] diffusion training Loss: 0.053091409616172314
2024-11-05 03:43:03,462 - INFO - [diffusion][Epoch 8937] diffusion learning rate: 0.001
2024-11-05 03:43:03,464 - INFO - [diffusion][Epoch 8937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:03,465 - INFO - [diffusion][Epoch 8938] Epoch 8939/12000
2024-11-05 03:43:07,510 - INFO - [diffusion][Epoch 8938] diffusion training Loss: 0.05294511280953884
2024-11-05 03:43:07,513 - INFO - [diffusion][Epoch 8938] diffusion learning rate: 0.001
2024-11-05 03:43:07,516 - INFO - [diffusion][Epoch 8938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:07,517 - INFO - [diffusion][Epoch 8939] Epoch 8940/12000
2024-11-05 03:43:11,509 - INFO - [diffusion][Epoch 8939] diffusion training Loss: 0.05551868584007025
2024-11-05 03:43:11,511 - INFO - [diffusion][Epoch 8939] diffusion learning rate: 0.001
2024-11-05 03:43:11,513 - INFO - [diffusion][Epoch 8939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:11,514 - INFO - [diffusion][Epoch 8940] Epoch 8941/12000
2024-11-05 03:43:15,530 - INFO - [diffusion][Epoch 8940] diffusion training Loss: 0.050424471497535706
2024-11-05 03:43:15,532 - INFO - [diffusion][Epoch 8940] diffusion learning rate: 0.001
2024-11-05 03:43:15,534 - INFO - [diffusion][Epoch 8940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:15,535 - INFO - [diffusion][Epoch 8941] Epoch 8942/12000
2024-11-05 03:43:19,614 - INFO - [diffusion][Epoch 8941] diffusion training Loss: 0.05826211627572775
2024-11-05 03:43:19,616 - INFO - [diffusion][Epoch 8941] diffusion learning rate: 0.001
2024-11-05 03:43:19,657 - INFO - [diffusion][Epoch 8941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:19,658 - INFO - [diffusion][Epoch 8942] Epoch 8943/12000
2024-11-05 03:43:23,848 - INFO - [diffusion][Epoch 8942] diffusion training Loss: 0.05252858903259039
2024-11-05 03:43:23,851 - INFO - [diffusion][Epoch 8942] diffusion learning rate: 0.001
2024-11-05 03:43:23,853 - INFO - [diffusion][Epoch 8942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:23,854 - INFO - [diffusion][Epoch 8943] Epoch 8944/12000
2024-11-05 03:43:27,884 - INFO - [diffusion][Epoch 8943] diffusion training Loss: 0.049019946716725826
2024-11-05 03:43:27,886 - INFO - [diffusion][Epoch 8943] diffusion learning rate: 0.001
2024-11-05 03:43:27,889 - INFO - [diffusion][Epoch 8943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:27,890 - INFO - [diffusion][Epoch 8944] Epoch 8945/12000
2024-11-05 03:43:31,991 - INFO - [diffusion][Epoch 8944] diffusion training Loss: 0.04866925626993179
2024-11-05 03:43:31,993 - INFO - [diffusion][Epoch 8944] diffusion learning rate: 0.001
2024-11-05 03:43:31,995 - INFO - [diffusion][Epoch 8944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:31,996 - INFO - [diffusion][Epoch 8945] Epoch 8946/12000
2024-11-05 03:43:36,027 - INFO - [diffusion][Epoch 8945] diffusion training Loss: 0.05660425126552582
2024-11-05 03:43:36,029 - INFO - [diffusion][Epoch 8945] diffusion learning rate: 0.001
2024-11-05 03:43:36,031 - INFO - [diffusion][Epoch 8945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:36,032 - INFO - [diffusion][Epoch 8946] Epoch 8947/12000
2024-11-05 03:43:39,961 - INFO - [diffusion][Epoch 8946] diffusion training Loss: 0.05571442469954491
2024-11-05 03:43:39,963 - INFO - [diffusion][Epoch 8946] diffusion learning rate: 0.001
2024-11-05 03:43:39,965 - INFO - [diffusion][Epoch 8946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:39,966 - INFO - [diffusion][Epoch 8947] Epoch 8948/12000
2024-11-05 03:43:43,910 - INFO - [diffusion][Epoch 8947] diffusion training Loss: 0.05133288633078337
2024-11-05 03:43:43,913 - INFO - [diffusion][Epoch 8947] diffusion learning rate: 0.001
2024-11-05 03:43:43,915 - INFO - [diffusion][Epoch 8947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:43,916 - INFO - [diffusion][Epoch 8948] Epoch 8949/12000
2024-11-05 03:43:47,936 - INFO - [diffusion][Epoch 8948] diffusion training Loss: 0.05411530937999487
2024-11-05 03:43:47,938 - INFO - [diffusion][Epoch 8948] diffusion learning rate: 0.001
2024-11-05 03:43:47,939 - INFO - [diffusion][Epoch 8948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:47,941 - INFO - [diffusion][Epoch 8949] Epoch 8950/12000
2024-11-05 03:43:51,877 - INFO - [diffusion][Epoch 8949] diffusion training Loss: 0.05708326119929552
2024-11-05 03:43:51,879 - INFO - [diffusion][Epoch 8949] diffusion learning rate: 0.001
2024-11-05 03:43:51,907 - INFO - [diffusion][Epoch 8949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:51,909 - INFO - [diffusion][Epoch 8950] Epoch 8951/12000
2024-11-05 03:43:55,883 - INFO - [diffusion][Epoch 8950] diffusion training Loss: 0.04735878016799688
2024-11-05 03:43:55,885 - INFO - [diffusion][Epoch 8950] diffusion learning rate: 0.001
2024-11-05 03:43:55,887 - INFO - [diffusion][Epoch 8950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:43:55,888 - INFO - [diffusion][Epoch 8951] Epoch 8952/12000
2024-11-05 03:43:59,999 - INFO - [diffusion][Epoch 8951] diffusion training Loss: 0.0539817875251174
2024-11-05 03:44:00,002 - INFO - [diffusion][Epoch 8951] diffusion learning rate: 0.001
2024-11-05 03:44:00,004 - INFO - [diffusion][Epoch 8951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:00,006 - INFO - [diffusion][Epoch 8952] Epoch 8953/12000
2024-11-05 03:44:03,919 - INFO - [diffusion][Epoch 8952] diffusion training Loss: 0.05117049068212509
2024-11-05 03:44:03,921 - INFO - [diffusion][Epoch 8952] diffusion learning rate: 0.001
2024-11-05 03:44:03,922 - INFO - [diffusion][Epoch 8952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:03,924 - INFO - [diffusion][Epoch 8953] Epoch 8954/12000
2024-11-05 03:44:07,960 - INFO - [diffusion][Epoch 8953] diffusion training Loss: 0.049792274832725525
2024-11-05 03:44:07,963 - INFO - [diffusion][Epoch 8953] diffusion learning rate: 0.001
2024-11-05 03:44:07,965 - INFO - [diffusion][Epoch 8953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:07,966 - INFO - [diffusion][Epoch 8954] Epoch 8955/12000
2024-11-05 03:44:12,005 - INFO - [diffusion][Epoch 8954] diffusion training Loss: 0.05443229619413614
2024-11-05 03:44:12,007 - INFO - [diffusion][Epoch 8954] diffusion learning rate: 0.001
2024-11-05 03:44:12,009 - INFO - [diffusion][Epoch 8954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:12,010 - INFO - [diffusion][Epoch 8955] Epoch 8956/12000
2024-11-05 03:44:16,030 - INFO - [diffusion][Epoch 8955] diffusion training Loss: 0.050034516490995884
2024-11-05 03:44:16,032 - INFO - [diffusion][Epoch 8955] diffusion learning rate: 0.001
2024-11-05 03:44:16,034 - INFO - [diffusion][Epoch 8955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:16,035 - INFO - [diffusion][Epoch 8956] Epoch 8957/12000
2024-11-05 03:44:20,011 - INFO - [diffusion][Epoch 8956] diffusion training Loss: 0.054841697216033936
2024-11-05 03:44:20,013 - INFO - [diffusion][Epoch 8956] diffusion learning rate: 0.001
2024-11-05 03:44:20,016 - INFO - [diffusion][Epoch 8956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:20,017 - INFO - [diffusion][Epoch 8957] Epoch 8958/12000
2024-11-05 03:44:23,960 - INFO - [diffusion][Epoch 8957] diffusion training Loss: 0.04806019552052021
2024-11-05 03:44:23,962 - INFO - [diffusion][Epoch 8957] diffusion learning rate: 0.001
2024-11-05 03:44:23,987 - INFO - [diffusion][Epoch 8957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:23,988 - INFO - [diffusion][Epoch 8958] Epoch 8959/12000
2024-11-05 03:44:28,132 - INFO - [diffusion][Epoch 8958] diffusion training Loss: 0.051966654136776924
2024-11-05 03:44:28,134 - INFO - [diffusion][Epoch 8958] diffusion learning rate: 0.001
2024-11-05 03:44:28,137 - INFO - [diffusion][Epoch 8958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:28,139 - INFO - [diffusion][Epoch 8959] Epoch 8960/12000
2024-11-05 03:44:32,168 - INFO - [diffusion][Epoch 8959] diffusion training Loss: 0.04916562419384718
2024-11-05 03:44:32,170 - INFO - [diffusion][Epoch 8959] diffusion learning rate: 0.001
2024-11-05 03:44:32,172 - INFO - [diffusion][Epoch 8959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:32,173 - INFO - [diffusion][Epoch 8960] Epoch 8961/12000
2024-11-05 03:44:36,174 - INFO - [diffusion][Epoch 8960] diffusion training Loss: 0.05497373826801777
2024-11-05 03:44:36,176 - INFO - [diffusion][Epoch 8960] diffusion learning rate: 0.001
2024-11-05 03:44:36,178 - INFO - [diffusion][Epoch 8960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:36,179 - INFO - [diffusion][Epoch 8961] Epoch 8962/12000
2024-11-05 03:44:40,096 - INFO - [diffusion][Epoch 8961] diffusion training Loss: 0.05024139489978552
2024-11-05 03:44:40,098 - INFO - [diffusion][Epoch 8961] diffusion learning rate: 0.001
2024-11-05 03:44:40,123 - INFO - [diffusion][Epoch 8961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:40,125 - INFO - [diffusion][Epoch 8962] Epoch 8963/12000
2024-11-05 03:44:44,138 - INFO - [diffusion][Epoch 8962] diffusion training Loss: 0.047689175233244896
2024-11-05 03:44:44,140 - INFO - [diffusion][Epoch 8962] diffusion learning rate: 0.001
2024-11-05 03:44:44,142 - INFO - [diffusion][Epoch 8962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:44,144 - INFO - [diffusion][Epoch 8963] Epoch 8964/12000
2024-11-05 03:44:48,171 - INFO - [diffusion][Epoch 8963] diffusion training Loss: 0.052328857593238354
2024-11-05 03:44:48,173 - INFO - [diffusion][Epoch 8963] diffusion learning rate: 0.001
2024-11-05 03:44:48,175 - INFO - [diffusion][Epoch 8963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:48,176 - INFO - [diffusion][Epoch 8964] Epoch 8965/12000
2024-11-05 03:44:52,293 - INFO - [diffusion][Epoch 8964] diffusion training Loss: 0.04557511955499649
2024-11-05 03:44:52,295 - INFO - [diffusion][Epoch 8964] diffusion learning rate: 0.001
2024-11-05 03:44:52,297 - INFO - [diffusion][Epoch 8964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:52,298 - INFO - [diffusion][Epoch 8965] Epoch 8966/12000
2024-11-05 03:44:56,376 - INFO - [diffusion][Epoch 8965] diffusion training Loss: 0.051601567305624485
2024-11-05 03:44:56,378 - INFO - [diffusion][Epoch 8965] diffusion learning rate: 0.001
2024-11-05 03:44:56,380 - INFO - [diffusion][Epoch 8965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:44:56,381 - INFO - [diffusion][Epoch 8966] Epoch 8967/12000
2024-11-05 03:45:00,481 - INFO - [diffusion][Epoch 8966] diffusion training Loss: 0.054079645313322544
2024-11-05 03:45:00,483 - INFO - [diffusion][Epoch 8966] diffusion learning rate: 0.001
2024-11-05 03:45:00,485 - INFO - [diffusion][Epoch 8966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:00,486 - INFO - [diffusion][Epoch 8967] Epoch 8968/12000
2024-11-05 03:45:04,589 - INFO - [diffusion][Epoch 8967] diffusion training Loss: 0.048847172409296036
2024-11-05 03:45:04,591 - INFO - [diffusion][Epoch 8967] diffusion learning rate: 0.001
2024-11-05 03:45:04,593 - INFO - [diffusion][Epoch 8967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:04,594 - INFO - [diffusion][Epoch 8968] Epoch 8969/12000
2024-11-05 03:45:08,501 - INFO - [diffusion][Epoch 8968] diffusion training Loss: 0.05460265651345253
2024-11-05 03:45:08,504 - INFO - [diffusion][Epoch 8968] diffusion learning rate: 0.001
2024-11-05 03:45:08,507 - INFO - [diffusion][Epoch 8968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:08,509 - INFO - [diffusion][Epoch 8969] Epoch 8970/12000
2024-11-05 03:45:12,547 - INFO - [diffusion][Epoch 8969] diffusion training Loss: 0.04996232967823744
2024-11-05 03:45:12,804 - INFO - [diffusion][Epoch 8969] diffusion learning rate: 0.001
2024-11-05 03:45:12,807 - INFO - [diffusion][Epoch 8969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:12,809 - INFO - [diffusion][Epoch 8970] Epoch 8971/12000
2024-11-05 03:45:16,942 - INFO - [diffusion][Epoch 8970] diffusion training Loss: 0.04783366248011589
2024-11-05 03:45:16,944 - INFO - [diffusion][Epoch 8970] diffusion learning rate: 0.001
2024-11-05 03:45:16,946 - INFO - [diffusion][Epoch 8970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:16,947 - INFO - [diffusion][Epoch 8971] Epoch 8972/12000
2024-11-05 03:45:20,942 - INFO - [diffusion][Epoch 8971] diffusion training Loss: 0.05274310056120157
2024-11-05 03:45:20,944 - INFO - [diffusion][Epoch 8971] diffusion learning rate: 0.001
2024-11-05 03:45:20,946 - INFO - [diffusion][Epoch 8971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:20,947 - INFO - [diffusion][Epoch 8972] Epoch 8973/12000
2024-11-05 03:45:25,145 - INFO - [diffusion][Epoch 8972] diffusion training Loss: 0.05082315392792225
2024-11-05 03:45:25,147 - INFO - [diffusion][Epoch 8972] diffusion learning rate: 0.001
2024-11-05 03:45:25,149 - INFO - [diffusion][Epoch 8972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:25,151 - INFO - [diffusion][Epoch 8973] Epoch 8974/12000
2024-11-05 03:45:29,199 - INFO - [diffusion][Epoch 8973] diffusion training Loss: 0.05046447552740574
2024-11-05 03:45:29,201 - INFO - [diffusion][Epoch 8973] diffusion learning rate: 0.001
2024-11-05 03:45:29,203 - INFO - [diffusion][Epoch 8973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:29,204 - INFO - [diffusion][Epoch 8974] Epoch 8975/12000
2024-11-05 03:45:33,287 - INFO - [diffusion][Epoch 8974] diffusion training Loss: 0.05219980236142874
2024-11-05 03:45:33,290 - INFO - [diffusion][Epoch 8974] diffusion learning rate: 0.001
2024-11-05 03:45:33,291 - INFO - [diffusion][Epoch 8974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:33,293 - INFO - [diffusion][Epoch 8975] Epoch 8976/12000
2024-11-05 03:45:37,334 - INFO - [diffusion][Epoch 8975] diffusion training Loss: 0.04882862698286772
2024-11-05 03:45:37,336 - INFO - [diffusion][Epoch 8975] diffusion learning rate: 0.001
2024-11-05 03:45:37,338 - INFO - [diffusion][Epoch 8975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:37,340 - INFO - [diffusion][Epoch 8976] Epoch 8977/12000
2024-11-05 03:45:41,229 - INFO - [diffusion][Epoch 8976] diffusion training Loss: 0.05197130888700485
2024-11-05 03:45:41,231 - INFO - [diffusion][Epoch 8976] diffusion learning rate: 0.001
2024-11-05 03:45:41,233 - INFO - [diffusion][Epoch 8976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:41,234 - INFO - [diffusion][Epoch 8977] Epoch 8978/12000
2024-11-05 03:45:45,387 - INFO - [diffusion][Epoch 8977] diffusion training Loss: 0.04988867603242397
2024-11-05 03:45:45,389 - INFO - [diffusion][Epoch 8977] diffusion learning rate: 0.001
2024-11-05 03:45:45,391 - INFO - [diffusion][Epoch 8977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:45,393 - INFO - [diffusion][Epoch 8978] Epoch 8979/12000
2024-11-05 03:45:49,352 - INFO - [diffusion][Epoch 8978] diffusion training Loss: 0.050878213718533516
2024-11-05 03:45:49,354 - INFO - [diffusion][Epoch 8978] diffusion learning rate: 0.001
2024-11-05 03:45:49,355 - INFO - [diffusion][Epoch 8978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:49,357 - INFO - [diffusion][Epoch 8979] Epoch 8980/12000
2024-11-05 03:45:53,361 - INFO - [diffusion][Epoch 8979] diffusion training Loss: 0.05062702298164368
2024-11-05 03:45:53,363 - INFO - [diffusion][Epoch 8979] diffusion learning rate: 0.001
2024-11-05 03:45:53,365 - INFO - [diffusion][Epoch 8979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:53,366 - INFO - [diffusion][Epoch 8980] Epoch 8981/12000
2024-11-05 03:45:57,468 - INFO - [diffusion][Epoch 8980] diffusion training Loss: 0.05082707665860653
2024-11-05 03:45:57,470 - INFO - [diffusion][Epoch 8980] diffusion learning rate: 0.001
2024-11-05 03:45:57,472 - INFO - [diffusion][Epoch 8980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:45:57,473 - INFO - [diffusion][Epoch 8981] Epoch 8982/12000
2024-11-05 03:46:01,406 - INFO - [diffusion][Epoch 8981] diffusion training Loss: 0.04851116985082626
2024-11-05 03:46:01,408 - INFO - [diffusion][Epoch 8981] diffusion learning rate: 0.001
2024-11-05 03:46:01,410 - INFO - [diffusion][Epoch 8981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:01,411 - INFO - [diffusion][Epoch 8982] Epoch 8983/12000
2024-11-05 03:46:05,353 - INFO - [diffusion][Epoch 8982] diffusion training Loss: 0.050825320184230804
2024-11-05 03:46:05,355 - INFO - [diffusion][Epoch 8982] diffusion learning rate: 0.001
2024-11-05 03:46:05,356 - INFO - [diffusion][Epoch 8982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:05,357 - INFO - [diffusion][Epoch 8983] Epoch 8984/12000
2024-11-05 03:46:09,313 - INFO - [diffusion][Epoch 8983] diffusion training Loss: 0.054099421948194504
2024-11-05 03:46:09,315 - INFO - [diffusion][Epoch 8983] diffusion learning rate: 0.001
2024-11-05 03:46:09,317 - INFO - [diffusion][Epoch 8983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:09,318 - INFO - [diffusion][Epoch 8984] Epoch 8985/12000
2024-11-05 03:46:13,386 - INFO - [diffusion][Epoch 8984] diffusion training Loss: 0.051740712486207485
2024-11-05 03:46:13,388 - INFO - [diffusion][Epoch 8984] diffusion learning rate: 0.001
2024-11-05 03:46:13,390 - INFO - [diffusion][Epoch 8984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:13,391 - INFO - [diffusion][Epoch 8985] Epoch 8986/12000
2024-11-05 03:46:17,465 - INFO - [diffusion][Epoch 8985] diffusion training Loss: 0.04925564490258694
2024-11-05 03:46:17,467 - INFO - [diffusion][Epoch 8985] diffusion learning rate: 0.001
2024-11-05 03:46:17,468 - INFO - [diffusion][Epoch 8985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:17,470 - INFO - [diffusion][Epoch 8986] Epoch 8987/12000
2024-11-05 03:46:21,479 - INFO - [diffusion][Epoch 8986] diffusion training Loss: 0.050752812065184116
2024-11-05 03:46:21,481 - INFO - [diffusion][Epoch 8986] diffusion learning rate: 0.001
2024-11-05 03:46:21,482 - INFO - [diffusion][Epoch 8986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:21,484 - INFO - [diffusion][Epoch 8987] Epoch 8988/12000
2024-11-05 03:46:25,575 - INFO - [diffusion][Epoch 8987] diffusion training Loss: 0.05256037041544914
2024-11-05 03:46:25,577 - INFO - [diffusion][Epoch 8987] diffusion learning rate: 0.001
2024-11-05 03:46:25,580 - INFO - [diffusion][Epoch 8987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:25,581 - INFO - [diffusion][Epoch 8988] Epoch 8989/12000
2024-11-05 03:46:29,711 - INFO - [diffusion][Epoch 8988] diffusion training Loss: 0.052376228384673595
2024-11-05 03:46:29,713 - INFO - [diffusion][Epoch 8988] diffusion learning rate: 0.001
2024-11-05 03:46:29,715 - INFO - [diffusion][Epoch 8988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:29,716 - INFO - [diffusion][Epoch 8989] Epoch 8990/12000
2024-11-05 03:46:33,796 - INFO - [diffusion][Epoch 8989] diffusion training Loss: 0.053102350793778896
2024-11-05 03:46:33,798 - INFO - [diffusion][Epoch 8989] diffusion learning rate: 0.001
2024-11-05 03:46:33,799 - INFO - [diffusion][Epoch 8989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:33,801 - INFO - [diffusion][Epoch 8990] Epoch 8991/12000
2024-11-05 03:46:37,987 - INFO - [diffusion][Epoch 8990] diffusion training Loss: 0.05149173829704523
2024-11-05 03:46:37,989 - INFO - [diffusion][Epoch 8990] diffusion learning rate: 0.001
2024-11-05 03:46:37,991 - INFO - [diffusion][Epoch 8990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:37,992 - INFO - [diffusion][Epoch 8991] Epoch 8992/12000
2024-11-05 03:46:42,139 - INFO - [diffusion][Epoch 8991] diffusion training Loss: 0.053850533440709114
2024-11-05 03:46:42,141 - INFO - [diffusion][Epoch 8991] diffusion learning rate: 0.001
2024-11-05 03:46:42,143 - INFO - [diffusion][Epoch 8991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:42,145 - INFO - [diffusion][Epoch 8992] Epoch 8993/12000
2024-11-05 03:46:46,994 - INFO - [diffusion][Epoch 8992] diffusion training Loss: 0.049677771516144276
2024-11-05 03:46:46,996 - INFO - [diffusion][Epoch 8992] diffusion learning rate: 0.001
2024-11-05 03:46:46,998 - INFO - [diffusion][Epoch 8992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:46,999 - INFO - [diffusion][Epoch 8993] Epoch 8994/12000
2024-11-05 03:46:51,034 - INFO - [diffusion][Epoch 8993] diffusion training Loss: 0.05190046038478613
2024-11-05 03:46:51,036 - INFO - [diffusion][Epoch 8993] diffusion learning rate: 0.001
2024-11-05 03:46:51,038 - INFO - [diffusion][Epoch 8993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:51,039 - INFO - [diffusion][Epoch 8994] Epoch 8995/12000
2024-11-05 03:46:55,032 - INFO - [diffusion][Epoch 8994] diffusion training Loss: 0.0506526930257678
2024-11-05 03:46:55,034 - INFO - [diffusion][Epoch 8994] diffusion learning rate: 0.001
2024-11-05 03:46:55,035 - INFO - [diffusion][Epoch 8994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:55,036 - INFO - [diffusion][Epoch 8995] Epoch 8996/12000
2024-11-05 03:46:59,133 - INFO - [diffusion][Epoch 8995] diffusion training Loss: 0.049265545792877674
2024-11-05 03:46:59,135 - INFO - [diffusion][Epoch 8995] diffusion learning rate: 0.001
2024-11-05 03:46:59,137 - INFO - [diffusion][Epoch 8995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:46:59,138 - INFO - [diffusion][Epoch 8996] Epoch 8997/12000
2024-11-05 03:47:03,235 - INFO - [diffusion][Epoch 8996] diffusion training Loss: 0.05759059637784958
2024-11-05 03:47:03,237 - INFO - [diffusion][Epoch 8996] diffusion learning rate: 0.001
2024-11-05 03:47:03,239 - INFO - [diffusion][Epoch 8996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:03,240 - INFO - [diffusion][Epoch 8997] Epoch 8998/12000
2024-11-05 03:47:07,354 - INFO - [diffusion][Epoch 8997] diffusion training Loss: 0.053979355841875076
2024-11-05 03:47:07,356 - INFO - [diffusion][Epoch 8997] diffusion learning rate: 0.001
2024-11-05 03:47:07,357 - INFO - [diffusion][Epoch 8997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:07,359 - INFO - [diffusion][Epoch 8998] Epoch 8999/12000
2024-11-05 03:47:11,509 - INFO - [diffusion][Epoch 8998] diffusion training Loss: 0.05354429129511118
2024-11-05 03:47:11,512 - INFO - [diffusion][Epoch 8998] diffusion learning rate: 0.001
2024-11-05 03:47:11,514 - INFO - [diffusion][Epoch 8998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:11,515 - INFO - [diffusion][Epoch 8999] Epoch 9000/12000
2024-11-05 03:47:15,600 - INFO - [diffusion][Epoch 8999] diffusion training Loss: 0.053148177452385426
2024-11-05 03:47:15,602 - INFO - [diffusion][Epoch 8999] diffusion learning rate: 0.001
2024-11-05 03:47:15,792 - INFO - [diffusion][Epoch 8999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:15,794 - INFO - [diffusion][Epoch 9000] Epoch 9001/12000
2024-11-05 03:47:19,831 - INFO - [diffusion][Epoch 9000] diffusion training Loss: 0.0491181081160903
2024-11-05 03:47:19,834 - INFO - [diffusion][Epoch 9000] diffusion learning rate: 0.001
2024-11-05 03:47:19,835 - INFO - [diffusion][Epoch 9000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:19,837 - INFO - [diffusion][Epoch 9001] Epoch 9002/12000
2024-11-05 03:47:23,955 - INFO - [diffusion][Epoch 9001] diffusion training Loss: 0.05417999718338251
2024-11-05 03:47:23,957 - INFO - [diffusion][Epoch 9001] diffusion learning rate: 0.001
2024-11-05 03:47:23,958 - INFO - [diffusion][Epoch 9001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:23,960 - INFO - [diffusion][Epoch 9002] Epoch 9003/12000
2024-11-05 03:47:28,025 - INFO - [diffusion][Epoch 9002] diffusion training Loss: 0.0521389152854681
2024-11-05 03:47:28,027 - INFO - [diffusion][Epoch 9002] diffusion learning rate: 0.001
2024-11-05 03:47:28,029 - INFO - [diffusion][Epoch 9002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:28,033 - INFO - [diffusion][Epoch 9003] Epoch 9004/12000
2024-11-05 03:47:32,085 - INFO - [diffusion][Epoch 9003] diffusion training Loss: 0.06023301277309656
2024-11-05 03:47:32,087 - INFO - [diffusion][Epoch 9003] diffusion learning rate: 0.001
2024-11-05 03:47:32,089 - INFO - [diffusion][Epoch 9003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:32,090 - INFO - [diffusion][Epoch 9004] Epoch 9005/12000
2024-11-05 03:47:36,188 - INFO - [diffusion][Epoch 9004] diffusion training Loss: 0.05447242595255375
2024-11-05 03:47:36,191 - INFO - [diffusion][Epoch 9004] diffusion learning rate: 0.001
2024-11-05 03:47:36,193 - INFO - [diffusion][Epoch 9004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:36,194 - INFO - [diffusion][Epoch 9005] Epoch 9006/12000
2024-11-05 03:47:40,240 - INFO - [diffusion][Epoch 9005] diffusion training Loss: 0.05084240064024925
2024-11-05 03:47:40,242 - INFO - [diffusion][Epoch 9005] diffusion learning rate: 0.001
2024-11-05 03:47:40,244 - INFO - [diffusion][Epoch 9005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:40,246 - INFO - [diffusion][Epoch 9006] Epoch 9007/12000
2024-11-05 03:47:44,457 - INFO - [diffusion][Epoch 9006] diffusion training Loss: 0.05790264904499054
2024-11-05 03:47:44,460 - INFO - [diffusion][Epoch 9006] diffusion learning rate: 0.001
2024-11-05 03:47:44,461 - INFO - [diffusion][Epoch 9006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:44,463 - INFO - [diffusion][Epoch 9007] Epoch 9008/12000
2024-11-05 03:47:48,421 - INFO - [diffusion][Epoch 9007] diffusion training Loss: 0.049148961901664734
2024-11-05 03:47:48,424 - INFO - [diffusion][Epoch 9007] diffusion learning rate: 0.001
2024-11-05 03:47:48,426 - INFO - [diffusion][Epoch 9007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:48,427 - INFO - [diffusion][Epoch 9008] Epoch 9009/12000
2024-11-05 03:47:52,586 - INFO - [diffusion][Epoch 9008] diffusion training Loss: 0.05249684862792492
2024-11-05 03:47:52,588 - INFO - [diffusion][Epoch 9008] diffusion learning rate: 0.001
2024-11-05 03:47:52,590 - INFO - [diffusion][Epoch 9008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:52,591 - INFO - [diffusion][Epoch 9009] Epoch 9010/12000
2024-11-05 03:47:56,608 - INFO - [diffusion][Epoch 9009] diffusion training Loss: 0.048646533861756325
2024-11-05 03:47:56,610 - INFO - [diffusion][Epoch 9009] diffusion learning rate: 0.001
2024-11-05 03:47:56,612 - INFO - [diffusion][Epoch 9009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:47:56,614 - INFO - [diffusion][Epoch 9010] Epoch 9011/12000
2024-11-05 03:48:00,678 - INFO - [diffusion][Epoch 9010] diffusion training Loss: 0.04936015233397484
2024-11-05 03:48:00,680 - INFO - [diffusion][Epoch 9010] diffusion learning rate: 0.001
2024-11-05 03:48:00,682 - INFO - [diffusion][Epoch 9010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:00,684 - INFO - [diffusion][Epoch 9011] Epoch 9012/12000
2024-11-05 03:48:04,750 - INFO - [diffusion][Epoch 9011] diffusion training Loss: 0.05522421468049288
2024-11-05 03:48:04,753 - INFO - [diffusion][Epoch 9011] diffusion learning rate: 0.001
2024-11-05 03:48:04,755 - INFO - [diffusion][Epoch 9011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:04,756 - INFO - [diffusion][Epoch 9012] Epoch 9013/12000
2024-11-05 03:48:09,146 - INFO - [diffusion][Epoch 9012] diffusion training Loss: 0.054838099516928196
2024-11-05 03:48:09,148 - INFO - [diffusion][Epoch 9012] diffusion learning rate: 0.001
2024-11-05 03:48:09,150 - INFO - [diffusion][Epoch 9012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:09,151 - INFO - [diffusion][Epoch 9013] Epoch 9014/12000
2024-11-05 03:48:13,192 - INFO - [diffusion][Epoch 9013] diffusion training Loss: 0.058308977633714676
2024-11-05 03:48:13,195 - INFO - [diffusion][Epoch 9013] diffusion learning rate: 0.001
2024-11-05 03:48:13,197 - INFO - [diffusion][Epoch 9013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:13,198 - INFO - [diffusion][Epoch 9014] Epoch 9015/12000
2024-11-05 03:48:17,337 - INFO - [diffusion][Epoch 9014] diffusion training Loss: 0.05033718794584274
2024-11-05 03:48:17,394 - INFO - [diffusion][Epoch 9014] diffusion learning rate: 0.001
2024-11-05 03:48:17,396 - INFO - [diffusion][Epoch 9014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:17,397 - INFO - [diffusion][Epoch 9015] Epoch 9016/12000
2024-11-05 03:48:21,527 - INFO - [diffusion][Epoch 9015] diffusion training Loss: 0.04509647469967604
2024-11-05 03:48:21,529 - INFO - [diffusion][Epoch 9015] diffusion learning rate: 0.001
2024-11-05 03:48:21,532 - INFO - [diffusion][Epoch 9015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:21,533 - INFO - [diffusion][Epoch 9016] Epoch 9017/12000
2024-11-05 03:48:25,683 - INFO - [diffusion][Epoch 9016] diffusion training Loss: 0.04717428516596556
2024-11-05 03:48:25,685 - INFO - [diffusion][Epoch 9016] diffusion learning rate: 0.001
2024-11-05 03:48:25,687 - INFO - [diffusion][Epoch 9016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:25,688 - INFO - [diffusion][Epoch 9017] Epoch 9018/12000
2024-11-05 03:48:29,803 - INFO - [diffusion][Epoch 9017] diffusion training Loss: 0.0524256881326437
2024-11-05 03:48:29,806 - INFO - [diffusion][Epoch 9017] diffusion learning rate: 0.001
2024-11-05 03:48:29,808 - INFO - [diffusion][Epoch 9017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:29,809 - INFO - [diffusion][Epoch 9018] Epoch 9019/12000
2024-11-05 03:48:34,005 - INFO - [diffusion][Epoch 9018] diffusion training Loss: 0.05552899930626154
2024-11-05 03:48:34,007 - INFO - [diffusion][Epoch 9018] diffusion learning rate: 0.001
2024-11-05 03:48:34,009 - INFO - [diffusion][Epoch 9018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:34,010 - INFO - [diffusion][Epoch 9019] Epoch 9020/12000
2024-11-05 03:48:38,087 - INFO - [diffusion][Epoch 9019] diffusion training Loss: 0.049892702139914036
2024-11-05 03:48:38,089 - INFO - [diffusion][Epoch 9019] diffusion learning rate: 0.001
2024-11-05 03:48:38,091 - INFO - [diffusion][Epoch 9019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:38,093 - INFO - [diffusion][Epoch 9020] Epoch 9021/12000
2024-11-05 03:48:42,163 - INFO - [diffusion][Epoch 9020] diffusion training Loss: 0.05306090321391821
2024-11-05 03:48:42,165 - INFO - [diffusion][Epoch 9020] diffusion learning rate: 0.001
2024-11-05 03:48:42,167 - INFO - [diffusion][Epoch 9020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:42,168 - INFO - [diffusion][Epoch 9021] Epoch 9022/12000
2024-11-05 03:48:46,232 - INFO - [diffusion][Epoch 9021] diffusion training Loss: 0.05279676988720894
2024-11-05 03:48:46,234 - INFO - [diffusion][Epoch 9021] diffusion learning rate: 0.001
2024-11-05 03:48:46,236 - INFO - [diffusion][Epoch 9021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:46,237 - INFO - [diffusion][Epoch 9022] Epoch 9023/12000
2024-11-05 03:48:50,303 - INFO - [diffusion][Epoch 9022] diffusion training Loss: 0.049447461031377316
2024-11-05 03:48:50,305 - INFO - [diffusion][Epoch 9022] diffusion learning rate: 0.001
2024-11-05 03:48:50,307 - INFO - [diffusion][Epoch 9022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:50,308 - INFO - [diffusion][Epoch 9023] Epoch 9024/12000
2024-11-05 03:48:54,312 - INFO - [diffusion][Epoch 9023] diffusion training Loss: 0.04892123397439718
2024-11-05 03:48:54,314 - INFO - [diffusion][Epoch 9023] diffusion learning rate: 0.001
2024-11-05 03:48:54,316 - INFO - [diffusion][Epoch 9023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:54,317 - INFO - [diffusion][Epoch 9024] Epoch 9025/12000
2024-11-05 03:48:58,329 - INFO - [diffusion][Epoch 9024] diffusion training Loss: 0.049202374182641506
2024-11-05 03:48:58,331 - INFO - [diffusion][Epoch 9024] diffusion learning rate: 0.001
2024-11-05 03:48:58,332 - INFO - [diffusion][Epoch 9024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:48:58,334 - INFO - [diffusion][Epoch 9025] Epoch 9026/12000
2024-11-05 03:49:02,210 - INFO - [diffusion][Epoch 9025] diffusion training Loss: 0.05166485533118248
2024-11-05 03:49:02,213 - INFO - [diffusion][Epoch 9025] diffusion learning rate: 0.001
2024-11-05 03:49:02,215 - INFO - [diffusion][Epoch 9025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:02,216 - INFO - [diffusion][Epoch 9026] Epoch 9027/12000
2024-11-05 03:49:06,337 - INFO - [diffusion][Epoch 9026] diffusion training Loss: 0.057970390655100346
2024-11-05 03:49:06,340 - INFO - [diffusion][Epoch 9026] diffusion learning rate: 0.001
2024-11-05 03:49:06,342 - INFO - [diffusion][Epoch 9026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:06,343 - INFO - [diffusion][Epoch 9027] Epoch 9028/12000
2024-11-05 03:49:10,431 - INFO - [diffusion][Epoch 9027] diffusion training Loss: 0.055838814936578274
2024-11-05 03:49:10,433 - INFO - [diffusion][Epoch 9027] diffusion learning rate: 0.001
2024-11-05 03:49:10,435 - INFO - [diffusion][Epoch 9027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:10,436 - INFO - [diffusion][Epoch 9028] Epoch 9029/12000
2024-11-05 03:49:14,548 - INFO - [diffusion][Epoch 9028] diffusion training Loss: 0.05273423623293638
2024-11-05 03:49:14,551 - INFO - [diffusion][Epoch 9028] diffusion learning rate: 0.001
2024-11-05 03:49:14,552 - INFO - [diffusion][Epoch 9028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:14,554 - INFO - [diffusion][Epoch 9029] Epoch 9030/12000
2024-11-05 03:49:18,702 - INFO - [diffusion][Epoch 9029] diffusion training Loss: 0.056305828504264355
2024-11-05 03:49:18,704 - INFO - [diffusion][Epoch 9029] diffusion learning rate: 0.001
2024-11-05 03:49:18,706 - INFO - [diffusion][Epoch 9029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:18,707 - INFO - [diffusion][Epoch 9030] Epoch 9031/12000
2024-11-05 03:49:22,812 - INFO - [diffusion][Epoch 9030] diffusion training Loss: 0.052038502879440784
2024-11-05 03:49:22,814 - INFO - [diffusion][Epoch 9030] diffusion learning rate: 0.001
2024-11-05 03:49:22,816 - INFO - [diffusion][Epoch 9030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:22,817 - INFO - [diffusion][Epoch 9031] Epoch 9032/12000
2024-11-05 03:49:26,898 - INFO - [diffusion][Epoch 9031] diffusion training Loss: 0.056613761000335217
2024-11-05 03:49:26,900 - INFO - [diffusion][Epoch 9031] diffusion learning rate: 0.001
2024-11-05 03:49:26,902 - INFO - [diffusion][Epoch 9031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:26,903 - INFO - [diffusion][Epoch 9032] Epoch 9033/12000
2024-11-05 03:49:30,873 - INFO - [diffusion][Epoch 9032] diffusion training Loss: 0.05172525905072689
2024-11-05 03:49:30,875 - INFO - [diffusion][Epoch 9032] diffusion learning rate: 0.001
2024-11-05 03:49:30,876 - INFO - [diffusion][Epoch 9032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:30,878 - INFO - [diffusion][Epoch 9033] Epoch 9034/12000
2024-11-05 03:49:34,921 - INFO - [diffusion][Epoch 9033] diffusion training Loss: 0.05157048720866442
2024-11-05 03:49:34,923 - INFO - [diffusion][Epoch 9033] diffusion learning rate: 0.001
2024-11-05 03:49:34,924 - INFO - [diffusion][Epoch 9033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:34,925 - INFO - [diffusion][Epoch 9034] Epoch 9035/12000
2024-11-05 03:49:39,424 - INFO - [diffusion][Epoch 9034] diffusion training Loss: 0.05298160761594772
2024-11-05 03:49:39,426 - INFO - [diffusion][Epoch 9034] diffusion learning rate: 0.001
2024-11-05 03:49:39,428 - INFO - [diffusion][Epoch 9034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:39,429 - INFO - [diffusion][Epoch 9035] Epoch 9036/12000
2024-11-05 03:49:43,613 - INFO - [diffusion][Epoch 9035] diffusion training Loss: 0.050728341564536095
2024-11-05 03:49:43,615 - INFO - [diffusion][Epoch 9035] diffusion learning rate: 0.001
2024-11-05 03:49:43,617 - INFO - [diffusion][Epoch 9035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:43,618 - INFO - [diffusion][Epoch 9036] Epoch 9037/12000
2024-11-05 03:49:47,722 - INFO - [diffusion][Epoch 9036] diffusion training Loss: 0.052243493497371674
2024-11-05 03:49:47,724 - INFO - [diffusion][Epoch 9036] diffusion learning rate: 0.001
2024-11-05 03:49:47,726 - INFO - [diffusion][Epoch 9036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:47,727 - INFO - [diffusion][Epoch 9037] Epoch 9038/12000
2024-11-05 03:49:51,599 - INFO - [diffusion][Epoch 9037] diffusion training Loss: 0.05027940031141043
2024-11-05 03:49:51,602 - INFO - [diffusion][Epoch 9037] diffusion learning rate: 0.001
2024-11-05 03:49:51,604 - INFO - [diffusion][Epoch 9037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:51,606 - INFO - [diffusion][Epoch 9038] Epoch 9039/12000
2024-11-05 03:49:55,522 - INFO - [diffusion][Epoch 9038] diffusion training Loss: 0.049404594115912914
2024-11-05 03:49:55,524 - INFO - [diffusion][Epoch 9038] diffusion learning rate: 0.001
2024-11-05 03:49:55,526 - INFO - [diffusion][Epoch 9038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:55,528 - INFO - [diffusion][Epoch 9039] Epoch 9040/12000
2024-11-05 03:49:59,537 - INFO - [diffusion][Epoch 9039] diffusion training Loss: 0.05386712122708559
2024-11-05 03:49:59,540 - INFO - [diffusion][Epoch 9039] diffusion learning rate: 0.001
2024-11-05 03:49:59,542 - INFO - [diffusion][Epoch 9039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:49:59,543 - INFO - [diffusion][Epoch 9040] Epoch 9041/12000
2024-11-05 03:50:03,555 - INFO - [diffusion][Epoch 9040] diffusion training Loss: 0.05575805623084307
2024-11-05 03:50:03,557 - INFO - [diffusion][Epoch 9040] diffusion learning rate: 0.001
2024-11-05 03:50:03,559 - INFO - [diffusion][Epoch 9040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:03,560 - INFO - [diffusion][Epoch 9041] Epoch 9042/12000
2024-11-05 03:50:07,547 - INFO - [diffusion][Epoch 9041] diffusion training Loss: 0.049977680668234825
2024-11-05 03:50:07,549 - INFO - [diffusion][Epoch 9041] diffusion learning rate: 0.001
2024-11-05 03:50:07,551 - INFO - [diffusion][Epoch 9041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:07,552 - INFO - [diffusion][Epoch 9042] Epoch 9043/12000
2024-11-05 03:50:11,547 - INFO - [diffusion][Epoch 9042] diffusion training Loss: 0.05050122831016779
2024-11-05 03:50:11,549 - INFO - [diffusion][Epoch 9042] diffusion learning rate: 0.001
2024-11-05 03:50:11,551 - INFO - [diffusion][Epoch 9042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:11,552 - INFO - [diffusion][Epoch 9043] Epoch 9044/12000
2024-11-05 03:50:15,718 - INFO - [diffusion][Epoch 9043] diffusion training Loss: 0.05030720867216587
2024-11-05 03:50:15,721 - INFO - [diffusion][Epoch 9043] diffusion learning rate: 0.001
2024-11-05 03:50:15,723 - INFO - [diffusion][Epoch 9043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:15,724 - INFO - [diffusion][Epoch 9044] Epoch 9045/12000
2024-11-05 03:50:19,779 - INFO - [diffusion][Epoch 9044] diffusion training Loss: 0.05679196771234274
2024-11-05 03:50:19,782 - INFO - [diffusion][Epoch 9044] diffusion learning rate: 0.001
2024-11-05 03:50:19,783 - INFO - [diffusion][Epoch 9044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:19,785 - INFO - [diffusion][Epoch 9045] Epoch 9046/12000
2024-11-05 03:50:23,716 - INFO - [diffusion][Epoch 9045] diffusion training Loss: 0.05105189140886068
2024-11-05 03:50:23,718 - INFO - [diffusion][Epoch 9045] diffusion learning rate: 0.001
2024-11-05 03:50:23,720 - INFO - [diffusion][Epoch 9045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:23,721 - INFO - [diffusion][Epoch 9046] Epoch 9047/12000
2024-11-05 03:50:27,681 - INFO - [diffusion][Epoch 9046] diffusion training Loss: 0.05440677236765623
2024-11-05 03:50:27,683 - INFO - [diffusion][Epoch 9046] diffusion learning rate: 0.001
2024-11-05 03:50:27,685 - INFO - [diffusion][Epoch 9046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:27,686 - INFO - [diffusion][Epoch 9047] Epoch 9048/12000
2024-11-05 03:50:31,690 - INFO - [diffusion][Epoch 9047] diffusion training Loss: 0.04978127498179674
2024-11-05 03:50:31,692 - INFO - [diffusion][Epoch 9047] diffusion learning rate: 0.001
2024-11-05 03:50:31,694 - INFO - [diffusion][Epoch 9047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:31,695 - INFO - [diffusion][Epoch 9048] Epoch 9049/12000
2024-11-05 03:50:35,574 - INFO - [diffusion][Epoch 9048] diffusion training Loss: 0.05055971350520849
2024-11-05 03:50:35,576 - INFO - [diffusion][Epoch 9048] diffusion learning rate: 0.001
2024-11-05 03:50:35,577 - INFO - [diffusion][Epoch 9048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:35,579 - INFO - [diffusion][Epoch 9049] Epoch 9050/12000
2024-11-05 03:50:39,552 - INFO - [diffusion][Epoch 9049] diffusion training Loss: 0.05580653343349695
2024-11-05 03:50:39,554 - INFO - [diffusion][Epoch 9049] diffusion learning rate: 0.001
2024-11-05 03:50:39,556 - INFO - [diffusion][Epoch 9049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:39,558 - INFO - [diffusion][Epoch 9050] Epoch 9051/12000
2024-11-05 03:50:43,673 - INFO - [diffusion][Epoch 9050] diffusion training Loss: 0.054033877328038216
2024-11-05 03:50:43,675 - INFO - [diffusion][Epoch 9050] diffusion learning rate: 0.001
2024-11-05 03:50:43,677 - INFO - [diffusion][Epoch 9050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:43,678 - INFO - [diffusion][Epoch 9051] Epoch 9052/12000
2024-11-05 03:50:47,712 - INFO - [diffusion][Epoch 9051] diffusion training Loss: 0.048472664318978786
2024-11-05 03:50:47,714 - INFO - [diffusion][Epoch 9051] diffusion learning rate: 0.001
2024-11-05 03:50:47,716 - INFO - [diffusion][Epoch 9051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:47,718 - INFO - [diffusion][Epoch 9052] Epoch 9053/12000
2024-11-05 03:50:51,826 - INFO - [diffusion][Epoch 9052] diffusion training Loss: 0.05314730200916529
2024-11-05 03:50:51,828 - INFO - [diffusion][Epoch 9052] diffusion learning rate: 0.001
2024-11-05 03:50:51,830 - INFO - [diffusion][Epoch 9052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:51,831 - INFO - [diffusion][Epoch 9053] Epoch 9054/12000
2024-11-05 03:50:55,894 - INFO - [diffusion][Epoch 9053] diffusion training Loss: 0.05165486875921488
2024-11-05 03:50:55,896 - INFO - [diffusion][Epoch 9053] diffusion learning rate: 0.001
2024-11-05 03:50:55,897 - INFO - [diffusion][Epoch 9053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:50:55,899 - INFO - [diffusion][Epoch 9054] Epoch 9055/12000
2024-11-05 03:51:00,017 - INFO - [diffusion][Epoch 9054] diffusion training Loss: 0.04956998396664858
2024-11-05 03:51:00,019 - INFO - [diffusion][Epoch 9054] diffusion learning rate: 0.001
2024-11-05 03:51:00,020 - INFO - [diffusion][Epoch 9054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:00,022 - INFO - [diffusion][Epoch 9055] Epoch 9056/12000
2024-11-05 03:51:04,129 - INFO - [diffusion][Epoch 9055] diffusion training Loss: 0.052353267557919025
2024-11-05 03:51:04,132 - INFO - [diffusion][Epoch 9055] diffusion learning rate: 0.001
2024-11-05 03:51:04,135 - INFO - [diffusion][Epoch 9055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:04,136 - INFO - [diffusion][Epoch 9056] Epoch 9057/12000
2024-11-05 03:51:08,395 - INFO - [diffusion][Epoch 9056] diffusion training Loss: 0.04609903134405613
2024-11-05 03:51:08,397 - INFO - [diffusion][Epoch 9056] diffusion learning rate: 0.001
2024-11-05 03:51:08,399 - INFO - [diffusion][Epoch 9056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:08,400 - INFO - [diffusion][Epoch 9057] Epoch 9058/12000
2024-11-05 03:51:12,433 - INFO - [diffusion][Epoch 9057] diffusion training Loss: 0.049370432272553444
2024-11-05 03:51:12,435 - INFO - [diffusion][Epoch 9057] diffusion learning rate: 0.001
2024-11-05 03:51:12,436 - INFO - [diffusion][Epoch 9057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:12,438 - INFO - [diffusion][Epoch 9058] Epoch 9059/12000
2024-11-05 03:51:16,531 - INFO - [diffusion][Epoch 9058] diffusion training Loss: 0.046706439927220345
2024-11-05 03:51:16,534 - INFO - [diffusion][Epoch 9058] diffusion learning rate: 0.001
2024-11-05 03:51:16,535 - INFO - [diffusion][Epoch 9058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:16,537 - INFO - [diffusion][Epoch 9059] Epoch 9060/12000
2024-11-05 03:51:20,614 - INFO - [diffusion][Epoch 9059] diffusion training Loss: 0.05291913542896509
2024-11-05 03:51:20,616 - INFO - [diffusion][Epoch 9059] diffusion learning rate: 0.001
2024-11-05 03:51:20,618 - INFO - [diffusion][Epoch 9059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:20,619 - INFO - [diffusion][Epoch 9060] Epoch 9061/12000
2024-11-05 03:51:24,760 - INFO - [diffusion][Epoch 9060] diffusion training Loss: 0.05097776558250189
2024-11-05 03:51:24,763 - INFO - [diffusion][Epoch 9060] diffusion learning rate: 0.001
2024-11-05 03:51:24,765 - INFO - [diffusion][Epoch 9060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:24,766 - INFO - [diffusion][Epoch 9061] Epoch 9062/12000
2024-11-05 03:51:28,873 - INFO - [diffusion][Epoch 9061] diffusion training Loss: 0.049171299673616886
2024-11-05 03:51:28,875 - INFO - [diffusion][Epoch 9061] diffusion learning rate: 0.001
2024-11-05 03:51:28,877 - INFO - [diffusion][Epoch 9061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:28,878 - INFO - [diffusion][Epoch 9062] Epoch 9063/12000
2024-11-05 03:51:32,988 - INFO - [diffusion][Epoch 9062] diffusion training Loss: 0.054044291377067566
2024-11-05 03:51:32,991 - INFO - [diffusion][Epoch 9062] diffusion learning rate: 0.001
2024-11-05 03:51:32,993 - INFO - [diffusion][Epoch 9062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:32,995 - INFO - [diffusion][Epoch 9063] Epoch 9064/12000
2024-11-05 03:51:37,081 - INFO - [diffusion][Epoch 9063] diffusion training Loss: 0.0519630154594779
2024-11-05 03:51:37,083 - INFO - [diffusion][Epoch 9063] diffusion learning rate: 0.001
2024-11-05 03:51:37,085 - INFO - [diffusion][Epoch 9063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:37,086 - INFO - [diffusion][Epoch 9064] Epoch 9065/12000
2024-11-05 03:51:41,198 - INFO - [diffusion][Epoch 9064] diffusion training Loss: 0.0507247606292367
2024-11-05 03:51:41,200 - INFO - [diffusion][Epoch 9064] diffusion learning rate: 0.001
2024-11-05 03:51:41,202 - INFO - [diffusion][Epoch 9064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:41,203 - INFO - [diffusion][Epoch 9065] Epoch 9066/12000
2024-11-05 03:51:45,288 - INFO - [diffusion][Epoch 9065] diffusion training Loss: 0.051856095902621746
2024-11-05 03:51:45,290 - INFO - [diffusion][Epoch 9065] diffusion learning rate: 0.001
2024-11-05 03:51:45,292 - INFO - [diffusion][Epoch 9065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:45,293 - INFO - [diffusion][Epoch 9066] Epoch 9067/12000
2024-11-05 03:51:49,358 - INFO - [diffusion][Epoch 9066] diffusion training Loss: 0.05163284298032522
2024-11-05 03:51:49,360 - INFO - [diffusion][Epoch 9066] diffusion learning rate: 0.001
2024-11-05 03:51:49,362 - INFO - [diffusion][Epoch 9066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:49,363 - INFO - [diffusion][Epoch 9067] Epoch 9068/12000
2024-11-05 03:51:53,471 - INFO - [diffusion][Epoch 9067] diffusion training Loss: 0.05210242886096239
2024-11-05 03:51:53,473 - INFO - [diffusion][Epoch 9067] diffusion learning rate: 0.001
2024-11-05 03:51:53,475 - INFO - [diffusion][Epoch 9067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:53,477 - INFO - [diffusion][Epoch 9068] Epoch 9069/12000
2024-11-05 03:51:57,613 - INFO - [diffusion][Epoch 9068] diffusion training Loss: 0.0501819858327508
2024-11-05 03:51:57,615 - INFO - [diffusion][Epoch 9068] diffusion learning rate: 0.001
2024-11-05 03:51:57,653 - INFO - [diffusion][Epoch 9068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:51:57,654 - INFO - [diffusion][Epoch 9069] Epoch 9070/12000
2024-11-05 03:52:01,716 - INFO - [diffusion][Epoch 9069] diffusion training Loss: 0.0537978820502758
2024-11-05 03:52:01,718 - INFO - [diffusion][Epoch 9069] diffusion learning rate: 0.001
2024-11-05 03:52:01,720 - INFO - [diffusion][Epoch 9069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:01,721 - INFO - [diffusion][Epoch 9070] Epoch 9071/12000
2024-11-05 03:52:05,862 - INFO - [diffusion][Epoch 9070] diffusion training Loss: 0.0523852976039052
2024-11-05 03:52:05,864 - INFO - [diffusion][Epoch 9070] diffusion learning rate: 0.001
2024-11-05 03:52:05,866 - INFO - [diffusion][Epoch 9070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:05,867 - INFO - [diffusion][Epoch 9071] Epoch 9072/12000
2024-11-05 03:52:09,856 - INFO - [diffusion][Epoch 9071] diffusion training Loss: 0.05370173044502735
2024-11-05 03:52:09,858 - INFO - [diffusion][Epoch 9071] diffusion learning rate: 0.001
2024-11-05 03:52:09,860 - INFO - [diffusion][Epoch 9071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:09,861 - INFO - [diffusion][Epoch 9072] Epoch 9073/12000
2024-11-05 03:52:13,863 - INFO - [diffusion][Epoch 9072] diffusion training Loss: 0.048469497822225094
2024-11-05 03:52:13,865 - INFO - [diffusion][Epoch 9072] diffusion learning rate: 0.001
2024-11-05 03:52:13,866 - INFO - [diffusion][Epoch 9072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:13,868 - INFO - [diffusion][Epoch 9073] Epoch 9074/12000
2024-11-05 03:52:18,024 - INFO - [diffusion][Epoch 9073] diffusion training Loss: 0.04892855789512396
2024-11-05 03:52:18,026 - INFO - [diffusion][Epoch 9073] diffusion learning rate: 0.001
2024-11-05 03:52:18,028 - INFO - [diffusion][Epoch 9073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:18,030 - INFO - [diffusion][Epoch 9074] Epoch 9075/12000
2024-11-05 03:52:22,033 - INFO - [diffusion][Epoch 9074] diffusion training Loss: 0.052909886464476585
2024-11-05 03:52:22,035 - INFO - [diffusion][Epoch 9074] diffusion learning rate: 0.001
2024-11-05 03:52:22,037 - INFO - [diffusion][Epoch 9074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:22,038 - INFO - [diffusion][Epoch 9075] Epoch 9076/12000
2024-11-05 03:52:26,044 - INFO - [diffusion][Epoch 9075] diffusion training Loss: 0.05354822985827923
2024-11-05 03:52:26,046 - INFO - [diffusion][Epoch 9075] diffusion learning rate: 0.001
2024-11-05 03:52:26,048 - INFO - [diffusion][Epoch 9075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:26,049 - INFO - [diffusion][Epoch 9076] Epoch 9077/12000
2024-11-05 03:52:30,258 - INFO - [diffusion][Epoch 9076] diffusion training Loss: 0.051988123916089535
2024-11-05 03:52:30,260 - INFO - [diffusion][Epoch 9076] diffusion learning rate: 0.001
2024-11-05 03:52:30,262 - INFO - [diffusion][Epoch 9076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:30,263 - INFO - [diffusion][Epoch 9077] Epoch 9078/12000
2024-11-05 03:52:34,381 - INFO - [diffusion][Epoch 9077] diffusion training Loss: 0.05388513021171093
2024-11-05 03:52:34,383 - INFO - [diffusion][Epoch 9077] diffusion learning rate: 0.001
2024-11-05 03:52:34,385 - INFO - [diffusion][Epoch 9077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:34,386 - INFO - [diffusion][Epoch 9078] Epoch 9079/12000
2024-11-05 03:52:38,570 - INFO - [diffusion][Epoch 9078] diffusion training Loss: 0.046158396638929844
2024-11-05 03:52:38,574 - INFO - [diffusion][Epoch 9078] diffusion learning rate: 0.001
2024-11-05 03:52:38,575 - INFO - [diffusion][Epoch 9078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:38,577 - INFO - [diffusion][Epoch 9079] Epoch 9080/12000
2024-11-05 03:52:42,630 - INFO - [diffusion][Epoch 9079] diffusion training Loss: 0.05390725936740637
2024-11-05 03:52:42,632 - INFO - [diffusion][Epoch 9079] diffusion learning rate: 0.001
2024-11-05 03:52:42,634 - INFO - [diffusion][Epoch 9079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:42,635 - INFO - [diffusion][Epoch 9080] Epoch 9081/12000
2024-11-05 03:52:46,752 - INFO - [diffusion][Epoch 9080] diffusion training Loss: 0.052062065340578556
2024-11-05 03:52:46,754 - INFO - [diffusion][Epoch 9080] diffusion learning rate: 0.001
2024-11-05 03:52:46,784 - INFO - [diffusion][Epoch 9080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:46,786 - INFO - [diffusion][Epoch 9081] Epoch 9082/12000
2024-11-05 03:52:50,853 - INFO - [diffusion][Epoch 9081] diffusion training Loss: 0.05350359249860048
2024-11-05 03:52:50,855 - INFO - [diffusion][Epoch 9081] diffusion learning rate: 0.001
2024-11-05 03:52:50,857 - INFO - [diffusion][Epoch 9081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:50,858 - INFO - [diffusion][Epoch 9082] Epoch 9083/12000
2024-11-05 03:52:54,962 - INFO - [diffusion][Epoch 9082] diffusion training Loss: 0.047976162284612656
2024-11-05 03:52:54,963 - INFO - [diffusion][Epoch 9082] diffusion learning rate: 0.001
2024-11-05 03:52:54,965 - INFO - [diffusion][Epoch 9082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:54,966 - INFO - [diffusion][Epoch 9083] Epoch 9084/12000
2024-11-05 03:52:59,030 - INFO - [diffusion][Epoch 9083] diffusion training Loss: 0.04784600995481014
2024-11-05 03:52:59,032 - INFO - [diffusion][Epoch 9083] diffusion learning rate: 0.001
2024-11-05 03:52:59,034 - INFO - [diffusion][Epoch 9083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:52:59,035 - INFO - [diffusion][Epoch 9084] Epoch 9085/12000
2024-11-05 03:53:03,164 - INFO - [diffusion][Epoch 9084] diffusion training Loss: 0.0518591208383441
2024-11-05 03:53:03,166 - INFO - [diffusion][Epoch 9084] diffusion learning rate: 0.001
2024-11-05 03:53:03,168 - INFO - [diffusion][Epoch 9084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:03,169 - INFO - [diffusion][Epoch 9085] Epoch 9086/12000
2024-11-05 03:53:07,293 - INFO - [diffusion][Epoch 9085] diffusion training Loss: 0.05343604739755392
2024-11-05 03:53:07,295 - INFO - [diffusion][Epoch 9085] diffusion learning rate: 0.001
2024-11-05 03:53:07,329 - INFO - [diffusion][Epoch 9085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:07,330 - INFO - [diffusion][Epoch 9086] Epoch 9087/12000
2024-11-05 03:53:11,420 - INFO - [diffusion][Epoch 9086] diffusion training Loss: 0.051658764481544495
2024-11-05 03:53:11,423 - INFO - [diffusion][Epoch 9086] diffusion learning rate: 0.001
2024-11-05 03:53:11,424 - INFO - [diffusion][Epoch 9086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:11,426 - INFO - [diffusion][Epoch 9087] Epoch 9088/12000
2024-11-05 03:53:15,538 - INFO - [diffusion][Epoch 9087] diffusion training Loss: 0.048077430576086044
2024-11-05 03:53:15,540 - INFO - [diffusion][Epoch 9087] diffusion learning rate: 0.001
2024-11-05 03:53:15,541 - INFO - [diffusion][Epoch 9087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:15,543 - INFO - [diffusion][Epoch 9088] Epoch 9089/12000
2024-11-05 03:53:19,679 - INFO - [diffusion][Epoch 9088] diffusion training Loss: 0.0526478411629796
2024-11-05 03:53:19,681 - INFO - [diffusion][Epoch 9088] diffusion learning rate: 0.001
2024-11-05 03:53:19,683 - INFO - [diffusion][Epoch 9088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:19,685 - INFO - [diffusion][Epoch 9089] Epoch 9090/12000
2024-11-05 03:53:23,727 - INFO - [diffusion][Epoch 9089] diffusion training Loss: 0.046948106959462166
2024-11-05 03:53:23,729 - INFO - [diffusion][Epoch 9089] diffusion learning rate: 0.001
2024-11-05 03:53:23,731 - INFO - [diffusion][Epoch 9089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:23,732 - INFO - [diffusion][Epoch 9090] Epoch 9091/12000
2024-11-05 03:53:27,796 - INFO - [diffusion][Epoch 9090] diffusion training Loss: 0.05510479677468538
2024-11-05 03:53:27,798 - INFO - [diffusion][Epoch 9090] diffusion learning rate: 0.001
2024-11-05 03:53:27,800 - INFO - [diffusion][Epoch 9090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:27,801 - INFO - [diffusion][Epoch 9091] Epoch 9092/12000
2024-11-05 03:53:31,800 - INFO - [diffusion][Epoch 9091] diffusion training Loss: 0.053809854201972485
2024-11-05 03:53:31,802 - INFO - [diffusion][Epoch 9091] diffusion learning rate: 0.001
2024-11-05 03:53:31,804 - INFO - [diffusion][Epoch 9091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:31,805 - INFO - [diffusion][Epoch 9092] Epoch 9093/12000
2024-11-05 03:53:35,954 - INFO - [diffusion][Epoch 9092] diffusion training Loss: 0.05488458089530468
2024-11-05 03:53:35,957 - INFO - [diffusion][Epoch 9092] diffusion learning rate: 0.001
2024-11-05 03:53:35,959 - INFO - [diffusion][Epoch 9092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:35,961 - INFO - [diffusion][Epoch 9093] Epoch 9094/12000
2024-11-05 03:53:40,078 - INFO - [diffusion][Epoch 9093] diffusion training Loss: 0.05062191467732191
2024-11-05 03:53:40,080 - INFO - [diffusion][Epoch 9093] diffusion learning rate: 0.001
2024-11-05 03:53:40,082 - INFO - [diffusion][Epoch 9093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:40,083 - INFO - [diffusion][Epoch 9094] Epoch 9095/12000
2024-11-05 03:53:44,119 - INFO - [diffusion][Epoch 9094] diffusion training Loss: 0.04865215625613928
2024-11-05 03:53:44,121 - INFO - [diffusion][Epoch 9094] diffusion learning rate: 0.001
2024-11-05 03:53:44,123 - INFO - [diffusion][Epoch 9094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:44,125 - INFO - [diffusion][Epoch 9095] Epoch 9096/12000
2024-11-05 03:53:48,300 - INFO - [diffusion][Epoch 9095] diffusion training Loss: 0.05372076295316219
2024-11-05 03:53:48,303 - INFO - [diffusion][Epoch 9095] diffusion learning rate: 0.001
2024-11-05 03:53:48,305 - INFO - [diffusion][Epoch 9095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:48,306 - INFO - [diffusion][Epoch 9096] Epoch 9097/12000
2024-11-05 03:53:52,503 - INFO - [diffusion][Epoch 9096] diffusion training Loss: 0.05516640469431877
2024-11-05 03:53:52,505 - INFO - [diffusion][Epoch 9096] diffusion learning rate: 0.001
2024-11-05 03:53:52,507 - INFO - [diffusion][Epoch 9096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:52,509 - INFO - [diffusion][Epoch 9097] Epoch 9098/12000
2024-11-05 03:53:56,792 - INFO - [diffusion][Epoch 9097] diffusion training Loss: 0.04834545310586691
2024-11-05 03:53:56,794 - INFO - [diffusion][Epoch 9097] diffusion learning rate: 0.001
2024-11-05 03:53:56,796 - INFO - [diffusion][Epoch 9097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:53:56,797 - INFO - [diffusion][Epoch 9098] Epoch 9099/12000
2024-11-05 03:54:00,782 - INFO - [diffusion][Epoch 9098] diffusion training Loss: 0.05022033490240574
2024-11-05 03:54:00,786 - INFO - [diffusion][Epoch 9098] diffusion learning rate: 0.001
2024-11-05 03:54:00,788 - INFO - [diffusion][Epoch 9098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:00,790 - INFO - [diffusion][Epoch 9099] Epoch 9100/12000
2024-11-05 03:54:04,982 - INFO - [diffusion][Epoch 9099] diffusion training Loss: 0.05104684270918369
2024-11-05 03:54:04,985 - INFO - [diffusion][Epoch 9099] diffusion learning rate: 0.001
2024-11-05 03:54:04,987 - INFO - [diffusion][Epoch 9099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:04,988 - INFO - [diffusion][Epoch 9100] Epoch 9101/12000
2024-11-05 03:54:09,152 - INFO - [diffusion][Epoch 9100] diffusion training Loss: 0.05167634040117264
2024-11-05 03:54:09,154 - INFO - [diffusion][Epoch 9100] diffusion learning rate: 0.001
2024-11-05 03:54:09,156 - INFO - [diffusion][Epoch 9100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:09,157 - INFO - [diffusion][Epoch 9101] Epoch 9102/12000
2024-11-05 03:54:13,349 - INFO - [diffusion][Epoch 9101] diffusion training Loss: 0.05141224339604378
2024-11-05 03:54:13,352 - INFO - [diffusion][Epoch 9101] diffusion learning rate: 0.001
2024-11-05 03:54:13,354 - INFO - [diffusion][Epoch 9101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:13,355 - INFO - [diffusion][Epoch 9102] Epoch 9103/12000
2024-11-05 03:54:17,398 - INFO - [diffusion][Epoch 9102] diffusion training Loss: 0.04919150099158287
2024-11-05 03:54:17,400 - INFO - [diffusion][Epoch 9102] diffusion learning rate: 0.001
2024-11-05 03:54:17,402 - INFO - [diffusion][Epoch 9102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:17,404 - INFO - [diffusion][Epoch 9103] Epoch 9104/12000
2024-11-05 03:54:21,512 - INFO - [diffusion][Epoch 9103] diffusion training Loss: 0.055106960237026215
2024-11-05 03:54:21,515 - INFO - [diffusion][Epoch 9103] diffusion learning rate: 0.001
2024-11-05 03:54:21,516 - INFO - [diffusion][Epoch 9103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:21,518 - INFO - [diffusion][Epoch 9104] Epoch 9105/12000
2024-11-05 03:54:25,657 - INFO - [diffusion][Epoch 9104] diffusion training Loss: 0.0492863692343235
2024-11-05 03:54:25,659 - INFO - [diffusion][Epoch 9104] diffusion learning rate: 0.001
2024-11-05 03:54:25,678 - INFO - [diffusion][Epoch 9104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:25,679 - INFO - [diffusion][Epoch 9105] Epoch 9106/12000
2024-11-05 03:54:29,813 - INFO - [diffusion][Epoch 9105] diffusion training Loss: 0.05142508540302515
2024-11-05 03:54:29,815 - INFO - [diffusion][Epoch 9105] diffusion learning rate: 0.001
2024-11-05 03:54:29,817 - INFO - [diffusion][Epoch 9105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:29,818 - INFO - [diffusion][Epoch 9106] Epoch 9107/12000
2024-11-05 03:54:33,935 - INFO - [diffusion][Epoch 9106] diffusion training Loss: 0.04934225417673588
2024-11-05 03:54:33,938 - INFO - [diffusion][Epoch 9106] diffusion learning rate: 0.001
2024-11-05 03:54:33,940 - INFO - [diffusion][Epoch 9106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:33,941 - INFO - [diffusion][Epoch 9107] Epoch 9108/12000
2024-11-05 03:54:37,919 - INFO - [diffusion][Epoch 9107] diffusion training Loss: 0.05576719529926777
2024-11-05 03:54:37,921 - INFO - [diffusion][Epoch 9107] diffusion learning rate: 0.001
2024-11-05 03:54:37,923 - INFO - [diffusion][Epoch 9107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:37,924 - INFO - [diffusion][Epoch 9108] Epoch 9109/12000
2024-11-05 03:54:42,042 - INFO - [diffusion][Epoch 9108] diffusion training Loss: 0.05443938449025154
2024-11-05 03:54:42,044 - INFO - [diffusion][Epoch 9108] diffusion learning rate: 0.001
2024-11-05 03:54:42,046 - INFO - [diffusion][Epoch 9108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:42,048 - INFO - [diffusion][Epoch 9109] Epoch 9110/12000
2024-11-05 03:54:46,149 - INFO - [diffusion][Epoch 9109] diffusion training Loss: 0.04842814803123474
2024-11-05 03:54:46,151 - INFO - [diffusion][Epoch 9109] diffusion learning rate: 0.001
2024-11-05 03:54:46,153 - INFO - [diffusion][Epoch 9109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:46,155 - INFO - [diffusion][Epoch 9110] Epoch 9111/12000
2024-11-05 03:54:50,234 - INFO - [diffusion][Epoch 9110] diffusion training Loss: 0.048844704404473305
2024-11-05 03:54:50,236 - INFO - [diffusion][Epoch 9110] diffusion learning rate: 0.001
2024-11-05 03:54:50,238 - INFO - [diffusion][Epoch 9110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:50,239 - INFO - [diffusion][Epoch 9111] Epoch 9112/12000
2024-11-05 03:54:54,370 - INFO - [diffusion][Epoch 9111] diffusion training Loss: 0.04997816029936075
2024-11-05 03:54:54,372 - INFO - [diffusion][Epoch 9111] diffusion learning rate: 0.001
2024-11-05 03:54:54,374 - INFO - [diffusion][Epoch 9111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:54,375 - INFO - [diffusion][Epoch 9112] Epoch 9113/12000
2024-11-05 03:54:58,535 - INFO - [diffusion][Epoch 9112] diffusion training Loss: 0.051480636931955814
2024-11-05 03:54:58,537 - INFO - [diffusion][Epoch 9112] diffusion learning rate: 0.001
2024-11-05 03:54:58,538 - INFO - [diffusion][Epoch 9112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:54:58,540 - INFO - [diffusion][Epoch 9113] Epoch 9114/12000
2024-11-05 03:55:02,657 - INFO - [diffusion][Epoch 9113] diffusion training Loss: 0.049103536643087864
2024-11-05 03:55:02,659 - INFO - [diffusion][Epoch 9113] diffusion learning rate: 0.001
2024-11-05 03:55:02,661 - INFO - [diffusion][Epoch 9113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:02,662 - INFO - [diffusion][Epoch 9114] Epoch 9115/12000
2024-11-05 03:55:06,585 - INFO - [diffusion][Epoch 9114] diffusion training Loss: 0.04807068407535553
2024-11-05 03:55:06,587 - INFO - [diffusion][Epoch 9114] diffusion learning rate: 0.001
2024-11-05 03:55:06,589 - INFO - [diffusion][Epoch 9114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:06,590 - INFO - [diffusion][Epoch 9115] Epoch 9116/12000
2024-11-05 03:55:10,638 - INFO - [diffusion][Epoch 9115] diffusion training Loss: 0.05650060065090656
2024-11-05 03:55:10,640 - INFO - [diffusion][Epoch 9115] diffusion learning rate: 0.001
2024-11-05 03:55:10,642 - INFO - [diffusion][Epoch 9115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:10,643 - INFO - [diffusion][Epoch 9116] Epoch 9117/12000
2024-11-05 03:55:14,759 - INFO - [diffusion][Epoch 9116] diffusion training Loss: 0.04600239731371403
2024-11-05 03:55:14,761 - INFO - [diffusion][Epoch 9116] diffusion learning rate: 0.001
2024-11-05 03:55:14,762 - INFO - [diffusion][Epoch 9116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:14,763 - INFO - [diffusion][Epoch 9117] Epoch 9118/12000
2024-11-05 03:55:19,124 - INFO - [diffusion][Epoch 9117] diffusion training Loss: 0.04812956880778074
2024-11-05 03:55:19,126 - INFO - [diffusion][Epoch 9117] diffusion learning rate: 0.001
2024-11-05 03:55:19,128 - INFO - [diffusion][Epoch 9117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:19,129 - INFO - [diffusion][Epoch 9118] Epoch 9119/12000
2024-11-05 03:55:23,143 - INFO - [diffusion][Epoch 9118] diffusion training Loss: 0.04909973870962858
2024-11-05 03:55:23,146 - INFO - [diffusion][Epoch 9118] diffusion learning rate: 0.001
2024-11-05 03:55:23,148 - INFO - [diffusion][Epoch 9118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:23,149 - INFO - [diffusion][Epoch 9119] Epoch 9120/12000
2024-11-05 03:55:27,240 - INFO - [diffusion][Epoch 9119] diffusion training Loss: 0.04646413866430521
2024-11-05 03:55:27,242 - INFO - [diffusion][Epoch 9119] diffusion learning rate: 0.001
2024-11-05 03:55:27,244 - INFO - [diffusion][Epoch 9119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:27,245 - INFO - [diffusion][Epoch 9120] Epoch 9121/12000
2024-11-05 03:55:31,295 - INFO - [diffusion][Epoch 9120] diffusion training Loss: 0.04794098902493715
2024-11-05 03:55:31,298 - INFO - [diffusion][Epoch 9120] diffusion learning rate: 0.001
2024-11-05 03:55:31,299 - INFO - [diffusion][Epoch 9120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:31,301 - INFO - [diffusion][Epoch 9121] Epoch 9122/12000
2024-11-05 03:55:35,412 - INFO - [diffusion][Epoch 9121] diffusion training Loss: 0.05046476237475872
2024-11-05 03:55:35,414 - INFO - [diffusion][Epoch 9121] diffusion learning rate: 0.001
2024-11-05 03:55:35,416 - INFO - [diffusion][Epoch 9121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:35,417 - INFO - [diffusion][Epoch 9122] Epoch 9123/12000
2024-11-05 03:55:39,491 - INFO - [diffusion][Epoch 9122] diffusion training Loss: 0.047851269133388996
2024-11-05 03:55:39,493 - INFO - [diffusion][Epoch 9122] diffusion learning rate: 0.001
2024-11-05 03:55:39,494 - INFO - [diffusion][Epoch 9122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:39,496 - INFO - [diffusion][Epoch 9123] Epoch 9124/12000
2024-11-05 03:55:43,561 - INFO - [diffusion][Epoch 9123] diffusion training Loss: 0.04916827101260424
2024-11-05 03:55:43,563 - INFO - [diffusion][Epoch 9123] diffusion learning rate: 0.001
2024-11-05 03:55:43,565 - INFO - [diffusion][Epoch 9123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:43,566 - INFO - [diffusion][Epoch 9124] Epoch 9125/12000
2024-11-05 03:55:47,445 - INFO - [diffusion][Epoch 9124] diffusion training Loss: 0.048428766429424286
2024-11-05 03:55:47,447 - INFO - [diffusion][Epoch 9124] diffusion learning rate: 0.001
2024-11-05 03:55:47,449 - INFO - [diffusion][Epoch 9124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:47,450 - INFO - [diffusion][Epoch 9125] Epoch 9126/12000
2024-11-05 03:55:51,439 - INFO - [diffusion][Epoch 9125] diffusion training Loss: 0.05015831347554922
2024-11-05 03:55:51,441 - INFO - [diffusion][Epoch 9125] diffusion learning rate: 0.001
2024-11-05 03:55:51,443 - INFO - [diffusion][Epoch 9125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:51,444 - INFO - [diffusion][Epoch 9126] Epoch 9127/12000
2024-11-05 03:55:55,508 - INFO - [diffusion][Epoch 9126] diffusion training Loss: 0.054665093310177326
2024-11-05 03:55:55,510 - INFO - [diffusion][Epoch 9126] diffusion learning rate: 0.001
2024-11-05 03:55:55,512 - INFO - [diffusion][Epoch 9126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:55,513 - INFO - [diffusion][Epoch 9127] Epoch 9128/12000
2024-11-05 03:55:59,563 - INFO - [diffusion][Epoch 9127] diffusion training Loss: 0.052104586735367775
2024-11-05 03:55:59,565 - INFO - [diffusion][Epoch 9127] diffusion learning rate: 0.001
2024-11-05 03:55:59,567 - INFO - [diffusion][Epoch 9127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:55:59,568 - INFO - [diffusion][Epoch 9128] Epoch 9129/12000
2024-11-05 03:56:03,603 - INFO - [diffusion][Epoch 9128] diffusion training Loss: 0.05806950945407152
2024-11-05 03:56:03,605 - INFO - [diffusion][Epoch 9128] diffusion learning rate: 0.001
2024-11-05 03:56:03,607 - INFO - [diffusion][Epoch 9128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:03,608 - INFO - [diffusion][Epoch 9129] Epoch 9130/12000
2024-11-05 03:56:07,593 - INFO - [diffusion][Epoch 9129] diffusion training Loss: 0.04987385030835867
2024-11-05 03:56:07,595 - INFO - [diffusion][Epoch 9129] diffusion learning rate: 0.001
2024-11-05 03:56:07,597 - INFO - [diffusion][Epoch 9129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:07,598 - INFO - [diffusion][Epoch 9130] Epoch 9131/12000
2024-11-05 03:56:11,633 - INFO - [diffusion][Epoch 9130] diffusion training Loss: 0.05042798910290003
2024-11-05 03:56:11,635 - INFO - [diffusion][Epoch 9130] diffusion learning rate: 0.001
2024-11-05 03:56:11,636 - INFO - [diffusion][Epoch 9130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:11,638 - INFO - [diffusion][Epoch 9131] Epoch 9132/12000
2024-11-05 03:56:15,601 - INFO - [diffusion][Epoch 9131] diffusion training Loss: 0.05066229123622179
2024-11-05 03:56:15,603 - INFO - [diffusion][Epoch 9131] diffusion learning rate: 0.001
2024-11-05 03:56:15,605 - INFO - [diffusion][Epoch 9131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:15,606 - INFO - [diffusion][Epoch 9132] Epoch 9133/12000
2024-11-05 03:56:19,660 - INFO - [diffusion][Epoch 9132] diffusion training Loss: 0.0610993392765522
2024-11-05 03:56:19,662 - INFO - [diffusion][Epoch 9132] diffusion learning rate: 0.001
2024-11-05 03:56:19,664 - INFO - [diffusion][Epoch 9132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:19,665 - INFO - [diffusion][Epoch 9133] Epoch 9134/12000
2024-11-05 03:56:23,619 - INFO - [diffusion][Epoch 9133] diffusion training Loss: 0.05151974689215422
2024-11-05 03:56:23,622 - INFO - [diffusion][Epoch 9133] diffusion learning rate: 0.001
2024-11-05 03:56:23,625 - INFO - [diffusion][Epoch 9133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:23,626 - INFO - [diffusion][Epoch 9134] Epoch 9135/12000
2024-11-05 03:56:27,666 - INFO - [diffusion][Epoch 9134] diffusion training Loss: 0.048331799916923046
2024-11-05 03:56:27,668 - INFO - [diffusion][Epoch 9134] diffusion learning rate: 0.001
2024-11-05 03:56:27,670 - INFO - [diffusion][Epoch 9134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:27,671 - INFO - [diffusion][Epoch 9135] Epoch 9136/12000
2024-11-05 03:56:31,592 - INFO - [diffusion][Epoch 9135] diffusion training Loss: 0.04929626826196909
2024-11-05 03:56:31,594 - INFO - [diffusion][Epoch 9135] diffusion learning rate: 0.001
2024-11-05 03:56:31,596 - INFO - [diffusion][Epoch 9135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:31,597 - INFO - [diffusion][Epoch 9136] Epoch 9137/12000
2024-11-05 03:56:35,696 - INFO - [diffusion][Epoch 9136] diffusion training Loss: 0.0521174781024456
2024-11-05 03:56:35,698 - INFO - [diffusion][Epoch 9136] diffusion learning rate: 0.001
2024-11-05 03:56:35,700 - INFO - [diffusion][Epoch 9136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:35,701 - INFO - [diffusion][Epoch 9137] Epoch 9138/12000
2024-11-05 03:56:40,034 - INFO - [diffusion][Epoch 9137] diffusion training Loss: 0.0523602245375514
2024-11-05 03:56:40,037 - INFO - [diffusion][Epoch 9137] diffusion learning rate: 0.001
2024-11-05 03:56:40,039 - INFO - [diffusion][Epoch 9137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:40,041 - INFO - [diffusion][Epoch 9138] Epoch 9139/12000
2024-11-05 03:56:44,044 - INFO - [diffusion][Epoch 9138] diffusion training Loss: 0.051336782053112984
2024-11-05 03:56:44,045 - INFO - [diffusion][Epoch 9138] diffusion learning rate: 0.001
2024-11-05 03:56:44,047 - INFO - [diffusion][Epoch 9138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:44,048 - INFO - [diffusion][Epoch 9139] Epoch 9140/12000
2024-11-05 03:56:48,057 - INFO - [diffusion][Epoch 9139] diffusion training Loss: 0.051279217936098576
2024-11-05 03:56:48,059 - INFO - [diffusion][Epoch 9139] diffusion learning rate: 0.001
2024-11-05 03:56:48,061 - INFO - [diffusion][Epoch 9139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:48,063 - INFO - [diffusion][Epoch 9140] Epoch 9141/12000
2024-11-05 03:56:52,020 - INFO - [diffusion][Epoch 9140] diffusion training Loss: 0.04692343343049288
2024-11-05 03:56:52,022 - INFO - [diffusion][Epoch 9140] diffusion learning rate: 0.001
2024-11-05 03:56:52,023 - INFO - [diffusion][Epoch 9140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:52,025 - INFO - [diffusion][Epoch 9141] Epoch 9142/12000
2024-11-05 03:56:56,010 - INFO - [diffusion][Epoch 9141] diffusion training Loss: 0.05051491502672434
2024-11-05 03:56:56,012 - INFO - [diffusion][Epoch 9141] diffusion learning rate: 0.001
2024-11-05 03:56:56,014 - INFO - [diffusion][Epoch 9141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:56:56,015 - INFO - [diffusion][Epoch 9142] Epoch 9143/12000
2024-11-05 03:57:00,181 - INFO - [diffusion][Epoch 9142] diffusion training Loss: 0.051575652323663235
2024-11-05 03:57:00,183 - INFO - [diffusion][Epoch 9142] diffusion learning rate: 0.001
2024-11-05 03:57:00,185 - INFO - [diffusion][Epoch 9142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:00,187 - INFO - [diffusion][Epoch 9143] Epoch 9144/12000
2024-11-05 03:57:04,237 - INFO - [diffusion][Epoch 9143] diffusion training Loss: 0.04751112870872021
2024-11-05 03:57:04,240 - INFO - [diffusion][Epoch 9143] diffusion learning rate: 0.001
2024-11-05 03:57:04,242 - INFO - [diffusion][Epoch 9143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:04,243 - INFO - [diffusion][Epoch 9144] Epoch 9145/12000
2024-11-05 03:57:08,341 - INFO - [diffusion][Epoch 9144] diffusion training Loss: 0.044313292019069195
2024-11-05 03:57:08,343 - INFO - [diffusion][Epoch 9144] diffusion learning rate: 0.001
2024-11-05 03:57:08,345 - INFO - [diffusion][Epoch 9144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:08,346 - INFO - [diffusion][Epoch 9145] Epoch 9146/12000
2024-11-05 03:57:12,505 - INFO - [diffusion][Epoch 9145] diffusion training Loss: 0.054381899535655975
2024-11-05 03:57:12,695 - INFO - [diffusion][Epoch 9145] diffusion learning rate: 0.001
2024-11-05 03:57:12,697 - INFO - [diffusion][Epoch 9145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:12,698 - INFO - [diffusion][Epoch 9146] Epoch 9147/12000
2024-11-05 03:57:16,790 - INFO - [diffusion][Epoch 9146] diffusion training Loss: 0.05395745672285557
2024-11-05 03:57:16,792 - INFO - [diffusion][Epoch 9146] diffusion learning rate: 0.001
2024-11-05 03:57:16,799 - INFO - [diffusion][Epoch 9146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:16,800 - INFO - [diffusion][Epoch 9147] Epoch 9148/12000
2024-11-05 03:57:20,965 - INFO - [diffusion][Epoch 9147] diffusion training Loss: 0.05238171014934778
2024-11-05 03:57:20,967 - INFO - [diffusion][Epoch 9147] diffusion learning rate: 0.001
2024-11-05 03:57:20,968 - INFO - [diffusion][Epoch 9147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:20,970 - INFO - [diffusion][Epoch 9148] Epoch 9149/12000
2024-11-05 03:57:25,025 - INFO - [diffusion][Epoch 9148] diffusion training Loss: 0.052494547329843044
2024-11-05 03:57:25,028 - INFO - [diffusion][Epoch 9148] diffusion learning rate: 0.001
2024-11-05 03:57:25,030 - INFO - [diffusion][Epoch 9148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:25,032 - INFO - [diffusion][Epoch 9149] Epoch 9150/12000
2024-11-05 03:57:29,194 - INFO - [diffusion][Epoch 9149] diffusion training Loss: 0.0492336256429553
2024-11-05 03:57:29,196 - INFO - [diffusion][Epoch 9149] diffusion learning rate: 0.001
2024-11-05 03:57:29,198 - INFO - [diffusion][Epoch 9149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:29,199 - INFO - [diffusion][Epoch 9150] Epoch 9151/12000
2024-11-05 03:57:33,216 - INFO - [diffusion][Epoch 9150] diffusion training Loss: 0.050060066394507885
2024-11-05 03:57:33,218 - INFO - [diffusion][Epoch 9150] diffusion learning rate: 0.001
2024-11-05 03:57:33,220 - INFO - [diffusion][Epoch 9150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:33,221 - INFO - [diffusion][Epoch 9151] Epoch 9152/12000
2024-11-05 03:57:37,282 - INFO - [diffusion][Epoch 9151] diffusion training Loss: 0.055172571912407875
2024-11-05 03:57:37,284 - INFO - [diffusion][Epoch 9151] diffusion learning rate: 0.001
2024-11-05 03:57:37,286 - INFO - [diffusion][Epoch 9151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:37,287 - INFO - [diffusion][Epoch 9152] Epoch 9153/12000
2024-11-05 03:57:41,356 - INFO - [diffusion][Epoch 9152] diffusion training Loss: 0.04780000541359186
2024-11-05 03:57:41,358 - INFO - [diffusion][Epoch 9152] diffusion learning rate: 0.001
2024-11-05 03:57:41,359 - INFO - [diffusion][Epoch 9152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:41,361 - INFO - [diffusion][Epoch 9153] Epoch 9154/12000
2024-11-05 03:57:45,353 - INFO - [diffusion][Epoch 9153] diffusion training Loss: 0.0491392882540822
2024-11-05 03:57:45,355 - INFO - [diffusion][Epoch 9153] diffusion learning rate: 0.001
2024-11-05 03:57:45,356 - INFO - [diffusion][Epoch 9153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:45,358 - INFO - [diffusion][Epoch 9154] Epoch 9155/12000
2024-11-05 03:57:49,352 - INFO - [diffusion][Epoch 9154] diffusion training Loss: 0.051016367971897125
2024-11-05 03:57:49,354 - INFO - [diffusion][Epoch 9154] diffusion learning rate: 0.001
2024-11-05 03:57:49,356 - INFO - [diffusion][Epoch 9154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:49,357 - INFO - [diffusion][Epoch 9155] Epoch 9156/12000
2024-11-05 03:57:53,374 - INFO - [diffusion][Epoch 9155] diffusion training Loss: 0.04953262396156788
2024-11-05 03:57:53,377 - INFO - [diffusion][Epoch 9155] diffusion learning rate: 0.001
2024-11-05 03:57:53,379 - INFO - [diffusion][Epoch 9155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:53,380 - INFO - [diffusion][Epoch 9156] Epoch 9157/12000
2024-11-05 03:57:57,424 - INFO - [diffusion][Epoch 9156] diffusion training Loss: 0.05379642453044653
2024-11-05 03:57:57,426 - INFO - [diffusion][Epoch 9156] diffusion learning rate: 0.001
2024-11-05 03:57:57,427 - INFO - [diffusion][Epoch 9156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:57:57,428 - INFO - [diffusion][Epoch 9157] Epoch 9158/12000
2024-11-05 03:58:01,536 - INFO - [diffusion][Epoch 9157] diffusion training Loss: 0.052774650044739246
2024-11-05 03:58:01,538 - INFO - [diffusion][Epoch 9157] diffusion learning rate: 0.001
2024-11-05 03:58:01,540 - INFO - [diffusion][Epoch 9157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:01,541 - INFO - [diffusion][Epoch 9158] Epoch 9159/12000
2024-11-05 03:58:05,929 - INFO - [diffusion][Epoch 9158] diffusion training Loss: 0.05507811438292265
2024-11-05 03:58:05,931 - INFO - [diffusion][Epoch 9158] diffusion learning rate: 0.001
2024-11-05 03:58:05,933 - INFO - [diffusion][Epoch 9158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:05,935 - INFO - [diffusion][Epoch 9159] Epoch 9160/12000
2024-11-05 03:58:10,069 - INFO - [diffusion][Epoch 9159] diffusion training Loss: 0.05179954133927822
2024-11-05 03:58:10,071 - INFO - [diffusion][Epoch 9159] diffusion learning rate: 0.001
2024-11-05 03:58:10,073 - INFO - [diffusion][Epoch 9159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:10,074 - INFO - [diffusion][Epoch 9160] Epoch 9161/12000
2024-11-05 03:58:14,086 - INFO - [diffusion][Epoch 9160] diffusion training Loss: 0.052314748987555504
2024-11-05 03:58:14,088 - INFO - [diffusion][Epoch 9160] diffusion learning rate: 0.001
2024-11-05 03:58:14,090 - INFO - [diffusion][Epoch 9160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:14,091 - INFO - [diffusion][Epoch 9161] Epoch 9162/12000
2024-11-05 03:58:18,159 - INFO - [diffusion][Epoch 9161] diffusion training Loss: 0.052903542295098305
2024-11-05 03:58:18,161 - INFO - [diffusion][Epoch 9161] diffusion learning rate: 0.001
2024-11-05 03:58:18,163 - INFO - [diffusion][Epoch 9161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:18,164 - INFO - [diffusion][Epoch 9162] Epoch 9163/12000
2024-11-05 03:58:22,203 - INFO - [diffusion][Epoch 9162] diffusion training Loss: 0.05391834769397974
2024-11-05 03:58:22,205 - INFO - [diffusion][Epoch 9162] diffusion learning rate: 0.001
2024-11-05 03:58:22,207 - INFO - [diffusion][Epoch 9162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:22,208 - INFO - [diffusion][Epoch 9163] Epoch 9164/12000
2024-11-05 03:58:26,269 - INFO - [diffusion][Epoch 9163] diffusion training Loss: 0.054117352701723576
2024-11-05 03:58:26,271 - INFO - [diffusion][Epoch 9163] diffusion learning rate: 0.001
2024-11-05 03:58:26,273 - INFO - [diffusion][Epoch 9163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:26,274 - INFO - [diffusion][Epoch 9164] Epoch 9165/12000
2024-11-05 03:58:30,432 - INFO - [diffusion][Epoch 9164] diffusion training Loss: 0.05859342310577631
2024-11-05 03:58:30,434 - INFO - [diffusion][Epoch 9164] diffusion learning rate: 0.001
2024-11-05 03:58:30,436 - INFO - [diffusion][Epoch 9164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:30,437 - INFO - [diffusion][Epoch 9165] Epoch 9166/12000
2024-11-05 03:58:34,597 - INFO - [diffusion][Epoch 9165] diffusion training Loss: 0.05075777601450682
2024-11-05 03:58:34,599 - INFO - [diffusion][Epoch 9165] diffusion learning rate: 0.001
2024-11-05 03:58:34,600 - INFO - [diffusion][Epoch 9165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:34,602 - INFO - [diffusion][Epoch 9166] Epoch 9167/12000
2024-11-05 03:58:38,724 - INFO - [diffusion][Epoch 9166] diffusion training Loss: 0.05186615139245987
2024-11-05 03:58:38,726 - INFO - [diffusion][Epoch 9166] diffusion learning rate: 0.001
2024-11-05 03:58:38,729 - INFO - [diffusion][Epoch 9166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:38,730 - INFO - [diffusion][Epoch 9167] Epoch 9168/12000
2024-11-05 03:58:42,864 - INFO - [diffusion][Epoch 9167] diffusion training Loss: 0.05061595793813467
2024-11-05 03:58:42,866 - INFO - [diffusion][Epoch 9167] diffusion learning rate: 0.001
2024-11-05 03:58:42,868 - INFO - [diffusion][Epoch 9167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:42,869 - INFO - [diffusion][Epoch 9168] Epoch 9169/12000
2024-11-05 03:58:46,922 - INFO - [diffusion][Epoch 9168] diffusion training Loss: 0.05452014785259962
2024-11-05 03:58:46,924 - INFO - [diffusion][Epoch 9168] diffusion learning rate: 0.001
2024-11-05 03:58:46,926 - INFO - [diffusion][Epoch 9168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:46,927 - INFO - [diffusion][Epoch 9169] Epoch 9170/12000
2024-11-05 03:58:51,053 - INFO - [diffusion][Epoch 9169] diffusion training Loss: 0.05435945559293032
2024-11-05 03:58:51,055 - INFO - [diffusion][Epoch 9169] diffusion learning rate: 0.001
2024-11-05 03:58:51,056 - INFO - [diffusion][Epoch 9169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:51,058 - INFO - [diffusion][Epoch 9170] Epoch 9171/12000
2024-11-05 03:58:55,197 - INFO - [diffusion][Epoch 9170] diffusion training Loss: 0.05936586111783981
2024-11-05 03:58:55,199 - INFO - [diffusion][Epoch 9170] diffusion learning rate: 0.001
2024-11-05 03:58:55,201 - INFO - [diffusion][Epoch 9170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:55,202 - INFO - [diffusion][Epoch 9171] Epoch 9172/12000
2024-11-05 03:58:59,285 - INFO - [diffusion][Epoch 9171] diffusion training Loss: 0.05872991215437651
2024-11-05 03:58:59,290 - INFO - [diffusion][Epoch 9171] diffusion learning rate: 0.001
2024-11-05 03:58:59,292 - INFO - [diffusion][Epoch 9171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:58:59,294 - INFO - [diffusion][Epoch 9172] Epoch 9173/12000
2024-11-05 03:59:03,235 - INFO - [diffusion][Epoch 9172] diffusion training Loss: 0.050467525608837605
2024-11-05 03:59:03,237 - INFO - [diffusion][Epoch 9172] diffusion learning rate: 0.001
2024-11-05 03:59:03,238 - INFO - [diffusion][Epoch 9172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:03,240 - INFO - [diffusion][Epoch 9173] Epoch 9174/12000
2024-11-05 03:59:07,355 - INFO - [diffusion][Epoch 9173] diffusion training Loss: 0.04965617693960667
2024-11-05 03:59:07,357 - INFO - [diffusion][Epoch 9173] diffusion learning rate: 0.001
2024-11-05 03:59:07,359 - INFO - [diffusion][Epoch 9173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:07,360 - INFO - [diffusion][Epoch 9174] Epoch 9175/12000
2024-11-05 03:59:11,414 - INFO - [diffusion][Epoch 9174] diffusion training Loss: 0.050290798768401146
2024-11-05 03:59:11,415 - INFO - [diffusion][Epoch 9174] diffusion learning rate: 0.001
2024-11-05 03:59:11,417 - INFO - [diffusion][Epoch 9174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:11,418 - INFO - [diffusion][Epoch 9175] Epoch 9176/12000
2024-11-05 03:59:15,412 - INFO - [diffusion][Epoch 9175] diffusion training Loss: 0.050533004105091095
2024-11-05 03:59:15,414 - INFO - [diffusion][Epoch 9175] diffusion learning rate: 0.001
2024-11-05 03:59:15,415 - INFO - [diffusion][Epoch 9175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:15,417 - INFO - [diffusion][Epoch 9176] Epoch 9177/12000
2024-11-05 03:59:19,531 - INFO - [diffusion][Epoch 9176] diffusion training Loss: 0.05057928804308176
2024-11-05 03:59:19,533 - INFO - [diffusion][Epoch 9176] diffusion learning rate: 0.001
2024-11-05 03:59:19,534 - INFO - [diffusion][Epoch 9176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:19,535 - INFO - [diffusion][Epoch 9177] Epoch 9178/12000
2024-11-05 03:59:23,600 - INFO - [diffusion][Epoch 9177] diffusion training Loss: 0.04961044527590275
2024-11-05 03:59:23,601 - INFO - [diffusion][Epoch 9177] diffusion learning rate: 0.001
2024-11-05 03:59:23,603 - INFO - [diffusion][Epoch 9177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:23,604 - INFO - [diffusion][Epoch 9178] Epoch 9179/12000
2024-11-05 03:59:27,806 - INFO - [diffusion][Epoch 9178] diffusion training Loss: 0.051147932186722755
2024-11-05 03:59:27,809 - INFO - [diffusion][Epoch 9178] diffusion learning rate: 0.001
2024-11-05 03:59:27,811 - INFO - [diffusion][Epoch 9178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:27,812 - INFO - [diffusion][Epoch 9179] Epoch 9180/12000
2024-11-05 03:59:31,714 - INFO - [diffusion][Epoch 9179] diffusion training Loss: 0.0526787368580699
2024-11-05 03:59:31,716 - INFO - [diffusion][Epoch 9179] diffusion learning rate: 0.001
2024-11-05 03:59:31,718 - INFO - [diffusion][Epoch 9179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:31,719 - INFO - [diffusion][Epoch 9180] Epoch 9181/12000
2024-11-05 03:59:35,816 - INFO - [diffusion][Epoch 9180] diffusion training Loss: 0.047972419299185276
2024-11-05 03:59:35,819 - INFO - [diffusion][Epoch 9180] diffusion learning rate: 0.001
2024-11-05 03:59:35,820 - INFO - [diffusion][Epoch 9180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:35,822 - INFO - [diffusion][Epoch 9181] Epoch 9182/12000
2024-11-05 03:59:39,800 - INFO - [diffusion][Epoch 9181] diffusion training Loss: 0.048439464531838894
2024-11-05 03:59:39,802 - INFO - [diffusion][Epoch 9181] diffusion learning rate: 0.001
2024-11-05 03:59:39,804 - INFO - [diffusion][Epoch 9181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:39,806 - INFO - [diffusion][Epoch 9182] Epoch 9183/12000
2024-11-05 03:59:43,823 - INFO - [diffusion][Epoch 9182] diffusion training Loss: 0.0458621010184288
2024-11-05 03:59:43,825 - INFO - [diffusion][Epoch 9182] diffusion learning rate: 0.001
2024-11-05 03:59:43,826 - INFO - [diffusion][Epoch 9182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:43,828 - INFO - [diffusion][Epoch 9183] Epoch 9184/12000
2024-11-05 03:59:47,793 - INFO - [diffusion][Epoch 9183] diffusion training Loss: 0.04652135539799929
2024-11-05 03:59:47,796 - INFO - [diffusion][Epoch 9183] diffusion learning rate: 0.001
2024-11-05 03:59:47,798 - INFO - [diffusion][Epoch 9183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:47,799 - INFO - [diffusion][Epoch 9184] Epoch 9185/12000
2024-11-05 03:59:51,918 - INFO - [diffusion][Epoch 9184] diffusion training Loss: 0.04989311005920172
2024-11-05 03:59:51,920 - INFO - [diffusion][Epoch 9184] diffusion learning rate: 0.001
2024-11-05 03:59:51,923 - INFO - [diffusion][Epoch 9184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:51,924 - INFO - [diffusion][Epoch 9185] Epoch 9186/12000
2024-11-05 03:59:55,882 - INFO - [diffusion][Epoch 9185] diffusion training Loss: 0.05118872132152319
2024-11-05 03:59:55,884 - INFO - [diffusion][Epoch 9185] diffusion learning rate: 0.001
2024-11-05 03:59:55,886 - INFO - [diffusion][Epoch 9185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:55,887 - INFO - [diffusion][Epoch 9186] Epoch 9187/12000
2024-11-05 03:59:59,975 - INFO - [diffusion][Epoch 9186] diffusion training Loss: 0.04859626106917858
2024-11-05 03:59:59,979 - INFO - [diffusion][Epoch 9186] diffusion learning rate: 0.001
2024-11-05 03:59:59,981 - INFO - [diffusion][Epoch 9186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 03:59:59,982 - INFO - [diffusion][Epoch 9187] Epoch 9188/12000
2024-11-05 04:00:04,080 - INFO - [diffusion][Epoch 9187] diffusion training Loss: 0.05863174423575401
2024-11-05 04:00:04,082 - INFO - [diffusion][Epoch 9187] diffusion learning rate: 0.001
2024-11-05 04:00:04,084 - INFO - [diffusion][Epoch 9187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:04,085 - INFO - [diffusion][Epoch 9188] Epoch 9189/12000
2024-11-05 04:00:08,022 - INFO - [diffusion][Epoch 9188] diffusion training Loss: 0.05621605925261974
2024-11-05 04:00:08,024 - INFO - [diffusion][Epoch 9188] diffusion learning rate: 0.001
2024-11-05 04:00:08,051 - INFO - [diffusion][Epoch 9188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:08,052 - INFO - [diffusion][Epoch 9189] Epoch 9190/12000
2024-11-05 04:00:11,999 - INFO - [diffusion][Epoch 9189] diffusion training Loss: 0.04973836988210678
2024-11-05 04:00:12,002 - INFO - [diffusion][Epoch 9189] diffusion learning rate: 0.001
2024-11-05 04:00:12,004 - INFO - [diffusion][Epoch 9189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:12,006 - INFO - [diffusion][Epoch 9190] Epoch 9191/12000
2024-11-05 04:00:15,944 - INFO - [diffusion][Epoch 9190] diffusion training Loss: 0.0511693162843585
2024-11-05 04:00:15,946 - INFO - [diffusion][Epoch 9190] diffusion learning rate: 0.001
2024-11-05 04:00:15,948 - INFO - [diffusion][Epoch 9190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:15,949 - INFO - [diffusion][Epoch 9191] Epoch 9192/12000
2024-11-05 04:00:19,950 - INFO - [diffusion][Epoch 9191] diffusion training Loss: 0.049640790559351444
2024-11-05 04:00:19,952 - INFO - [diffusion][Epoch 9191] diffusion learning rate: 0.001
2024-11-05 04:00:19,964 - INFO - [diffusion][Epoch 9191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:19,965 - INFO - [diffusion][Epoch 9192] Epoch 9193/12000
2024-11-05 04:00:23,967 - INFO - [diffusion][Epoch 9192] diffusion training Loss: 0.048138500191271305
2024-11-05 04:00:23,970 - INFO - [diffusion][Epoch 9192] diffusion learning rate: 0.001
2024-11-05 04:00:23,972 - INFO - [diffusion][Epoch 9192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:23,973 - INFO - [diffusion][Epoch 9193] Epoch 9194/12000
2024-11-05 04:00:28,014 - INFO - [diffusion][Epoch 9193] diffusion training Loss: 0.051936445757746696
2024-11-05 04:00:28,019 - INFO - [diffusion][Epoch 9193] diffusion learning rate: 0.001
2024-11-05 04:00:28,021 - INFO - [diffusion][Epoch 9193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:28,022 - INFO - [diffusion][Epoch 9194] Epoch 9195/12000
2024-11-05 04:00:31,993 - INFO - [diffusion][Epoch 9194] diffusion training Loss: 0.050713387317955494
2024-11-05 04:00:31,996 - INFO - [diffusion][Epoch 9194] diffusion learning rate: 0.001
2024-11-05 04:00:31,999 - INFO - [diffusion][Epoch 9194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:32,000 - INFO - [diffusion][Epoch 9195] Epoch 9196/12000
2024-11-05 04:00:36,134 - INFO - [diffusion][Epoch 9195] diffusion training Loss: 0.047813112847507
2024-11-05 04:00:36,136 - INFO - [diffusion][Epoch 9195] diffusion learning rate: 0.001
2024-11-05 04:00:36,138 - INFO - [diffusion][Epoch 9195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:36,139 - INFO - [diffusion][Epoch 9196] Epoch 9197/12000
2024-11-05 04:00:40,244 - INFO - [diffusion][Epoch 9196] diffusion training Loss: 0.05265538766980171
2024-11-05 04:00:40,246 - INFO - [diffusion][Epoch 9196] diffusion learning rate: 0.001
2024-11-05 04:00:40,248 - INFO - [diffusion][Epoch 9196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:40,249 - INFO - [diffusion][Epoch 9197] Epoch 9198/12000
2024-11-05 04:00:44,179 - INFO - [diffusion][Epoch 9197] diffusion training Loss: 0.05051973182708025
2024-11-05 04:00:44,182 - INFO - [diffusion][Epoch 9197] diffusion learning rate: 0.001
2024-11-05 04:00:44,183 - INFO - [diffusion][Epoch 9197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:44,185 - INFO - [diffusion][Epoch 9198] Epoch 9199/12000
2024-11-05 04:00:48,895 - INFO - [diffusion][Epoch 9198] diffusion training Loss: 0.05240531638264656
2024-11-05 04:00:48,897 - INFO - [diffusion][Epoch 9198] diffusion learning rate: 0.001
2024-11-05 04:00:48,899 - INFO - [diffusion][Epoch 9198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:48,900 - INFO - [diffusion][Epoch 9199] Epoch 9200/12000
2024-11-05 04:00:52,871 - INFO - [diffusion][Epoch 9199] diffusion training Loss: 0.054050530306994915
2024-11-05 04:00:52,873 - INFO - [diffusion][Epoch 9199] diffusion learning rate: 0.001
2024-11-05 04:00:52,874 - INFO - [diffusion][Epoch 9199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:52,876 - INFO - [diffusion][Epoch 9200] Epoch 9201/12000
2024-11-05 04:00:56,974 - INFO - [diffusion][Epoch 9200] diffusion training Loss: 0.04919248912483454
2024-11-05 04:00:56,976 - INFO - [diffusion][Epoch 9200] diffusion learning rate: 0.001
2024-11-05 04:00:56,978 - INFO - [diffusion][Epoch 9200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:00:56,979 - INFO - [diffusion][Epoch 9201] Epoch 9202/12000
2024-11-05 04:01:00,959 - INFO - [diffusion][Epoch 9201] diffusion training Loss: 0.05068035330623388
2024-11-05 04:01:00,961 - INFO - [diffusion][Epoch 9201] diffusion learning rate: 0.001
2024-11-05 04:01:00,963 - INFO - [diffusion][Epoch 9201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:00,964 - INFO - [diffusion][Epoch 9202] Epoch 9203/12000
2024-11-05 04:01:04,879 - INFO - [diffusion][Epoch 9202] diffusion training Loss: 0.054793293587863445
2024-11-05 04:01:04,881 - INFO - [diffusion][Epoch 9202] diffusion learning rate: 0.001
2024-11-05 04:01:04,883 - INFO - [diffusion][Epoch 9202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:04,884 - INFO - [diffusion][Epoch 9203] Epoch 9204/12000
2024-11-05 04:01:08,792 - INFO - [diffusion][Epoch 9203] diffusion training Loss: 0.04712971765547991
2024-11-05 04:01:08,794 - INFO - [diffusion][Epoch 9203] diffusion learning rate: 0.001
2024-11-05 04:01:08,796 - INFO - [diffusion][Epoch 9203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:08,797 - INFO - [diffusion][Epoch 9204] Epoch 9205/12000
2024-11-05 04:01:12,953 - INFO - [diffusion][Epoch 9204] diffusion training Loss: 0.052380421198904514
2024-11-05 04:01:12,954 - INFO - [diffusion][Epoch 9204] diffusion learning rate: 0.001
2024-11-05 04:01:12,956 - INFO - [diffusion][Epoch 9204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:12,957 - INFO - [diffusion][Epoch 9205] Epoch 9206/12000
2024-11-05 04:01:17,128 - INFO - [diffusion][Epoch 9205] diffusion training Loss: 0.052845134399831295
2024-11-05 04:01:17,130 - INFO - [diffusion][Epoch 9205] diffusion learning rate: 0.001
2024-11-05 04:01:17,132 - INFO - [diffusion][Epoch 9205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:17,133 - INFO - [diffusion][Epoch 9206] Epoch 9207/12000
2024-11-05 04:01:21,246 - INFO - [diffusion][Epoch 9206] diffusion training Loss: 0.049520691856741905
2024-11-05 04:01:21,248 - INFO - [diffusion][Epoch 9206] diffusion learning rate: 0.001
2024-11-05 04:01:21,250 - INFO - [diffusion][Epoch 9206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:21,251 - INFO - [diffusion][Epoch 9207] Epoch 9208/12000
2024-11-05 04:01:25,327 - INFO - [diffusion][Epoch 9207] diffusion training Loss: 0.054008674807846546
2024-11-05 04:01:25,330 - INFO - [diffusion][Epoch 9207] diffusion learning rate: 0.001
2024-11-05 04:01:25,332 - INFO - [diffusion][Epoch 9207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:25,333 - INFO - [diffusion][Epoch 9208] Epoch 9209/12000
2024-11-05 04:01:29,411 - INFO - [diffusion][Epoch 9208] diffusion training Loss: 0.05755850579589605
2024-11-05 04:01:29,414 - INFO - [diffusion][Epoch 9208] diffusion learning rate: 0.001
2024-11-05 04:01:29,416 - INFO - [diffusion][Epoch 9208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:29,417 - INFO - [diffusion][Epoch 9209] Epoch 9210/12000
2024-11-05 04:01:33,585 - INFO - [diffusion][Epoch 9209] diffusion training Loss: 0.05760420951992273
2024-11-05 04:01:33,587 - INFO - [diffusion][Epoch 9209] diffusion learning rate: 0.001
2024-11-05 04:01:33,613 - INFO - [diffusion][Epoch 9209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:33,615 - INFO - [diffusion][Epoch 9210] Epoch 9211/12000
2024-11-05 04:01:37,731 - INFO - [diffusion][Epoch 9210] diffusion training Loss: 0.05197897180914879
2024-11-05 04:01:37,733 - INFO - [diffusion][Epoch 9210] diffusion learning rate: 0.001
2024-11-05 04:01:37,735 - INFO - [diffusion][Epoch 9210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:37,736 - INFO - [diffusion][Epoch 9211] Epoch 9212/12000
2024-11-05 04:01:41,780 - INFO - [diffusion][Epoch 9211] diffusion training Loss: 0.05039109382778406
2024-11-05 04:01:41,783 - INFO - [diffusion][Epoch 9211] diffusion learning rate: 0.001
2024-11-05 04:01:41,785 - INFO - [diffusion][Epoch 9211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:41,787 - INFO - [diffusion][Epoch 9212] Epoch 9213/12000
2024-11-05 04:01:45,774 - INFO - [diffusion][Epoch 9212] diffusion training Loss: 0.05158117599785328
2024-11-05 04:01:45,776 - INFO - [diffusion][Epoch 9212] diffusion learning rate: 0.001
2024-11-05 04:01:45,778 - INFO - [diffusion][Epoch 9212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:45,780 - INFO - [diffusion][Epoch 9213] Epoch 9214/12000
2024-11-05 04:01:49,907 - INFO - [diffusion][Epoch 9213] diffusion training Loss: 0.05442804470658302
2024-11-05 04:01:49,910 - INFO - [diffusion][Epoch 9213] diffusion learning rate: 0.001
2024-11-05 04:01:49,912 - INFO - [diffusion][Epoch 9213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:49,913 - INFO - [diffusion][Epoch 9214] Epoch 9215/12000
2024-11-05 04:01:54,041 - INFO - [diffusion][Epoch 9214] diffusion training Loss: 0.0482828700914979
2024-11-05 04:01:54,043 - INFO - [diffusion][Epoch 9214] diffusion learning rate: 0.001
2024-11-05 04:01:54,045 - INFO - [diffusion][Epoch 9214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:54,046 - INFO - [diffusion][Epoch 9215] Epoch 9216/12000
2024-11-05 04:01:58,055 - INFO - [diffusion][Epoch 9215] diffusion training Loss: 0.05523028410971165
2024-11-05 04:01:58,059 - INFO - [diffusion][Epoch 9215] diffusion learning rate: 0.001
2024-11-05 04:01:58,060 - INFO - [diffusion][Epoch 9215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:01:58,062 - INFO - [diffusion][Epoch 9216] Epoch 9217/12000
2024-11-05 04:02:02,141 - INFO - [diffusion][Epoch 9216] diffusion training Loss: 0.06094413064420223
2024-11-05 04:02:02,144 - INFO - [diffusion][Epoch 9216] diffusion learning rate: 0.001
2024-11-05 04:02:02,145 - INFO - [diffusion][Epoch 9216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:02,147 - INFO - [diffusion][Epoch 9217] Epoch 9218/12000
2024-11-05 04:02:06,224 - INFO - [diffusion][Epoch 9217] diffusion training Loss: 0.04874471761286259
2024-11-05 04:02:06,226 - INFO - [diffusion][Epoch 9217] diffusion learning rate: 0.001
2024-11-05 04:02:06,258 - INFO - [diffusion][Epoch 9217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:06,260 - INFO - [diffusion][Epoch 9218] Epoch 9219/12000
2024-11-05 04:02:10,729 - INFO - [diffusion][Epoch 9218] diffusion training Loss: 0.04827818367630243
2024-11-05 04:02:10,731 - INFO - [diffusion][Epoch 9218] diffusion learning rate: 0.001
2024-11-05 04:02:10,732 - INFO - [diffusion][Epoch 9218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:10,734 - INFO - [diffusion][Epoch 9219] Epoch 9220/12000
2024-11-05 04:02:14,905 - INFO - [diffusion][Epoch 9219] diffusion training Loss: 0.051769365556538105
2024-11-05 04:02:14,907 - INFO - [diffusion][Epoch 9219] diffusion learning rate: 0.001
2024-11-05 04:02:14,910 - INFO - [diffusion][Epoch 9219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:14,911 - INFO - [diffusion][Epoch 9220] Epoch 9221/12000
2024-11-05 04:02:18,975 - INFO - [diffusion][Epoch 9220] diffusion training Loss: 0.05539829470217228
2024-11-05 04:02:18,977 - INFO - [diffusion][Epoch 9220] diffusion learning rate: 0.001
2024-11-05 04:02:18,979 - INFO - [diffusion][Epoch 9220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:18,980 - INFO - [diffusion][Epoch 9221] Epoch 9222/12000
2024-11-05 04:02:22,962 - INFO - [diffusion][Epoch 9221] diffusion training Loss: 0.04874699003994465
2024-11-05 04:02:22,964 - INFO - [diffusion][Epoch 9221] diffusion learning rate: 0.001
2024-11-05 04:02:22,966 - INFO - [diffusion][Epoch 9221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:22,967 - INFO - [diffusion][Epoch 9222] Epoch 9223/12000
2024-11-05 04:02:27,072 - INFO - [diffusion][Epoch 9222] diffusion training Loss: 0.050477697513997555
2024-11-05 04:02:27,075 - INFO - [diffusion][Epoch 9222] diffusion learning rate: 0.001
2024-11-05 04:02:27,076 - INFO - [diffusion][Epoch 9222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:27,078 - INFO - [diffusion][Epoch 9223] Epoch 9224/12000
2024-11-05 04:02:31,215 - INFO - [diffusion][Epoch 9223] diffusion training Loss: 0.049826561473309994
2024-11-05 04:02:31,217 - INFO - [diffusion][Epoch 9223] diffusion learning rate: 0.001
2024-11-05 04:02:31,219 - INFO - [diffusion][Epoch 9223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:31,220 - INFO - [diffusion][Epoch 9224] Epoch 9225/12000
2024-11-05 04:02:35,293 - INFO - [diffusion][Epoch 9224] diffusion training Loss: 0.054784719832241535
2024-11-05 04:02:35,295 - INFO - [diffusion][Epoch 9224] diffusion learning rate: 0.001
2024-11-05 04:02:35,297 - INFO - [diffusion][Epoch 9224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:35,298 - INFO - [diffusion][Epoch 9225] Epoch 9226/12000
2024-11-05 04:02:39,385 - INFO - [diffusion][Epoch 9225] diffusion training Loss: 0.053957645781338215
2024-11-05 04:02:39,387 - INFO - [diffusion][Epoch 9225] diffusion learning rate: 0.001
2024-11-05 04:02:39,389 - INFO - [diffusion][Epoch 9225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:39,390 - INFO - [diffusion][Epoch 9226] Epoch 9227/12000
2024-11-05 04:02:43,521 - INFO - [diffusion][Epoch 9226] diffusion training Loss: 0.053756752982735634
2024-11-05 04:02:43,523 - INFO - [diffusion][Epoch 9226] diffusion learning rate: 0.001
2024-11-05 04:02:43,525 - INFO - [diffusion][Epoch 9226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:43,526 - INFO - [diffusion][Epoch 9227] Epoch 9228/12000
2024-11-05 04:02:47,690 - INFO - [diffusion][Epoch 9227] diffusion training Loss: 0.052435072138905525
2024-11-05 04:02:47,692 - INFO - [diffusion][Epoch 9227] diffusion learning rate: 0.001
2024-11-05 04:02:47,694 - INFO - [diffusion][Epoch 9227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:47,695 - INFO - [diffusion][Epoch 9228] Epoch 9229/12000
2024-11-05 04:02:51,855 - INFO - [diffusion][Epoch 9228] diffusion training Loss: 0.05229628551751375
2024-11-05 04:02:51,857 - INFO - [diffusion][Epoch 9228] diffusion learning rate: 0.001
2024-11-05 04:02:51,859 - INFO - [diffusion][Epoch 9228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:51,860 - INFO - [diffusion][Epoch 9229] Epoch 9230/12000
2024-11-05 04:02:55,823 - INFO - [diffusion][Epoch 9229] diffusion training Loss: 0.052167858928442
2024-11-05 04:02:55,825 - INFO - [diffusion][Epoch 9229] diffusion learning rate: 0.001
2024-11-05 04:02:55,826 - INFO - [diffusion][Epoch 9229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:55,828 - INFO - [diffusion][Epoch 9230] Epoch 9231/12000
2024-11-05 04:02:59,915 - INFO - [diffusion][Epoch 9230] diffusion training Loss: 0.0531446635723114
2024-11-05 04:02:59,917 - INFO - [diffusion][Epoch 9230] diffusion learning rate: 0.001
2024-11-05 04:02:59,920 - INFO - [diffusion][Epoch 9230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:02:59,921 - INFO - [diffusion][Epoch 9231] Epoch 9232/12000
2024-11-05 04:03:03,969 - INFO - [diffusion][Epoch 9231] diffusion training Loss: 0.045784251764416695
2024-11-05 04:03:03,970 - INFO - [diffusion][Epoch 9231] diffusion learning rate: 0.001
2024-11-05 04:03:03,972 - INFO - [diffusion][Epoch 9231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:03,973 - INFO - [diffusion][Epoch 9232] Epoch 9233/12000
2024-11-05 04:03:07,902 - INFO - [diffusion][Epoch 9232] diffusion training Loss: 0.0508252028375864
2024-11-05 04:03:07,904 - INFO - [diffusion][Epoch 9232] diffusion learning rate: 0.001
2024-11-05 04:03:07,906 - INFO - [diffusion][Epoch 9232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:07,907 - INFO - [diffusion][Epoch 9233] Epoch 9234/12000
2024-11-05 04:03:12,045 - INFO - [diffusion][Epoch 9233] diffusion training Loss: 0.04726705141365528
2024-11-05 04:03:12,047 - INFO - [diffusion][Epoch 9233] diffusion learning rate: 0.001
2024-11-05 04:03:12,048 - INFO - [diffusion][Epoch 9233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:12,050 - INFO - [diffusion][Epoch 9234] Epoch 9235/12000
2024-11-05 04:03:16,343 - INFO - [diffusion][Epoch 9234] diffusion training Loss: 0.04974138643592596
2024-11-05 04:03:16,345 - INFO - [diffusion][Epoch 9234] diffusion learning rate: 0.001
2024-11-05 04:03:16,347 - INFO - [diffusion][Epoch 9234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:16,349 - INFO - [diffusion][Epoch 9235] Epoch 9236/12000
2024-11-05 04:03:20,388 - INFO - [diffusion][Epoch 9235] diffusion training Loss: 0.054532142356038094
2024-11-05 04:03:20,390 - INFO - [diffusion][Epoch 9235] diffusion learning rate: 0.001
2024-11-05 04:03:20,392 - INFO - [diffusion][Epoch 9235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:20,393 - INFO - [diffusion][Epoch 9236] Epoch 9237/12000
2024-11-05 04:03:24,561 - INFO - [diffusion][Epoch 9236] diffusion training Loss: 0.053500449284911156
2024-11-05 04:03:24,563 - INFO - [diffusion][Epoch 9236] diffusion learning rate: 0.001
2024-11-05 04:03:24,565 - INFO - [diffusion][Epoch 9236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:24,566 - INFO - [diffusion][Epoch 9237] Epoch 9238/12000
2024-11-05 04:03:28,463 - INFO - [diffusion][Epoch 9237] diffusion training Loss: 0.04972292948514223
2024-11-05 04:03:28,465 - INFO - [diffusion][Epoch 9237] diffusion learning rate: 0.001
2024-11-05 04:03:28,466 - INFO - [diffusion][Epoch 9237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:28,467 - INFO - [diffusion][Epoch 9238] Epoch 9239/12000
2024-11-05 04:03:32,694 - INFO - [diffusion][Epoch 9238] diffusion training Loss: 0.05441428814083338
2024-11-05 04:03:32,697 - INFO - [diffusion][Epoch 9238] diffusion learning rate: 0.001
2024-11-05 04:03:32,699 - INFO - [diffusion][Epoch 9238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:32,700 - INFO - [diffusion][Epoch 9239] Epoch 9240/12000
2024-11-05 04:03:36,835 - INFO - [diffusion][Epoch 9239] diffusion training Loss: 0.05045581888407469
2024-11-05 04:03:36,837 - INFO - [diffusion][Epoch 9239] diffusion learning rate: 0.001
2024-11-05 04:03:36,839 - INFO - [diffusion][Epoch 9239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:36,840 - INFO - [diffusion][Epoch 9240] Epoch 9241/12000
2024-11-05 04:03:41,042 - INFO - [diffusion][Epoch 9240] diffusion training Loss: 0.04721687361598015
2024-11-05 04:03:41,044 - INFO - [diffusion][Epoch 9240] diffusion learning rate: 0.001
2024-11-05 04:03:41,046 - INFO - [diffusion][Epoch 9240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:41,047 - INFO - [diffusion][Epoch 9241] Epoch 9242/12000
2024-11-05 04:03:45,303 - INFO - [diffusion][Epoch 9241] diffusion training Loss: 0.0458010034635663
2024-11-05 04:03:45,305 - INFO - [diffusion][Epoch 9241] diffusion learning rate: 0.001
2024-11-05 04:03:45,307 - INFO - [diffusion][Epoch 9241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:45,308 - INFO - [diffusion][Epoch 9242] Epoch 9243/12000
2024-11-05 04:03:49,396 - INFO - [diffusion][Epoch 9242] diffusion training Loss: 0.05523701570928097
2024-11-05 04:03:49,399 - INFO - [diffusion][Epoch 9242] diffusion learning rate: 0.001
2024-11-05 04:03:49,401 - INFO - [diffusion][Epoch 9242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:49,402 - INFO - [diffusion][Epoch 9243] Epoch 9244/12000
2024-11-05 04:03:53,613 - INFO - [diffusion][Epoch 9243] diffusion training Loss: 0.050397252663969994
2024-11-05 04:03:53,616 - INFO - [diffusion][Epoch 9243] diffusion learning rate: 0.001
2024-11-05 04:03:53,619 - INFO - [diffusion][Epoch 9243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:53,620 - INFO - [diffusion][Epoch 9244] Epoch 9245/12000
2024-11-05 04:03:57,776 - INFO - [diffusion][Epoch 9244] diffusion training Loss: 0.05138430651277304
2024-11-05 04:03:57,779 - INFO - [diffusion][Epoch 9244] diffusion learning rate: 0.001
2024-11-05 04:03:57,781 - INFO - [diffusion][Epoch 9244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:03:57,785 - INFO - [diffusion][Epoch 9245] Epoch 9246/12000
2024-11-05 04:04:01,888 - INFO - [diffusion][Epoch 9245] diffusion training Loss: 0.04666868317872286
2024-11-05 04:04:01,891 - INFO - [diffusion][Epoch 9245] diffusion learning rate: 0.001
2024-11-05 04:04:01,893 - INFO - [diffusion][Epoch 9245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:01,894 - INFO - [diffusion][Epoch 9246] Epoch 9247/12000
2024-11-05 04:04:05,953 - INFO - [diffusion][Epoch 9246] diffusion training Loss: 0.04603266529738903
2024-11-05 04:04:05,955 - INFO - [diffusion][Epoch 9246] diffusion learning rate: 0.001
2024-11-05 04:04:05,956 - INFO - [diffusion][Epoch 9246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:05,958 - INFO - [diffusion][Epoch 9247] Epoch 9248/12000
2024-11-05 04:04:09,970 - INFO - [diffusion][Epoch 9247] diffusion training Loss: 0.05092003662139177
2024-11-05 04:04:09,972 - INFO - [diffusion][Epoch 9247] diffusion learning rate: 0.001
2024-11-05 04:04:09,974 - INFO - [diffusion][Epoch 9247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:09,976 - INFO - [diffusion][Epoch 9248] Epoch 9249/12000
2024-11-05 04:04:14,012 - INFO - [diffusion][Epoch 9248] diffusion training Loss: 0.04688825085759163
2024-11-05 04:04:14,014 - INFO - [diffusion][Epoch 9248] diffusion learning rate: 0.001
2024-11-05 04:04:14,016 - INFO - [diffusion][Epoch 9248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:14,017 - INFO - [diffusion][Epoch 9249] Epoch 9250/12000
2024-11-05 04:04:18,028 - INFO - [diffusion][Epoch 9249] diffusion training Loss: 0.047734749503433704
2024-11-05 04:04:18,030 - INFO - [diffusion][Epoch 9249] diffusion learning rate: 0.001
2024-11-05 04:04:18,032 - INFO - [diffusion][Epoch 9249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:18,033 - INFO - [diffusion][Epoch 9250] Epoch 9251/12000
2024-11-05 04:04:22,084 - INFO - [diffusion][Epoch 9250] diffusion training Loss: 0.05382375232875347
2024-11-05 04:04:22,086 - INFO - [diffusion][Epoch 9250] diffusion learning rate: 0.001
2024-11-05 04:04:22,088 - INFO - [diffusion][Epoch 9250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:22,089 - INFO - [diffusion][Epoch 9251] Epoch 9252/12000
2024-11-05 04:04:26,231 - INFO - [diffusion][Epoch 9251] diffusion training Loss: 0.04474032763391733
2024-11-05 04:04:26,233 - INFO - [diffusion][Epoch 9251] diffusion learning rate: 0.001
2024-11-05 04:04:26,235 - INFO - [diffusion][Epoch 9251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:26,236 - INFO - [diffusion][Epoch 9252] Epoch 9253/12000
2024-11-05 04:04:30,324 - INFO - [diffusion][Epoch 9252] diffusion training Loss: 0.05139535199850798
2024-11-05 04:04:30,326 - INFO - [diffusion][Epoch 9252] diffusion learning rate: 0.001
2024-11-05 04:04:30,328 - INFO - [diffusion][Epoch 9252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:30,329 - INFO - [diffusion][Epoch 9253] Epoch 9254/12000
2024-11-05 04:04:34,344 - INFO - [diffusion][Epoch 9253] diffusion training Loss: 0.053557151928544044
2024-11-05 04:04:34,347 - INFO - [diffusion][Epoch 9253] diffusion learning rate: 0.001
2024-11-05 04:04:34,349 - INFO - [diffusion][Epoch 9253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:34,350 - INFO - [diffusion][Epoch 9254] Epoch 9255/12000
2024-11-05 04:04:38,560 - INFO - [diffusion][Epoch 9254] diffusion training Loss: 0.052220191806554794
2024-11-05 04:04:38,562 - INFO - [diffusion][Epoch 9254] diffusion learning rate: 0.001
2024-11-05 04:04:38,564 - INFO - [diffusion][Epoch 9254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:38,565 - INFO - [diffusion][Epoch 9255] Epoch 9256/12000
2024-11-05 04:04:42,628 - INFO - [diffusion][Epoch 9255] diffusion training Loss: 0.04741976223886013
2024-11-05 04:04:42,630 - INFO - [diffusion][Epoch 9255] diffusion learning rate: 0.001
2024-11-05 04:04:42,632 - INFO - [diffusion][Epoch 9255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:42,633 - INFO - [diffusion][Epoch 9256] Epoch 9257/12000
2024-11-05 04:04:46,585 - INFO - [diffusion][Epoch 9256] diffusion training Loss: 0.0471013318747282
2024-11-05 04:04:46,588 - INFO - [diffusion][Epoch 9256] diffusion learning rate: 0.001
2024-11-05 04:04:46,590 - INFO - [diffusion][Epoch 9256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:46,591 - INFO - [diffusion][Epoch 9257] Epoch 9258/12000
2024-11-05 04:04:50,572 - INFO - [diffusion][Epoch 9257] diffusion training Loss: 0.055847957730293274
2024-11-05 04:04:50,575 - INFO - [diffusion][Epoch 9257] diffusion learning rate: 0.001
2024-11-05 04:04:50,577 - INFO - [diffusion][Epoch 9257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:50,579 - INFO - [diffusion][Epoch 9258] Epoch 9259/12000
2024-11-05 04:04:54,571 - INFO - [diffusion][Epoch 9258] diffusion training Loss: 0.05319290515035391
2024-11-05 04:04:54,573 - INFO - [diffusion][Epoch 9258] diffusion learning rate: 0.001
2024-11-05 04:04:54,574 - INFO - [diffusion][Epoch 9258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:54,576 - INFO - [diffusion][Epoch 9259] Epoch 9260/12000
2024-11-05 04:04:58,899 - INFO - [diffusion][Epoch 9259] diffusion training Loss: 0.04755238350480795
2024-11-05 04:04:58,901 - INFO - [diffusion][Epoch 9259] diffusion learning rate: 0.001
2024-11-05 04:04:58,902 - INFO - [diffusion][Epoch 9259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:04:58,904 - INFO - [diffusion][Epoch 9260] Epoch 9261/12000
2024-11-05 04:05:03,028 - INFO - [diffusion][Epoch 9260] diffusion training Loss: 0.04838281124830246
2024-11-05 04:05:03,030 - INFO - [diffusion][Epoch 9260] diffusion learning rate: 0.001
2024-11-05 04:05:03,032 - INFO - [diffusion][Epoch 9260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:03,033 - INFO - [diffusion][Epoch 9261] Epoch 9262/12000
2024-11-05 04:05:07,086 - INFO - [diffusion][Epoch 9261] diffusion training Loss: 0.05656895227730274
2024-11-05 04:05:07,088 - INFO - [diffusion][Epoch 9261] diffusion learning rate: 0.001
2024-11-05 04:05:07,090 - INFO - [diffusion][Epoch 9261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:07,091 - INFO - [diffusion][Epoch 9262] Epoch 9263/12000
2024-11-05 04:05:11,092 - INFO - [diffusion][Epoch 9262] diffusion training Loss: 0.04657171666622162
2024-11-05 04:05:11,094 - INFO - [diffusion][Epoch 9262] diffusion learning rate: 0.001
2024-11-05 04:05:11,096 - INFO - [diffusion][Epoch 9262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:11,097 - INFO - [diffusion][Epoch 9263] Epoch 9264/12000
2024-11-05 04:05:15,104 - INFO - [diffusion][Epoch 9263] diffusion training Loss: 0.050164634361863136
2024-11-05 04:05:15,107 - INFO - [diffusion][Epoch 9263] diffusion learning rate: 0.001
2024-11-05 04:05:15,109 - INFO - [diffusion][Epoch 9263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:15,110 - INFO - [diffusion][Epoch 9264] Epoch 9265/12000
2024-11-05 04:05:19,169 - INFO - [diffusion][Epoch 9264] diffusion training Loss: 0.050886345095932484
2024-11-05 04:05:19,172 - INFO - [diffusion][Epoch 9264] diffusion learning rate: 0.001
2024-11-05 04:05:19,174 - INFO - [diffusion][Epoch 9264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:19,175 - INFO - [diffusion][Epoch 9265] Epoch 9266/12000
2024-11-05 04:05:23,214 - INFO - [diffusion][Epoch 9265] diffusion training Loss: 0.0497996686026454
2024-11-05 04:05:23,216 - INFO - [diffusion][Epoch 9265] diffusion learning rate: 0.001
2024-11-05 04:05:23,218 - INFO - [diffusion][Epoch 9265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:23,219 - INFO - [diffusion][Epoch 9266] Epoch 9267/12000
2024-11-05 04:05:27,302 - INFO - [diffusion][Epoch 9266] diffusion training Loss: 0.04875083360821009
2024-11-05 04:05:27,304 - INFO - [diffusion][Epoch 9266] diffusion learning rate: 0.001
2024-11-05 04:05:27,354 - INFO - [diffusion][Epoch 9266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:27,355 - INFO - [diffusion][Epoch 9267] Epoch 9268/12000
2024-11-05 04:05:31,458 - INFO - [diffusion][Epoch 9267] diffusion training Loss: 0.04844916611909866
2024-11-05 04:05:31,461 - INFO - [diffusion][Epoch 9267] diffusion learning rate: 0.001
2024-11-05 04:05:31,467 - INFO - [diffusion][Epoch 9267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:31,469 - INFO - [diffusion][Epoch 9268] Epoch 9269/12000
2024-11-05 04:05:35,787 - INFO - [diffusion][Epoch 9268] diffusion training Loss: 0.04618136025965214
2024-11-05 04:05:35,790 - INFO - [diffusion][Epoch 9268] diffusion learning rate: 0.001
2024-11-05 04:05:35,793 - INFO - [diffusion][Epoch 9268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:35,794 - INFO - [diffusion][Epoch 9269] Epoch 9270/12000
2024-11-05 04:05:39,893 - INFO - [diffusion][Epoch 9269] diffusion training Loss: 0.04976371396332979
2024-11-05 04:05:39,895 - INFO - [diffusion][Epoch 9269] diffusion learning rate: 0.001
2024-11-05 04:05:39,897 - INFO - [diffusion][Epoch 9269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:39,898 - INFO - [diffusion][Epoch 9270] Epoch 9271/12000
2024-11-05 04:05:44,137 - INFO - [diffusion][Epoch 9270] diffusion training Loss: 0.05155322700738907
2024-11-05 04:05:44,139 - INFO - [diffusion][Epoch 9270] diffusion learning rate: 0.001
2024-11-05 04:05:44,171 - INFO - [diffusion][Epoch 9270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:44,172 - INFO - [diffusion][Epoch 9271] Epoch 9272/12000
2024-11-05 04:05:48,372 - INFO - [diffusion][Epoch 9271] diffusion training Loss: 0.05170809477567673
2024-11-05 04:05:48,374 - INFO - [diffusion][Epoch 9271] diffusion learning rate: 0.001
2024-11-05 04:05:48,377 - INFO - [diffusion][Epoch 9271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:48,379 - INFO - [diffusion][Epoch 9272] Epoch 9273/12000
2024-11-05 04:05:52,624 - INFO - [diffusion][Epoch 9272] diffusion training Loss: 0.05940677411854267
2024-11-05 04:05:52,626 - INFO - [diffusion][Epoch 9272] diffusion learning rate: 0.001
2024-11-05 04:05:52,628 - INFO - [diffusion][Epoch 9272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:52,630 - INFO - [diffusion][Epoch 9273] Epoch 9274/12000
2024-11-05 04:05:56,736 - INFO - [diffusion][Epoch 9273] diffusion training Loss: 0.052372416481375694
2024-11-05 04:05:56,738 - INFO - [diffusion][Epoch 9273] diffusion learning rate: 0.001
2024-11-05 04:05:56,740 - INFO - [diffusion][Epoch 9273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:05:56,741 - INFO - [diffusion][Epoch 9274] Epoch 9275/12000
2024-11-05 04:06:00,863 - INFO - [diffusion][Epoch 9274] diffusion training Loss: 0.05278848111629486
2024-11-05 04:06:00,865 - INFO - [diffusion][Epoch 9274] diffusion learning rate: 0.001
2024-11-05 04:06:00,867 - INFO - [diffusion][Epoch 9274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:00,868 - INFO - [diffusion][Epoch 9275] Epoch 9276/12000
2024-11-05 04:06:04,855 - INFO - [diffusion][Epoch 9275] diffusion training Loss: 0.04798795375972986
2024-11-05 04:06:04,857 - INFO - [diffusion][Epoch 9275] diffusion learning rate: 0.001
2024-11-05 04:06:04,859 - INFO - [diffusion][Epoch 9275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:04,860 - INFO - [diffusion][Epoch 9276] Epoch 9277/12000
2024-11-05 04:06:08,857 - INFO - [diffusion][Epoch 9276] diffusion training Loss: 0.05251224432140589
2024-11-05 04:06:08,859 - INFO - [diffusion][Epoch 9276] diffusion learning rate: 0.001
2024-11-05 04:06:08,861 - INFO - [diffusion][Epoch 9276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:08,863 - INFO - [diffusion][Epoch 9277] Epoch 9278/12000
2024-11-05 04:06:12,892 - INFO - [diffusion][Epoch 9277] diffusion training Loss: 0.052810496650636196
2024-11-05 04:06:12,894 - INFO - [diffusion][Epoch 9277] diffusion learning rate: 0.001
2024-11-05 04:06:12,896 - INFO - [diffusion][Epoch 9277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:12,897 - INFO - [diffusion][Epoch 9278] Epoch 9279/12000
2024-11-05 04:06:16,855 - INFO - [diffusion][Epoch 9278] diffusion training Loss: 0.05263402499258518
2024-11-05 04:06:16,857 - INFO - [diffusion][Epoch 9278] diffusion learning rate: 0.001
2024-11-05 04:06:16,859 - INFO - [diffusion][Epoch 9278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:16,860 - INFO - [diffusion][Epoch 9279] Epoch 9280/12000
2024-11-05 04:06:21,029 - INFO - [diffusion][Epoch 9279] diffusion training Loss: 0.05050819460302591
2024-11-05 04:06:21,031 - INFO - [diffusion][Epoch 9279] diffusion learning rate: 0.001
2024-11-05 04:06:21,033 - INFO - [diffusion][Epoch 9279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:21,034 - INFO - [diffusion][Epoch 9280] Epoch 9281/12000
2024-11-05 04:06:25,218 - INFO - [diffusion][Epoch 9280] diffusion training Loss: 0.05098042171448469
2024-11-05 04:06:25,220 - INFO - [diffusion][Epoch 9280] diffusion learning rate: 0.001
2024-11-05 04:06:25,222 - INFO - [diffusion][Epoch 9280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:25,224 - INFO - [diffusion][Epoch 9281] Epoch 9282/12000
2024-11-05 04:06:29,418 - INFO - [diffusion][Epoch 9281] diffusion training Loss: 0.05020548962056637
2024-11-05 04:06:29,420 - INFO - [diffusion][Epoch 9281] diffusion learning rate: 0.001
2024-11-05 04:06:29,423 - INFO - [diffusion][Epoch 9281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:29,424 - INFO - [diffusion][Epoch 9282] Epoch 9283/12000
2024-11-05 04:06:33,535 - INFO - [diffusion][Epoch 9282] diffusion training Loss: 0.05406724475324154
2024-11-05 04:06:33,537 - INFO - [diffusion][Epoch 9282] diffusion learning rate: 0.001
2024-11-05 04:06:33,539 - INFO - [diffusion][Epoch 9282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:33,540 - INFO - [diffusion][Epoch 9283] Epoch 9284/12000
2024-11-05 04:06:37,493 - INFO - [diffusion][Epoch 9283] diffusion training Loss: 0.049787809140980244
2024-11-05 04:06:37,496 - INFO - [diffusion][Epoch 9283] diffusion learning rate: 0.001
2024-11-05 04:06:37,498 - INFO - [diffusion][Epoch 9283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:37,499 - INFO - [diffusion][Epoch 9284] Epoch 9285/12000
2024-11-05 04:06:41,564 - INFO - [diffusion][Epoch 9284] diffusion training Loss: 0.05758738052099943
2024-11-05 04:06:41,567 - INFO - [diffusion][Epoch 9284] diffusion learning rate: 0.001
2024-11-05 04:06:41,570 - INFO - [diffusion][Epoch 9284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:41,571 - INFO - [diffusion][Epoch 9285] Epoch 9286/12000
2024-11-05 04:06:45,700 - INFO - [diffusion][Epoch 9285] diffusion training Loss: 0.050551433116197586
2024-11-05 04:06:45,702 - INFO - [diffusion][Epoch 9285] diffusion learning rate: 0.001
2024-11-05 04:06:45,704 - INFO - [diffusion][Epoch 9285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:45,705 - INFO - [diffusion][Epoch 9286] Epoch 9287/12000
2024-11-05 04:06:49,771 - INFO - [diffusion][Epoch 9286] diffusion training Loss: 0.047356520779430866
2024-11-05 04:06:49,773 - INFO - [diffusion][Epoch 9286] diffusion learning rate: 0.001
2024-11-05 04:06:49,775 - INFO - [diffusion][Epoch 9286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:49,776 - INFO - [diffusion][Epoch 9287] Epoch 9288/12000
2024-11-05 04:06:53,808 - INFO - [diffusion][Epoch 9287] diffusion training Loss: 0.050349161028862
2024-11-05 04:06:53,810 - INFO - [diffusion][Epoch 9287] diffusion learning rate: 0.001
2024-11-05 04:06:53,811 - INFO - [diffusion][Epoch 9287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:53,813 - INFO - [diffusion][Epoch 9288] Epoch 9289/12000
2024-11-05 04:06:57,898 - INFO - [diffusion][Epoch 9288] diffusion training Loss: 0.04868212528526783
2024-11-05 04:06:57,902 - INFO - [diffusion][Epoch 9288] diffusion learning rate: 0.001
2024-11-05 04:06:57,904 - INFO - [diffusion][Epoch 9288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:06:57,905 - INFO - [diffusion][Epoch 9289] Epoch 9290/12000
2024-11-05 04:07:02,085 - INFO - [diffusion][Epoch 9289] diffusion training Loss: 0.04485739395022392
2024-11-05 04:07:02,087 - INFO - [diffusion][Epoch 9289] diffusion learning rate: 0.001
2024-11-05 04:07:02,089 - INFO - [diffusion][Epoch 9289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:02,091 - INFO - [diffusion][Epoch 9290] Epoch 9291/12000
2024-11-05 04:07:06,077 - INFO - [diffusion][Epoch 9290] diffusion training Loss: 0.0542061161249876
2024-11-05 04:07:06,079 - INFO - [diffusion][Epoch 9290] diffusion learning rate: 0.001
2024-11-05 04:07:06,081 - INFO - [diffusion][Epoch 9290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:06,083 - INFO - [diffusion][Epoch 9291] Epoch 9292/12000
2024-11-05 04:07:10,208 - INFO - [diffusion][Epoch 9291] diffusion training Loss: 0.051802391186356544
2024-11-05 04:07:10,211 - INFO - [diffusion][Epoch 9291] diffusion learning rate: 0.001
2024-11-05 04:07:10,213 - INFO - [diffusion][Epoch 9291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:10,218 - INFO - [diffusion][Epoch 9292] Epoch 9293/12000
2024-11-05 04:07:14,398 - INFO - [diffusion][Epoch 9292] diffusion training Loss: 0.046938820742070675
2024-11-05 04:07:14,400 - INFO - [diffusion][Epoch 9292] diffusion learning rate: 0.001
2024-11-05 04:07:14,403 - INFO - [diffusion][Epoch 9292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:14,404 - INFO - [diffusion][Epoch 9293] Epoch 9294/12000
2024-11-05 04:07:18,465 - INFO - [diffusion][Epoch 9293] diffusion training Loss: 0.0525421192869544
2024-11-05 04:07:18,467 - INFO - [diffusion][Epoch 9293] diffusion learning rate: 0.001
2024-11-05 04:07:18,469 - INFO - [diffusion][Epoch 9293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:18,471 - INFO - [diffusion][Epoch 9294] Epoch 9295/12000
2024-11-05 04:07:22,528 - INFO - [diffusion][Epoch 9294] diffusion training Loss: 0.05800909083336592
2024-11-05 04:07:22,530 - INFO - [diffusion][Epoch 9294] diffusion learning rate: 0.001
2024-11-05 04:07:22,532 - INFO - [diffusion][Epoch 9294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:22,533 - INFO - [diffusion][Epoch 9295] Epoch 9296/12000
2024-11-05 04:07:26,650 - INFO - [diffusion][Epoch 9295] diffusion training Loss: 0.05095014534890652
2024-11-05 04:07:26,652 - INFO - [diffusion][Epoch 9295] diffusion learning rate: 0.001
2024-11-05 04:07:26,654 - INFO - [diffusion][Epoch 9295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:26,655 - INFO - [diffusion][Epoch 9296] Epoch 9297/12000
2024-11-05 04:07:30,741 - INFO - [diffusion][Epoch 9296] diffusion training Loss: 0.05174036044627428
2024-11-05 04:07:30,743 - INFO - [diffusion][Epoch 9296] diffusion learning rate: 0.001
2024-11-05 04:07:30,745 - INFO - [diffusion][Epoch 9296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:30,747 - INFO - [diffusion][Epoch 9297] Epoch 9298/12000
2024-11-05 04:07:34,969 - INFO - [diffusion][Epoch 9297] diffusion training Loss: 0.054195803590118885
2024-11-05 04:07:34,971 - INFO - [diffusion][Epoch 9297] diffusion learning rate: 0.001
2024-11-05 04:07:34,973 - INFO - [diffusion][Epoch 9297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:34,974 - INFO - [diffusion][Epoch 9298] Epoch 9299/12000
2024-11-05 04:07:39,174 - INFO - [diffusion][Epoch 9298] diffusion training Loss: 0.0514184245839715
2024-11-05 04:07:39,177 - INFO - [diffusion][Epoch 9298] diffusion learning rate: 0.001
2024-11-05 04:07:39,179 - INFO - [diffusion][Epoch 9298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:39,181 - INFO - [diffusion][Epoch 9299] Epoch 9300/12000
2024-11-05 04:07:43,419 - INFO - [diffusion][Epoch 9299] diffusion training Loss: 0.053713603876531124
2024-11-05 04:07:43,421 - INFO - [diffusion][Epoch 9299] diffusion learning rate: 0.001
2024-11-05 04:07:43,423 - INFO - [diffusion][Epoch 9299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:43,424 - INFO - [diffusion][Epoch 9300] Epoch 9301/12000
2024-11-05 04:07:47,490 - INFO - [diffusion][Epoch 9300] diffusion training Loss: 0.05630964133888483
2024-11-05 04:07:47,492 - INFO - [diffusion][Epoch 9300] diffusion learning rate: 0.001
2024-11-05 04:07:47,493 - INFO - [diffusion][Epoch 9300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:47,495 - INFO - [diffusion][Epoch 9301] Epoch 9302/12000
2024-11-05 04:07:51,540 - INFO - [diffusion][Epoch 9301] diffusion training Loss: 0.05317582190036774
2024-11-05 04:07:51,559 - INFO - [diffusion][Epoch 9301] diffusion learning rate: 0.001
2024-11-05 04:07:51,561 - INFO - [diffusion][Epoch 9301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:51,562 - INFO - [diffusion][Epoch 9302] Epoch 9303/12000
2024-11-05 04:07:55,724 - INFO - [diffusion][Epoch 9302] diffusion training Loss: 0.05327555816620588
2024-11-05 04:07:55,726 - INFO - [diffusion][Epoch 9302] diffusion learning rate: 0.001
2024-11-05 04:07:55,727 - INFO - [diffusion][Epoch 9302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:55,729 - INFO - [diffusion][Epoch 9303] Epoch 9304/12000
2024-11-05 04:07:59,829 - INFO - [diffusion][Epoch 9303] diffusion training Loss: 0.05149079579859972
2024-11-05 04:07:59,832 - INFO - [diffusion][Epoch 9303] diffusion learning rate: 0.001
2024-11-05 04:07:59,834 - INFO - [diffusion][Epoch 9303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:07:59,835 - INFO - [diffusion][Epoch 9304] Epoch 9305/12000
2024-11-05 04:08:03,784 - INFO - [diffusion][Epoch 9304] diffusion training Loss: 0.048300052992999554
2024-11-05 04:08:03,786 - INFO - [diffusion][Epoch 9304] diffusion learning rate: 0.001
2024-11-05 04:08:03,788 - INFO - [diffusion][Epoch 9304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:03,791 - INFO - [diffusion][Epoch 9305] Epoch 9306/12000
2024-11-05 04:08:07,796 - INFO - [diffusion][Epoch 9305] diffusion training Loss: 0.050708298571407795
2024-11-05 04:08:07,798 - INFO - [diffusion][Epoch 9305] diffusion learning rate: 0.001
2024-11-05 04:08:07,800 - INFO - [diffusion][Epoch 9305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:07,801 - INFO - [diffusion][Epoch 9306] Epoch 9307/12000
2024-11-05 04:08:11,814 - INFO - [diffusion][Epoch 9306] diffusion training Loss: 0.04880516231060028
2024-11-05 04:08:11,816 - INFO - [diffusion][Epoch 9306] diffusion learning rate: 0.001
2024-11-05 04:08:11,818 - INFO - [diffusion][Epoch 9306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:11,820 - INFO - [diffusion][Epoch 9307] Epoch 9308/12000
2024-11-05 04:08:15,858 - INFO - [diffusion][Epoch 9307] diffusion training Loss: 0.048843635246157646
2024-11-05 04:08:15,859 - INFO - [diffusion][Epoch 9307] diffusion learning rate: 0.001
2024-11-05 04:08:15,861 - INFO - [diffusion][Epoch 9307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:15,863 - INFO - [diffusion][Epoch 9308] Epoch 9309/12000
2024-11-05 04:08:19,938 - INFO - [diffusion][Epoch 9308] diffusion training Loss: 0.051364107988774776
2024-11-05 04:08:19,941 - INFO - [diffusion][Epoch 9308] diffusion learning rate: 0.001
2024-11-05 04:08:19,943 - INFO - [diffusion][Epoch 9308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:19,944 - INFO - [diffusion][Epoch 9309] Epoch 9310/12000
2024-11-05 04:08:23,922 - INFO - [diffusion][Epoch 9309] diffusion training Loss: 0.051523152738809586
2024-11-05 04:08:23,924 - INFO - [diffusion][Epoch 9309] diffusion learning rate: 0.001
2024-11-05 04:08:23,926 - INFO - [diffusion][Epoch 9309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:23,928 - INFO - [diffusion][Epoch 9310] Epoch 9311/12000
2024-11-05 04:08:28,125 - INFO - [diffusion][Epoch 9310] diffusion training Loss: 0.04598262906074524
2024-11-05 04:08:28,127 - INFO - [diffusion][Epoch 9310] diffusion learning rate: 0.001
2024-11-05 04:08:28,129 - INFO - [diffusion][Epoch 9310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:28,130 - INFO - [diffusion][Epoch 9311] Epoch 9312/12000
2024-11-05 04:08:32,167 - INFO - [diffusion][Epoch 9311] diffusion training Loss: 0.04967332258820534
2024-11-05 04:08:32,168 - INFO - [diffusion][Epoch 9311] diffusion learning rate: 0.001
2024-11-05 04:08:32,170 - INFO - [diffusion][Epoch 9311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:32,172 - INFO - [diffusion][Epoch 9312] Epoch 9313/12000
2024-11-05 04:08:36,300 - INFO - [diffusion][Epoch 9312] diffusion training Loss: 0.05027170851826668
2024-11-05 04:08:36,302 - INFO - [diffusion][Epoch 9312] diffusion learning rate: 0.001
2024-11-05 04:08:36,304 - INFO - [diffusion][Epoch 9312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:36,306 - INFO - [diffusion][Epoch 9313] Epoch 9314/12000
2024-11-05 04:08:40,507 - INFO - [diffusion][Epoch 9313] diffusion training Loss: 0.04387394059449434
2024-11-05 04:08:40,510 - INFO - [diffusion][Epoch 9313] diffusion learning rate: 0.001
2024-11-05 04:08:40,512 - INFO - [diffusion][Epoch 9313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:40,513 - INFO - [diffusion][Epoch 9314] Epoch 9315/12000
2024-11-05 04:08:44,543 - INFO - [diffusion][Epoch 9314] diffusion training Loss: 0.04775806702673435
2024-11-05 04:08:44,545 - INFO - [diffusion][Epoch 9314] diffusion learning rate: 0.001
2024-11-05 04:08:44,548 - INFO - [diffusion][Epoch 9314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:44,549 - INFO - [diffusion][Epoch 9315] Epoch 9316/12000
2024-11-05 04:08:48,596 - INFO - [diffusion][Epoch 9315] diffusion training Loss: 0.05539677385240793
2024-11-05 04:08:48,598 - INFO - [diffusion][Epoch 9315] diffusion learning rate: 0.001
2024-11-05 04:08:48,602 - INFO - [diffusion][Epoch 9315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:48,603 - INFO - [diffusion][Epoch 9316] Epoch 9317/12000
2024-11-05 04:08:52,729 - INFO - [diffusion][Epoch 9316] diffusion training Loss: 0.051496402360498905
2024-11-05 04:08:52,731 - INFO - [diffusion][Epoch 9316] diffusion learning rate: 0.001
2024-11-05 04:08:52,733 - INFO - [diffusion][Epoch 9316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:52,734 - INFO - [diffusion][Epoch 9317] Epoch 9318/12000
2024-11-05 04:08:56,782 - INFO - [diffusion][Epoch 9317] diffusion training Loss: 0.052095756866037846
2024-11-05 04:08:56,784 - INFO - [diffusion][Epoch 9317] diffusion learning rate: 0.001
2024-11-05 04:08:56,786 - INFO - [diffusion][Epoch 9317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:08:56,788 - INFO - [diffusion][Epoch 9318] Epoch 9319/12000
2024-11-05 04:09:00,689 - INFO - [diffusion][Epoch 9318] diffusion training Loss: 0.0536770299077034
2024-11-05 04:09:00,691 - INFO - [diffusion][Epoch 9318] diffusion learning rate: 0.001
2024-11-05 04:09:00,693 - INFO - [diffusion][Epoch 9318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:00,694 - INFO - [diffusion][Epoch 9319] Epoch 9320/12000
2024-11-05 04:09:04,934 - INFO - [diffusion][Epoch 9319] diffusion training Loss: 0.05022158659994602
2024-11-05 04:09:04,936 - INFO - [diffusion][Epoch 9319] diffusion learning rate: 0.001
2024-11-05 04:09:04,938 - INFO - [diffusion][Epoch 9319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:04,939 - INFO - [diffusion][Epoch 9320] Epoch 9321/12000
2024-11-05 04:09:08,939 - INFO - [diffusion][Epoch 9320] diffusion training Loss: 0.053869739174842834
2024-11-05 04:09:08,942 - INFO - [diffusion][Epoch 9320] diffusion learning rate: 0.001
2024-11-05 04:09:08,944 - INFO - [diffusion][Epoch 9320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:08,945 - INFO - [diffusion][Epoch 9321] Epoch 9322/12000
2024-11-05 04:09:13,045 - INFO - [diffusion][Epoch 9321] diffusion training Loss: 0.04853491112589836
2024-11-05 04:09:13,047 - INFO - [diffusion][Epoch 9321] diffusion learning rate: 0.001
2024-11-05 04:09:13,049 - INFO - [diffusion][Epoch 9321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:13,051 - INFO - [diffusion][Epoch 9322] Epoch 9323/12000
2024-11-05 04:09:17,316 - INFO - [diffusion][Epoch 9322] diffusion training Loss: 0.048978282138705254
2024-11-05 04:09:17,318 - INFO - [diffusion][Epoch 9322] diffusion learning rate: 0.001
2024-11-05 04:09:17,320 - INFO - [diffusion][Epoch 9322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:17,321 - INFO - [diffusion][Epoch 9323] Epoch 9324/12000
2024-11-05 04:09:21,433 - INFO - [diffusion][Epoch 9323] diffusion training Loss: 0.05781648401170969
2024-11-05 04:09:21,436 - INFO - [diffusion][Epoch 9323] diffusion learning rate: 0.001
2024-11-05 04:09:21,438 - INFO - [diffusion][Epoch 9323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:21,440 - INFO - [diffusion][Epoch 9324] Epoch 9325/12000
2024-11-05 04:09:25,665 - INFO - [diffusion][Epoch 9324] diffusion training Loss: 0.05305387079715729
2024-11-05 04:09:25,667 - INFO - [diffusion][Epoch 9324] diffusion learning rate: 0.001
2024-11-05 04:09:25,669 - INFO - [diffusion][Epoch 9324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:25,671 - INFO - [diffusion][Epoch 9325] Epoch 9326/12000
2024-11-05 04:09:29,865 - INFO - [diffusion][Epoch 9325] diffusion training Loss: 0.04776551201939583
2024-11-05 04:09:29,867 - INFO - [diffusion][Epoch 9325] diffusion learning rate: 0.001
2024-11-05 04:09:29,869 - INFO - [diffusion][Epoch 9325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:29,871 - INFO - [diffusion][Epoch 9326] Epoch 9327/12000
2024-11-05 04:09:33,969 - INFO - [diffusion][Epoch 9326] diffusion training Loss: 0.05194947961717844
2024-11-05 04:09:33,973 - INFO - [diffusion][Epoch 9326] diffusion learning rate: 0.001
2024-11-05 04:09:33,975 - INFO - [diffusion][Epoch 9326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:33,976 - INFO - [diffusion][Epoch 9327] Epoch 9328/12000
2024-11-05 04:09:38,193 - INFO - [diffusion][Epoch 9327] diffusion training Loss: 0.05219361465424299
2024-11-05 04:09:38,196 - INFO - [diffusion][Epoch 9327] diffusion learning rate: 0.001
2024-11-05 04:09:38,198 - INFO - [diffusion][Epoch 9327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:38,199 - INFO - [diffusion][Epoch 9328] Epoch 9329/12000
2024-11-05 04:09:42,408 - INFO - [diffusion][Epoch 9328] diffusion training Loss: 0.05081765167415142
2024-11-05 04:09:42,411 - INFO - [diffusion][Epoch 9328] diffusion learning rate: 0.001
2024-11-05 04:09:42,413 - INFO - [diffusion][Epoch 9328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:42,414 - INFO - [diffusion][Epoch 9329] Epoch 9330/12000
2024-11-05 04:09:46,512 - INFO - [diffusion][Epoch 9329] diffusion training Loss: 0.05060090031474829
2024-11-05 04:09:46,514 - INFO - [diffusion][Epoch 9329] diffusion learning rate: 0.001
2024-11-05 04:09:46,516 - INFO - [diffusion][Epoch 9329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:46,517 - INFO - [diffusion][Epoch 9330] Epoch 9331/12000
2024-11-05 04:09:50,515 - INFO - [diffusion][Epoch 9330] diffusion training Loss: 0.047192555852234364
2024-11-05 04:09:50,517 - INFO - [diffusion][Epoch 9330] diffusion learning rate: 0.001
2024-11-05 04:09:50,518 - INFO - [diffusion][Epoch 9330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:50,520 - INFO - [diffusion][Epoch 9331] Epoch 9332/12000
2024-11-05 04:09:54,603 - INFO - [diffusion][Epoch 9331] diffusion training Loss: 0.04752493742853403
2024-11-05 04:09:54,605 - INFO - [diffusion][Epoch 9331] diffusion learning rate: 0.001
2024-11-05 04:09:54,638 - INFO - [diffusion][Epoch 9331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:54,639 - INFO - [diffusion][Epoch 9332] Epoch 9333/12000
2024-11-05 04:09:58,607 - INFO - [diffusion][Epoch 9332] diffusion training Loss: 0.04716899525374174
2024-11-05 04:09:58,610 - INFO - [diffusion][Epoch 9332] diffusion learning rate: 0.001
2024-11-05 04:09:58,613 - INFO - [diffusion][Epoch 9332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:09:58,614 - INFO - [diffusion][Epoch 9333] Epoch 9334/12000
2024-11-05 04:10:02,733 - INFO - [diffusion][Epoch 9333] diffusion training Loss: 0.0574318366125226
2024-11-05 04:10:02,735 - INFO - [diffusion][Epoch 9333] diffusion learning rate: 0.001
2024-11-05 04:10:02,737 - INFO - [diffusion][Epoch 9333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:02,738 - INFO - [diffusion][Epoch 9334] Epoch 9335/12000
2024-11-05 04:10:06,876 - INFO - [diffusion][Epoch 9334] diffusion training Loss: 0.056805153377354145
2024-11-05 04:10:06,878 - INFO - [diffusion][Epoch 9334] diffusion learning rate: 0.001
2024-11-05 04:10:06,881 - INFO - [diffusion][Epoch 9334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:06,882 - INFO - [diffusion][Epoch 9335] Epoch 9336/12000
2024-11-05 04:10:10,909 - INFO - [diffusion][Epoch 9335] diffusion training Loss: 0.05470402352511883
2024-11-05 04:10:10,911 - INFO - [diffusion][Epoch 9335] diffusion learning rate: 0.001
2024-11-05 04:10:10,913 - INFO - [diffusion][Epoch 9335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:10,915 - INFO - [diffusion][Epoch 9336] Epoch 9337/12000
2024-11-05 04:10:14,988 - INFO - [diffusion][Epoch 9336] diffusion training Loss: 0.05251914635300636
2024-11-05 04:10:14,990 - INFO - [diffusion][Epoch 9336] diffusion learning rate: 0.001
2024-11-05 04:10:14,992 - INFO - [diffusion][Epoch 9336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:14,993 - INFO - [diffusion][Epoch 9337] Epoch 9338/12000
2024-11-05 04:10:19,045 - INFO - [diffusion][Epoch 9337] diffusion training Loss: 0.04689894989132881
2024-11-05 04:10:19,047 - INFO - [diffusion][Epoch 9337] diffusion learning rate: 0.001
2024-11-05 04:10:19,049 - INFO - [diffusion][Epoch 9337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:19,050 - INFO - [diffusion][Epoch 9338] Epoch 9339/12000
2024-11-05 04:10:22,977 - INFO - [diffusion][Epoch 9338] diffusion training Loss: 0.04896480683237314
2024-11-05 04:10:22,979 - INFO - [diffusion][Epoch 9338] diffusion learning rate: 0.001
2024-11-05 04:10:22,981 - INFO - [diffusion][Epoch 9338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:22,983 - INFO - [diffusion][Epoch 9339] Epoch 9340/12000
2024-11-05 04:10:27,157 - INFO - [diffusion][Epoch 9339] diffusion training Loss: 0.05131051689386368
2024-11-05 04:10:27,163 - INFO - [diffusion][Epoch 9339] diffusion learning rate: 0.001
2024-11-05 04:10:27,165 - INFO - [diffusion][Epoch 9339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:27,167 - INFO - [diffusion][Epoch 9340] Epoch 9341/12000
2024-11-05 04:10:31,709 - INFO - [diffusion][Epoch 9340] diffusion training Loss: 0.04767401237040758
2024-11-05 04:10:31,711 - INFO - [diffusion][Epoch 9340] diffusion learning rate: 0.001
2024-11-05 04:10:31,713 - INFO - [diffusion][Epoch 9340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:31,714 - INFO - [diffusion][Epoch 9341] Epoch 9342/12000
2024-11-05 04:10:35,819 - INFO - [diffusion][Epoch 9341] diffusion training Loss: 0.050208632834255695
2024-11-05 04:10:35,821 - INFO - [diffusion][Epoch 9341] diffusion learning rate: 0.001
2024-11-05 04:10:35,823 - INFO - [diffusion][Epoch 9341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:35,824 - INFO - [diffusion][Epoch 9342] Epoch 9343/12000
2024-11-05 04:10:39,884 - INFO - [diffusion][Epoch 9342] diffusion training Loss: 0.04653038643300533
2024-11-05 04:10:39,886 - INFO - [diffusion][Epoch 9342] diffusion learning rate: 0.001
2024-11-05 04:10:39,887 - INFO - [diffusion][Epoch 9342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:39,889 - INFO - [diffusion][Epoch 9343] Epoch 9344/12000
2024-11-05 04:10:44,002 - INFO - [diffusion][Epoch 9343] diffusion training Loss: 0.05291959550231695
2024-11-05 04:10:44,004 - INFO - [diffusion][Epoch 9343] diffusion learning rate: 0.001
2024-11-05 04:10:44,006 - INFO - [diffusion][Epoch 9343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:44,007 - INFO - [diffusion][Epoch 9344] Epoch 9345/12000
2024-11-05 04:10:48,123 - INFO - [diffusion][Epoch 9344] diffusion training Loss: 0.05300508812069893
2024-11-05 04:10:48,126 - INFO - [diffusion][Epoch 9344] diffusion learning rate: 0.001
2024-11-05 04:10:48,128 - INFO - [diffusion][Epoch 9344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:48,129 - INFO - [diffusion][Epoch 9345] Epoch 9346/12000
2024-11-05 04:10:52,292 - INFO - [diffusion][Epoch 9345] diffusion training Loss: 0.046892838552594185
2024-11-05 04:10:52,295 - INFO - [diffusion][Epoch 9345] diffusion learning rate: 0.001
2024-11-05 04:10:52,297 - INFO - [diffusion][Epoch 9345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:52,298 - INFO - [diffusion][Epoch 9346] Epoch 9347/12000
2024-11-05 04:10:56,365 - INFO - [diffusion][Epoch 9346] diffusion training Loss: 0.051413124427199364
2024-11-05 04:10:56,367 - INFO - [diffusion][Epoch 9346] diffusion learning rate: 0.001
2024-11-05 04:10:56,369 - INFO - [diffusion][Epoch 9346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:10:56,370 - INFO - [diffusion][Epoch 9347] Epoch 9348/12000
2024-11-05 04:11:00,375 - INFO - [diffusion][Epoch 9347] diffusion training Loss: 0.04869467951357365
2024-11-05 04:11:00,377 - INFO - [diffusion][Epoch 9347] diffusion learning rate: 0.001
2024-11-05 04:11:00,378 - INFO - [diffusion][Epoch 9347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:00,380 - INFO - [diffusion][Epoch 9348] Epoch 9349/12000
2024-11-05 04:11:04,605 - INFO - [diffusion][Epoch 9348] diffusion training Loss: 0.0494013512507081
2024-11-05 04:11:04,607 - INFO - [diffusion][Epoch 9348] diffusion learning rate: 0.001
2024-11-05 04:11:04,609 - INFO - [diffusion][Epoch 9348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:04,611 - INFO - [diffusion][Epoch 9349] Epoch 9350/12000
2024-11-05 04:11:08,698 - INFO - [diffusion][Epoch 9349] diffusion training Loss: 0.054804219864308834
2024-11-05 04:11:08,700 - INFO - [diffusion][Epoch 9349] diffusion learning rate: 0.001
2024-11-05 04:11:08,702 - INFO - [diffusion][Epoch 9349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:08,703 - INFO - [diffusion][Epoch 9350] Epoch 9351/12000
2024-11-05 04:11:12,873 - INFO - [diffusion][Epoch 9350] diffusion training Loss: 0.048770525492727757
2024-11-05 04:11:12,876 - INFO - [diffusion][Epoch 9350] diffusion learning rate: 0.001
2024-11-05 04:11:12,903 - INFO - [diffusion][Epoch 9350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:12,904 - INFO - [diffusion][Epoch 9351] Epoch 9352/12000
2024-11-05 04:11:17,087 - INFO - [diffusion][Epoch 9351] diffusion training Loss: 0.04762277565896511
2024-11-05 04:11:17,089 - INFO - [diffusion][Epoch 9351] diffusion learning rate: 0.001
2024-11-05 04:11:17,091 - INFO - [diffusion][Epoch 9351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:17,093 - INFO - [diffusion][Epoch 9352] Epoch 9353/12000
2024-11-05 04:11:21,197 - INFO - [diffusion][Epoch 9352] diffusion training Loss: 0.04970576427876949
2024-11-05 04:11:21,199 - INFO - [diffusion][Epoch 9352] diffusion learning rate: 0.001
2024-11-05 04:11:21,201 - INFO - [diffusion][Epoch 9352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:21,202 - INFO - [diffusion][Epoch 9353] Epoch 9354/12000
2024-11-05 04:11:25,335 - INFO - [diffusion][Epoch 9353] diffusion training Loss: 0.053317220881581306
2024-11-05 04:11:25,338 - INFO - [diffusion][Epoch 9353] diffusion learning rate: 0.001
2024-11-05 04:11:25,340 - INFO - [diffusion][Epoch 9353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:25,341 - INFO - [diffusion][Epoch 9354] Epoch 9355/12000
2024-11-05 04:11:29,578 - INFO - [diffusion][Epoch 9354] diffusion training Loss: 0.04728342778980732
2024-11-05 04:11:29,580 - INFO - [diffusion][Epoch 9354] diffusion learning rate: 0.001
2024-11-05 04:11:29,582 - INFO - [diffusion][Epoch 9354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:29,584 - INFO - [diffusion][Epoch 9355] Epoch 9356/12000
2024-11-05 04:11:33,818 - INFO - [diffusion][Epoch 9355] diffusion training Loss: 0.05077063571661711
2024-11-05 04:11:33,821 - INFO - [diffusion][Epoch 9355] diffusion learning rate: 0.001
2024-11-05 04:11:33,823 - INFO - [diffusion][Epoch 9355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:33,824 - INFO - [diffusion][Epoch 9356] Epoch 9357/12000
2024-11-05 04:11:37,833 - INFO - [diffusion][Epoch 9356] diffusion training Loss: 0.048676843754947186
2024-11-05 04:11:37,834 - INFO - [diffusion][Epoch 9356] diffusion learning rate: 0.001
2024-11-05 04:11:37,836 - INFO - [diffusion][Epoch 9356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:37,837 - INFO - [diffusion][Epoch 9357] Epoch 9358/12000
2024-11-05 04:11:41,780 - INFO - [diffusion][Epoch 9357] diffusion training Loss: 0.046170066110789776
2024-11-05 04:11:41,782 - INFO - [diffusion][Epoch 9357] diffusion learning rate: 0.001
2024-11-05 04:11:41,784 - INFO - [diffusion][Epoch 9357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:41,785 - INFO - [diffusion][Epoch 9358] Epoch 9359/12000
2024-11-05 04:11:45,712 - INFO - [diffusion][Epoch 9358] diffusion training Loss: 0.05290048196911812
2024-11-05 04:11:45,715 - INFO - [diffusion][Epoch 9358] diffusion learning rate: 0.001
2024-11-05 04:11:45,717 - INFO - [diffusion][Epoch 9358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:45,719 - INFO - [diffusion][Epoch 9359] Epoch 9360/12000
2024-11-05 04:11:49,870 - INFO - [diffusion][Epoch 9359] diffusion training Loss: 0.0458849910646677
2024-11-05 04:11:49,872 - INFO - [diffusion][Epoch 9359] diffusion learning rate: 0.001
2024-11-05 04:11:49,874 - INFO - [diffusion][Epoch 9359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:49,876 - INFO - [diffusion][Epoch 9360] Epoch 9361/12000
2024-11-05 04:11:54,568 - INFO - [diffusion][Epoch 9360] diffusion training Loss: 0.05018703266978264
2024-11-05 04:11:54,570 - INFO - [diffusion][Epoch 9360] diffusion learning rate: 0.001
2024-11-05 04:11:54,572 - INFO - [diffusion][Epoch 9360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:54,573 - INFO - [diffusion][Epoch 9361] Epoch 9362/12000
2024-11-05 04:11:58,677 - INFO - [diffusion][Epoch 9361] diffusion training Loss: 0.04992998670786619
2024-11-05 04:11:58,679 - INFO - [diffusion][Epoch 9361] diffusion learning rate: 0.001
2024-11-05 04:11:58,681 - INFO - [diffusion][Epoch 9361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:11:58,682 - INFO - [diffusion][Epoch 9362] Epoch 9363/12000
2024-11-05 04:12:02,716 - INFO - [diffusion][Epoch 9362] diffusion training Loss: 0.05111936666071415
2024-11-05 04:12:02,718 - INFO - [diffusion][Epoch 9362] diffusion learning rate: 0.001
2024-11-05 04:12:02,720 - INFO - [diffusion][Epoch 9362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:02,721 - INFO - [diffusion][Epoch 9363] Epoch 9364/12000
2024-11-05 04:12:06,781 - INFO - [diffusion][Epoch 9363] diffusion training Loss: 0.05250703729689121
2024-11-05 04:12:06,783 - INFO - [diffusion][Epoch 9363] diffusion learning rate: 0.001
2024-11-05 04:12:06,785 - INFO - [diffusion][Epoch 9363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:06,787 - INFO - [diffusion][Epoch 9364] Epoch 9365/12000
2024-11-05 04:12:10,854 - INFO - [diffusion][Epoch 9364] diffusion training Loss: 0.0520741967484355
2024-11-05 04:12:10,857 - INFO - [diffusion][Epoch 9364] diffusion learning rate: 0.001
2024-11-05 04:12:10,858 - INFO - [diffusion][Epoch 9364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:10,860 - INFO - [diffusion][Epoch 9365] Epoch 9366/12000
2024-11-05 04:12:14,965 - INFO - [diffusion][Epoch 9365] diffusion training Loss: 0.05589559953659773
2024-11-05 04:12:14,967 - INFO - [diffusion][Epoch 9365] diffusion learning rate: 0.001
2024-11-05 04:12:14,969 - INFO - [diffusion][Epoch 9365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:14,972 - INFO - [diffusion][Epoch 9366] Epoch 9367/12000
2024-11-05 04:12:19,019 - INFO - [diffusion][Epoch 9366] diffusion training Loss: 0.048230668529868126
2024-11-05 04:12:19,021 - INFO - [diffusion][Epoch 9366] diffusion learning rate: 0.001
2024-11-05 04:12:19,023 - INFO - [diffusion][Epoch 9366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:19,024 - INFO - [diffusion][Epoch 9367] Epoch 9368/12000
2024-11-05 04:12:23,281 - INFO - [diffusion][Epoch 9367] diffusion training Loss: 0.05448697600513697
2024-11-05 04:12:23,283 - INFO - [diffusion][Epoch 9367] diffusion learning rate: 0.001
2024-11-05 04:12:23,285 - INFO - [diffusion][Epoch 9367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:23,286 - INFO - [diffusion][Epoch 9368] Epoch 9369/12000
2024-11-05 04:12:27,429 - INFO - [diffusion][Epoch 9368] diffusion training Loss: 0.054563988000154495
2024-11-05 04:12:27,431 - INFO - [diffusion][Epoch 9368] diffusion learning rate: 0.001
2024-11-05 04:12:27,433 - INFO - [diffusion][Epoch 9368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:27,434 - INFO - [diffusion][Epoch 9369] Epoch 9370/12000
2024-11-05 04:12:31,574 - INFO - [diffusion][Epoch 9369] diffusion training Loss: 0.04557918291538954
2024-11-05 04:12:31,576 - INFO - [diffusion][Epoch 9369] diffusion learning rate: 0.001
2024-11-05 04:12:31,578 - INFO - [diffusion][Epoch 9369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:31,579 - INFO - [diffusion][Epoch 9370] Epoch 9371/12000
2024-11-05 04:12:35,736 - INFO - [diffusion][Epoch 9370] diffusion training Loss: 0.05330923572182655
2024-11-05 04:12:35,738 - INFO - [diffusion][Epoch 9370] diffusion learning rate: 0.001
2024-11-05 04:12:35,740 - INFO - [diffusion][Epoch 9370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:35,741 - INFO - [diffusion][Epoch 9371] Epoch 9372/12000
2024-11-05 04:12:39,900 - INFO - [diffusion][Epoch 9371] diffusion training Loss: 0.048611726611852646
2024-11-05 04:12:39,902 - INFO - [diffusion][Epoch 9371] diffusion learning rate: 0.001
2024-11-05 04:12:39,903 - INFO - [diffusion][Epoch 9371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:39,904 - INFO - [diffusion][Epoch 9372] Epoch 9373/12000
2024-11-05 04:12:43,863 - INFO - [diffusion][Epoch 9372] diffusion training Loss: 0.05253412015736103
2024-11-05 04:12:43,865 - INFO - [diffusion][Epoch 9372] diffusion learning rate: 0.001
2024-11-05 04:12:43,880 - INFO - [diffusion][Epoch 9372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:43,882 - INFO - [diffusion][Epoch 9373] Epoch 9374/12000
2024-11-05 04:12:48,050 - INFO - [diffusion][Epoch 9373] diffusion training Loss: 0.05145963281393051
2024-11-05 04:12:48,053 - INFO - [diffusion][Epoch 9373] diffusion learning rate: 0.001
2024-11-05 04:12:48,055 - INFO - [diffusion][Epoch 9373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:48,056 - INFO - [diffusion][Epoch 9374] Epoch 9375/12000
2024-11-05 04:12:52,155 - INFO - [diffusion][Epoch 9374] diffusion training Loss: 0.04509706888347864
2024-11-05 04:12:52,157 - INFO - [diffusion][Epoch 9374] diffusion learning rate: 0.001
2024-11-05 04:12:52,159 - INFO - [diffusion][Epoch 9374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:52,160 - INFO - [diffusion][Epoch 9375] Epoch 9376/12000
2024-11-05 04:12:56,276 - INFO - [diffusion][Epoch 9375] diffusion training Loss: 0.04952459130436182
2024-11-05 04:12:56,278 - INFO - [diffusion][Epoch 9375] diffusion learning rate: 0.001
2024-11-05 04:12:56,280 - INFO - [diffusion][Epoch 9375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:12:56,282 - INFO - [diffusion][Epoch 9376] Epoch 9377/12000
2024-11-05 04:13:00,407 - INFO - [diffusion][Epoch 9376] diffusion training Loss: 0.051525301299989223
2024-11-05 04:13:00,410 - INFO - [diffusion][Epoch 9376] diffusion learning rate: 0.001
2024-11-05 04:13:00,411 - INFO - [diffusion][Epoch 9376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:00,413 - INFO - [diffusion][Epoch 9377] Epoch 9378/12000
2024-11-05 04:13:04,495 - INFO - [diffusion][Epoch 9377] diffusion training Loss: 0.053998205810785294
2024-11-05 04:13:04,497 - INFO - [diffusion][Epoch 9377] diffusion learning rate: 0.001
2024-11-05 04:13:04,499 - INFO - [diffusion][Epoch 9377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:04,501 - INFO - [diffusion][Epoch 9378] Epoch 9379/12000
2024-11-05 04:13:08,688 - INFO - [diffusion][Epoch 9378] diffusion training Loss: 0.05163880065083504
2024-11-05 04:13:08,691 - INFO - [diffusion][Epoch 9378] diffusion learning rate: 0.001
2024-11-05 04:13:08,693 - INFO - [diffusion][Epoch 9378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:08,694 - INFO - [diffusion][Epoch 9379] Epoch 9380/12000
2024-11-05 04:13:12,869 - INFO - [diffusion][Epoch 9379] diffusion training Loss: 0.049803285859525204
2024-11-05 04:13:12,871 - INFO - [diffusion][Epoch 9379] diffusion learning rate: 0.001
2024-11-05 04:13:12,873 - INFO - [diffusion][Epoch 9379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:12,876 - INFO - [diffusion][Epoch 9380] Epoch 9381/12000
2024-11-05 04:13:16,930 - INFO - [diffusion][Epoch 9380] diffusion training Loss: 0.0473387036472559
2024-11-05 04:13:16,932 - INFO - [diffusion][Epoch 9380] diffusion learning rate: 0.001
2024-11-05 04:13:16,934 - INFO - [diffusion][Epoch 9380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:16,935 - INFO - [diffusion][Epoch 9381] Epoch 9382/12000
2024-11-05 04:13:21,166 - INFO - [diffusion][Epoch 9381] diffusion training Loss: 0.05498254671692848
2024-11-05 04:13:21,169 - INFO - [diffusion][Epoch 9381] diffusion learning rate: 0.001
2024-11-05 04:13:21,171 - INFO - [diffusion][Epoch 9381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:21,173 - INFO - [diffusion][Epoch 9382] Epoch 9383/12000
2024-11-05 04:13:25,254 - INFO - [diffusion][Epoch 9382] diffusion training Loss: 0.04845320247113705
2024-11-05 04:13:25,257 - INFO - [diffusion][Epoch 9382] diffusion learning rate: 0.001
2024-11-05 04:13:25,259 - INFO - [diffusion][Epoch 9382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:25,260 - INFO - [diffusion][Epoch 9383] Epoch 9384/12000
2024-11-05 04:13:29,384 - INFO - [diffusion][Epoch 9383] diffusion training Loss: 0.050669729709625244
2024-11-05 04:13:29,386 - INFO - [diffusion][Epoch 9383] diffusion learning rate: 0.001
2024-11-05 04:13:29,388 - INFO - [diffusion][Epoch 9383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:29,389 - INFO - [diffusion][Epoch 9384] Epoch 9385/12000
2024-11-05 04:13:33,277 - INFO - [diffusion][Epoch 9384] diffusion training Loss: 0.04726949706673622
2024-11-05 04:13:33,280 - INFO - [diffusion][Epoch 9384] diffusion learning rate: 0.001
2024-11-05 04:13:33,282 - INFO - [diffusion][Epoch 9384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:33,283 - INFO - [diffusion][Epoch 9385] Epoch 9386/12000
2024-11-05 04:13:37,360 - INFO - [diffusion][Epoch 9385] diffusion training Loss: 0.04956720396876335
2024-11-05 04:13:37,363 - INFO - [diffusion][Epoch 9385] diffusion learning rate: 0.001
2024-11-05 04:13:37,364 - INFO - [diffusion][Epoch 9385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:37,366 - INFO - [diffusion][Epoch 9386] Epoch 9387/12000
2024-11-05 04:13:41,469 - INFO - [diffusion][Epoch 9386] diffusion training Loss: 0.05272297468036413
2024-11-05 04:13:41,471 - INFO - [diffusion][Epoch 9386] diffusion learning rate: 0.001
2024-11-05 04:13:41,473 - INFO - [diffusion][Epoch 9386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:41,474 - INFO - [diffusion][Epoch 9387] Epoch 9388/12000
2024-11-05 04:13:45,532 - INFO - [diffusion][Epoch 9387] diffusion training Loss: 0.052500976249575615
2024-11-05 04:13:45,534 - INFO - [diffusion][Epoch 9387] diffusion learning rate: 0.001
2024-11-05 04:13:45,539 - INFO - [diffusion][Epoch 9387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:45,540 - INFO - [diffusion][Epoch 9388] Epoch 9389/12000
2024-11-05 04:13:49,636 - INFO - [diffusion][Epoch 9388] diffusion training Loss: 0.05131334252655506
2024-11-05 04:13:49,639 - INFO - [diffusion][Epoch 9388] diffusion learning rate: 0.001
2024-11-05 04:13:49,640 - INFO - [diffusion][Epoch 9388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:49,642 - INFO - [diffusion][Epoch 9389] Epoch 9390/12000
2024-11-05 04:13:53,799 - INFO - [diffusion][Epoch 9389] diffusion training Loss: 0.053191469982266426
2024-11-05 04:13:53,802 - INFO - [diffusion][Epoch 9389] diffusion learning rate: 0.001
2024-11-05 04:13:53,803 - INFO - [diffusion][Epoch 9389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:53,805 - INFO - [diffusion][Epoch 9390] Epoch 9391/12000
2024-11-05 04:13:58,057 - INFO - [diffusion][Epoch 9390] diffusion training Loss: 0.04695778153836727
2024-11-05 04:13:58,059 - INFO - [diffusion][Epoch 9390] diffusion learning rate: 0.001
2024-11-05 04:13:58,061 - INFO - [diffusion][Epoch 9390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:13:58,063 - INFO - [diffusion][Epoch 9391] Epoch 9392/12000
2024-11-05 04:14:02,271 - INFO - [diffusion][Epoch 9391] diffusion training Loss: 0.051663244143128395
2024-11-05 04:14:02,273 - INFO - [diffusion][Epoch 9391] diffusion learning rate: 0.001
2024-11-05 04:14:02,275 - INFO - [diffusion][Epoch 9391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:02,276 - INFO - [diffusion][Epoch 9392] Epoch 9393/12000
2024-11-05 04:14:06,384 - INFO - [diffusion][Epoch 9392] diffusion training Loss: 0.0511954827234149
2024-11-05 04:14:06,386 - INFO - [diffusion][Epoch 9392] diffusion learning rate: 0.001
2024-11-05 04:14:06,387 - INFO - [diffusion][Epoch 9392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:06,389 - INFO - [diffusion][Epoch 9393] Epoch 9394/12000
2024-11-05 04:14:10,414 - INFO - [diffusion][Epoch 9393] diffusion training Loss: 0.05146054271608591
2024-11-05 04:14:10,417 - INFO - [diffusion][Epoch 9393] diffusion learning rate: 0.001
2024-11-05 04:14:10,419 - INFO - [diffusion][Epoch 9393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:10,420 - INFO - [diffusion][Epoch 9394] Epoch 9395/12000
2024-11-05 04:14:14,450 - INFO - [diffusion][Epoch 9394] diffusion training Loss: 0.04405011981725693
2024-11-05 04:14:14,452 - INFO - [diffusion][Epoch 9394] diffusion learning rate: 0.001
2024-11-05 04:14:14,454 - INFO - [diffusion][Epoch 9394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:14,455 - INFO - [diffusion][Epoch 9395] Epoch 9396/12000
2024-11-05 04:14:18,372 - INFO - [diffusion][Epoch 9395] diffusion training Loss: 0.05075978673994541
2024-11-05 04:14:18,376 - INFO - [diffusion][Epoch 9395] diffusion learning rate: 0.001
2024-11-05 04:14:18,378 - INFO - [diffusion][Epoch 9395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:18,380 - INFO - [diffusion][Epoch 9396] Epoch 9397/12000
2024-11-05 04:14:22,505 - INFO - [diffusion][Epoch 9396] diffusion training Loss: 0.05030984152108431
2024-11-05 04:14:22,507 - INFO - [diffusion][Epoch 9396] diffusion learning rate: 0.001
2024-11-05 04:14:22,509 - INFO - [diffusion][Epoch 9396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:22,510 - INFO - [diffusion][Epoch 9397] Epoch 9398/12000
2024-11-05 04:14:26,471 - INFO - [diffusion][Epoch 9397] diffusion training Loss: 0.05490928702056408
2024-11-05 04:14:26,472 - INFO - [diffusion][Epoch 9397] diffusion learning rate: 0.001
2024-11-05 04:14:26,474 - INFO - [diffusion][Epoch 9397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:26,475 - INFO - [diffusion][Epoch 9398] Epoch 9399/12000
2024-11-05 04:14:30,423 - INFO - [diffusion][Epoch 9398] diffusion training Loss: 0.05605096463114023
2024-11-05 04:14:30,425 - INFO - [diffusion][Epoch 9398] diffusion learning rate: 0.001
2024-11-05 04:14:30,427 - INFO - [diffusion][Epoch 9398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:30,429 - INFO - [diffusion][Epoch 9399] Epoch 9400/12000
2024-11-05 04:14:34,536 - INFO - [diffusion][Epoch 9399] diffusion training Loss: 0.053755315952003
2024-11-05 04:14:34,538 - INFO - [diffusion][Epoch 9399] diffusion learning rate: 0.001
2024-11-05 04:14:34,540 - INFO - [diffusion][Epoch 9399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:34,541 - INFO - [diffusion][Epoch 9400] Epoch 9401/12000
2024-11-05 04:14:38,664 - INFO - [diffusion][Epoch 9400] diffusion training Loss: 0.047695718705654144
2024-11-05 04:14:38,666 - INFO - [diffusion][Epoch 9400] diffusion learning rate: 0.001
2024-11-05 04:14:38,668 - INFO - [diffusion][Epoch 9400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:38,669 - INFO - [diffusion][Epoch 9401] Epoch 9402/12000
2024-11-05 04:14:42,827 - INFO - [diffusion][Epoch 9401] diffusion training Loss: 0.047354704700410366
2024-11-05 04:14:42,829 - INFO - [diffusion][Epoch 9401] diffusion learning rate: 0.001
2024-11-05 04:14:42,830 - INFO - [diffusion][Epoch 9401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:42,832 - INFO - [diffusion][Epoch 9402] Epoch 9403/12000
2024-11-05 04:14:46,984 - INFO - [diffusion][Epoch 9402] diffusion training Loss: 0.0510385911911726
2024-11-05 04:14:46,986 - INFO - [diffusion][Epoch 9402] diffusion learning rate: 0.001
2024-11-05 04:14:46,988 - INFO - [diffusion][Epoch 9402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:46,990 - INFO - [diffusion][Epoch 9403] Epoch 9404/12000
2024-11-05 04:14:51,390 - INFO - [diffusion][Epoch 9403] diffusion training Loss: 0.05231634899973869
2024-11-05 04:14:51,392 - INFO - [diffusion][Epoch 9403] diffusion learning rate: 0.001
2024-11-05 04:14:51,394 - INFO - [diffusion][Epoch 9403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:51,395 - INFO - [diffusion][Epoch 9404] Epoch 9405/12000
2024-11-05 04:14:55,448 - INFO - [diffusion][Epoch 9404] diffusion training Loss: 0.05221865698695183
2024-11-05 04:14:55,450 - INFO - [diffusion][Epoch 9404] diffusion learning rate: 0.001
2024-11-05 04:14:55,452 - INFO - [diffusion][Epoch 9404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:55,453 - INFO - [diffusion][Epoch 9405] Epoch 9406/12000
2024-11-05 04:14:59,607 - INFO - [diffusion][Epoch 9405] diffusion training Loss: 0.047563266940414906
2024-11-05 04:14:59,608 - INFO - [diffusion][Epoch 9405] diffusion learning rate: 0.001
2024-11-05 04:14:59,610 - INFO - [diffusion][Epoch 9405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:14:59,611 - INFO - [diffusion][Epoch 9406] Epoch 9407/12000
2024-11-05 04:15:03,705 - INFO - [diffusion][Epoch 9406] diffusion training Loss: 0.05567870009690523
2024-11-05 04:15:03,706 - INFO - [diffusion][Epoch 9406] diffusion learning rate: 0.001
2024-11-05 04:15:03,709 - INFO - [diffusion][Epoch 9406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:03,710 - INFO - [diffusion][Epoch 9407] Epoch 9408/12000
2024-11-05 04:15:07,877 - INFO - [diffusion][Epoch 9407] diffusion training Loss: 0.049712782725691795
2024-11-05 04:15:07,879 - INFO - [diffusion][Epoch 9407] diffusion learning rate: 0.001
2024-11-05 04:15:07,881 - INFO - [diffusion][Epoch 9407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:07,882 - INFO - [diffusion][Epoch 9408] Epoch 9409/12000
2024-11-05 04:15:12,052 - INFO - [diffusion][Epoch 9408] diffusion training Loss: 0.04738146252930164
2024-11-05 04:15:12,053 - INFO - [diffusion][Epoch 9408] diffusion learning rate: 0.001
2024-11-05 04:15:12,055 - INFO - [diffusion][Epoch 9408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:12,057 - INFO - [diffusion][Epoch 9409] Epoch 9410/12000
2024-11-05 04:15:16,218 - INFO - [diffusion][Epoch 9409] diffusion training Loss: 0.0493212528526783
2024-11-05 04:15:16,220 - INFO - [diffusion][Epoch 9409] diffusion learning rate: 0.001
2024-11-05 04:15:16,222 - INFO - [diffusion][Epoch 9409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:16,223 - INFO - [diffusion][Epoch 9410] Epoch 9411/12000
2024-11-05 04:15:20,510 - INFO - [diffusion][Epoch 9410] diffusion training Loss: 0.052946846932172775
2024-11-05 04:15:20,512 - INFO - [diffusion][Epoch 9410] diffusion learning rate: 0.001
2024-11-05 04:15:20,516 - INFO - [diffusion][Epoch 9410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:20,517 - INFO - [diffusion][Epoch 9411] Epoch 9412/12000
2024-11-05 04:15:24,589 - INFO - [diffusion][Epoch 9411] diffusion training Loss: 0.052023894153535366
2024-11-05 04:15:24,591 - INFO - [diffusion][Epoch 9411] diffusion learning rate: 0.001
2024-11-05 04:15:24,593 - INFO - [diffusion][Epoch 9411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:24,594 - INFO - [diffusion][Epoch 9412] Epoch 9413/12000
2024-11-05 04:15:28,649 - INFO - [diffusion][Epoch 9412] diffusion training Loss: 0.05298037361353636
2024-11-05 04:15:28,651 - INFO - [diffusion][Epoch 9412] diffusion learning rate: 0.001
2024-11-05 04:15:28,653 - INFO - [diffusion][Epoch 9412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:28,655 - INFO - [diffusion][Epoch 9413] Epoch 9414/12000
2024-11-05 04:15:32,619 - INFO - [diffusion][Epoch 9413] diffusion training Loss: 0.04884224943816662
2024-11-05 04:15:32,622 - INFO - [diffusion][Epoch 9413] diffusion learning rate: 0.001
2024-11-05 04:15:32,624 - INFO - [diffusion][Epoch 9413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:32,625 - INFO - [diffusion][Epoch 9414] Epoch 9415/12000
2024-11-05 04:15:36,710 - INFO - [diffusion][Epoch 9414] diffusion training Loss: 0.05053002666682005
2024-11-05 04:15:36,712 - INFO - [diffusion][Epoch 9414] diffusion learning rate: 0.001
2024-11-05 04:15:36,714 - INFO - [diffusion][Epoch 9414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:36,715 - INFO - [diffusion][Epoch 9415] Epoch 9416/12000
2024-11-05 04:15:40,632 - INFO - [diffusion][Epoch 9415] diffusion training Loss: 0.052158922888338566
2024-11-05 04:15:40,635 - INFO - [diffusion][Epoch 9415] diffusion learning rate: 0.001
2024-11-05 04:15:40,639 - INFO - [diffusion][Epoch 9415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:40,641 - INFO - [diffusion][Epoch 9416] Epoch 9417/12000
2024-11-05 04:15:44,665 - INFO - [diffusion][Epoch 9416] diffusion training Loss: 0.051089062355458736
2024-11-05 04:15:44,667 - INFO - [diffusion][Epoch 9416] diffusion learning rate: 0.001
2024-11-05 04:15:44,669 - INFO - [diffusion][Epoch 9416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:44,671 - INFO - [diffusion][Epoch 9417] Epoch 9418/12000
2024-11-05 04:15:48,774 - INFO - [diffusion][Epoch 9417] diffusion training Loss: 0.05469926819205284
2024-11-05 04:15:48,776 - INFO - [diffusion][Epoch 9417] diffusion learning rate: 0.001
2024-11-05 04:15:48,778 - INFO - [diffusion][Epoch 9417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:48,780 - INFO - [diffusion][Epoch 9418] Epoch 9419/12000
2024-11-05 04:15:52,922 - INFO - [diffusion][Epoch 9418] diffusion training Loss: 0.04814805090427399
2024-11-05 04:15:52,925 - INFO - [diffusion][Epoch 9418] diffusion learning rate: 0.001
2024-11-05 04:15:52,927 - INFO - [diffusion][Epoch 9418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:52,928 - INFO - [diffusion][Epoch 9419] Epoch 9420/12000
2024-11-05 04:15:57,069 - INFO - [diffusion][Epoch 9419] diffusion training Loss: 0.052276832051575184
2024-11-05 04:15:57,071 - INFO - [diffusion][Epoch 9419] diffusion learning rate: 0.001
2024-11-05 04:15:57,073 - INFO - [diffusion][Epoch 9419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:15:57,074 - INFO - [diffusion][Epoch 9420] Epoch 9421/12000
2024-11-05 04:16:01,219 - INFO - [diffusion][Epoch 9420] diffusion training Loss: 0.05183625314384699
2024-11-05 04:16:01,221 - INFO - [diffusion][Epoch 9420] diffusion learning rate: 0.001
2024-11-05 04:16:01,223 - INFO - [diffusion][Epoch 9420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:01,225 - INFO - [diffusion][Epoch 9421] Epoch 9422/12000
2024-11-05 04:16:05,367 - INFO - [diffusion][Epoch 9421] diffusion training Loss: 0.04960653372108936
2024-11-05 04:16:05,369 - INFO - [diffusion][Epoch 9421] diffusion learning rate: 0.001
2024-11-05 04:16:05,371 - INFO - [diffusion][Epoch 9421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:05,372 - INFO - [diffusion][Epoch 9422] Epoch 9423/12000
2024-11-05 04:16:09,469 - INFO - [diffusion][Epoch 9422] diffusion training Loss: 0.05501474812626839
2024-11-05 04:16:09,471 - INFO - [diffusion][Epoch 9422] diffusion learning rate: 0.001
2024-11-05 04:16:09,473 - INFO - [diffusion][Epoch 9422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:09,474 - INFO - [diffusion][Epoch 9423] Epoch 9424/12000
2024-11-05 04:16:13,530 - INFO - [diffusion][Epoch 9423] diffusion training Loss: 0.04799453169107437
2024-11-05 04:16:13,532 - INFO - [diffusion][Epoch 9423] diffusion learning rate: 0.001
2024-11-05 04:16:13,534 - INFO - [diffusion][Epoch 9423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:13,535 - INFO - [diffusion][Epoch 9424] Epoch 9425/12000
2024-11-05 04:16:17,923 - INFO - [diffusion][Epoch 9424] diffusion training Loss: 0.05007011443376541
2024-11-05 04:16:17,925 - INFO - [diffusion][Epoch 9424] diffusion learning rate: 0.001
2024-11-05 04:16:17,927 - INFO - [diffusion][Epoch 9424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:17,928 - INFO - [diffusion][Epoch 9425] Epoch 9426/12000
2024-11-05 04:16:22,081 - INFO - [diffusion][Epoch 9425] diffusion training Loss: 0.0515938987955451
2024-11-05 04:16:22,083 - INFO - [diffusion][Epoch 9425] diffusion learning rate: 0.001
2024-11-05 04:16:22,085 - INFO - [diffusion][Epoch 9425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:22,086 - INFO - [diffusion][Epoch 9426] Epoch 9427/12000
2024-11-05 04:16:26,118 - INFO - [diffusion][Epoch 9426] diffusion training Loss: 0.044687542133033276
2024-11-05 04:16:26,120 - INFO - [diffusion][Epoch 9426] diffusion learning rate: 0.001
2024-11-05 04:16:26,122 - INFO - [diffusion][Epoch 9426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:26,123 - INFO - [diffusion][Epoch 9427] Epoch 9428/12000
2024-11-05 04:16:30,413 - INFO - [diffusion][Epoch 9427] diffusion training Loss: 0.05090923793613911
2024-11-05 04:16:30,415 - INFO - [diffusion][Epoch 9427] diffusion learning rate: 0.001
2024-11-05 04:16:30,417 - INFO - [diffusion][Epoch 9427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:30,419 - INFO - [diffusion][Epoch 9428] Epoch 9429/12000
2024-11-05 04:16:34,534 - INFO - [diffusion][Epoch 9428] diffusion training Loss: 0.05097396858036518
2024-11-05 04:16:34,536 - INFO - [diffusion][Epoch 9428] diffusion learning rate: 0.001
2024-11-05 04:16:34,538 - INFO - [diffusion][Epoch 9428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:34,539 - INFO - [diffusion][Epoch 9429] Epoch 9430/12000
2024-11-05 04:16:38,637 - INFO - [diffusion][Epoch 9429] diffusion training Loss: 0.05120280012488365
2024-11-05 04:16:38,639 - INFO - [diffusion][Epoch 9429] diffusion learning rate: 0.001
2024-11-05 04:16:38,642 - INFO - [diffusion][Epoch 9429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:38,644 - INFO - [diffusion][Epoch 9430] Epoch 9431/12000
2024-11-05 04:16:42,781 - INFO - [diffusion][Epoch 9430] diffusion training Loss: 0.050869083032011986
2024-11-05 04:16:42,783 - INFO - [diffusion][Epoch 9430] diffusion learning rate: 0.001
2024-11-05 04:16:42,785 - INFO - [diffusion][Epoch 9430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:42,787 - INFO - [diffusion][Epoch 9431] Epoch 9432/12000
2024-11-05 04:16:46,938 - INFO - [diffusion][Epoch 9431] diffusion training Loss: 0.05403298884630203
2024-11-05 04:16:46,940 - INFO - [diffusion][Epoch 9431] diffusion learning rate: 0.001
2024-11-05 04:16:46,942 - INFO - [diffusion][Epoch 9431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:46,943 - INFO - [diffusion][Epoch 9432] Epoch 9433/12000
2024-11-05 04:16:51,020 - INFO - [diffusion][Epoch 9432] diffusion training Loss: 0.05200049560517073
2024-11-05 04:16:51,022 - INFO - [diffusion][Epoch 9432] diffusion learning rate: 0.001
2024-11-05 04:16:51,024 - INFO - [diffusion][Epoch 9432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:51,025 - INFO - [diffusion][Epoch 9433] Epoch 9434/12000
2024-11-05 04:16:55,059 - INFO - [diffusion][Epoch 9433] diffusion training Loss: 0.048442864790558815
2024-11-05 04:16:55,062 - INFO - [diffusion][Epoch 9433] diffusion learning rate: 0.001
2024-11-05 04:16:55,064 - INFO - [diffusion][Epoch 9433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:55,065 - INFO - [diffusion][Epoch 9434] Epoch 9435/12000
2024-11-05 04:16:59,188 - INFO - [diffusion][Epoch 9434] diffusion training Loss: 0.05084722116589546
2024-11-05 04:16:59,191 - INFO - [diffusion][Epoch 9434] diffusion learning rate: 0.001
2024-11-05 04:16:59,197 - INFO - [diffusion][Epoch 9434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:16:59,199 - INFO - [diffusion][Epoch 9435] Epoch 9436/12000
2024-11-05 04:17:03,428 - INFO - [diffusion][Epoch 9435] diffusion training Loss: 0.049061430618166924
2024-11-05 04:17:03,432 - INFO - [diffusion][Epoch 9435] diffusion learning rate: 0.001
2024-11-05 04:17:03,458 - INFO - [diffusion][Epoch 9435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:03,459 - INFO - [diffusion][Epoch 9436] Epoch 9437/12000
2024-11-05 04:17:07,559 - INFO - [diffusion][Epoch 9436] diffusion training Loss: 0.044508202001452446
2024-11-05 04:17:07,561 - INFO - [diffusion][Epoch 9436] diffusion learning rate: 0.001
2024-11-05 04:17:07,563 - INFO - [diffusion][Epoch 9436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:07,565 - INFO - [diffusion][Epoch 9437] Epoch 9438/12000
2024-11-05 04:17:11,482 - INFO - [diffusion][Epoch 9437] diffusion training Loss: 0.049395215697586536
2024-11-05 04:17:11,484 - INFO - [diffusion][Epoch 9437] diffusion learning rate: 0.001
2024-11-05 04:17:11,486 - INFO - [diffusion][Epoch 9437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:11,487 - INFO - [diffusion][Epoch 9438] Epoch 9439/12000
2024-11-05 04:17:15,487 - INFO - [diffusion][Epoch 9438] diffusion training Loss: 0.05008301790803671
2024-11-05 04:17:15,490 - INFO - [diffusion][Epoch 9438] diffusion learning rate: 0.001
2024-11-05 04:17:15,492 - INFO - [diffusion][Epoch 9438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:15,493 - INFO - [diffusion][Epoch 9439] Epoch 9440/12000
2024-11-05 04:17:19,560 - INFO - [diffusion][Epoch 9439] diffusion training Loss: 0.05212153401225805
2024-11-05 04:17:19,562 - INFO - [diffusion][Epoch 9439] diffusion learning rate: 0.001
2024-11-05 04:17:19,564 - INFO - [diffusion][Epoch 9439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:19,565 - INFO - [diffusion][Epoch 9440] Epoch 9441/12000
2024-11-05 04:17:23,688 - INFO - [diffusion][Epoch 9440] diffusion training Loss: 0.04755961336195469
2024-11-05 04:17:23,690 - INFO - [diffusion][Epoch 9440] diffusion learning rate: 0.001
2024-11-05 04:17:23,692 - INFO - [diffusion][Epoch 9440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:23,693 - INFO - [diffusion][Epoch 9441] Epoch 9442/12000
2024-11-05 04:17:27,690 - INFO - [diffusion][Epoch 9441] diffusion training Loss: 0.050623309798538685
2024-11-05 04:17:27,692 - INFO - [diffusion][Epoch 9441] diffusion learning rate: 0.001
2024-11-05 04:17:27,694 - INFO - [diffusion][Epoch 9441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:27,695 - INFO - [diffusion][Epoch 9442] Epoch 9443/12000
2024-11-05 04:17:31,812 - INFO - [diffusion][Epoch 9442] diffusion training Loss: 0.05564418341964483
2024-11-05 04:17:31,814 - INFO - [diffusion][Epoch 9442] diffusion learning rate: 0.001
2024-11-05 04:17:31,816 - INFO - [diffusion][Epoch 9442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:31,817 - INFO - [diffusion][Epoch 9443] Epoch 9444/12000
2024-11-05 04:17:35,795 - INFO - [diffusion][Epoch 9443] diffusion training Loss: 0.05442491639405489
2024-11-05 04:17:35,797 - INFO - [diffusion][Epoch 9443] diffusion learning rate: 0.001
2024-11-05 04:17:35,799 - INFO - [diffusion][Epoch 9443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:35,802 - INFO - [diffusion][Epoch 9444] Epoch 9445/12000
2024-11-05 04:17:40,173 - INFO - [diffusion][Epoch 9444] diffusion training Loss: 0.053969175554811954
2024-11-05 04:17:40,177 - INFO - [diffusion][Epoch 9444] diffusion learning rate: 0.001
2024-11-05 04:17:40,179 - INFO - [diffusion][Epoch 9444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:40,180 - INFO - [diffusion][Epoch 9445] Epoch 9446/12000
2024-11-05 04:17:44,427 - INFO - [diffusion][Epoch 9445] diffusion training Loss: 0.04869431722909212
2024-11-05 04:17:44,429 - INFO - [diffusion][Epoch 9445] diffusion learning rate: 0.001
2024-11-05 04:17:44,430 - INFO - [diffusion][Epoch 9445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:44,431 - INFO - [diffusion][Epoch 9446] Epoch 9447/12000
2024-11-05 04:17:48,508 - INFO - [diffusion][Epoch 9446] diffusion training Loss: 0.04899084288626909
2024-11-05 04:17:48,511 - INFO - [diffusion][Epoch 9446] diffusion learning rate: 0.001
2024-11-05 04:17:48,513 - INFO - [diffusion][Epoch 9446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:48,516 - INFO - [diffusion][Epoch 9447] Epoch 9448/12000
2024-11-05 04:17:52,587 - INFO - [diffusion][Epoch 9447] diffusion training Loss: 0.05344454199075699
2024-11-05 04:17:52,589 - INFO - [diffusion][Epoch 9447] diffusion learning rate: 0.001
2024-11-05 04:17:52,605 - INFO - [diffusion][Epoch 9447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:52,606 - INFO - [diffusion][Epoch 9448] Epoch 9449/12000
2024-11-05 04:17:56,729 - INFO - [diffusion][Epoch 9448] diffusion training Loss: 0.051424997858703136
2024-11-05 04:17:56,732 - INFO - [diffusion][Epoch 9448] diffusion learning rate: 0.001
2024-11-05 04:17:56,734 - INFO - [diffusion][Epoch 9448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:17:56,736 - INFO - [diffusion][Epoch 9449] Epoch 9450/12000
2024-11-05 04:18:00,711 - INFO - [diffusion][Epoch 9449] diffusion training Loss: 0.0491188932210207
2024-11-05 04:18:00,713 - INFO - [diffusion][Epoch 9449] diffusion learning rate: 0.001
2024-11-05 04:18:00,715 - INFO - [diffusion][Epoch 9449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:00,716 - INFO - [diffusion][Epoch 9450] Epoch 9451/12000
2024-11-05 04:18:04,706 - INFO - [diffusion][Epoch 9450] diffusion training Loss: 0.05082602333277464
2024-11-05 04:18:04,708 - INFO - [diffusion][Epoch 9450] diffusion learning rate: 0.001
2024-11-05 04:18:04,710 - INFO - [diffusion][Epoch 9450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:04,712 - INFO - [diffusion][Epoch 9451] Epoch 9452/12000
2024-11-05 04:18:08,843 - INFO - [diffusion][Epoch 9451] diffusion training Loss: 0.049171083606779575
2024-11-05 04:18:08,845 - INFO - [diffusion][Epoch 9451] diffusion learning rate: 0.001
2024-11-05 04:18:08,847 - INFO - [diffusion][Epoch 9451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:08,848 - INFO - [diffusion][Epoch 9452] Epoch 9453/12000
2024-11-05 04:18:12,908 - INFO - [diffusion][Epoch 9452] diffusion training Loss: 0.047893266193568707
2024-11-05 04:18:12,910 - INFO - [diffusion][Epoch 9452] diffusion learning rate: 0.001
2024-11-05 04:18:12,911 - INFO - [diffusion][Epoch 9452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:12,913 - INFO - [diffusion][Epoch 9453] Epoch 9454/12000
2024-11-05 04:18:16,971 - INFO - [diffusion][Epoch 9453] diffusion training Loss: 0.04711896926164627
2024-11-05 04:18:16,973 - INFO - [diffusion][Epoch 9453] diffusion learning rate: 0.001
2024-11-05 04:18:16,975 - INFO - [diffusion][Epoch 9453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:16,977 - INFO - [diffusion][Epoch 9454] Epoch 9455/12000
2024-11-05 04:18:21,089 - INFO - [diffusion][Epoch 9454] diffusion training Loss: 0.04938607569783926
2024-11-05 04:18:21,091 - INFO - [diffusion][Epoch 9454] diffusion learning rate: 0.001
2024-11-05 04:18:21,093 - INFO - [diffusion][Epoch 9454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:21,094 - INFO - [diffusion][Epoch 9455] Epoch 9456/12000
2024-11-05 04:18:25,043 - INFO - [diffusion][Epoch 9455] diffusion training Loss: 0.052965027280151844
2024-11-05 04:18:25,045 - INFO - [diffusion][Epoch 9455] diffusion learning rate: 0.001
2024-11-05 04:18:25,047 - INFO - [diffusion][Epoch 9455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:25,048 - INFO - [diffusion][Epoch 9456] Epoch 9457/12000
2024-11-05 04:18:29,100 - INFO - [diffusion][Epoch 9456] diffusion training Loss: 0.048562921583652496
2024-11-05 04:18:29,102 - INFO - [diffusion][Epoch 9456] diffusion learning rate: 0.001
2024-11-05 04:18:29,103 - INFO - [diffusion][Epoch 9456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:29,105 - INFO - [diffusion][Epoch 9457] Epoch 9458/12000
2024-11-05 04:18:33,308 - INFO - [diffusion][Epoch 9457] diffusion training Loss: 0.046442936174571514
2024-11-05 04:18:33,310 - INFO - [diffusion][Epoch 9457] diffusion learning rate: 0.001
2024-11-05 04:18:33,312 - INFO - [diffusion][Epoch 9457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:33,313 - INFO - [diffusion][Epoch 9458] Epoch 9459/12000
2024-11-05 04:18:37,396 - INFO - [diffusion][Epoch 9458] diffusion training Loss: 0.05193795915693045
2024-11-05 04:18:37,398 - INFO - [diffusion][Epoch 9458] diffusion learning rate: 0.001
2024-11-05 04:18:37,400 - INFO - [diffusion][Epoch 9458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:37,401 - INFO - [diffusion][Epoch 9459] Epoch 9460/12000
2024-11-05 04:18:41,506 - INFO - [diffusion][Epoch 9459] diffusion training Loss: 0.050399262458086014
2024-11-05 04:18:41,508 - INFO - [diffusion][Epoch 9459] diffusion learning rate: 0.001
2024-11-05 04:18:41,510 - INFO - [diffusion][Epoch 9459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:41,511 - INFO - [diffusion][Epoch 9460] Epoch 9461/12000
2024-11-05 04:18:45,624 - INFO - [diffusion][Epoch 9460] diffusion training Loss: 0.05029836297035217
2024-11-05 04:18:45,626 - INFO - [diffusion][Epoch 9460] diffusion learning rate: 0.001
2024-11-05 04:18:45,628 - INFO - [diffusion][Epoch 9460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:45,629 - INFO - [diffusion][Epoch 9461] Epoch 9462/12000
2024-11-05 04:18:49,764 - INFO - [diffusion][Epoch 9461] diffusion training Loss: 0.04962106794118881
2024-11-05 04:18:49,766 - INFO - [diffusion][Epoch 9461] diffusion learning rate: 0.001
2024-11-05 04:18:49,768 - INFO - [diffusion][Epoch 9461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:49,769 - INFO - [diffusion][Epoch 9462] Epoch 9463/12000
2024-11-05 04:18:53,827 - INFO - [diffusion][Epoch 9462] diffusion training Loss: 0.05460804607719183
2024-11-05 04:18:53,829 - INFO - [diffusion][Epoch 9462] diffusion learning rate: 0.001
2024-11-05 04:18:53,831 - INFO - [diffusion][Epoch 9462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:53,832 - INFO - [diffusion][Epoch 9463] Epoch 9464/12000
2024-11-05 04:18:57,945 - INFO - [diffusion][Epoch 9463] diffusion training Loss: 0.050376204773783684
2024-11-05 04:18:57,948 - INFO - [diffusion][Epoch 9463] diffusion learning rate: 0.001
2024-11-05 04:18:57,950 - INFO - [diffusion][Epoch 9463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:18:57,952 - INFO - [diffusion][Epoch 9464] Epoch 9465/12000
2024-11-05 04:19:02,190 - INFO - [diffusion][Epoch 9464] diffusion training Loss: 0.048412020318210125
2024-11-05 04:19:02,193 - INFO - [diffusion][Epoch 9464] diffusion learning rate: 0.001
2024-11-05 04:19:02,195 - INFO - [diffusion][Epoch 9464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:02,196 - INFO - [diffusion][Epoch 9465] Epoch 9466/12000
2024-11-05 04:19:06,252 - INFO - [diffusion][Epoch 9465] diffusion training Loss: 0.04613697715103626
2024-11-05 04:19:06,254 - INFO - [diffusion][Epoch 9465] diffusion learning rate: 0.001
2024-11-05 04:19:06,255 - INFO - [diffusion][Epoch 9465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:06,257 - INFO - [diffusion][Epoch 9466] Epoch 9467/12000
2024-11-05 04:19:10,410 - INFO - [diffusion][Epoch 9466] diffusion training Loss: 0.049767822958528996
2024-11-05 04:19:10,412 - INFO - [diffusion][Epoch 9466] diffusion learning rate: 0.001
2024-11-05 04:19:10,413 - INFO - [diffusion][Epoch 9466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:10,415 - INFO - [diffusion][Epoch 9467] Epoch 9468/12000
2024-11-05 04:19:14,340 - INFO - [diffusion][Epoch 9467] diffusion training Loss: 0.04777379706501961
2024-11-05 04:19:14,342 - INFO - [diffusion][Epoch 9467] diffusion learning rate: 0.001
2024-11-05 04:19:14,345 - INFO - [diffusion][Epoch 9467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:14,346 - INFO - [diffusion][Epoch 9468] Epoch 9469/12000
2024-11-05 04:19:18,435 - INFO - [diffusion][Epoch 9468] diffusion training Loss: 0.049338811077177525
2024-11-05 04:19:18,437 - INFO - [diffusion][Epoch 9468] diffusion learning rate: 0.001
2024-11-05 04:19:18,439 - INFO - [diffusion][Epoch 9468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:18,441 - INFO - [diffusion][Epoch 9469] Epoch 9470/12000
2024-11-05 04:19:22,636 - INFO - [diffusion][Epoch 9469] diffusion training Loss: 0.04749851208180189
2024-11-05 04:19:22,639 - INFO - [diffusion][Epoch 9469] diffusion learning rate: 0.001
2024-11-05 04:19:22,641 - INFO - [diffusion][Epoch 9469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:22,643 - INFO - [diffusion][Epoch 9470] Epoch 9471/12000
2024-11-05 04:19:26,702 - INFO - [diffusion][Epoch 9470] diffusion training Loss: 0.04692018870264292
2024-11-05 04:19:26,704 - INFO - [diffusion][Epoch 9470] diffusion learning rate: 0.001
2024-11-05 04:19:26,706 - INFO - [diffusion][Epoch 9470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:26,707 - INFO - [diffusion][Epoch 9471] Epoch 9472/12000
2024-11-05 04:19:30,949 - INFO - [diffusion][Epoch 9471] diffusion training Loss: 0.05146863032132387
2024-11-05 04:19:30,951 - INFO - [diffusion][Epoch 9471] diffusion learning rate: 0.001
2024-11-05 04:19:30,953 - INFO - [diffusion][Epoch 9471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:30,954 - INFO - [diffusion][Epoch 9472] Epoch 9473/12000
2024-11-05 04:19:35,090 - INFO - [diffusion][Epoch 9472] diffusion training Loss: 0.0506955636665225
2024-11-05 04:19:35,092 - INFO - [diffusion][Epoch 9472] diffusion learning rate: 0.001
2024-11-05 04:19:35,094 - INFO - [diffusion][Epoch 9472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:35,095 - INFO - [diffusion][Epoch 9473] Epoch 9474/12000
2024-11-05 04:19:39,272 - INFO - [diffusion][Epoch 9473] diffusion training Loss: 0.05025221407413483
2024-11-05 04:19:39,274 - INFO - [diffusion][Epoch 9473] diffusion learning rate: 0.001
2024-11-05 04:19:39,276 - INFO - [diffusion][Epoch 9473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:39,278 - INFO - [diffusion][Epoch 9474] Epoch 9475/12000
2024-11-05 04:19:43,335 - INFO - [diffusion][Epoch 9474] diffusion training Loss: 0.05317999515682459
2024-11-05 04:19:43,337 - INFO - [diffusion][Epoch 9474] diffusion learning rate: 0.001
2024-11-05 04:19:43,356 - INFO - [diffusion][Epoch 9474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:43,357 - INFO - [diffusion][Epoch 9475] Epoch 9476/12000
2024-11-05 04:19:47,365 - INFO - [diffusion][Epoch 9475] diffusion training Loss: 0.048688728362321854
2024-11-05 04:19:47,366 - INFO - [diffusion][Epoch 9475] diffusion learning rate: 0.001
2024-11-05 04:19:47,368 - INFO - [diffusion][Epoch 9475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:47,370 - INFO - [diffusion][Epoch 9476] Epoch 9477/12000
2024-11-05 04:19:51,460 - INFO - [diffusion][Epoch 9476] diffusion training Loss: 0.04859214276075363
2024-11-05 04:19:51,462 - INFO - [diffusion][Epoch 9476] diffusion learning rate: 0.001
2024-11-05 04:19:51,464 - INFO - [diffusion][Epoch 9476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:51,466 - INFO - [diffusion][Epoch 9477] Epoch 9478/12000
2024-11-05 04:19:55,613 - INFO - [diffusion][Epoch 9477] diffusion training Loss: 0.0504577960819006
2024-11-05 04:19:55,616 - INFO - [diffusion][Epoch 9477] diffusion learning rate: 0.001
2024-11-05 04:19:55,617 - INFO - [diffusion][Epoch 9477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:55,619 - INFO - [diffusion][Epoch 9478] Epoch 9479/12000
2024-11-05 04:19:59,743 - INFO - [diffusion][Epoch 9478] diffusion training Loss: 0.05085256230086088
2024-11-05 04:19:59,745 - INFO - [diffusion][Epoch 9478] diffusion learning rate: 0.001
2024-11-05 04:19:59,747 - INFO - [diffusion][Epoch 9478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:19:59,748 - INFO - [diffusion][Epoch 9479] Epoch 9480/12000
2024-11-05 04:20:03,836 - INFO - [diffusion][Epoch 9479] diffusion training Loss: 0.04566923342645168
2024-11-05 04:20:03,839 - INFO - [diffusion][Epoch 9479] diffusion learning rate: 0.001
2024-11-05 04:20:03,841 - INFO - [diffusion][Epoch 9479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:03,842 - INFO - [diffusion][Epoch 9480] Epoch 9481/12000
2024-11-05 04:20:07,792 - INFO - [diffusion][Epoch 9480] diffusion training Loss: 0.05054157134145498
2024-11-05 04:20:07,794 - INFO - [diffusion][Epoch 9480] diffusion learning rate: 0.001
2024-11-05 04:20:07,796 - INFO - [diffusion][Epoch 9480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:07,798 - INFO - [diffusion][Epoch 9481] Epoch 9482/12000
2024-11-05 04:20:11,941 - INFO - [diffusion][Epoch 9481] diffusion training Loss: 0.05266693979501724
2024-11-05 04:20:11,944 - INFO - [diffusion][Epoch 9481] diffusion learning rate: 0.001
2024-11-05 04:20:11,946 - INFO - [diffusion][Epoch 9481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:11,948 - INFO - [diffusion][Epoch 9482] Epoch 9483/12000
2024-11-05 04:20:16,109 - INFO - [diffusion][Epoch 9482] diffusion training Loss: 0.05256683100014925
2024-11-05 04:20:16,111 - INFO - [diffusion][Epoch 9482] diffusion learning rate: 0.001
2024-11-05 04:20:16,112 - INFO - [diffusion][Epoch 9482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:16,114 - INFO - [diffusion][Epoch 9483] Epoch 9484/12000
2024-11-05 04:20:20,264 - INFO - [diffusion][Epoch 9483] diffusion training Loss: 0.05335076246410608
2024-11-05 04:20:20,266 - INFO - [diffusion][Epoch 9483] diffusion learning rate: 0.001
2024-11-05 04:20:20,268 - INFO - [diffusion][Epoch 9483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:20,269 - INFO - [diffusion][Epoch 9484] Epoch 9485/12000
2024-11-05 04:20:24,409 - INFO - [diffusion][Epoch 9484] diffusion training Loss: 0.054856632836163044
2024-11-05 04:20:24,411 - INFO - [diffusion][Epoch 9484] diffusion learning rate: 0.001
2024-11-05 04:20:24,413 - INFO - [diffusion][Epoch 9484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:24,414 - INFO - [diffusion][Epoch 9485] Epoch 9486/12000
2024-11-05 04:20:28,390 - INFO - [diffusion][Epoch 9485] diffusion training Loss: 0.05409201141446829
2024-11-05 04:20:28,393 - INFO - [diffusion][Epoch 9485] diffusion learning rate: 0.001
2024-11-05 04:20:28,394 - INFO - [diffusion][Epoch 9485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:28,396 - INFO - [diffusion][Epoch 9486] Epoch 9487/12000
2024-11-05 04:20:32,773 - INFO - [diffusion][Epoch 9486] diffusion training Loss: 0.054376767948269844
2024-11-05 04:20:32,774 - INFO - [diffusion][Epoch 9486] diffusion learning rate: 0.001
2024-11-05 04:20:32,776 - INFO - [diffusion][Epoch 9486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:32,778 - INFO - [diffusion][Epoch 9487] Epoch 9488/12000
2024-11-05 04:20:36,963 - INFO - [diffusion][Epoch 9487] diffusion training Loss: 0.0527391005307436
2024-11-05 04:20:36,965 - INFO - [diffusion][Epoch 9487] diffusion learning rate: 0.001
2024-11-05 04:20:36,967 - INFO - [diffusion][Epoch 9487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:36,968 - INFO - [diffusion][Epoch 9488] Epoch 9489/12000
2024-11-05 04:20:40,989 - INFO - [diffusion][Epoch 9488] diffusion training Loss: 0.05363168753683567
2024-11-05 04:20:40,991 - INFO - [diffusion][Epoch 9488] diffusion learning rate: 0.001
2024-11-05 04:20:40,993 - INFO - [diffusion][Epoch 9488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:40,994 - INFO - [diffusion][Epoch 9489] Epoch 9490/12000
2024-11-05 04:20:45,025 - INFO - [diffusion][Epoch 9489] diffusion training Loss: 0.05619867891073227
2024-11-05 04:20:45,027 - INFO - [diffusion][Epoch 9489] diffusion learning rate: 0.001
2024-11-05 04:20:45,029 - INFO - [diffusion][Epoch 9489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:45,030 - INFO - [diffusion][Epoch 9490] Epoch 9491/12000
2024-11-05 04:20:49,137 - INFO - [diffusion][Epoch 9490] diffusion training Loss: 0.052382451482117176
2024-11-05 04:20:49,139 - INFO - [diffusion][Epoch 9490] diffusion learning rate: 0.001
2024-11-05 04:20:49,141 - INFO - [diffusion][Epoch 9490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:49,143 - INFO - [diffusion][Epoch 9491] Epoch 9492/12000
2024-11-05 04:20:53,140 - INFO - [diffusion][Epoch 9491] diffusion training Loss: 0.048573825508356094
2024-11-05 04:20:53,142 - INFO - [diffusion][Epoch 9491] diffusion learning rate: 0.001
2024-11-05 04:20:53,144 - INFO - [diffusion][Epoch 9491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:53,145 - INFO - [diffusion][Epoch 9492] Epoch 9493/12000
2024-11-05 04:20:57,195 - INFO - [diffusion][Epoch 9492] diffusion training Loss: 0.05090968497097492
2024-11-05 04:20:57,197 - INFO - [diffusion][Epoch 9492] diffusion learning rate: 0.001
2024-11-05 04:20:57,199 - INFO - [diffusion][Epoch 9492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:20:57,200 - INFO - [diffusion][Epoch 9493] Epoch 9494/12000
2024-11-05 04:21:01,231 - INFO - [diffusion][Epoch 9493] diffusion training Loss: 0.052547882311046124
2024-11-05 04:21:01,238 - INFO - [diffusion][Epoch 9493] diffusion learning rate: 0.001
2024-11-05 04:21:01,240 - INFO - [diffusion][Epoch 9493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:01,241 - INFO - [diffusion][Epoch 9494] Epoch 9495/12000
2024-11-05 04:21:05,143 - INFO - [diffusion][Epoch 9494] diffusion training Loss: 0.04737362451851368
2024-11-05 04:21:05,144 - INFO - [diffusion][Epoch 9494] diffusion learning rate: 0.001
2024-11-05 04:21:05,146 - INFO - [diffusion][Epoch 9494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:05,147 - INFO - [diffusion][Epoch 9495] Epoch 9496/12000
2024-11-05 04:21:09,176 - INFO - [diffusion][Epoch 9495] diffusion training Loss: 0.04882596433162689
2024-11-05 04:21:09,177 - INFO - [diffusion][Epoch 9495] diffusion learning rate: 0.001
2024-11-05 04:21:09,179 - INFO - [diffusion][Epoch 9495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:09,180 - INFO - [diffusion][Epoch 9496] Epoch 9497/12000
2024-11-05 04:21:13,208 - INFO - [diffusion][Epoch 9496] diffusion training Loss: 0.04885317198932171
2024-11-05 04:21:13,210 - INFO - [diffusion][Epoch 9496] diffusion learning rate: 0.001
2024-11-05 04:21:13,212 - INFO - [diffusion][Epoch 9496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:13,213 - INFO - [diffusion][Epoch 9497] Epoch 9498/12000
2024-11-05 04:21:17,338 - INFO - [diffusion][Epoch 9497] diffusion training Loss: 0.04651395324617624
2024-11-05 04:21:17,340 - INFO - [diffusion][Epoch 9497] diffusion learning rate: 0.001
2024-11-05 04:21:17,342 - INFO - [diffusion][Epoch 9497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:17,344 - INFO - [diffusion][Epoch 9498] Epoch 9499/12000
2024-11-05 04:21:21,423 - INFO - [diffusion][Epoch 9498] diffusion training Loss: 0.04879541415721178
2024-11-05 04:21:21,425 - INFO - [diffusion][Epoch 9498] diffusion learning rate: 0.001
2024-11-05 04:21:21,427 - INFO - [diffusion][Epoch 9498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:21,428 - INFO - [diffusion][Epoch 9499] Epoch 9500/12000
2024-11-05 04:21:25,376 - INFO - [diffusion][Epoch 9499] diffusion training Loss: 0.05478145368397236
2024-11-05 04:21:25,378 - INFO - [diffusion][Epoch 9499] diffusion learning rate: 0.001
2024-11-05 04:21:25,380 - INFO - [diffusion][Epoch 9499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:25,381 - INFO - [diffusion][Epoch 9500] Epoch 9501/12000
2024-11-05 04:21:29,298 - INFO - [diffusion][Epoch 9500] diffusion training Loss: 0.04901602491736412
2024-11-05 04:21:29,300 - INFO - [diffusion][Epoch 9500] diffusion learning rate: 0.001
2024-11-05 04:21:29,302 - INFO - [diffusion][Epoch 9500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:29,303 - INFO - [diffusion][Epoch 9501] Epoch 9502/12000
2024-11-05 04:21:33,374 - INFO - [diffusion][Epoch 9501] diffusion training Loss: 0.050457607954740524
2024-11-05 04:21:33,376 - INFO - [diffusion][Epoch 9501] diffusion learning rate: 0.001
2024-11-05 04:21:33,378 - INFO - [diffusion][Epoch 9501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:33,379 - INFO - [diffusion][Epoch 9502] Epoch 9503/12000
2024-11-05 04:21:37,476 - INFO - [diffusion][Epoch 9502] diffusion training Loss: 0.05061327386647463
2024-11-05 04:21:37,478 - INFO - [diffusion][Epoch 9502] diffusion learning rate: 0.001
2024-11-05 04:21:37,479 - INFO - [diffusion][Epoch 9502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:37,481 - INFO - [diffusion][Epoch 9503] Epoch 9504/12000
2024-11-05 04:21:41,651 - INFO - [diffusion][Epoch 9503] diffusion training Loss: 0.04677671380341053
2024-11-05 04:21:41,653 - INFO - [diffusion][Epoch 9503] diffusion learning rate: 0.001
2024-11-05 04:21:41,655 - INFO - [diffusion][Epoch 9503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:41,656 - INFO - [diffusion][Epoch 9504] Epoch 9505/12000
2024-11-05 04:21:45,791 - INFO - [diffusion][Epoch 9504] diffusion training Loss: 0.04919544141739607
2024-11-05 04:21:45,794 - INFO - [diffusion][Epoch 9504] diffusion learning rate: 0.001
2024-11-05 04:21:45,796 - INFO - [diffusion][Epoch 9504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:45,798 - INFO - [diffusion][Epoch 9505] Epoch 9506/12000
2024-11-05 04:21:49,938 - INFO - [diffusion][Epoch 9505] diffusion training Loss: 0.047292119823396206
2024-11-05 04:21:49,940 - INFO - [diffusion][Epoch 9505] diffusion learning rate: 0.001
2024-11-05 04:21:49,942 - INFO - [diffusion][Epoch 9505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:49,943 - INFO - [diffusion][Epoch 9506] Epoch 9507/12000
2024-11-05 04:21:54,045 - INFO - [diffusion][Epoch 9506] diffusion training Loss: 0.05336153320968151
2024-11-05 04:21:54,047 - INFO - [diffusion][Epoch 9506] diffusion learning rate: 0.001
2024-11-05 04:21:54,048 - INFO - [diffusion][Epoch 9506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:54,049 - INFO - [diffusion][Epoch 9507] Epoch 9508/12000
2024-11-05 04:21:57,957 - INFO - [diffusion][Epoch 9507] diffusion training Loss: 0.0548963388428092
2024-11-05 04:21:57,961 - INFO - [diffusion][Epoch 9507] diffusion learning rate: 0.001
2024-11-05 04:21:57,963 - INFO - [diffusion][Epoch 9507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:21:57,965 - INFO - [diffusion][Epoch 9508] Epoch 9509/12000
2024-11-05 04:22:02,605 - INFO - [diffusion][Epoch 9508] diffusion training Loss: 0.05231716204434633
2024-11-05 04:22:02,608 - INFO - [diffusion][Epoch 9508] diffusion learning rate: 0.001
2024-11-05 04:22:02,609 - INFO - [diffusion][Epoch 9508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:02,611 - INFO - [diffusion][Epoch 9509] Epoch 9510/12000
2024-11-05 04:22:06,620 - INFO - [diffusion][Epoch 9509] diffusion training Loss: 0.053612373769283295
2024-11-05 04:22:06,623 - INFO - [diffusion][Epoch 9509] diffusion learning rate: 0.001
2024-11-05 04:22:06,625 - INFO - [diffusion][Epoch 9509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:06,627 - INFO - [diffusion][Epoch 9510] Epoch 9511/12000
2024-11-05 04:22:10,547 - INFO - [diffusion][Epoch 9510] diffusion training Loss: 0.05062579736113548
2024-11-05 04:22:10,549 - INFO - [diffusion][Epoch 9510] diffusion learning rate: 0.001
2024-11-05 04:22:10,551 - INFO - [diffusion][Epoch 9510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:10,552 - INFO - [diffusion][Epoch 9511] Epoch 9512/12000
2024-11-05 04:22:14,584 - INFO - [diffusion][Epoch 9511] diffusion training Loss: 0.04935647640377283
2024-11-05 04:22:14,586 - INFO - [diffusion][Epoch 9511] diffusion learning rate: 0.001
2024-11-05 04:22:14,588 - INFO - [diffusion][Epoch 9511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:14,589 - INFO - [diffusion][Epoch 9512] Epoch 9513/12000
2024-11-05 04:22:18,529 - INFO - [diffusion][Epoch 9512] diffusion training Loss: 0.04881247133016586
2024-11-05 04:22:18,531 - INFO - [diffusion][Epoch 9512] diffusion learning rate: 0.001
2024-11-05 04:22:18,532 - INFO - [diffusion][Epoch 9512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:18,534 - INFO - [diffusion][Epoch 9513] Epoch 9514/12000
2024-11-05 04:22:22,639 - INFO - [diffusion][Epoch 9513] diffusion training Loss: 0.05440641660243273
2024-11-05 04:22:22,641 - INFO - [diffusion][Epoch 9513] diffusion learning rate: 0.001
2024-11-05 04:22:22,643 - INFO - [diffusion][Epoch 9513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:22,644 - INFO - [diffusion][Epoch 9514] Epoch 9515/12000
2024-11-05 04:22:26,676 - INFO - [diffusion][Epoch 9514] diffusion training Loss: 0.04906936455518007
2024-11-05 04:22:26,678 - INFO - [diffusion][Epoch 9514] diffusion learning rate: 0.001
2024-11-05 04:22:26,679 - INFO - [diffusion][Epoch 9514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:26,681 - INFO - [diffusion][Epoch 9515] Epoch 9516/12000
2024-11-05 04:22:30,582 - INFO - [diffusion][Epoch 9515] diffusion training Loss: 0.04801038559526205
2024-11-05 04:22:30,584 - INFO - [diffusion][Epoch 9515] diffusion learning rate: 0.001
2024-11-05 04:22:30,586 - INFO - [diffusion][Epoch 9515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:30,587 - INFO - [diffusion][Epoch 9516] Epoch 9517/12000
2024-11-05 04:22:34,469 - INFO - [diffusion][Epoch 9516] diffusion training Loss: 0.0506806680932641
2024-11-05 04:22:34,471 - INFO - [diffusion][Epoch 9516] diffusion learning rate: 0.001
2024-11-05 04:22:34,473 - INFO - [diffusion][Epoch 9516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:34,474 - INFO - [diffusion][Epoch 9517] Epoch 9518/12000
2024-11-05 04:22:38,563 - INFO - [diffusion][Epoch 9517] diffusion training Loss: 0.04840059392154217
2024-11-05 04:22:38,565 - INFO - [diffusion][Epoch 9517] diffusion learning rate: 0.001
2024-11-05 04:22:38,567 - INFO - [diffusion][Epoch 9517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:38,568 - INFO - [diffusion][Epoch 9518] Epoch 9519/12000
2024-11-05 04:22:42,671 - INFO - [diffusion][Epoch 9518] diffusion training Loss: 0.05131803825497627
2024-11-05 04:22:42,673 - INFO - [diffusion][Epoch 9518] diffusion learning rate: 0.001
2024-11-05 04:22:42,674 - INFO - [diffusion][Epoch 9518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:42,676 - INFO - [diffusion][Epoch 9519] Epoch 9520/12000
2024-11-05 04:22:46,636 - INFO - [diffusion][Epoch 9519] diffusion training Loss: 0.05048934742808342
2024-11-05 04:22:46,638 - INFO - [diffusion][Epoch 9519] diffusion learning rate: 0.001
2024-11-05 04:22:46,640 - INFO - [diffusion][Epoch 9519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:46,642 - INFO - [diffusion][Epoch 9520] Epoch 9521/12000
2024-11-05 04:22:50,643 - INFO - [diffusion][Epoch 9520] diffusion training Loss: 0.051804968155920506
2024-11-05 04:22:50,645 - INFO - [diffusion][Epoch 9520] diffusion learning rate: 0.001
2024-11-05 04:22:50,647 - INFO - [diffusion][Epoch 9520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:50,649 - INFO - [diffusion][Epoch 9521] Epoch 9522/12000
2024-11-05 04:22:54,680 - INFO - [diffusion][Epoch 9521] diffusion training Loss: 0.05287287849932909
2024-11-05 04:22:54,682 - INFO - [diffusion][Epoch 9521] diffusion learning rate: 0.001
2024-11-05 04:22:54,684 - INFO - [diffusion][Epoch 9521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:54,686 - INFO - [diffusion][Epoch 9522] Epoch 9523/12000
2024-11-05 04:22:58,855 - INFO - [diffusion][Epoch 9522] diffusion training Loss: 0.048493290320038795
2024-11-05 04:22:58,857 - INFO - [diffusion][Epoch 9522] diffusion learning rate: 0.001
2024-11-05 04:22:58,859 - INFO - [diffusion][Epoch 9522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:22:58,860 - INFO - [diffusion][Epoch 9523] Epoch 9524/12000
2024-11-05 04:23:02,956 - INFO - [diffusion][Epoch 9523] diffusion training Loss: 0.05006752908229828
2024-11-05 04:23:02,960 - INFO - [diffusion][Epoch 9523] diffusion learning rate: 0.001
2024-11-05 04:23:02,962 - INFO - [diffusion][Epoch 9523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:02,964 - INFO - [diffusion][Epoch 9524] Epoch 9525/12000
2024-11-05 04:23:06,887 - INFO - [diffusion][Epoch 9524] diffusion training Loss: 0.05027995724231005
2024-11-05 04:23:06,890 - INFO - [diffusion][Epoch 9524] diffusion learning rate: 0.001
2024-11-05 04:23:06,892 - INFO - [diffusion][Epoch 9524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:06,893 - INFO - [diffusion][Epoch 9525] Epoch 9526/12000
2024-11-05 04:23:10,995 - INFO - [diffusion][Epoch 9525] diffusion training Loss: 0.05333025101572275
2024-11-05 04:23:10,997 - INFO - [diffusion][Epoch 9525] diffusion learning rate: 0.001
2024-11-05 04:23:10,998 - INFO - [diffusion][Epoch 9525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:10,999 - INFO - [diffusion][Epoch 9526] Epoch 9527/12000
2024-11-05 04:23:15,023 - INFO - [diffusion][Epoch 9526] diffusion training Loss: 0.04578283615410328
2024-11-05 04:23:15,025 - INFO - [diffusion][Epoch 9526] diffusion learning rate: 0.001
2024-11-05 04:23:15,026 - INFO - [diffusion][Epoch 9526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:15,027 - INFO - [diffusion][Epoch 9527] Epoch 9528/12000
2024-11-05 04:23:19,118 - INFO - [diffusion][Epoch 9527] diffusion training Loss: 0.04877919703722
2024-11-05 04:23:19,120 - INFO - [diffusion][Epoch 9527] diffusion learning rate: 0.001
2024-11-05 04:23:19,122 - INFO - [diffusion][Epoch 9527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:19,124 - INFO - [diffusion][Epoch 9528] Epoch 9529/12000
2024-11-05 04:23:23,254 - INFO - [diffusion][Epoch 9528] diffusion training Loss: 0.04906818177551031
2024-11-05 04:23:23,256 - INFO - [diffusion][Epoch 9528] diffusion learning rate: 0.001
2024-11-05 04:23:23,257 - INFO - [diffusion][Epoch 9528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:23,259 - INFO - [diffusion][Epoch 9529] Epoch 9530/12000
2024-11-05 04:23:27,321 - INFO - [diffusion][Epoch 9529] diffusion training Loss: 0.052017525769770145
2024-11-05 04:23:27,323 - INFO - [diffusion][Epoch 9529] diffusion learning rate: 0.001
2024-11-05 04:23:27,325 - INFO - [diffusion][Epoch 9529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:27,326 - INFO - [diffusion][Epoch 9530] Epoch 9531/12000
2024-11-05 04:23:31,290 - INFO - [diffusion][Epoch 9530] diffusion training Loss: 0.052689661271870136
2024-11-05 04:23:31,294 - INFO - [diffusion][Epoch 9530] diffusion learning rate: 0.001
2024-11-05 04:23:31,295 - INFO - [diffusion][Epoch 9530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:31,297 - INFO - [diffusion][Epoch 9531] Epoch 9532/12000
2024-11-05 04:23:35,389 - INFO - [diffusion][Epoch 9531] diffusion training Loss: 0.051718080416321754
2024-11-05 04:23:35,391 - INFO - [diffusion][Epoch 9531] diffusion learning rate: 0.001
2024-11-05 04:23:35,393 - INFO - [diffusion][Epoch 9531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:35,394 - INFO - [diffusion][Epoch 9532] Epoch 9533/12000
2024-11-05 04:23:39,427 - INFO - [diffusion][Epoch 9532] diffusion training Loss: 0.04906108696013689
2024-11-05 04:23:39,429 - INFO - [diffusion][Epoch 9532] diffusion learning rate: 0.001
2024-11-05 04:23:39,431 - INFO - [diffusion][Epoch 9532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:39,432 - INFO - [diffusion][Epoch 9533] Epoch 9534/12000
2024-11-05 04:23:43,519 - INFO - [diffusion][Epoch 9533] diffusion training Loss: 0.050045134499669075
2024-11-05 04:23:43,521 - INFO - [diffusion][Epoch 9533] diffusion learning rate: 0.001
2024-11-05 04:23:43,522 - INFO - [diffusion][Epoch 9533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:43,524 - INFO - [diffusion][Epoch 9534] Epoch 9535/12000
2024-11-05 04:23:47,606 - INFO - [diffusion][Epoch 9534] diffusion training Loss: 0.04918426275253296
2024-11-05 04:23:47,608 - INFO - [diffusion][Epoch 9534] diffusion learning rate: 0.001
2024-11-05 04:23:47,610 - INFO - [diffusion][Epoch 9534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:47,611 - INFO - [diffusion][Epoch 9535] Epoch 9536/12000
2024-11-05 04:23:51,535 - INFO - [diffusion][Epoch 9535] diffusion training Loss: 0.049928407184779644
2024-11-05 04:23:51,537 - INFO - [diffusion][Epoch 9535] diffusion learning rate: 0.001
2024-11-05 04:23:51,539 - INFO - [diffusion][Epoch 9535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:51,540 - INFO - [diffusion][Epoch 9536] Epoch 9537/12000
2024-11-05 04:23:55,679 - INFO - [diffusion][Epoch 9536] diffusion training Loss: 0.05117592215538025
2024-11-05 04:23:55,681 - INFO - [diffusion][Epoch 9536] diffusion learning rate: 0.001
2024-11-05 04:23:55,683 - INFO - [diffusion][Epoch 9536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:55,684 - INFO - [diffusion][Epoch 9537] Epoch 9538/12000
2024-11-05 04:23:59,786 - INFO - [diffusion][Epoch 9537] diffusion training Loss: 0.042636074125766754
2024-11-05 04:23:59,788 - INFO - [diffusion][Epoch 9537] diffusion learning rate: 0.001
2024-11-05 04:23:59,790 - INFO - [diffusion][Epoch 9537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:23:59,791 - INFO - [diffusion][Epoch 9538] Epoch 9539/12000
2024-11-05 04:24:03,826 - INFO - [diffusion][Epoch 9538] diffusion training Loss: 0.0508381687104702
2024-11-05 04:24:03,829 - INFO - [diffusion][Epoch 9538] diffusion learning rate: 0.001
2024-11-05 04:24:03,831 - INFO - [diffusion][Epoch 9538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:03,832 - INFO - [diffusion][Epoch 9539] Epoch 9540/12000
2024-11-05 04:24:07,861 - INFO - [diffusion][Epoch 9539] diffusion training Loss: 0.05049203895032406
2024-11-05 04:24:07,863 - INFO - [diffusion][Epoch 9539] diffusion learning rate: 0.001
2024-11-05 04:24:07,865 - INFO - [diffusion][Epoch 9539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:07,866 - INFO - [diffusion][Epoch 9540] Epoch 9541/12000
2024-11-05 04:24:11,993 - INFO - [diffusion][Epoch 9540] diffusion training Loss: 0.04483554605394602
2024-11-05 04:24:11,995 - INFO - [diffusion][Epoch 9540] diffusion learning rate: 0.001
2024-11-05 04:24:11,997 - INFO - [diffusion][Epoch 9540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:11,998 - INFO - [diffusion][Epoch 9541] Epoch 9542/12000
2024-11-05 04:24:15,906 - INFO - [diffusion][Epoch 9541] diffusion training Loss: 0.045146224088966846
2024-11-05 04:24:15,907 - INFO - [diffusion][Epoch 9541] diffusion learning rate: 0.001
2024-11-05 04:24:15,909 - INFO - [diffusion][Epoch 9541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:15,910 - INFO - [diffusion][Epoch 9542] Epoch 9543/12000
2024-11-05 04:24:19,757 - INFO - [diffusion][Epoch 9542] diffusion training Loss: 0.052948652766644955
2024-11-05 04:24:19,759 - INFO - [diffusion][Epoch 9542] diffusion learning rate: 0.001
2024-11-05 04:24:19,761 - INFO - [diffusion][Epoch 9542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:19,763 - INFO - [diffusion][Epoch 9543] Epoch 9544/12000
2024-11-05 04:24:23,925 - INFO - [diffusion][Epoch 9543] diffusion training Loss: 0.05169456731528044
2024-11-05 04:24:23,927 - INFO - [diffusion][Epoch 9543] diffusion learning rate: 0.001
2024-11-05 04:24:23,929 - INFO - [diffusion][Epoch 9543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:23,932 - INFO - [diffusion][Epoch 9544] Epoch 9545/12000
2024-11-05 04:24:28,070 - INFO - [diffusion][Epoch 9544] diffusion training Loss: 0.04834602214396
2024-11-05 04:24:28,072 - INFO - [diffusion][Epoch 9544] diffusion learning rate: 0.001
2024-11-05 04:24:28,100 - INFO - [diffusion][Epoch 9544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:28,101 - INFO - [diffusion][Epoch 9545] Epoch 9546/12000
2024-11-05 04:24:32,114 - INFO - [diffusion][Epoch 9545] diffusion training Loss: 0.04782767966389656
2024-11-05 04:24:32,117 - INFO - [diffusion][Epoch 9545] diffusion learning rate: 0.001
2024-11-05 04:24:32,119 - INFO - [diffusion][Epoch 9545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:32,120 - INFO - [diffusion][Epoch 9546] Epoch 9547/12000
2024-11-05 04:24:36,170 - INFO - [diffusion][Epoch 9546] diffusion training Loss: 0.05255388841032982
2024-11-05 04:24:36,172 - INFO - [diffusion][Epoch 9546] diffusion learning rate: 0.001
2024-11-05 04:24:36,174 - INFO - [diffusion][Epoch 9546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:36,176 - INFO - [diffusion][Epoch 9547] Epoch 9548/12000
2024-11-05 04:24:40,229 - INFO - [diffusion][Epoch 9547] diffusion training Loss: 0.048387257382273674
2024-11-05 04:24:40,231 - INFO - [diffusion][Epoch 9547] diffusion learning rate: 0.001
2024-11-05 04:24:40,233 - INFO - [diffusion][Epoch 9547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:40,234 - INFO - [diffusion][Epoch 9548] Epoch 9549/12000
2024-11-05 04:24:44,246 - INFO - [diffusion][Epoch 9548] diffusion training Loss: 0.05118168890476227
2024-11-05 04:24:44,248 - INFO - [diffusion][Epoch 9548] diffusion learning rate: 0.001
2024-11-05 04:24:44,282 - INFO - [diffusion][Epoch 9548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:44,283 - INFO - [diffusion][Epoch 9549] Epoch 9550/12000
2024-11-05 04:24:48,371 - INFO - [diffusion][Epoch 9549] diffusion training Loss: 0.04724127799272537
2024-11-05 04:24:48,373 - INFO - [diffusion][Epoch 9549] diffusion learning rate: 0.001
2024-11-05 04:24:48,375 - INFO - [diffusion][Epoch 9549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:48,376 - INFO - [diffusion][Epoch 9550] Epoch 9551/12000
2024-11-05 04:24:52,548 - INFO - [diffusion][Epoch 9550] diffusion training Loss: 0.05164326820522547
2024-11-05 04:24:52,550 - INFO - [diffusion][Epoch 9550] diffusion learning rate: 0.001
2024-11-05 04:24:52,553 - INFO - [diffusion][Epoch 9550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:52,555 - INFO - [diffusion][Epoch 9551] Epoch 9552/12000
2024-11-05 04:24:56,578 - INFO - [diffusion][Epoch 9551] diffusion training Loss: 0.047508412040770054
2024-11-05 04:24:56,580 - INFO - [diffusion][Epoch 9551] diffusion learning rate: 0.001
2024-11-05 04:24:56,582 - INFO - [diffusion][Epoch 9551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:24:56,583 - INFO - [diffusion][Epoch 9552] Epoch 9553/12000
2024-11-05 04:25:00,683 - INFO - [diffusion][Epoch 9552] diffusion training Loss: 0.050261519849300385
2024-11-05 04:25:00,685 - INFO - [diffusion][Epoch 9552] diffusion learning rate: 0.001
2024-11-05 04:25:00,687 - INFO - [diffusion][Epoch 9552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:00,688 - INFO - [diffusion][Epoch 9553] Epoch 9554/12000
2024-11-05 04:25:04,749 - INFO - [diffusion][Epoch 9553] diffusion training Loss: 0.050027661956846714
2024-11-05 04:25:04,752 - INFO - [diffusion][Epoch 9553] diffusion learning rate: 0.001
2024-11-05 04:25:04,754 - INFO - [diffusion][Epoch 9553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:04,755 - INFO - [diffusion][Epoch 9554] Epoch 9555/12000
2024-11-05 04:25:08,735 - INFO - [diffusion][Epoch 9554] diffusion training Loss: 0.05107137095183134
2024-11-05 04:25:08,739 - INFO - [diffusion][Epoch 9554] diffusion learning rate: 0.001
2024-11-05 04:25:08,742 - INFO - [diffusion][Epoch 9554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:08,744 - INFO - [diffusion][Epoch 9555] Epoch 9556/12000
2024-11-05 04:25:12,626 - INFO - [diffusion][Epoch 9555] diffusion training Loss: 0.04855176620185375
2024-11-05 04:25:12,798 - INFO - [diffusion][Epoch 9555] diffusion learning rate: 0.001
2024-11-05 04:25:12,800 - INFO - [diffusion][Epoch 9555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:12,801 - INFO - [diffusion][Epoch 9556] Epoch 9557/12000
2024-11-05 04:25:16,827 - INFO - [diffusion][Epoch 9556] diffusion training Loss: 0.0481233187019825
2024-11-05 04:25:16,829 - INFO - [diffusion][Epoch 9556] diffusion learning rate: 0.001
2024-11-05 04:25:16,830 - INFO - [diffusion][Epoch 9556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:16,832 - INFO - [diffusion][Epoch 9557] Epoch 9558/12000
2024-11-05 04:25:20,996 - INFO - [diffusion][Epoch 9557] diffusion training Loss: 0.044481550343334675
2024-11-05 04:25:20,998 - INFO - [diffusion][Epoch 9557] diffusion learning rate: 0.001
2024-11-05 04:25:21,000 - INFO - [diffusion][Epoch 9557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:21,001 - INFO - [diffusion][Epoch 9558] Epoch 9559/12000
2024-11-05 04:25:25,061 - INFO - [diffusion][Epoch 9558] diffusion training Loss: 0.05588190536946058
2024-11-05 04:25:25,063 - INFO - [diffusion][Epoch 9558] diffusion learning rate: 0.001
2024-11-05 04:25:25,065 - INFO - [diffusion][Epoch 9558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:25,067 - INFO - [diffusion][Epoch 9559] Epoch 9560/12000
2024-11-05 04:25:29,154 - INFO - [diffusion][Epoch 9559] diffusion training Loss: 0.05017689522355795
2024-11-05 04:25:29,156 - INFO - [diffusion][Epoch 9559] diffusion learning rate: 0.001
2024-11-05 04:25:29,159 - INFO - [diffusion][Epoch 9559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:29,160 - INFO - [diffusion][Epoch 9560] Epoch 9561/12000
2024-11-05 04:25:33,251 - INFO - [diffusion][Epoch 9560] diffusion training Loss: 0.048189677298069
2024-11-05 04:25:33,254 - INFO - [diffusion][Epoch 9560] diffusion learning rate: 0.001
2024-11-05 04:25:33,256 - INFO - [diffusion][Epoch 9560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:33,257 - INFO - [diffusion][Epoch 9561] Epoch 9562/12000
2024-11-05 04:25:37,232 - INFO - [diffusion][Epoch 9561] diffusion training Loss: 0.051456687040627
2024-11-05 04:25:37,234 - INFO - [diffusion][Epoch 9561] diffusion learning rate: 0.001
2024-11-05 04:25:37,236 - INFO - [diffusion][Epoch 9561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:37,237 - INFO - [diffusion][Epoch 9562] Epoch 9563/12000
2024-11-05 04:25:41,183 - INFO - [diffusion][Epoch 9562] diffusion training Loss: 0.05846607591956854
2024-11-05 04:25:41,185 - INFO - [diffusion][Epoch 9562] diffusion learning rate: 0.001
2024-11-05 04:25:41,187 - INFO - [diffusion][Epoch 9562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:41,188 - INFO - [diffusion][Epoch 9563] Epoch 9564/12000
2024-11-05 04:25:45,178 - INFO - [diffusion][Epoch 9563] diffusion training Loss: 0.048790755681693554
2024-11-05 04:25:45,180 - INFO - [diffusion][Epoch 9563] diffusion learning rate: 0.001
2024-11-05 04:25:45,208 - INFO - [diffusion][Epoch 9563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:45,209 - INFO - [diffusion][Epoch 9564] Epoch 9565/12000
2024-11-05 04:25:49,327 - INFO - [diffusion][Epoch 9564] diffusion training Loss: 0.04958089999854565
2024-11-05 04:25:49,329 - INFO - [diffusion][Epoch 9564] diffusion learning rate: 0.001
2024-11-05 04:25:49,331 - INFO - [diffusion][Epoch 9564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:49,332 - INFO - [diffusion][Epoch 9565] Epoch 9566/12000
2024-11-05 04:25:53,476 - INFO - [diffusion][Epoch 9565] diffusion training Loss: 0.04924940504133701
2024-11-05 04:25:53,478 - INFO - [diffusion][Epoch 9565] diffusion learning rate: 0.001
2024-11-05 04:25:53,480 - INFO - [diffusion][Epoch 9565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:53,481 - INFO - [diffusion][Epoch 9566] Epoch 9567/12000
2024-11-05 04:25:57,604 - INFO - [diffusion][Epoch 9566] diffusion training Loss: 0.05190757196396589
2024-11-05 04:25:57,606 - INFO - [diffusion][Epoch 9566] diffusion learning rate: 0.001
2024-11-05 04:25:57,608 - INFO - [diffusion][Epoch 9566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:25:57,609 - INFO - [diffusion][Epoch 9567] Epoch 9568/12000
2024-11-05 04:26:01,728 - INFO - [diffusion][Epoch 9567] diffusion training Loss: 0.049613616429269314
2024-11-05 04:26:01,730 - INFO - [diffusion][Epoch 9567] diffusion learning rate: 0.001
2024-11-05 04:26:01,732 - INFO - [diffusion][Epoch 9567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:01,733 - INFO - [diffusion][Epoch 9568] Epoch 9569/12000
2024-11-05 04:26:05,874 - INFO - [diffusion][Epoch 9568] diffusion training Loss: 0.051137564703822136
2024-11-05 04:26:05,877 - INFO - [diffusion][Epoch 9568] diffusion learning rate: 0.001
2024-11-05 04:26:05,879 - INFO - [diffusion][Epoch 9568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:05,880 - INFO - [diffusion][Epoch 9569] Epoch 9570/12000
2024-11-05 04:26:09,979 - INFO - [diffusion][Epoch 9569] diffusion training Loss: 0.048619676381349564
2024-11-05 04:26:09,981 - INFO - [diffusion][Epoch 9569] diffusion learning rate: 0.001
2024-11-05 04:26:09,983 - INFO - [diffusion][Epoch 9569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:09,985 - INFO - [diffusion][Epoch 9570] Epoch 9571/12000
2024-11-05 04:26:14,077 - INFO - [diffusion][Epoch 9570] diffusion training Loss: 0.05201875977218151
2024-11-05 04:26:14,079 - INFO - [diffusion][Epoch 9570] diffusion learning rate: 0.001
2024-11-05 04:26:14,082 - INFO - [diffusion][Epoch 9570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:14,084 - INFO - [diffusion][Epoch 9571] Epoch 9572/12000
2024-11-05 04:26:18,044 - INFO - [diffusion][Epoch 9571] diffusion training Loss: 0.05170580465346575
2024-11-05 04:26:18,046 - INFO - [diffusion][Epoch 9571] diffusion learning rate: 0.001
2024-11-05 04:26:18,047 - INFO - [diffusion][Epoch 9571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:18,049 - INFO - [diffusion][Epoch 9572] Epoch 9573/12000
2024-11-05 04:26:22,289 - INFO - [diffusion][Epoch 9572] diffusion training Loss: 0.05222446843981743
2024-11-05 04:26:22,291 - INFO - [diffusion][Epoch 9572] diffusion learning rate: 0.001
2024-11-05 04:26:22,292 - INFO - [diffusion][Epoch 9572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:22,293 - INFO - [diffusion][Epoch 9573] Epoch 9574/12000
2024-11-05 04:26:26,350 - INFO - [diffusion][Epoch 9573] diffusion training Loss: 0.052389999851584435
2024-11-05 04:26:26,352 - INFO - [diffusion][Epoch 9573] diffusion learning rate: 0.001
2024-11-05 04:26:26,382 - INFO - [diffusion][Epoch 9573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:26,383 - INFO - [diffusion][Epoch 9574] Epoch 9575/12000
2024-11-05 04:26:30,324 - INFO - [diffusion][Epoch 9574] diffusion training Loss: 0.05184496100991964
2024-11-05 04:26:30,325 - INFO - [diffusion][Epoch 9574] diffusion learning rate: 0.001
2024-11-05 04:26:30,327 - INFO - [diffusion][Epoch 9574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:30,329 - INFO - [diffusion][Epoch 9575] Epoch 9576/12000
2024-11-05 04:26:34,345 - INFO - [diffusion][Epoch 9575] diffusion training Loss: 0.051283848471939564
2024-11-05 04:26:34,348 - INFO - [diffusion][Epoch 9575] diffusion learning rate: 0.001
2024-11-05 04:26:34,350 - INFO - [diffusion][Epoch 9575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:34,351 - INFO - [diffusion][Epoch 9576] Epoch 9577/12000
2024-11-05 04:26:38,373 - INFO - [diffusion][Epoch 9576] diffusion training Loss: 0.051529779098927975
2024-11-05 04:26:38,375 - INFO - [diffusion][Epoch 9576] diffusion learning rate: 0.001
2024-11-05 04:26:38,377 - INFO - [diffusion][Epoch 9576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:38,378 - INFO - [diffusion][Epoch 9577] Epoch 9578/12000
2024-11-05 04:26:42,402 - INFO - [diffusion][Epoch 9577] diffusion training Loss: 0.04787831753492355
2024-11-05 04:26:42,404 - INFO - [diffusion][Epoch 9577] diffusion learning rate: 0.001
2024-11-05 04:26:42,406 - INFO - [diffusion][Epoch 9577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:42,407 - INFO - [diffusion][Epoch 9578] Epoch 9579/12000
2024-11-05 04:26:46,265 - INFO - [diffusion][Epoch 9578] diffusion training Loss: 0.05098738893866539
2024-11-05 04:26:46,267 - INFO - [diffusion][Epoch 9578] diffusion learning rate: 0.001
2024-11-05 04:26:46,268 - INFO - [diffusion][Epoch 9578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:46,269 - INFO - [diffusion][Epoch 9579] Epoch 9580/12000
2024-11-05 04:26:50,213 - INFO - [diffusion][Epoch 9579] diffusion training Loss: 0.04788981191813946
2024-11-05 04:26:50,214 - INFO - [diffusion][Epoch 9579] diffusion learning rate: 0.001
2024-11-05 04:26:50,216 - INFO - [diffusion][Epoch 9579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:50,217 - INFO - [diffusion][Epoch 9580] Epoch 9581/12000
2024-11-05 04:26:54,234 - INFO - [diffusion][Epoch 9580] diffusion training Loss: 0.04737513232976198
2024-11-05 04:26:54,236 - INFO - [diffusion][Epoch 9580] diffusion learning rate: 0.001
2024-11-05 04:26:54,265 - INFO - [diffusion][Epoch 9580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:54,266 - INFO - [diffusion][Epoch 9581] Epoch 9582/12000
2024-11-05 04:26:58,316 - INFO - [diffusion][Epoch 9581] diffusion training Loss: 0.054930408485233784
2024-11-05 04:26:58,318 - INFO - [diffusion][Epoch 9581] diffusion learning rate: 0.001
2024-11-05 04:26:58,320 - INFO - [diffusion][Epoch 9581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:26:58,321 - INFO - [diffusion][Epoch 9582] Epoch 9583/12000
2024-11-05 04:27:02,335 - INFO - [diffusion][Epoch 9582] diffusion training Loss: 0.05073408968746662
2024-11-05 04:27:02,337 - INFO - [diffusion][Epoch 9582] diffusion learning rate: 0.001
2024-11-05 04:27:02,339 - INFO - [diffusion][Epoch 9582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:02,340 - INFO - [diffusion][Epoch 9583] Epoch 9584/12000
2024-11-05 04:27:06,439 - INFO - [diffusion][Epoch 9583] diffusion training Loss: 0.051723118871450424
2024-11-05 04:27:06,442 - INFO - [diffusion][Epoch 9583] diffusion learning rate: 0.001
2024-11-05 04:27:06,444 - INFO - [diffusion][Epoch 9583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:06,445 - INFO - [diffusion][Epoch 9584] Epoch 9585/12000
2024-11-05 04:27:10,505 - INFO - [diffusion][Epoch 9584] diffusion training Loss: 0.051182460971176624
2024-11-05 04:27:10,507 - INFO - [diffusion][Epoch 9584] diffusion learning rate: 0.001
2024-11-05 04:27:10,508 - INFO - [diffusion][Epoch 9584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:10,510 - INFO - [diffusion][Epoch 9585] Epoch 9586/12000
2024-11-05 04:27:14,621 - INFO - [diffusion][Epoch 9585] diffusion training Loss: 0.05096953082829714
2024-11-05 04:27:14,623 - INFO - [diffusion][Epoch 9585] diffusion learning rate: 0.001
2024-11-05 04:27:14,624 - INFO - [diffusion][Epoch 9585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:14,625 - INFO - [diffusion][Epoch 9586] Epoch 9587/12000
2024-11-05 04:27:18,668 - INFO - [diffusion][Epoch 9586] diffusion training Loss: 0.049207087606191635
2024-11-05 04:27:18,670 - INFO - [diffusion][Epoch 9586] diffusion learning rate: 0.001
2024-11-05 04:27:18,672 - INFO - [diffusion][Epoch 9586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:18,673 - INFO - [diffusion][Epoch 9587] Epoch 9588/12000
2024-11-05 04:27:22,724 - INFO - [diffusion][Epoch 9587] diffusion training Loss: 0.051729368045926094
2024-11-05 04:27:22,726 - INFO - [diffusion][Epoch 9587] diffusion learning rate: 0.001
2024-11-05 04:27:22,756 - INFO - [diffusion][Epoch 9587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:22,757 - INFO - [diffusion][Epoch 9588] Epoch 9589/12000
2024-11-05 04:27:26,814 - INFO - [diffusion][Epoch 9588] diffusion training Loss: 0.050574165768921375
2024-11-05 04:27:26,817 - INFO - [diffusion][Epoch 9588] diffusion learning rate: 0.001
2024-11-05 04:27:26,819 - INFO - [diffusion][Epoch 9588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:26,820 - INFO - [diffusion][Epoch 9589] Epoch 9590/12000
2024-11-05 04:27:30,753 - INFO - [diffusion][Epoch 9589] diffusion training Loss: 0.04869819059967995
2024-11-05 04:27:30,755 - INFO - [diffusion][Epoch 9589] diffusion learning rate: 0.001
2024-11-05 04:27:30,756 - INFO - [diffusion][Epoch 9589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:30,758 - INFO - [diffusion][Epoch 9590] Epoch 9591/12000
2024-11-05 04:27:34,855 - INFO - [diffusion][Epoch 9590] diffusion training Loss: 0.051112580113112926
2024-11-05 04:27:34,858 - INFO - [diffusion][Epoch 9590] diffusion learning rate: 0.001
2024-11-05 04:27:34,860 - INFO - [diffusion][Epoch 9590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:34,861 - INFO - [diffusion][Epoch 9591] Epoch 9592/12000
2024-11-05 04:27:39,004 - INFO - [diffusion][Epoch 9591] diffusion training Loss: 0.047465563751757145
2024-11-05 04:27:39,006 - INFO - [diffusion][Epoch 9591] diffusion learning rate: 0.001
2024-11-05 04:27:39,034 - INFO - [diffusion][Epoch 9591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:39,035 - INFO - [diffusion][Epoch 9592] Epoch 9593/12000
2024-11-05 04:27:43,209 - INFO - [diffusion][Epoch 9592] diffusion training Loss: 0.048697615042328835
2024-11-05 04:27:43,211 - INFO - [diffusion][Epoch 9592] diffusion learning rate: 0.001
2024-11-05 04:27:43,213 - INFO - [diffusion][Epoch 9592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:43,215 - INFO - [diffusion][Epoch 9593] Epoch 9594/12000
2024-11-05 04:27:48,070 - INFO - [diffusion][Epoch 9593] diffusion training Loss: 0.051528746262192726
2024-11-05 04:27:48,072 - INFO - [diffusion][Epoch 9593] diffusion learning rate: 0.001
2024-11-05 04:27:48,074 - INFO - [diffusion][Epoch 9593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:48,075 - INFO - [diffusion][Epoch 9594] Epoch 9595/12000
2024-11-05 04:27:52,195 - INFO - [diffusion][Epoch 9594] diffusion training Loss: 0.049497148022055626
2024-11-05 04:27:52,197 - INFO - [diffusion][Epoch 9594] diffusion learning rate: 0.001
2024-11-05 04:27:52,199 - INFO - [diffusion][Epoch 9594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:52,200 - INFO - [diffusion][Epoch 9595] Epoch 9596/12000
2024-11-05 04:27:56,225 - INFO - [diffusion][Epoch 9595] diffusion training Loss: 0.05245420057326555
2024-11-05 04:27:56,227 - INFO - [diffusion][Epoch 9595] diffusion learning rate: 0.001
2024-11-05 04:27:56,229 - INFO - [diffusion][Epoch 9595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:27:56,233 - INFO - [diffusion][Epoch 9596] Epoch 9597/12000
2024-11-05 04:28:00,279 - INFO - [diffusion][Epoch 9596] diffusion training Loss: 0.04873315151780844
2024-11-05 04:28:00,282 - INFO - [diffusion][Epoch 9596] diffusion learning rate: 0.001
2024-11-05 04:28:00,283 - INFO - [diffusion][Epoch 9596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:00,285 - INFO - [diffusion][Epoch 9597] Epoch 9598/12000
2024-11-05 04:28:04,411 - INFO - [diffusion][Epoch 9597] diffusion training Loss: 0.051045799627900124
2024-11-05 04:28:04,414 - INFO - [diffusion][Epoch 9597] diffusion learning rate: 0.001
2024-11-05 04:28:04,416 - INFO - [diffusion][Epoch 9597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:04,417 - INFO - [diffusion][Epoch 9598] Epoch 9599/12000
2024-11-05 04:28:08,544 - INFO - [diffusion][Epoch 9598] diffusion training Loss: 0.050545284524559975
2024-11-05 04:28:08,547 - INFO - [diffusion][Epoch 9598] diffusion learning rate: 0.001
2024-11-05 04:28:08,549 - INFO - [diffusion][Epoch 9598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:08,550 - INFO - [diffusion][Epoch 9599] Epoch 9600/12000
2024-11-05 04:28:12,755 - INFO - [diffusion][Epoch 9599] diffusion training Loss: 0.04704747721552849
2024-11-05 04:28:12,757 - INFO - [diffusion][Epoch 9599] diffusion learning rate: 0.001
2024-11-05 04:28:12,758 - INFO - [diffusion][Epoch 9599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:12,759 - INFO - [diffusion][Epoch 9600] Epoch 9601/12000
2024-11-05 04:28:16,836 - INFO - [diffusion][Epoch 9600] diffusion training Loss: 0.049645586870610714
2024-11-05 04:28:16,839 - INFO - [diffusion][Epoch 9600] diffusion learning rate: 0.001
2024-11-05 04:28:16,840 - INFO - [diffusion][Epoch 9600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:16,842 - INFO - [diffusion][Epoch 9601] Epoch 9602/12000
2024-11-05 04:28:21,044 - INFO - [diffusion][Epoch 9601] diffusion training Loss: 0.04752590134739876
2024-11-05 04:28:21,046 - INFO - [diffusion][Epoch 9601] diffusion learning rate: 0.001
2024-11-05 04:28:21,048 - INFO - [diffusion][Epoch 9601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:21,049 - INFO - [diffusion][Epoch 9602] Epoch 9603/12000
2024-11-05 04:28:25,175 - INFO - [diffusion][Epoch 9602] diffusion training Loss: 0.05420342553406954
2024-11-05 04:28:25,177 - INFO - [diffusion][Epoch 9602] diffusion learning rate: 0.001
2024-11-05 04:28:25,203 - INFO - [diffusion][Epoch 9602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:25,205 - INFO - [diffusion][Epoch 9603] Epoch 9604/12000
2024-11-05 04:28:29,324 - INFO - [diffusion][Epoch 9603] diffusion training Loss: 0.051467846147716045
2024-11-05 04:28:29,327 - INFO - [diffusion][Epoch 9603] diffusion learning rate: 0.001
2024-11-05 04:28:29,328 - INFO - [diffusion][Epoch 9603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:29,330 - INFO - [diffusion][Epoch 9604] Epoch 9605/12000
2024-11-05 04:28:33,438 - INFO - [diffusion][Epoch 9604] diffusion training Loss: 0.05015935469418764
2024-11-05 04:28:33,441 - INFO - [diffusion][Epoch 9604] diffusion learning rate: 0.001
2024-11-05 04:28:33,442 - INFO - [diffusion][Epoch 9604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:33,444 - INFO - [diffusion][Epoch 9605] Epoch 9606/12000
2024-11-05 04:28:37,546 - INFO - [diffusion][Epoch 9605] diffusion training Loss: 0.047999183647334576
2024-11-05 04:28:37,549 - INFO - [diffusion][Epoch 9605] diffusion learning rate: 0.001
2024-11-05 04:28:37,551 - INFO - [diffusion][Epoch 9605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:37,552 - INFO - [diffusion][Epoch 9606] Epoch 9607/12000
2024-11-05 04:28:41,690 - INFO - [diffusion][Epoch 9606] diffusion training Loss: 0.045741564594209194
2024-11-05 04:28:41,692 - INFO - [diffusion][Epoch 9606] diffusion learning rate: 0.001
2024-11-05 04:28:41,694 - INFO - [diffusion][Epoch 9606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:41,696 - INFO - [diffusion][Epoch 9607] Epoch 9608/12000
2024-11-05 04:28:45,740 - INFO - [diffusion][Epoch 9607] diffusion training Loss: 0.047169738449156284
2024-11-05 04:28:45,742 - INFO - [diffusion][Epoch 9607] diffusion learning rate: 0.001
2024-11-05 04:28:45,744 - INFO - [diffusion][Epoch 9607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:45,745 - INFO - [diffusion][Epoch 9608] Epoch 9609/12000
2024-11-05 04:28:49,730 - INFO - [diffusion][Epoch 9608] diffusion training Loss: 0.055500647984445095
2024-11-05 04:28:49,732 - INFO - [diffusion][Epoch 9608] diffusion learning rate: 0.001
2024-11-05 04:28:49,734 - INFO - [diffusion][Epoch 9608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:49,735 - INFO - [diffusion][Epoch 9609] Epoch 9610/12000
2024-11-05 04:28:53,850 - INFO - [diffusion][Epoch 9609] diffusion training Loss: 0.05126862972974777
2024-11-05 04:28:53,852 - INFO - [diffusion][Epoch 9609] diffusion learning rate: 0.001
2024-11-05 04:28:53,854 - INFO - [diffusion][Epoch 9609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:53,855 - INFO - [diffusion][Epoch 9610] Epoch 9611/12000
2024-11-05 04:28:57,985 - INFO - [diffusion][Epoch 9610] diffusion training Loss: 0.051561192609369755
2024-11-05 04:28:57,987 - INFO - [diffusion][Epoch 9610] diffusion learning rate: 0.001
2024-11-05 04:28:57,989 - INFO - [diffusion][Epoch 9610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:28:57,991 - INFO - [diffusion][Epoch 9611] Epoch 9612/12000
2024-11-05 04:29:02,023 - INFO - [diffusion][Epoch 9611] diffusion training Loss: 0.05143074598163366
2024-11-05 04:29:02,025 - INFO - [diffusion][Epoch 9611] diffusion learning rate: 0.001
2024-11-05 04:29:02,027 - INFO - [diffusion][Epoch 9611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:02,028 - INFO - [diffusion][Epoch 9612] Epoch 9613/12000
2024-11-05 04:29:06,078 - INFO - [diffusion][Epoch 9612] diffusion training Loss: 0.053679030388593674
2024-11-05 04:29:06,080 - INFO - [diffusion][Epoch 9612] diffusion learning rate: 0.001
2024-11-05 04:29:06,082 - INFO - [diffusion][Epoch 9612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:06,083 - INFO - [diffusion][Epoch 9613] Epoch 9614/12000
2024-11-05 04:29:10,831 - INFO - [diffusion][Epoch 9613] diffusion training Loss: 0.04355295654386282
2024-11-05 04:29:10,834 - INFO - [diffusion][Epoch 9613] diffusion learning rate: 0.001
2024-11-05 04:29:10,873 - INFO - [diffusion][Epoch 9613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:10,875 - INFO - [diffusion][Epoch 9614] Epoch 9615/12000
2024-11-05 04:29:14,835 - INFO - [diffusion][Epoch 9614] diffusion training Loss: 0.05180783849209547
2024-11-05 04:29:14,837 - INFO - [diffusion][Epoch 9614] diffusion learning rate: 0.001
2024-11-05 04:29:14,839 - INFO - [diffusion][Epoch 9614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:14,840 - INFO - [diffusion][Epoch 9615] Epoch 9616/12000
2024-11-05 04:29:18,963 - INFO - [diffusion][Epoch 9615] diffusion training Loss: 0.05199987068772316
2024-11-05 04:29:18,965 - INFO - [diffusion][Epoch 9615] diffusion learning rate: 0.001
2024-11-05 04:29:18,967 - INFO - [diffusion][Epoch 9615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:18,968 - INFO - [diffusion][Epoch 9616] Epoch 9617/12000
2024-11-05 04:29:23,075 - INFO - [diffusion][Epoch 9616] diffusion training Loss: 0.04989986401051283
2024-11-05 04:29:23,077 - INFO - [diffusion][Epoch 9616] diffusion learning rate: 0.001
2024-11-05 04:29:23,079 - INFO - [diffusion][Epoch 9616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:23,080 - INFO - [diffusion][Epoch 9617] Epoch 9618/12000
2024-11-05 04:29:27,061 - INFO - [diffusion][Epoch 9617] diffusion training Loss: 0.050695616751909256
2024-11-05 04:29:27,063 - INFO - [diffusion][Epoch 9617] diffusion learning rate: 0.001
2024-11-05 04:29:27,083 - INFO - [diffusion][Epoch 9617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:27,084 - INFO - [diffusion][Epoch 9618] Epoch 9619/12000
2024-11-05 04:29:30,897 - INFO - [diffusion][Epoch 9618] diffusion training Loss: 0.04871977772563696
2024-11-05 04:29:30,899 - INFO - [diffusion][Epoch 9618] diffusion learning rate: 0.001
2024-11-05 04:29:30,901 - INFO - [diffusion][Epoch 9618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:30,902 - INFO - [diffusion][Epoch 9619] Epoch 9620/12000
2024-11-05 04:29:34,947 - INFO - [diffusion][Epoch 9619] diffusion training Loss: 0.0509991142898798
2024-11-05 04:29:34,949 - INFO - [diffusion][Epoch 9619] diffusion learning rate: 0.001
2024-11-05 04:29:34,951 - INFO - [diffusion][Epoch 9619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:34,952 - INFO - [diffusion][Epoch 9620] Epoch 9621/12000
2024-11-05 04:29:39,010 - INFO - [diffusion][Epoch 9620] diffusion training Loss: 0.04651830904185772
2024-11-05 04:29:39,013 - INFO - [diffusion][Epoch 9620] diffusion learning rate: 0.001
2024-11-05 04:29:39,015 - INFO - [diffusion][Epoch 9620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:39,016 - INFO - [diffusion][Epoch 9621] Epoch 9622/12000
2024-11-05 04:29:43,133 - INFO - [diffusion][Epoch 9621] diffusion training Loss: 0.04354892857372761
2024-11-05 04:29:43,135 - INFO - [diffusion][Epoch 9621] diffusion learning rate: 0.001
2024-11-05 04:29:43,174 - INFO - [diffusion][Epoch 9621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:43,176 - INFO - [diffusion][Epoch 9622] Epoch 9623/12000
2024-11-05 04:29:47,208 - INFO - [diffusion][Epoch 9622] diffusion training Loss: 0.04742453247308731
2024-11-05 04:29:47,210 - INFO - [diffusion][Epoch 9622] diffusion learning rate: 0.001
2024-11-05 04:29:47,212 - INFO - [diffusion][Epoch 9622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:47,214 - INFO - [diffusion][Epoch 9623] Epoch 9624/12000
2024-11-05 04:29:51,419 - INFO - [diffusion][Epoch 9623] diffusion training Loss: 0.053807894699275494
2024-11-05 04:29:51,422 - INFO - [diffusion][Epoch 9623] diffusion learning rate: 0.001
2024-11-05 04:29:51,424 - INFO - [diffusion][Epoch 9623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:51,425 - INFO - [diffusion][Epoch 9624] Epoch 9625/12000
2024-11-05 04:29:55,545 - INFO - [diffusion][Epoch 9624] diffusion training Loss: 0.052507877349853516
2024-11-05 04:29:55,547 - INFO - [diffusion][Epoch 9624] diffusion learning rate: 0.001
2024-11-05 04:29:55,549 - INFO - [diffusion][Epoch 9624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:55,550 - INFO - [diffusion][Epoch 9625] Epoch 9626/12000
2024-11-05 04:29:59,628 - INFO - [diffusion][Epoch 9625] diffusion training Loss: 0.051655939780175686
2024-11-05 04:29:59,631 - INFO - [diffusion][Epoch 9625] diffusion learning rate: 0.001
2024-11-05 04:29:59,632 - INFO - [diffusion][Epoch 9625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:29:59,634 - INFO - [diffusion][Epoch 9626] Epoch 9627/12000
2024-11-05 04:30:03,738 - INFO - [diffusion][Epoch 9626] diffusion training Loss: 0.05180966295301914
2024-11-05 04:30:03,740 - INFO - [diffusion][Epoch 9626] diffusion learning rate: 0.001
2024-11-05 04:30:03,742 - INFO - [diffusion][Epoch 9626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:03,743 - INFO - [diffusion][Epoch 9627] Epoch 9628/12000
2024-11-05 04:30:07,846 - INFO - [diffusion][Epoch 9627] diffusion training Loss: 0.04341646935790777
2024-11-05 04:30:07,849 - INFO - [diffusion][Epoch 9627] diffusion learning rate: 0.001
2024-11-05 04:30:07,851 - INFO - [diffusion][Epoch 9627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:07,852 - INFO - [diffusion][Epoch 9628] Epoch 9629/12000
2024-11-05 04:30:11,962 - INFO - [diffusion][Epoch 9628] diffusion training Loss: 0.05873038712888956
2024-11-05 04:30:11,964 - INFO - [diffusion][Epoch 9628] diffusion learning rate: 0.001
2024-11-05 04:30:11,966 - INFO - [diffusion][Epoch 9628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:11,967 - INFO - [diffusion][Epoch 9629] Epoch 9630/12000
2024-11-05 04:30:16,052 - INFO - [diffusion][Epoch 9629] diffusion training Loss: 0.05068715661764145
2024-11-05 04:30:16,054 - INFO - [diffusion][Epoch 9629] diffusion learning rate: 0.001
2024-11-05 04:30:16,056 - INFO - [diffusion][Epoch 9629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:16,057 - INFO - [diffusion][Epoch 9630] Epoch 9631/12000
2024-11-05 04:30:20,189 - INFO - [diffusion][Epoch 9630] diffusion training Loss: 0.05048864521086216
2024-11-05 04:30:20,191 - INFO - [diffusion][Epoch 9630] diffusion learning rate: 0.001
2024-11-05 04:30:20,230 - INFO - [diffusion][Epoch 9630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:20,231 - INFO - [diffusion][Epoch 9631] Epoch 9632/12000
2024-11-05 04:30:24,169 - INFO - [diffusion][Epoch 9631] diffusion training Loss: 0.04903494752943516
2024-11-05 04:30:24,171 - INFO - [diffusion][Epoch 9631] diffusion learning rate: 0.001
2024-11-05 04:30:24,173 - INFO - [diffusion][Epoch 9631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:24,174 - INFO - [diffusion][Epoch 9632] Epoch 9633/12000
2024-11-05 04:30:28,201 - INFO - [diffusion][Epoch 9632] diffusion training Loss: 0.04893187992274761
2024-11-05 04:30:28,203 - INFO - [diffusion][Epoch 9632] diffusion learning rate: 0.001
2024-11-05 04:30:28,204 - INFO - [diffusion][Epoch 9632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:28,206 - INFO - [diffusion][Epoch 9633] Epoch 9634/12000
2024-11-05 04:30:32,289 - INFO - [diffusion][Epoch 9633] diffusion training Loss: 0.049636565148830414
2024-11-05 04:30:32,291 - INFO - [diffusion][Epoch 9633] diffusion learning rate: 0.001
2024-11-05 04:30:32,293 - INFO - [diffusion][Epoch 9633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:32,294 - INFO - [diffusion][Epoch 9634] Epoch 9635/12000
2024-11-05 04:30:36,428 - INFO - [diffusion][Epoch 9634] diffusion training Loss: 0.04949780274182558
2024-11-05 04:30:36,430 - INFO - [diffusion][Epoch 9634] diffusion learning rate: 0.001
2024-11-05 04:30:36,475 - INFO - [diffusion][Epoch 9634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:36,476 - INFO - [diffusion][Epoch 9635] Epoch 9636/12000
2024-11-05 04:30:40,405 - INFO - [diffusion][Epoch 9635] diffusion training Loss: 0.04934910498559475
2024-11-05 04:30:40,408 - INFO - [diffusion][Epoch 9635] diffusion learning rate: 0.001
2024-11-05 04:30:40,410 - INFO - [diffusion][Epoch 9635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:40,411 - INFO - [diffusion][Epoch 9636] Epoch 9637/12000
2024-11-05 04:30:44,493 - INFO - [diffusion][Epoch 9636] diffusion training Loss: 0.04797440581023693
2024-11-05 04:30:44,495 - INFO - [diffusion][Epoch 9636] diffusion learning rate: 0.001
2024-11-05 04:30:44,497 - INFO - [diffusion][Epoch 9636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:44,498 - INFO - [diffusion][Epoch 9637] Epoch 9638/12000
2024-11-05 04:30:48,650 - INFO - [diffusion][Epoch 9637] diffusion training Loss: 0.051909852772951126
2024-11-05 04:30:48,652 - INFO - [diffusion][Epoch 9637] diffusion learning rate: 0.001
2024-11-05 04:30:48,654 - INFO - [diffusion][Epoch 9637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:48,655 - INFO - [diffusion][Epoch 9638] Epoch 9639/12000
2024-11-05 04:30:52,790 - INFO - [diffusion][Epoch 9638] diffusion training Loss: 0.05575293954461813
2024-11-05 04:30:52,792 - INFO - [diffusion][Epoch 9638] diffusion learning rate: 0.001
2024-11-05 04:30:52,793 - INFO - [diffusion][Epoch 9638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:52,794 - INFO - [diffusion][Epoch 9639] Epoch 9640/12000
2024-11-05 04:30:56,943 - INFO - [diffusion][Epoch 9639] diffusion training Loss: 0.05167665984481573
2024-11-05 04:30:56,945 - INFO - [diffusion][Epoch 9639] diffusion learning rate: 0.001
2024-11-05 04:30:56,948 - INFO - [diffusion][Epoch 9639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:30:56,949 - INFO - [diffusion][Epoch 9640] Epoch 9641/12000
2024-11-05 04:31:01,101 - INFO - [diffusion][Epoch 9640] diffusion training Loss: 0.05266277678310871
2024-11-05 04:31:01,103 - INFO - [diffusion][Epoch 9640] diffusion learning rate: 0.001
2024-11-05 04:31:01,105 - INFO - [diffusion][Epoch 9640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:01,106 - INFO - [diffusion][Epoch 9641] Epoch 9642/12000
2024-11-05 04:31:05,252 - INFO - [diffusion][Epoch 9641] diffusion training Loss: 0.04910310544073582
2024-11-05 04:31:05,254 - INFO - [diffusion][Epoch 9641] diffusion learning rate: 0.001
2024-11-05 04:31:05,256 - INFO - [diffusion][Epoch 9641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:05,257 - INFO - [diffusion][Epoch 9642] Epoch 9643/12000
2024-11-05 04:31:09,362 - INFO - [diffusion][Epoch 9642] diffusion training Loss: 0.048634208738803864
2024-11-05 04:31:09,364 - INFO - [diffusion][Epoch 9642] diffusion learning rate: 0.001
2024-11-05 04:31:09,391 - INFO - [diffusion][Epoch 9642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:09,393 - INFO - [diffusion][Epoch 9643] Epoch 9644/12000
2024-11-05 04:31:13,577 - INFO - [diffusion][Epoch 9643] diffusion training Loss: 0.055060843005776405
2024-11-05 04:31:13,580 - INFO - [diffusion][Epoch 9643] diffusion learning rate: 0.001
2024-11-05 04:31:13,582 - INFO - [diffusion][Epoch 9643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:13,583 - INFO - [diffusion][Epoch 9644] Epoch 9645/12000
2024-11-05 04:31:17,757 - INFO - [diffusion][Epoch 9644] diffusion training Loss: 0.0491473525762558
2024-11-05 04:31:17,759 - INFO - [diffusion][Epoch 9644] diffusion learning rate: 0.001
2024-11-05 04:31:17,760 - INFO - [diffusion][Epoch 9644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:17,762 - INFO - [diffusion][Epoch 9645] Epoch 9646/12000
2024-11-05 04:31:21,881 - INFO - [diffusion][Epoch 9645] diffusion training Loss: 0.04802999831736088
2024-11-05 04:31:21,883 - INFO - [diffusion][Epoch 9645] diffusion learning rate: 0.001
2024-11-05 04:31:21,885 - INFO - [diffusion][Epoch 9645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:21,887 - INFO - [diffusion][Epoch 9646] Epoch 9647/12000
2024-11-05 04:31:25,970 - INFO - [diffusion][Epoch 9646] diffusion training Loss: 0.04519365541636944
2024-11-05 04:31:25,972 - INFO - [diffusion][Epoch 9646] diffusion learning rate: 0.001
2024-11-05 04:31:25,973 - INFO - [diffusion][Epoch 9646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:25,974 - INFO - [diffusion][Epoch 9647] Epoch 9648/12000
2024-11-05 04:31:30,186 - INFO - [diffusion][Epoch 9647] diffusion training Loss: 0.0523943156003952
2024-11-05 04:31:30,188 - INFO - [diffusion][Epoch 9647] diffusion learning rate: 0.001
2024-11-05 04:31:30,190 - INFO - [diffusion][Epoch 9647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:30,191 - INFO - [diffusion][Epoch 9648] Epoch 9649/12000
2024-11-05 04:31:34,307 - INFO - [diffusion][Epoch 9648] diffusion training Loss: 0.05166043993085623
2024-11-05 04:31:34,309 - INFO - [diffusion][Epoch 9648] diffusion learning rate: 0.001
2024-11-05 04:31:34,311 - INFO - [diffusion][Epoch 9648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:34,312 - INFO - [diffusion][Epoch 9649] Epoch 9650/12000
2024-11-05 04:31:38,359 - INFO - [diffusion][Epoch 9649] diffusion training Loss: 0.05619027465581894
2024-11-05 04:31:38,361 - INFO - [diffusion][Epoch 9649] diffusion learning rate: 0.001
2024-11-05 04:31:38,363 - INFO - [diffusion][Epoch 9649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:38,364 - INFO - [diffusion][Epoch 9650] Epoch 9651/12000
2024-11-05 04:31:42,443 - INFO - [diffusion][Epoch 9650] diffusion training Loss: 0.05548579432070255
2024-11-05 04:31:42,445 - INFO - [diffusion][Epoch 9650] diffusion learning rate: 0.001
2024-11-05 04:31:42,446 - INFO - [diffusion][Epoch 9650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:42,448 - INFO - [diffusion][Epoch 9651] Epoch 9652/12000
2024-11-05 04:31:46,519 - INFO - [diffusion][Epoch 9651] diffusion training Loss: 0.053305378183722496
2024-11-05 04:31:46,521 - INFO - [diffusion][Epoch 9651] diffusion learning rate: 0.001
2024-11-05 04:31:46,523 - INFO - [diffusion][Epoch 9651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:46,524 - INFO - [diffusion][Epoch 9652] Epoch 9653/12000
2024-11-05 04:31:50,508 - INFO - [diffusion][Epoch 9652] diffusion training Loss: 0.05062077194452286
2024-11-05 04:31:50,510 - INFO - [diffusion][Epoch 9652] diffusion learning rate: 0.001
2024-11-05 04:31:50,512 - INFO - [diffusion][Epoch 9652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:50,513 - INFO - [diffusion][Epoch 9653] Epoch 9654/12000
2024-11-05 04:31:54,581 - INFO - [diffusion][Epoch 9653] diffusion training Loss: 0.05357157904654741
2024-11-05 04:31:54,583 - INFO - [diffusion][Epoch 9653] diffusion learning rate: 0.001
2024-11-05 04:31:54,622 - INFO - [diffusion][Epoch 9653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:54,623 - INFO - [diffusion][Epoch 9654] Epoch 9655/12000
2024-11-05 04:31:58,631 - INFO - [diffusion][Epoch 9654] diffusion training Loss: 0.04760483093559742
2024-11-05 04:31:58,633 - INFO - [diffusion][Epoch 9654] diffusion learning rate: 0.001
2024-11-05 04:31:58,635 - INFO - [diffusion][Epoch 9654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:31:58,636 - INFO - [diffusion][Epoch 9655] Epoch 9656/12000
2024-11-05 04:32:02,787 - INFO - [diffusion][Epoch 9655] diffusion training Loss: 0.045458607375621796
2024-11-05 04:32:02,789 - INFO - [diffusion][Epoch 9655] diffusion learning rate: 0.001
2024-11-05 04:32:02,790 - INFO - [diffusion][Epoch 9655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:02,792 - INFO - [diffusion][Epoch 9656] Epoch 9657/12000
2024-11-05 04:32:07,194 - INFO - [diffusion][Epoch 9656] diffusion training Loss: 0.05303249787539244
2024-11-05 04:32:07,196 - INFO - [diffusion][Epoch 9656] diffusion learning rate: 0.001
2024-11-05 04:32:07,197 - INFO - [diffusion][Epoch 9656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:07,199 - INFO - [diffusion][Epoch 9657] Epoch 9658/12000
2024-11-05 04:32:11,374 - INFO - [diffusion][Epoch 9657] diffusion training Loss: 0.051054783165454865
2024-11-05 04:32:11,376 - INFO - [diffusion][Epoch 9657] diffusion learning rate: 0.001
2024-11-05 04:32:11,417 - INFO - [diffusion][Epoch 9657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:11,420 - INFO - [diffusion][Epoch 9658] Epoch 9659/12000
2024-11-05 04:32:15,582 - INFO - [diffusion][Epoch 9658] diffusion training Loss: 0.05134468711912632
2024-11-05 04:32:15,585 - INFO - [diffusion][Epoch 9658] diffusion learning rate: 0.001
2024-11-05 04:32:15,587 - INFO - [diffusion][Epoch 9658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:15,588 - INFO - [diffusion][Epoch 9659] Epoch 9660/12000
2024-11-05 04:32:19,751 - INFO - [diffusion][Epoch 9659] diffusion training Loss: 0.0521615045145154
2024-11-05 04:32:19,753 - INFO - [diffusion][Epoch 9659] diffusion learning rate: 0.001
2024-11-05 04:32:19,755 - INFO - [diffusion][Epoch 9659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:19,756 - INFO - [diffusion][Epoch 9660] Epoch 9661/12000
2024-11-05 04:32:23,930 - INFO - [diffusion][Epoch 9660] diffusion training Loss: 0.05437584966421127
2024-11-05 04:32:23,932 - INFO - [diffusion][Epoch 9660] diffusion learning rate: 0.001
2024-11-05 04:32:23,934 - INFO - [diffusion][Epoch 9660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:23,936 - INFO - [diffusion][Epoch 9661] Epoch 9662/12000
2024-11-05 04:32:27,969 - INFO - [diffusion][Epoch 9661] diffusion training Loss: 0.05235788878053427
2024-11-05 04:32:27,972 - INFO - [diffusion][Epoch 9661] diffusion learning rate: 0.001
2024-11-05 04:32:27,974 - INFO - [diffusion][Epoch 9661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:27,975 - INFO - [diffusion][Epoch 9662] Epoch 9663/12000
2024-11-05 04:32:32,110 - INFO - [diffusion][Epoch 9662] diffusion training Loss: 0.05111920740455389
2024-11-05 04:32:32,112 - INFO - [diffusion][Epoch 9662] diffusion learning rate: 0.001
2024-11-05 04:32:32,114 - INFO - [diffusion][Epoch 9662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:32,115 - INFO - [diffusion][Epoch 9663] Epoch 9664/12000
2024-11-05 04:32:36,169 - INFO - [diffusion][Epoch 9663] diffusion training Loss: 0.051164464093744755
2024-11-05 04:32:36,170 - INFO - [diffusion][Epoch 9663] diffusion learning rate: 0.001
2024-11-05 04:32:36,172 - INFO - [diffusion][Epoch 9663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:36,173 - INFO - [diffusion][Epoch 9664] Epoch 9665/12000
2024-11-05 04:32:40,255 - INFO - [diffusion][Epoch 9664] diffusion training Loss: 0.04656182415783405
2024-11-05 04:32:40,257 - INFO - [diffusion][Epoch 9664] diffusion learning rate: 0.001
2024-11-05 04:32:40,260 - INFO - [diffusion][Epoch 9664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:40,262 - INFO - [diffusion][Epoch 9665] Epoch 9666/12000
2024-11-05 04:32:44,222 - INFO - [diffusion][Epoch 9665] diffusion training Loss: 0.05142455827444792
2024-11-05 04:32:44,224 - INFO - [diffusion][Epoch 9665] diffusion learning rate: 0.001
2024-11-05 04:32:44,226 - INFO - [diffusion][Epoch 9665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:44,227 - INFO - [diffusion][Epoch 9666] Epoch 9667/12000
2024-11-05 04:32:48,261 - INFO - [diffusion][Epoch 9666] diffusion training Loss: 0.0554848900064826
2024-11-05 04:32:48,263 - INFO - [diffusion][Epoch 9666] diffusion learning rate: 0.001
2024-11-05 04:32:48,265 - INFO - [diffusion][Epoch 9666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:48,266 - INFO - [diffusion][Epoch 9667] Epoch 9668/12000
2024-11-05 04:32:52,324 - INFO - [diffusion][Epoch 9667] diffusion training Loss: 0.05036916956305504
2024-11-05 04:32:52,326 - INFO - [diffusion][Epoch 9667] diffusion learning rate: 0.001
2024-11-05 04:32:52,328 - INFO - [diffusion][Epoch 9667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:52,329 - INFO - [diffusion][Epoch 9668] Epoch 9669/12000
2024-11-05 04:32:56,343 - INFO - [diffusion][Epoch 9668] diffusion training Loss: 0.04734466504305601
2024-11-05 04:32:56,346 - INFO - [diffusion][Epoch 9668] diffusion learning rate: 0.001
2024-11-05 04:32:56,347 - INFO - [diffusion][Epoch 9668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:32:56,349 - INFO - [diffusion][Epoch 9669] Epoch 9670/12000
2024-11-05 04:33:00,266 - INFO - [diffusion][Epoch 9669] diffusion training Loss: 0.054599036462605
2024-11-05 04:33:00,268 - INFO - [diffusion][Epoch 9669] diffusion learning rate: 0.001
2024-11-05 04:33:00,271 - INFO - [diffusion][Epoch 9669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:00,272 - INFO - [diffusion][Epoch 9670] Epoch 9671/12000
2024-11-05 04:33:04,410 - INFO - [diffusion][Epoch 9670] diffusion training Loss: 0.04544520564377308
2024-11-05 04:33:04,412 - INFO - [diffusion][Epoch 9670] diffusion learning rate: 0.001
2024-11-05 04:33:04,414 - INFO - [diffusion][Epoch 9670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:04,415 - INFO - [diffusion][Epoch 9671] Epoch 9672/12000
2024-11-05 04:33:08,415 - INFO - [diffusion][Epoch 9671] diffusion training Loss: 0.05072853434830904
2024-11-05 04:33:08,417 - INFO - [diffusion][Epoch 9671] diffusion learning rate: 0.001
2024-11-05 04:33:08,418 - INFO - [diffusion][Epoch 9671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:08,420 - INFO - [diffusion][Epoch 9672] Epoch 9673/12000
2024-11-05 04:33:12,456 - INFO - [diffusion][Epoch 9672] diffusion training Loss: 0.05036821961402893
2024-11-05 04:33:12,458 - INFO - [diffusion][Epoch 9672] diffusion learning rate: 0.001
2024-11-05 04:33:12,460 - INFO - [diffusion][Epoch 9672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:12,461 - INFO - [diffusion][Epoch 9673] Epoch 9674/12000
2024-11-05 04:33:16,501 - INFO - [diffusion][Epoch 9673] diffusion training Loss: 0.052533332258462906
2024-11-05 04:33:16,504 - INFO - [diffusion][Epoch 9673] diffusion learning rate: 0.001
2024-11-05 04:33:16,505 - INFO - [diffusion][Epoch 9673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:16,507 - INFO - [diffusion][Epoch 9674] Epoch 9675/12000
2024-11-05 04:33:20,678 - INFO - [diffusion][Epoch 9674] diffusion training Loss: 0.055466920137405396
2024-11-05 04:33:20,680 - INFO - [diffusion][Epoch 9674] diffusion learning rate: 0.001
2024-11-05 04:33:20,682 - INFO - [diffusion][Epoch 9674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:20,683 - INFO - [diffusion][Epoch 9675] Epoch 9676/12000
2024-11-05 04:33:24,704 - INFO - [diffusion][Epoch 9675] diffusion training Loss: 0.04707439802587032
2024-11-05 04:33:24,706 - INFO - [diffusion][Epoch 9675] diffusion learning rate: 0.001
2024-11-05 04:33:24,747 - INFO - [diffusion][Epoch 9675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:24,748 - INFO - [diffusion][Epoch 9676] Epoch 9677/12000
2024-11-05 04:33:29,064 - INFO - [diffusion][Epoch 9676] diffusion training Loss: 0.052481893450021744
2024-11-05 04:33:29,066 - INFO - [diffusion][Epoch 9676] diffusion learning rate: 0.001
2024-11-05 04:33:29,068 - INFO - [diffusion][Epoch 9676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:29,069 - INFO - [diffusion][Epoch 9677] Epoch 9678/12000
2024-11-05 04:33:33,186 - INFO - [diffusion][Epoch 9677] diffusion training Loss: 0.05145838391035795
2024-11-05 04:33:33,188 - INFO - [diffusion][Epoch 9677] diffusion learning rate: 0.001
2024-11-05 04:33:33,190 - INFO - [diffusion][Epoch 9677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:33,191 - INFO - [diffusion][Epoch 9678] Epoch 9679/12000
2024-11-05 04:33:37,150 - INFO - [diffusion][Epoch 9678] diffusion training Loss: 0.05308595858514309
2024-11-05 04:33:37,151 - INFO - [diffusion][Epoch 9678] diffusion learning rate: 0.001
2024-11-05 04:33:37,153 - INFO - [diffusion][Epoch 9678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:37,154 - INFO - [diffusion][Epoch 9679] Epoch 9680/12000
2024-11-05 04:33:41,107 - INFO - [diffusion][Epoch 9679] diffusion training Loss: 0.05661175865679979
2024-11-05 04:33:41,109 - INFO - [diffusion][Epoch 9679] diffusion learning rate: 0.001
2024-11-05 04:33:41,111 - INFO - [diffusion][Epoch 9679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:41,112 - INFO - [diffusion][Epoch 9680] Epoch 9681/12000
2024-11-05 04:33:45,152 - INFO - [diffusion][Epoch 9680] diffusion training Loss: 0.05358857661485672
2024-11-05 04:33:45,154 - INFO - [diffusion][Epoch 9680] diffusion learning rate: 0.001
2024-11-05 04:33:45,156 - INFO - [diffusion][Epoch 9680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:45,157 - INFO - [diffusion][Epoch 9681] Epoch 9682/12000
2024-11-05 04:33:49,214 - INFO - [diffusion][Epoch 9681] diffusion training Loss: 0.05109455529600382
2024-11-05 04:33:49,216 - INFO - [diffusion][Epoch 9681] diffusion learning rate: 0.001
2024-11-05 04:33:49,217 - INFO - [diffusion][Epoch 9681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:49,218 - INFO - [diffusion][Epoch 9682] Epoch 9683/12000
2024-11-05 04:33:53,344 - INFO - [diffusion][Epoch 9682] diffusion training Loss: 0.04916814900934696
2024-11-05 04:33:53,347 - INFO - [diffusion][Epoch 9682] diffusion learning rate: 0.001
2024-11-05 04:33:53,349 - INFO - [diffusion][Epoch 9682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:53,351 - INFO - [diffusion][Epoch 9683] Epoch 9684/12000
2024-11-05 04:33:57,459 - INFO - [diffusion][Epoch 9683] diffusion training Loss: 0.04702438320964575
2024-11-05 04:33:57,461 - INFO - [diffusion][Epoch 9683] diffusion learning rate: 0.001
2024-11-05 04:33:57,463 - INFO - [diffusion][Epoch 9683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:33:57,464 - INFO - [diffusion][Epoch 9684] Epoch 9685/12000
2024-11-05 04:34:01,618 - INFO - [diffusion][Epoch 9684] diffusion training Loss: 0.04990122839808464
2024-11-05 04:34:01,620 - INFO - [diffusion][Epoch 9684] diffusion learning rate: 0.001
2024-11-05 04:34:01,622 - INFO - [diffusion][Epoch 9684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:01,624 - INFO - [diffusion][Epoch 9685] Epoch 9686/12000
2024-11-05 04:34:05,754 - INFO - [diffusion][Epoch 9685] diffusion training Loss: 0.05237482488155365
2024-11-05 04:34:05,756 - INFO - [diffusion][Epoch 9685] diffusion learning rate: 0.001
2024-11-05 04:34:05,807 - INFO - [diffusion][Epoch 9685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:05,809 - INFO - [diffusion][Epoch 9686] Epoch 9687/12000
2024-11-05 04:34:09,934 - INFO - [diffusion][Epoch 9686] diffusion training Loss: 0.04945600498467684
2024-11-05 04:34:09,936 - INFO - [diffusion][Epoch 9686] diffusion learning rate: 0.001
2024-11-05 04:34:09,938 - INFO - [diffusion][Epoch 9686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:09,939 - INFO - [diffusion][Epoch 9687] Epoch 9688/12000
2024-11-05 04:34:14,088 - INFO - [diffusion][Epoch 9687] diffusion training Loss: 0.049102505668997765
2024-11-05 04:34:14,090 - INFO - [diffusion][Epoch 9687] diffusion learning rate: 0.001
2024-11-05 04:34:14,092 - INFO - [diffusion][Epoch 9687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:14,093 - INFO - [diffusion][Epoch 9688] Epoch 9689/12000
2024-11-05 04:34:18,212 - INFO - [diffusion][Epoch 9688] diffusion training Loss: 0.050745291635394096
2024-11-05 04:34:18,215 - INFO - [diffusion][Epoch 9688] diffusion learning rate: 0.001
2024-11-05 04:34:18,217 - INFO - [diffusion][Epoch 9688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:18,218 - INFO - [diffusion][Epoch 9689] Epoch 9690/12000
2024-11-05 04:34:22,341 - INFO - [diffusion][Epoch 9689] diffusion training Loss: 0.05590737424790859
2024-11-05 04:34:22,343 - INFO - [diffusion][Epoch 9689] diffusion learning rate: 0.001
2024-11-05 04:34:22,344 - INFO - [diffusion][Epoch 9689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:22,346 - INFO - [diffusion][Epoch 9690] Epoch 9691/12000
2024-11-05 04:34:26,484 - INFO - [diffusion][Epoch 9690] diffusion training Loss: 0.05140432249754667
2024-11-05 04:34:26,486 - INFO - [diffusion][Epoch 9690] diffusion learning rate: 0.001
2024-11-05 04:34:26,488 - INFO - [diffusion][Epoch 9690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:26,490 - INFO - [diffusion][Epoch 9691] Epoch 9692/12000
2024-11-05 04:34:30,616 - INFO - [diffusion][Epoch 9691] diffusion training Loss: 0.0521057341247797
2024-11-05 04:34:30,618 - INFO - [diffusion][Epoch 9691] diffusion learning rate: 0.001
2024-11-05 04:34:30,620 - INFO - [diffusion][Epoch 9691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:30,621 - INFO - [diffusion][Epoch 9692] Epoch 9693/12000
2024-11-05 04:34:34,757 - INFO - [diffusion][Epoch 9692] diffusion training Loss: 0.049747045151889324
2024-11-05 04:34:34,759 - INFO - [diffusion][Epoch 9692] diffusion learning rate: 0.001
2024-11-05 04:34:34,811 - INFO - [diffusion][Epoch 9692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:34,812 - INFO - [diffusion][Epoch 9693] Epoch 9694/12000
2024-11-05 04:34:38,908 - INFO - [diffusion][Epoch 9693] diffusion training Loss: 0.051238976418972015
2024-11-05 04:34:38,910 - INFO - [diffusion][Epoch 9693] diffusion learning rate: 0.001
2024-11-05 04:34:38,912 - INFO - [diffusion][Epoch 9693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:38,913 - INFO - [diffusion][Epoch 9694] Epoch 9695/12000
2024-11-05 04:34:43,119 - INFO - [diffusion][Epoch 9694] diffusion training Loss: 0.05346412491053343
2024-11-05 04:34:43,121 - INFO - [diffusion][Epoch 9694] diffusion learning rate: 0.001
2024-11-05 04:34:43,123 - INFO - [diffusion][Epoch 9694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:43,124 - INFO - [diffusion][Epoch 9695] Epoch 9696/12000
2024-11-05 04:34:47,237 - INFO - [diffusion][Epoch 9695] diffusion training Loss: 0.04858783911913633
2024-11-05 04:34:47,239 - INFO - [diffusion][Epoch 9695] diffusion learning rate: 0.001
2024-11-05 04:34:47,241 - INFO - [diffusion][Epoch 9695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:47,243 - INFO - [diffusion][Epoch 9696] Epoch 9697/12000
2024-11-05 04:34:51,206 - INFO - [diffusion][Epoch 9696] diffusion training Loss: 0.05244817025959492
2024-11-05 04:34:51,208 - INFO - [diffusion][Epoch 9696] diffusion learning rate: 0.001
2024-11-05 04:34:51,210 - INFO - [diffusion][Epoch 9696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:51,211 - INFO - [diffusion][Epoch 9697] Epoch 9698/12000
2024-11-05 04:34:55,334 - INFO - [diffusion][Epoch 9697] diffusion training Loss: 0.053205667063593864
2024-11-05 04:34:55,336 - INFO - [diffusion][Epoch 9697] diffusion learning rate: 0.001
2024-11-05 04:34:55,338 - INFO - [diffusion][Epoch 9697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:55,339 - INFO - [diffusion][Epoch 9698] Epoch 9699/12000
2024-11-05 04:34:59,478 - INFO - [diffusion][Epoch 9698] diffusion training Loss: 0.05047533381730318
2024-11-05 04:34:59,480 - INFO - [diffusion][Epoch 9698] diffusion learning rate: 0.001
2024-11-05 04:34:59,481 - INFO - [diffusion][Epoch 9698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:34:59,483 - INFO - [diffusion][Epoch 9699] Epoch 9700/12000
2024-11-05 04:35:03,658 - INFO - [diffusion][Epoch 9699] diffusion training Loss: 0.05012931488454342
2024-11-05 04:35:03,660 - INFO - [diffusion][Epoch 9699] diffusion learning rate: 0.001
2024-11-05 04:35:03,662 - INFO - [diffusion][Epoch 9699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:03,663 - INFO - [diffusion][Epoch 9700] Epoch 9701/12000
2024-11-05 04:35:07,777 - INFO - [diffusion][Epoch 9700] diffusion training Loss: 0.05100336391478777
2024-11-05 04:35:07,780 - INFO - [diffusion][Epoch 9700] diffusion learning rate: 0.001
2024-11-05 04:35:07,838 - INFO - [diffusion][Epoch 9700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:07,840 - INFO - [diffusion][Epoch 9701] Epoch 9702/12000
2024-11-05 04:35:11,957 - INFO - [diffusion][Epoch 9701] diffusion training Loss: 0.05036184098571539
2024-11-05 04:35:11,959 - INFO - [diffusion][Epoch 9701] diffusion learning rate: 0.001
2024-11-05 04:35:11,961 - INFO - [diffusion][Epoch 9701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:11,962 - INFO - [diffusion][Epoch 9702] Epoch 9703/12000
2024-11-05 04:35:16,075 - INFO - [diffusion][Epoch 9702] diffusion training Loss: 0.04976559802889824
2024-11-05 04:35:16,078 - INFO - [diffusion][Epoch 9702] diffusion learning rate: 0.001
2024-11-05 04:35:16,080 - INFO - [diffusion][Epoch 9702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:16,081 - INFO - [diffusion][Epoch 9703] Epoch 9704/12000
2024-11-05 04:35:20,255 - INFO - [diffusion][Epoch 9703] diffusion training Loss: 0.049981389194726944
2024-11-05 04:35:20,259 - INFO - [diffusion][Epoch 9703] diffusion learning rate: 0.001
2024-11-05 04:35:20,261 - INFO - [diffusion][Epoch 9703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:20,262 - INFO - [diffusion][Epoch 9704] Epoch 9705/12000
2024-11-05 04:35:24,416 - INFO - [diffusion][Epoch 9704] diffusion training Loss: 0.0508409570902586
2024-11-05 04:35:24,419 - INFO - [diffusion][Epoch 9704] diffusion learning rate: 0.001
2024-11-05 04:35:24,421 - INFO - [diffusion][Epoch 9704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:24,423 - INFO - [diffusion][Epoch 9705] Epoch 9706/12000
2024-11-05 04:35:28,476 - INFO - [diffusion][Epoch 9705] diffusion training Loss: 0.051288037560880184
2024-11-05 04:35:28,479 - INFO - [diffusion][Epoch 9705] diffusion learning rate: 0.001
2024-11-05 04:35:28,480 - INFO - [diffusion][Epoch 9705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:28,482 - INFO - [diffusion][Epoch 9706] Epoch 9707/12000
2024-11-05 04:35:32,626 - INFO - [diffusion][Epoch 9706] diffusion training Loss: 0.0532486354932189
2024-11-05 04:35:32,627 - INFO - [diffusion][Epoch 9706] diffusion learning rate: 0.001
2024-11-05 04:35:32,629 - INFO - [diffusion][Epoch 9706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:32,631 - INFO - [diffusion][Epoch 9707] Epoch 9708/12000
2024-11-05 04:35:36,791 - INFO - [diffusion][Epoch 9707] diffusion training Loss: 0.0453949561342597
2024-11-05 04:35:36,793 - INFO - [diffusion][Epoch 9707] diffusion learning rate: 0.001
2024-11-05 04:35:36,795 - INFO - [diffusion][Epoch 9707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:36,796 - INFO - [diffusion][Epoch 9708] Epoch 9709/12000
2024-11-05 04:35:40,921 - INFO - [diffusion][Epoch 9708] diffusion training Loss: 0.05189790576696396
2024-11-05 04:35:40,923 - INFO - [diffusion][Epoch 9708] diffusion learning rate: 0.001
2024-11-05 04:35:40,924 - INFO - [diffusion][Epoch 9708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:40,926 - INFO - [diffusion][Epoch 9709] Epoch 9710/12000
2024-11-05 04:35:45,026 - INFO - [diffusion][Epoch 9709] diffusion training Loss: 0.050162969157099724
2024-11-05 04:35:45,028 - INFO - [diffusion][Epoch 9709] diffusion learning rate: 0.001
2024-11-05 04:35:45,030 - INFO - [diffusion][Epoch 9709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:45,031 - INFO - [diffusion][Epoch 9710] Epoch 9711/12000
2024-11-05 04:35:49,114 - INFO - [diffusion][Epoch 9710] diffusion training Loss: 0.05683751218020916
2024-11-05 04:35:49,116 - INFO - [diffusion][Epoch 9710] diffusion learning rate: 0.001
2024-11-05 04:35:49,117 - INFO - [diffusion][Epoch 9710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:49,119 - INFO - [diffusion][Epoch 9711] Epoch 9712/12000
2024-11-05 04:35:53,231 - INFO - [diffusion][Epoch 9711] diffusion training Loss: 0.055491517297923565
2024-11-05 04:35:53,233 - INFO - [diffusion][Epoch 9711] diffusion learning rate: 0.001
2024-11-05 04:35:53,235 - INFO - [diffusion][Epoch 9711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:53,236 - INFO - [diffusion][Epoch 9712] Epoch 9713/12000
2024-11-05 04:35:57,291 - INFO - [diffusion][Epoch 9712] diffusion training Loss: 0.047950261272490025
2024-11-05 04:35:57,293 - INFO - [diffusion][Epoch 9712] diffusion learning rate: 0.001
2024-11-05 04:35:57,294 - INFO - [diffusion][Epoch 9712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:35:57,296 - INFO - [diffusion][Epoch 9713] Epoch 9714/12000
2024-11-05 04:36:01,403 - INFO - [diffusion][Epoch 9713] diffusion training Loss: 0.04887752514332533
2024-11-05 04:36:01,405 - INFO - [diffusion][Epoch 9713] diffusion learning rate: 0.001
2024-11-05 04:36:01,407 - INFO - [diffusion][Epoch 9713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:01,408 - INFO - [diffusion][Epoch 9714] Epoch 9715/12000
2024-11-05 04:36:05,440 - INFO - [diffusion][Epoch 9714] diffusion training Loss: 0.050408453680574894
2024-11-05 04:36:05,442 - INFO - [diffusion][Epoch 9714] diffusion learning rate: 0.001
2024-11-05 04:36:05,444 - INFO - [diffusion][Epoch 9714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:05,445 - INFO - [diffusion][Epoch 9715] Epoch 9716/12000
2024-11-05 04:36:09,549 - INFO - [diffusion][Epoch 9715] diffusion training Loss: 0.058208088390529156
2024-11-05 04:36:09,552 - INFO - [diffusion][Epoch 9715] diffusion learning rate: 0.001
2024-11-05 04:36:09,554 - INFO - [diffusion][Epoch 9715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:09,555 - INFO - [diffusion][Epoch 9716] Epoch 9717/12000
2024-11-05 04:36:13,637 - INFO - [diffusion][Epoch 9716] diffusion training Loss: 0.046748495660722256
2024-11-05 04:36:13,639 - INFO - [diffusion][Epoch 9716] diffusion learning rate: 0.001
2024-11-05 04:36:13,641 - INFO - [diffusion][Epoch 9716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:13,642 - INFO - [diffusion][Epoch 9717] Epoch 9718/12000
2024-11-05 04:36:17,720 - INFO - [diffusion][Epoch 9717] diffusion training Loss: 0.054239701479673386
2024-11-05 04:36:17,721 - INFO - [diffusion][Epoch 9717] diffusion learning rate: 0.001
2024-11-05 04:36:17,723 - INFO - [diffusion][Epoch 9717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:17,724 - INFO - [diffusion][Epoch 9718] Epoch 9719/12000
2024-11-05 04:36:22,089 - INFO - [diffusion][Epoch 9718] diffusion training Loss: 0.054104920476675034
2024-11-05 04:36:22,092 - INFO - [diffusion][Epoch 9718] diffusion learning rate: 0.001
2024-11-05 04:36:22,093 - INFO - [diffusion][Epoch 9718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:22,095 - INFO - [diffusion][Epoch 9719] Epoch 9720/12000
2024-11-05 04:36:26,202 - INFO - [diffusion][Epoch 9719] diffusion training Loss: 0.047987889498472214
2024-11-05 04:36:26,204 - INFO - [diffusion][Epoch 9719] diffusion learning rate: 0.001
2024-11-05 04:36:26,206 - INFO - [diffusion][Epoch 9719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:26,207 - INFO - [diffusion][Epoch 9720] Epoch 9721/12000
2024-11-05 04:36:30,322 - INFO - [diffusion][Epoch 9720] diffusion training Loss: 0.048263583332300186
2024-11-05 04:36:30,324 - INFO - [diffusion][Epoch 9720] diffusion learning rate: 0.001
2024-11-05 04:36:30,326 - INFO - [diffusion][Epoch 9720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:30,327 - INFO - [diffusion][Epoch 9721] Epoch 9722/12000
2024-11-05 04:36:34,422 - INFO - [diffusion][Epoch 9721] diffusion training Loss: 0.05014800652861595
2024-11-05 04:36:34,424 - INFO - [diffusion][Epoch 9721] diffusion learning rate: 0.001
2024-11-05 04:36:34,516 - INFO - [diffusion][Epoch 9721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:34,517 - INFO - [diffusion][Epoch 9722] Epoch 9723/12000
2024-11-05 04:36:38,670 - INFO - [diffusion][Epoch 9722] diffusion training Loss: 0.04982149973511696
2024-11-05 04:36:38,672 - INFO - [diffusion][Epoch 9722] diffusion learning rate: 0.001
2024-11-05 04:36:38,674 - INFO - [diffusion][Epoch 9722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:38,675 - INFO - [diffusion][Epoch 9723] Epoch 9724/12000
2024-11-05 04:36:42,677 - INFO - [diffusion][Epoch 9723] diffusion training Loss: 0.05112859047949314
2024-11-05 04:36:42,679 - INFO - [diffusion][Epoch 9723] diffusion learning rate: 0.001
2024-11-05 04:36:42,681 - INFO - [diffusion][Epoch 9723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:42,682 - INFO - [diffusion][Epoch 9724] Epoch 9725/12000
2024-11-05 04:36:46,770 - INFO - [diffusion][Epoch 9724] diffusion training Loss: 0.05052943527698517
2024-11-05 04:36:46,772 - INFO - [diffusion][Epoch 9724] diffusion learning rate: 0.001
2024-11-05 04:36:46,774 - INFO - [diffusion][Epoch 9724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:46,776 - INFO - [diffusion][Epoch 9725] Epoch 9726/12000
2024-11-05 04:36:50,907 - INFO - [diffusion][Epoch 9725] diffusion training Loss: 0.04237197898328304
2024-11-05 04:36:50,909 - INFO - [diffusion][Epoch 9725] diffusion learning rate: 0.001
2024-11-05 04:36:50,911 - INFO - [diffusion][Epoch 9725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:50,912 - INFO - [diffusion][Epoch 9726] Epoch 9727/12000
2024-11-05 04:36:55,011 - INFO - [diffusion][Epoch 9726] diffusion training Loss: 0.04961508419364691
2024-11-05 04:36:55,013 - INFO - [diffusion][Epoch 9726] diffusion learning rate: 0.001
2024-11-05 04:36:55,015 - INFO - [diffusion][Epoch 9726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:55,016 - INFO - [diffusion][Epoch 9727] Epoch 9728/12000
2024-11-05 04:36:59,103 - INFO - [diffusion][Epoch 9727] diffusion training Loss: 0.048225296661257744
2024-11-05 04:36:59,105 - INFO - [diffusion][Epoch 9727] diffusion learning rate: 0.001
2024-11-05 04:36:59,107 - INFO - [diffusion][Epoch 9727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:36:59,108 - INFO - [diffusion][Epoch 9728] Epoch 9729/12000
2024-11-05 04:37:03,237 - INFO - [diffusion][Epoch 9728] diffusion training Loss: 0.04753637220710516
2024-11-05 04:37:03,239 - INFO - [diffusion][Epoch 9728] diffusion learning rate: 0.001
2024-11-05 04:37:03,241 - INFO - [diffusion][Epoch 9728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:03,243 - INFO - [diffusion][Epoch 9729] Epoch 9730/12000
2024-11-05 04:37:07,357 - INFO - [diffusion][Epoch 9729] diffusion training Loss: 0.05092698149383068
2024-11-05 04:37:07,359 - INFO - [diffusion][Epoch 9729] diffusion learning rate: 0.001
2024-11-05 04:37:07,383 - INFO - [diffusion][Epoch 9729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:07,384 - INFO - [diffusion][Epoch 9730] Epoch 9731/12000
2024-11-05 04:37:11,403 - INFO - [diffusion][Epoch 9730] diffusion training Loss: 0.05615031346678734
2024-11-05 04:37:11,405 - INFO - [diffusion][Epoch 9730] diffusion learning rate: 0.001
2024-11-05 04:37:11,407 - INFO - [diffusion][Epoch 9730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:11,408 - INFO - [diffusion][Epoch 9731] Epoch 9732/12000
2024-11-05 04:37:15,478 - INFO - [diffusion][Epoch 9731] diffusion training Loss: 0.04841878078877926
2024-11-05 04:37:15,480 - INFO - [diffusion][Epoch 9731] diffusion learning rate: 0.001
2024-11-05 04:37:15,481 - INFO - [diffusion][Epoch 9731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:15,483 - INFO - [diffusion][Epoch 9732] Epoch 9733/12000
2024-11-05 04:37:19,590 - INFO - [diffusion][Epoch 9732] diffusion training Loss: 0.04765440057963133
2024-11-05 04:37:19,592 - INFO - [diffusion][Epoch 9732] diffusion learning rate: 0.001
2024-11-05 04:37:19,594 - INFO - [diffusion][Epoch 9732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:19,595 - INFO - [diffusion][Epoch 9733] Epoch 9734/12000
2024-11-05 04:37:23,757 - INFO - [diffusion][Epoch 9733] diffusion training Loss: 0.04943019151687622
2024-11-05 04:37:23,760 - INFO - [diffusion][Epoch 9733] diffusion learning rate: 0.001
2024-11-05 04:37:23,762 - INFO - [diffusion][Epoch 9733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:23,763 - INFO - [diffusion][Epoch 9734] Epoch 9735/12000
2024-11-05 04:37:27,895 - INFO - [diffusion][Epoch 9734] diffusion training Loss: 0.053677698597311974
2024-11-05 04:37:27,897 - INFO - [diffusion][Epoch 9734] diffusion learning rate: 0.001
2024-11-05 04:37:27,899 - INFO - [diffusion][Epoch 9734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:27,900 - INFO - [diffusion][Epoch 9735] Epoch 9736/12000
2024-11-05 04:37:31,975 - INFO - [diffusion][Epoch 9735] diffusion training Loss: 0.04769843816757202
2024-11-05 04:37:31,977 - INFO - [diffusion][Epoch 9735] diffusion learning rate: 0.001
2024-11-05 04:37:31,979 - INFO - [diffusion][Epoch 9735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:31,980 - INFO - [diffusion][Epoch 9736] Epoch 9737/12000
2024-11-05 04:37:36,117 - INFO - [diffusion][Epoch 9736] diffusion training Loss: 0.048046862706542015
2024-11-05 04:37:36,119 - INFO - [diffusion][Epoch 9736] diffusion learning rate: 0.001
2024-11-05 04:37:36,120 - INFO - [diffusion][Epoch 9736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:36,122 - INFO - [diffusion][Epoch 9737] Epoch 9738/12000
2024-11-05 04:37:40,245 - INFO - [diffusion][Epoch 9737] diffusion training Loss: 0.04909658804535866
2024-11-05 04:37:40,247 - INFO - [diffusion][Epoch 9737] diffusion learning rate: 0.001
2024-11-05 04:37:40,249 - INFO - [diffusion][Epoch 9737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:40,250 - INFO - [diffusion][Epoch 9738] Epoch 9739/12000
2024-11-05 04:37:44,679 - INFO - [diffusion][Epoch 9738] diffusion training Loss: 0.04663895070552826
2024-11-05 04:37:44,681 - INFO - [diffusion][Epoch 9738] diffusion learning rate: 0.001
2024-11-05 04:37:44,683 - INFO - [diffusion][Epoch 9738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:44,684 - INFO - [diffusion][Epoch 9739] Epoch 9740/12000
2024-11-05 04:37:48,837 - INFO - [diffusion][Epoch 9739] diffusion training Loss: 0.04696204233914614
2024-11-05 04:37:48,839 - INFO - [diffusion][Epoch 9739] diffusion learning rate: 0.001
2024-11-05 04:37:48,841 - INFO - [diffusion][Epoch 9739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:48,842 - INFO - [diffusion][Epoch 9740] Epoch 9741/12000
2024-11-05 04:37:52,962 - INFO - [diffusion][Epoch 9740] diffusion training Loss: 0.04957819264382124
2024-11-05 04:37:52,964 - INFO - [diffusion][Epoch 9740] diffusion learning rate: 0.001
2024-11-05 04:37:52,992 - INFO - [diffusion][Epoch 9740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:52,993 - INFO - [diffusion][Epoch 9741] Epoch 9742/12000
2024-11-05 04:37:57,093 - INFO - [diffusion][Epoch 9741] diffusion training Loss: 0.051397399976849556
2024-11-05 04:37:57,095 - INFO - [diffusion][Epoch 9741] diffusion learning rate: 0.001
2024-11-05 04:37:57,097 - INFO - [diffusion][Epoch 9741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:37:57,098 - INFO - [diffusion][Epoch 9742] Epoch 9743/12000
2024-11-05 04:38:01,162 - INFO - [diffusion][Epoch 9742] diffusion training Loss: 0.04825037997215986
2024-11-05 04:38:01,164 - INFO - [diffusion][Epoch 9742] diffusion learning rate: 0.001
2024-11-05 04:38:01,166 - INFO - [diffusion][Epoch 9742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:01,167 - INFO - [diffusion][Epoch 9743] Epoch 9744/12000
2024-11-05 04:38:05,114 - INFO - [diffusion][Epoch 9743] diffusion training Loss: 0.04984543193131685
2024-11-05 04:38:05,116 - INFO - [diffusion][Epoch 9743] diffusion learning rate: 0.001
2024-11-05 04:38:05,118 - INFO - [diffusion][Epoch 9743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:05,119 - INFO - [diffusion][Epoch 9744] Epoch 9745/12000
2024-11-05 04:38:09,123 - INFO - [diffusion][Epoch 9744] diffusion training Loss: 0.05252183694392443
2024-11-05 04:38:09,125 - INFO - [diffusion][Epoch 9744] diffusion learning rate: 0.001
2024-11-05 04:38:09,154 - INFO - [diffusion][Epoch 9744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:09,155 - INFO - [diffusion][Epoch 9745] Epoch 9746/12000
2024-11-05 04:38:13,264 - INFO - [diffusion][Epoch 9745] diffusion training Loss: 0.04456712678074837
2024-11-05 04:38:13,267 - INFO - [diffusion][Epoch 9745] diffusion learning rate: 0.001
2024-11-05 04:38:13,268 - INFO - [diffusion][Epoch 9745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:13,270 - INFO - [diffusion][Epoch 9746] Epoch 9747/12000
2024-11-05 04:38:17,388 - INFO - [diffusion][Epoch 9746] diffusion training Loss: 0.05736274644732475
2024-11-05 04:38:17,390 - INFO - [diffusion][Epoch 9746] diffusion learning rate: 0.001
2024-11-05 04:38:17,392 - INFO - [diffusion][Epoch 9746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:17,393 - INFO - [diffusion][Epoch 9747] Epoch 9748/12000
2024-11-05 04:38:21,476 - INFO - [diffusion][Epoch 9747] diffusion training Loss: 0.04997654352337122
2024-11-05 04:38:21,478 - INFO - [diffusion][Epoch 9747] diffusion learning rate: 0.001
2024-11-05 04:38:21,480 - INFO - [diffusion][Epoch 9747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:21,481 - INFO - [diffusion][Epoch 9748] Epoch 9749/12000
2024-11-05 04:38:25,577 - INFO - [diffusion][Epoch 9748] diffusion training Loss: 0.047191779129207134
2024-11-05 04:38:25,579 - INFO - [diffusion][Epoch 9748] diffusion learning rate: 0.001
2024-11-05 04:38:25,581 - INFO - [diffusion][Epoch 9748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:25,582 - INFO - [diffusion][Epoch 9749] Epoch 9750/12000
2024-11-05 04:38:29,699 - INFO - [diffusion][Epoch 9749] diffusion training Loss: 0.04781640414148569
2024-11-05 04:38:29,701 - INFO - [diffusion][Epoch 9749] diffusion learning rate: 0.001
2024-11-05 04:38:29,741 - INFO - [diffusion][Epoch 9749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:29,742 - INFO - [diffusion][Epoch 9750] Epoch 9751/12000
2024-11-05 04:38:33,890 - INFO - [diffusion][Epoch 9750] diffusion training Loss: 0.053782396018505096
2024-11-05 04:38:33,893 - INFO - [diffusion][Epoch 9750] diffusion learning rate: 0.001
2024-11-05 04:38:33,895 - INFO - [diffusion][Epoch 9750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:33,896 - INFO - [diffusion][Epoch 9751] Epoch 9752/12000
2024-11-05 04:38:38,036 - INFO - [diffusion][Epoch 9751] diffusion training Loss: 0.0520954355597496
2024-11-05 04:38:38,038 - INFO - [diffusion][Epoch 9751] diffusion learning rate: 0.001
2024-11-05 04:38:38,040 - INFO - [diffusion][Epoch 9751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:38,041 - INFO - [diffusion][Epoch 9752] Epoch 9753/12000
2024-11-05 04:38:42,190 - INFO - [diffusion][Epoch 9752] diffusion training Loss: 0.046449560672044754
2024-11-05 04:38:42,192 - INFO - [diffusion][Epoch 9752] diffusion learning rate: 0.001
2024-11-05 04:38:42,193 - INFO - [diffusion][Epoch 9752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:42,196 - INFO - [diffusion][Epoch 9753] Epoch 9754/12000
2024-11-05 04:38:46,301 - INFO - [diffusion][Epoch 9753] diffusion training Loss: 0.04640574660152197
2024-11-05 04:38:46,303 - INFO - [diffusion][Epoch 9753] diffusion learning rate: 0.001
2024-11-05 04:38:46,305 - INFO - [diffusion][Epoch 9753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:46,307 - INFO - [diffusion][Epoch 9754] Epoch 9755/12000
2024-11-05 04:38:50,405 - INFO - [diffusion][Epoch 9754] diffusion training Loss: 0.04926959611475468
2024-11-05 04:38:50,407 - INFO - [diffusion][Epoch 9754] diffusion learning rate: 0.001
2024-11-05 04:38:50,409 - INFO - [diffusion][Epoch 9754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:50,410 - INFO - [diffusion][Epoch 9755] Epoch 9756/12000
2024-11-05 04:38:54,456 - INFO - [diffusion][Epoch 9755] diffusion training Loss: 0.052018712274730206
2024-11-05 04:38:54,458 - INFO - [diffusion][Epoch 9755] diffusion learning rate: 0.001
2024-11-05 04:38:54,460 - INFO - [diffusion][Epoch 9755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:54,462 - INFO - [diffusion][Epoch 9756] Epoch 9757/12000
2024-11-05 04:38:58,482 - INFO - [diffusion][Epoch 9756] diffusion training Loss: 0.047186209820210934
2024-11-05 04:38:58,484 - INFO - [diffusion][Epoch 9756] diffusion learning rate: 0.001
2024-11-05 04:38:58,486 - INFO - [diffusion][Epoch 9756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:38:58,487 - INFO - [diffusion][Epoch 9757] Epoch 9758/12000
2024-11-05 04:39:02,456 - INFO - [diffusion][Epoch 9757] diffusion training Loss: 0.05106611270457506
2024-11-05 04:39:02,458 - INFO - [diffusion][Epoch 9757] diffusion learning rate: 0.001
2024-11-05 04:39:02,460 - INFO - [diffusion][Epoch 9757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:02,461 - INFO - [diffusion][Epoch 9758] Epoch 9759/12000
2024-11-05 04:39:06,391 - INFO - [diffusion][Epoch 9758] diffusion training Loss: 0.048402187414467335
2024-11-05 04:39:06,393 - INFO - [diffusion][Epoch 9758] diffusion learning rate: 0.001
2024-11-05 04:39:06,395 - INFO - [diffusion][Epoch 9758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:06,396 - INFO - [diffusion][Epoch 9759] Epoch 9760/12000
2024-11-05 04:39:10,823 - INFO - [diffusion][Epoch 9759] diffusion training Loss: 0.048754855059087276
2024-11-05 04:39:10,825 - INFO - [diffusion][Epoch 9759] diffusion learning rate: 0.001
2024-11-05 04:39:10,827 - INFO - [diffusion][Epoch 9759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:10,828 - INFO - [diffusion][Epoch 9760] Epoch 9761/12000
2024-11-05 04:39:14,794 - INFO - [diffusion][Epoch 9760] diffusion training Loss: 0.04837713949382305
2024-11-05 04:39:14,796 - INFO - [diffusion][Epoch 9760] diffusion learning rate: 0.001
2024-11-05 04:39:14,798 - INFO - [diffusion][Epoch 9760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:14,800 - INFO - [diffusion][Epoch 9761] Epoch 9762/12000
2024-11-05 04:39:18,861 - INFO - [diffusion][Epoch 9761] diffusion training Loss: 0.04642247501760721
2024-11-05 04:39:18,863 - INFO - [diffusion][Epoch 9761] diffusion learning rate: 0.001
2024-11-05 04:39:18,892 - INFO - [diffusion][Epoch 9761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:18,894 - INFO - [diffusion][Epoch 9762] Epoch 9763/12000
2024-11-05 04:39:23,187 - INFO - [diffusion][Epoch 9762] diffusion training Loss: 0.05112222861498594
2024-11-05 04:39:23,189 - INFO - [diffusion][Epoch 9762] diffusion learning rate: 0.001
2024-11-05 04:39:23,191 - INFO - [diffusion][Epoch 9762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:23,192 - INFO - [diffusion][Epoch 9763] Epoch 9764/12000
2024-11-05 04:39:27,343 - INFO - [diffusion][Epoch 9763] diffusion training Loss: 0.05067802779376507
2024-11-05 04:39:27,346 - INFO - [diffusion][Epoch 9763] diffusion learning rate: 0.001
2024-11-05 04:39:27,348 - INFO - [diffusion][Epoch 9763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:27,349 - INFO - [diffusion][Epoch 9764] Epoch 9765/12000
2024-11-05 04:39:31,491 - INFO - [diffusion][Epoch 9764] diffusion training Loss: 0.049697247333824635
2024-11-05 04:39:31,494 - INFO - [diffusion][Epoch 9764] diffusion learning rate: 0.001
2024-11-05 04:39:31,495 - INFO - [diffusion][Epoch 9764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:31,497 - INFO - [diffusion][Epoch 9765] Epoch 9766/12000
2024-11-05 04:39:35,641 - INFO - [diffusion][Epoch 9765] diffusion training Loss: 0.04898562002927065
2024-11-05 04:39:35,644 - INFO - [diffusion][Epoch 9765] diffusion learning rate: 0.001
2024-11-05 04:39:35,646 - INFO - [diffusion][Epoch 9765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:35,647 - INFO - [diffusion][Epoch 9766] Epoch 9767/12000
2024-11-05 04:39:39,717 - INFO - [diffusion][Epoch 9766] diffusion training Loss: 0.05155657511204481
2024-11-05 04:39:39,719 - INFO - [diffusion][Epoch 9766] diffusion learning rate: 0.001
2024-11-05 04:39:39,721 - INFO - [diffusion][Epoch 9766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:39,722 - INFO - [diffusion][Epoch 9767] Epoch 9768/12000
2024-11-05 04:39:43,799 - INFO - [diffusion][Epoch 9767] diffusion training Loss: 0.04867906495928764
2024-11-05 04:39:43,801 - INFO - [diffusion][Epoch 9767] diffusion learning rate: 0.001
2024-11-05 04:39:43,802 - INFO - [diffusion][Epoch 9767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:43,803 - INFO - [diffusion][Epoch 9768] Epoch 9769/12000
2024-11-05 04:39:47,883 - INFO - [diffusion][Epoch 9768] diffusion training Loss: 0.05103789642453194
2024-11-05 04:39:47,885 - INFO - [diffusion][Epoch 9768] diffusion learning rate: 0.001
2024-11-05 04:39:47,887 - INFO - [diffusion][Epoch 9768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:47,888 - INFO - [diffusion][Epoch 9769] Epoch 9770/12000
2024-11-05 04:39:52,018 - INFO - [diffusion][Epoch 9769] diffusion training Loss: 0.04857164714485407
2024-11-05 04:39:52,020 - INFO - [diffusion][Epoch 9769] diffusion learning rate: 0.001
2024-11-05 04:39:52,022 - INFO - [diffusion][Epoch 9769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:52,023 - INFO - [diffusion][Epoch 9770] Epoch 9771/12000
2024-11-05 04:39:56,091 - INFO - [diffusion][Epoch 9770] diffusion training Loss: 0.04994232766330242
2024-11-05 04:39:56,092 - INFO - [diffusion][Epoch 9770] diffusion learning rate: 0.001
2024-11-05 04:39:56,094 - INFO - [diffusion][Epoch 9770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:39:56,095 - INFO - [diffusion][Epoch 9771] Epoch 9772/12000
2024-11-05 04:40:00,135 - INFO - [diffusion][Epoch 9771] diffusion training Loss: 0.05383352376520634
2024-11-05 04:40:00,137 - INFO - [diffusion][Epoch 9771] diffusion learning rate: 0.001
2024-11-05 04:40:00,138 - INFO - [diffusion][Epoch 9771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:00,140 - INFO - [diffusion][Epoch 9772] Epoch 9773/12000
2024-11-05 04:40:04,235 - INFO - [diffusion][Epoch 9772] diffusion training Loss: 0.055897509679198265
2024-11-05 04:40:04,237 - INFO - [diffusion][Epoch 9772] diffusion learning rate: 0.001
2024-11-05 04:40:04,239 - INFO - [diffusion][Epoch 9772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:04,240 - INFO - [diffusion][Epoch 9773] Epoch 9774/12000
2024-11-05 04:40:08,338 - INFO - [diffusion][Epoch 9773] diffusion training Loss: 0.049641299061477184
2024-11-05 04:40:08,340 - INFO - [diffusion][Epoch 9773] diffusion learning rate: 0.001
2024-11-05 04:40:08,342 - INFO - [diffusion][Epoch 9773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:08,343 - INFO - [diffusion][Epoch 9774] Epoch 9775/12000
2024-11-05 04:40:12,413 - INFO - [diffusion][Epoch 9774] diffusion training Loss: 0.04760399367660284
2024-11-05 04:40:12,433 - INFO - [diffusion][Epoch 9774] diffusion learning rate: 0.001
2024-11-05 04:40:12,435 - INFO - [diffusion][Epoch 9774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:12,436 - INFO - [diffusion][Epoch 9775] Epoch 9776/12000
2024-11-05 04:40:16,494 - INFO - [diffusion][Epoch 9775] diffusion training Loss: 0.0496728615835309
2024-11-05 04:40:16,496 - INFO - [diffusion][Epoch 9775] diffusion learning rate: 0.001
2024-11-05 04:40:16,498 - INFO - [diffusion][Epoch 9775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:16,500 - INFO - [diffusion][Epoch 9776] Epoch 9777/12000
2024-11-05 04:40:20,583 - INFO - [diffusion][Epoch 9776] diffusion training Loss: 0.05286296550184488
2024-11-05 04:40:20,585 - INFO - [diffusion][Epoch 9776] diffusion learning rate: 0.001
2024-11-05 04:40:20,587 - INFO - [diffusion][Epoch 9776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:20,588 - INFO - [diffusion][Epoch 9777] Epoch 9778/12000
2024-11-05 04:40:24,730 - INFO - [diffusion][Epoch 9777] diffusion training Loss: 0.05195336602628231
2024-11-05 04:40:24,732 - INFO - [diffusion][Epoch 9777] diffusion learning rate: 0.001
2024-11-05 04:40:24,734 - INFO - [diffusion][Epoch 9777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:24,735 - INFO - [diffusion][Epoch 9778] Epoch 9779/12000
2024-11-05 04:40:28,780 - INFO - [diffusion][Epoch 9778] diffusion training Loss: 0.04843105189502239
2024-11-05 04:40:28,783 - INFO - [diffusion][Epoch 9778] diffusion learning rate: 0.001
2024-11-05 04:40:28,786 - INFO - [diffusion][Epoch 9778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:28,787 - INFO - [diffusion][Epoch 9779] Epoch 9780/12000
2024-11-05 04:40:33,017 - INFO - [diffusion][Epoch 9779] diffusion training Loss: 0.04639051854610443
2024-11-05 04:40:33,019 - INFO - [diffusion][Epoch 9779] diffusion learning rate: 0.001
2024-11-05 04:40:33,021 - INFO - [diffusion][Epoch 9779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:33,022 - INFO - [diffusion][Epoch 9780] Epoch 9781/12000
2024-11-05 04:40:37,124 - INFO - [diffusion][Epoch 9780] diffusion training Loss: 0.04857879597693682
2024-11-05 04:40:37,126 - INFO - [diffusion][Epoch 9780] diffusion learning rate: 0.001
2024-11-05 04:40:37,128 - INFO - [diffusion][Epoch 9780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:37,129 - INFO - [diffusion][Epoch 9781] Epoch 9782/12000
2024-11-05 04:40:41,320 - INFO - [diffusion][Epoch 9781] diffusion training Loss: 0.05361371487379074
2024-11-05 04:40:41,322 - INFO - [diffusion][Epoch 9781] diffusion learning rate: 0.001
2024-11-05 04:40:41,324 - INFO - [diffusion][Epoch 9781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:41,325 - INFO - [diffusion][Epoch 9782] Epoch 9783/12000
2024-11-05 04:40:45,402 - INFO - [diffusion][Epoch 9782] diffusion training Loss: 0.05031617172062397
2024-11-05 04:40:45,404 - INFO - [diffusion][Epoch 9782] diffusion learning rate: 0.001
2024-11-05 04:40:45,406 - INFO - [diffusion][Epoch 9782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:45,407 - INFO - [diffusion][Epoch 9783] Epoch 9784/12000
2024-11-05 04:40:49,451 - INFO - [diffusion][Epoch 9783] diffusion training Loss: 0.05319345463067293
2024-11-05 04:40:49,453 - INFO - [diffusion][Epoch 9783] diffusion learning rate: 0.001
2024-11-05 04:40:49,455 - INFO - [diffusion][Epoch 9783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:49,456 - INFO - [diffusion][Epoch 9784] Epoch 9785/12000
2024-11-05 04:40:53,583 - INFO - [diffusion][Epoch 9784] diffusion training Loss: 0.05294742714613676
2024-11-05 04:40:53,585 - INFO - [diffusion][Epoch 9784] diffusion learning rate: 0.001
2024-11-05 04:40:53,587 - INFO - [diffusion][Epoch 9784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:53,589 - INFO - [diffusion][Epoch 9785] Epoch 9786/12000
2024-11-05 04:40:57,645 - INFO - [diffusion][Epoch 9785] diffusion training Loss: 0.05223133787512779
2024-11-05 04:40:57,647 - INFO - [diffusion][Epoch 9785] diffusion learning rate: 0.001
2024-11-05 04:40:57,649 - INFO - [diffusion][Epoch 9785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:40:57,650 - INFO - [diffusion][Epoch 9786] Epoch 9787/12000
2024-11-05 04:41:01,799 - INFO - [diffusion][Epoch 9786] diffusion training Loss: 0.04794766288250685
2024-11-05 04:41:01,801 - INFO - [diffusion][Epoch 9786] diffusion learning rate: 0.001
2024-11-05 04:41:01,802 - INFO - [diffusion][Epoch 9786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:01,804 - INFO - [diffusion][Epoch 9787] Epoch 9788/12000
2024-11-05 04:41:05,915 - INFO - [diffusion][Epoch 9787] diffusion training Loss: 0.05010204203426838
2024-11-05 04:41:05,917 - INFO - [diffusion][Epoch 9787] diffusion learning rate: 0.001
2024-11-05 04:41:05,919 - INFO - [diffusion][Epoch 9787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:05,920 - INFO - [diffusion][Epoch 9788] Epoch 9789/12000
2024-11-05 04:41:10,047 - INFO - [diffusion][Epoch 9788] diffusion training Loss: 0.05005986522883177
2024-11-05 04:41:10,049 - INFO - [diffusion][Epoch 9788] diffusion learning rate: 0.001
2024-11-05 04:41:10,051 - INFO - [diffusion][Epoch 9788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:10,052 - INFO - [diffusion][Epoch 9789] Epoch 9790/12000
2024-11-05 04:41:14,154 - INFO - [diffusion][Epoch 9789] diffusion training Loss: 0.05298346467316151
2024-11-05 04:41:14,156 - INFO - [diffusion][Epoch 9789] diffusion learning rate: 0.001
2024-11-05 04:41:14,184 - INFO - [diffusion][Epoch 9789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:14,185 - INFO - [diffusion][Epoch 9790] Epoch 9791/12000
2024-11-05 04:41:18,309 - INFO - [diffusion][Epoch 9790] diffusion training Loss: 0.050541533157229424
2024-11-05 04:41:18,311 - INFO - [diffusion][Epoch 9790] diffusion learning rate: 0.001
2024-11-05 04:41:18,313 - INFO - [diffusion][Epoch 9790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:18,314 - INFO - [diffusion][Epoch 9791] Epoch 9792/12000
2024-11-05 04:41:22,445 - INFO - [diffusion][Epoch 9791] diffusion training Loss: 0.05239632911980152
2024-11-05 04:41:22,447 - INFO - [diffusion][Epoch 9791] diffusion learning rate: 0.001
2024-11-05 04:41:22,449 - INFO - [diffusion][Epoch 9791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:22,450 - INFO - [diffusion][Epoch 9792] Epoch 9793/12000
2024-11-05 04:41:26,586 - INFO - [diffusion][Epoch 9792] diffusion training Loss: 0.04485192243009806
2024-11-05 04:41:26,588 - INFO - [diffusion][Epoch 9792] diffusion learning rate: 0.001
2024-11-05 04:41:26,589 - INFO - [diffusion][Epoch 9792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:26,591 - INFO - [diffusion][Epoch 9793] Epoch 9794/12000
2024-11-05 04:41:30,735 - INFO - [diffusion][Epoch 9793] diffusion training Loss: 0.05269319098442793
2024-11-05 04:41:30,737 - INFO - [diffusion][Epoch 9793] diffusion learning rate: 0.001
2024-11-05 04:41:30,739 - INFO - [diffusion][Epoch 9793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:30,740 - INFO - [diffusion][Epoch 9794] Epoch 9795/12000
2024-11-05 04:41:34,796 - INFO - [diffusion][Epoch 9794] diffusion training Loss: 0.04857063293457031
2024-11-05 04:41:34,798 - INFO - [diffusion][Epoch 9794] diffusion learning rate: 0.001
2024-11-05 04:41:34,800 - INFO - [diffusion][Epoch 9794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:34,801 - INFO - [diffusion][Epoch 9795] Epoch 9796/12000
2024-11-05 04:41:38,892 - INFO - [diffusion][Epoch 9795] diffusion training Loss: 0.05096018500626087
2024-11-05 04:41:38,893 - INFO - [diffusion][Epoch 9795] diffusion learning rate: 0.001
2024-11-05 04:41:38,896 - INFO - [diffusion][Epoch 9795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:38,897 - INFO - [diffusion][Epoch 9796] Epoch 9797/12000
2024-11-05 04:41:43,061 - INFO - [diffusion][Epoch 9796] diffusion training Loss: 0.055638840422034264
2024-11-05 04:41:43,063 - INFO - [diffusion][Epoch 9796] diffusion learning rate: 0.001
2024-11-05 04:41:43,065 - INFO - [diffusion][Epoch 9796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:43,066 - INFO - [diffusion][Epoch 9797] Epoch 9798/12000
2024-11-05 04:41:47,144 - INFO - [diffusion][Epoch 9797] diffusion training Loss: 0.04837398324161768
2024-11-05 04:41:47,147 - INFO - [diffusion][Epoch 9797] diffusion learning rate: 0.001
2024-11-05 04:41:47,149 - INFO - [diffusion][Epoch 9797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:47,152 - INFO - [diffusion][Epoch 9798] Epoch 9799/12000
2024-11-05 04:41:51,206 - INFO - [diffusion][Epoch 9798] diffusion training Loss: 0.04756058380007744
2024-11-05 04:41:51,208 - INFO - [diffusion][Epoch 9798] diffusion learning rate: 0.001
2024-11-05 04:41:51,209 - INFO - [diffusion][Epoch 9798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:51,211 - INFO - [diffusion][Epoch 9799] Epoch 9800/12000
2024-11-05 04:41:55,175 - INFO - [diffusion][Epoch 9799] diffusion training Loss: 0.05109167844057083
2024-11-05 04:41:55,177 - INFO - [diffusion][Epoch 9799] diffusion learning rate: 0.001
2024-11-05 04:41:55,179 - INFO - [diffusion][Epoch 9799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:55,180 - INFO - [diffusion][Epoch 9800] Epoch 9801/12000
2024-11-05 04:41:59,419 - INFO - [diffusion][Epoch 9800] diffusion training Loss: 0.049165849573910236
2024-11-05 04:41:59,421 - INFO - [diffusion][Epoch 9800] diffusion learning rate: 0.001
2024-11-05 04:41:59,423 - INFO - [diffusion][Epoch 9800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:41:59,424 - INFO - [diffusion][Epoch 9801] Epoch 9802/12000
2024-11-05 04:42:03,267 - INFO - [diffusion][Epoch 9801] diffusion training Loss: 0.04628225043416023
2024-11-05 04:42:03,269 - INFO - [diffusion][Epoch 9801] diffusion learning rate: 0.001
2024-11-05 04:42:03,297 - INFO - [diffusion][Epoch 9801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:03,298 - INFO - [diffusion][Epoch 9802] Epoch 9803/12000
2024-11-05 04:42:07,208 - INFO - [diffusion][Epoch 9802] diffusion training Loss: 0.04951019585132599
2024-11-05 04:42:07,210 - INFO - [diffusion][Epoch 9802] diffusion learning rate: 0.001
2024-11-05 04:42:07,212 - INFO - [diffusion][Epoch 9802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:07,214 - INFO - [diffusion][Epoch 9803] Epoch 9804/12000
2024-11-05 04:42:11,224 - INFO - [diffusion][Epoch 9803] diffusion training Loss: 0.04397098999470472
2024-11-05 04:42:11,226 - INFO - [diffusion][Epoch 9803] diffusion learning rate: 0.001
2024-11-05 04:42:11,228 - INFO - [diffusion][Epoch 9803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:11,229 - INFO - [diffusion][Epoch 9804] Epoch 9805/12000
2024-11-05 04:42:15,241 - INFO - [diffusion][Epoch 9804] diffusion training Loss: 0.042939113453030586
2024-11-05 04:42:15,243 - INFO - [diffusion][Epoch 9804] diffusion learning rate: 0.001
2024-11-05 04:42:15,245 - INFO - [diffusion][Epoch 9804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:15,247 - INFO - [diffusion][Epoch 9805] Epoch 9806/12000
2024-11-05 04:42:19,254 - INFO - [diffusion][Epoch 9805] diffusion training Loss: 0.050023822113871574
2024-11-05 04:42:19,256 - INFO - [diffusion][Epoch 9805] diffusion learning rate: 0.001
2024-11-05 04:42:19,258 - INFO - [diffusion][Epoch 9805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:19,259 - INFO - [diffusion][Epoch 9806] Epoch 9807/12000
2024-11-05 04:42:23,315 - INFO - [diffusion][Epoch 9806] diffusion training Loss: 0.049433743581175804
2024-11-05 04:42:23,318 - INFO - [diffusion][Epoch 9806] diffusion learning rate: 0.001
2024-11-05 04:42:23,320 - INFO - [diffusion][Epoch 9806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:23,321 - INFO - [diffusion][Epoch 9807] Epoch 9808/12000
2024-11-05 04:42:27,300 - INFO - [diffusion][Epoch 9807] diffusion training Loss: 0.04862980172038078
2024-11-05 04:42:27,302 - INFO - [diffusion][Epoch 9807] diffusion learning rate: 0.001
2024-11-05 04:42:27,304 - INFO - [diffusion][Epoch 9807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:27,306 - INFO - [diffusion][Epoch 9808] Epoch 9809/12000
2024-11-05 04:42:31,252 - INFO - [diffusion][Epoch 9808] diffusion training Loss: 0.04403146542608738
2024-11-05 04:42:31,254 - INFO - [diffusion][Epoch 9808] diffusion learning rate: 0.001
2024-11-05 04:42:31,256 - INFO - [diffusion][Epoch 9808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:31,257 - INFO - [diffusion][Epoch 9809] Epoch 9810/12000
2024-11-05 04:42:35,157 - INFO - [diffusion][Epoch 9809] diffusion training Loss: 0.05396029446274042
2024-11-05 04:42:35,159 - INFO - [diffusion][Epoch 9809] diffusion learning rate: 0.001
2024-11-05 04:42:35,160 - INFO - [diffusion][Epoch 9809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:35,162 - INFO - [diffusion][Epoch 9810] Epoch 9811/12000
2024-11-05 04:42:39,113 - INFO - [diffusion][Epoch 9810] diffusion training Loss: 0.05178340710699558
2024-11-05 04:42:39,115 - INFO - [diffusion][Epoch 9810] diffusion learning rate: 0.001
2024-11-05 04:42:39,117 - INFO - [diffusion][Epoch 9810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:39,118 - INFO - [diffusion][Epoch 9811] Epoch 9812/12000
2024-11-05 04:42:43,040 - INFO - [diffusion][Epoch 9811] diffusion training Loss: 0.05123499408364296
2024-11-05 04:42:43,042 - INFO - [diffusion][Epoch 9811] diffusion learning rate: 0.001
2024-11-05 04:42:43,044 - INFO - [diffusion][Epoch 9811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:43,046 - INFO - [diffusion][Epoch 9812] Epoch 9813/12000
2024-11-05 04:42:46,923 - INFO - [diffusion][Epoch 9812] diffusion training Loss: 0.04956288915127516
2024-11-05 04:42:46,925 - INFO - [diffusion][Epoch 9812] diffusion learning rate: 0.001
2024-11-05 04:42:46,927 - INFO - [diffusion][Epoch 9812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:46,928 - INFO - [diffusion][Epoch 9813] Epoch 9814/12000
2024-11-05 04:42:51,024 - INFO - [diffusion][Epoch 9813] diffusion training Loss: 0.0489032082259655
2024-11-05 04:42:51,025 - INFO - [diffusion][Epoch 9813] diffusion learning rate: 0.001
2024-11-05 04:42:51,027 - INFO - [diffusion][Epoch 9813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:51,028 - INFO - [diffusion][Epoch 9814] Epoch 9815/12000
2024-11-05 04:42:55,116 - INFO - [diffusion][Epoch 9814] diffusion training Loss: 0.049751500599086285
2024-11-05 04:42:55,118 - INFO - [diffusion][Epoch 9814] diffusion learning rate: 0.001
2024-11-05 04:42:55,120 - INFO - [diffusion][Epoch 9814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:55,122 - INFO - [diffusion][Epoch 9815] Epoch 9816/12000
2024-11-05 04:42:59,222 - INFO - [diffusion][Epoch 9815] diffusion training Loss: 0.05185424629598856
2024-11-05 04:42:59,224 - INFO - [diffusion][Epoch 9815] diffusion learning rate: 0.001
2024-11-05 04:42:59,226 - INFO - [diffusion][Epoch 9815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:42:59,228 - INFO - [diffusion][Epoch 9816] Epoch 9817/12000
2024-11-05 04:43:03,339 - INFO - [diffusion][Epoch 9816] diffusion training Loss: 0.05258506163954735
2024-11-05 04:43:03,341 - INFO - [diffusion][Epoch 9816] diffusion learning rate: 0.001
2024-11-05 04:43:03,343 - INFO - [diffusion][Epoch 9816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:03,344 - INFO - [diffusion][Epoch 9817] Epoch 9818/12000
2024-11-05 04:43:07,435 - INFO - [diffusion][Epoch 9817] diffusion training Loss: 0.05114118196070194
2024-11-05 04:43:07,437 - INFO - [diffusion][Epoch 9817] diffusion learning rate: 0.001
2024-11-05 04:43:07,439 - INFO - [diffusion][Epoch 9817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:07,440 - INFO - [diffusion][Epoch 9818] Epoch 9819/12000
2024-11-05 04:43:11,528 - INFO - [diffusion][Epoch 9818] diffusion training Loss: 0.050466522574424744
2024-11-05 04:43:11,530 - INFO - [diffusion][Epoch 9818] diffusion learning rate: 0.001
2024-11-05 04:43:11,532 - INFO - [diffusion][Epoch 9818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:11,533 - INFO - [diffusion][Epoch 9819] Epoch 9820/12000
2024-11-05 04:43:15,643 - INFO - [diffusion][Epoch 9819] diffusion training Loss: 0.055671160109341145
2024-11-05 04:43:15,645 - INFO - [diffusion][Epoch 9819] diffusion learning rate: 0.001
2024-11-05 04:43:15,647 - INFO - [diffusion][Epoch 9819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:15,649 - INFO - [diffusion][Epoch 9820] Epoch 9821/12000
2024-11-05 04:43:19,715 - INFO - [diffusion][Epoch 9820] diffusion training Loss: 0.048579684458673
2024-11-05 04:43:19,717 - INFO - [diffusion][Epoch 9820] diffusion learning rate: 0.001
2024-11-05 04:43:19,719 - INFO - [diffusion][Epoch 9820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:19,720 - INFO - [diffusion][Epoch 9821] Epoch 9822/12000
2024-11-05 04:43:24,038 - INFO - [diffusion][Epoch 9821] diffusion training Loss: 0.050359269604086876
2024-11-05 04:43:24,040 - INFO - [diffusion][Epoch 9821] diffusion learning rate: 0.001
2024-11-05 04:43:24,042 - INFO - [diffusion][Epoch 9821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:24,043 - INFO - [diffusion][Epoch 9822] Epoch 9823/12000
2024-11-05 04:43:27,964 - INFO - [diffusion][Epoch 9822] diffusion training Loss: 0.04946206230670214
2024-11-05 04:43:27,966 - INFO - [diffusion][Epoch 9822] diffusion learning rate: 0.001
2024-11-05 04:43:27,968 - INFO - [diffusion][Epoch 9822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:27,969 - INFO - [diffusion][Epoch 9823] Epoch 9824/12000
2024-11-05 04:43:32,084 - INFO - [diffusion][Epoch 9823] diffusion training Loss: 0.04780109133571386
2024-11-05 04:43:32,089 - INFO - [diffusion][Epoch 9823] diffusion learning rate: 0.001
2024-11-05 04:43:32,093 - INFO - [diffusion][Epoch 9823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:32,095 - INFO - [diffusion][Epoch 9824] Epoch 9825/12000
2024-11-05 04:43:36,080 - INFO - [diffusion][Epoch 9824] diffusion training Loss: 0.047427318058907986
2024-11-05 04:43:36,082 - INFO - [diffusion][Epoch 9824] diffusion learning rate: 0.001
2024-11-05 04:43:36,084 - INFO - [diffusion][Epoch 9824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:36,085 - INFO - [diffusion][Epoch 9825] Epoch 9826/12000
2024-11-05 04:43:40,172 - INFO - [diffusion][Epoch 9825] diffusion training Loss: 0.05327034741640091
2024-11-05 04:43:40,174 - INFO - [diffusion][Epoch 9825] diffusion learning rate: 0.001
2024-11-05 04:43:40,175 - INFO - [diffusion][Epoch 9825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:40,177 - INFO - [diffusion][Epoch 9826] Epoch 9827/12000
2024-11-05 04:43:44,246 - INFO - [diffusion][Epoch 9826] diffusion training Loss: 0.04227823670953512
2024-11-05 04:43:44,248 - INFO - [diffusion][Epoch 9826] diffusion learning rate: 0.001
2024-11-05 04:43:44,250 - INFO - [diffusion][Epoch 9826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:44,252 - INFO - [diffusion][Epoch 9827] Epoch 9828/12000
2024-11-05 04:43:48,359 - INFO - [diffusion][Epoch 9827] diffusion training Loss: 0.05504426918923855
2024-11-05 04:43:48,361 - INFO - [diffusion][Epoch 9827] diffusion learning rate: 0.001
2024-11-05 04:43:48,363 - INFO - [diffusion][Epoch 9827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:48,364 - INFO - [diffusion][Epoch 9828] Epoch 9829/12000
2024-11-05 04:43:52,529 - INFO - [diffusion][Epoch 9828] diffusion training Loss: 0.05085349082946777
2024-11-05 04:43:52,531 - INFO - [diffusion][Epoch 9828] diffusion learning rate: 0.001
2024-11-05 04:43:52,534 - INFO - [diffusion][Epoch 9828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:52,536 - INFO - [diffusion][Epoch 9829] Epoch 9830/12000
2024-11-05 04:43:56,455 - INFO - [diffusion][Epoch 9829] diffusion training Loss: 0.048020243644714355
2024-11-05 04:43:56,457 - INFO - [diffusion][Epoch 9829] diffusion learning rate: 0.001
2024-11-05 04:43:56,466 - INFO - [diffusion][Epoch 9829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:43:56,467 - INFO - [diffusion][Epoch 9830] Epoch 9831/12000
2024-11-05 04:44:00,492 - INFO - [diffusion][Epoch 9830] diffusion training Loss: 0.049803074449300766
2024-11-05 04:44:00,494 - INFO - [diffusion][Epoch 9830] diffusion learning rate: 0.001
2024-11-05 04:44:00,495 - INFO - [diffusion][Epoch 9830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:00,497 - INFO - [diffusion][Epoch 9831] Epoch 9832/12000
2024-11-05 04:44:04,593 - INFO - [diffusion][Epoch 9831] diffusion training Loss: 0.051495970226824284
2024-11-05 04:44:04,595 - INFO - [diffusion][Epoch 9831] diffusion learning rate: 0.001
2024-11-05 04:44:04,596 - INFO - [diffusion][Epoch 9831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:04,598 - INFO - [diffusion][Epoch 9832] Epoch 9833/12000
2024-11-05 04:44:08,695 - INFO - [diffusion][Epoch 9832] diffusion training Loss: 0.053360085003077984
2024-11-05 04:44:08,698 - INFO - [diffusion][Epoch 9832] diffusion learning rate: 0.001
2024-11-05 04:44:08,699 - INFO - [diffusion][Epoch 9832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:08,701 - INFO - [diffusion][Epoch 9833] Epoch 9834/12000
2024-11-05 04:44:12,858 - INFO - [diffusion][Epoch 9833] diffusion training Loss: 0.05534906592220068
2024-11-05 04:44:12,860 - INFO - [diffusion][Epoch 9833] diffusion learning rate: 0.001
2024-11-05 04:44:12,861 - INFO - [diffusion][Epoch 9833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:12,863 - INFO - [diffusion][Epoch 9834] Epoch 9835/12000
2024-11-05 04:44:16,963 - INFO - [diffusion][Epoch 9834] diffusion training Loss: 0.05063463840633631
2024-11-05 04:44:16,965 - INFO - [diffusion][Epoch 9834] diffusion learning rate: 0.001
2024-11-05 04:44:16,966 - INFO - [diffusion][Epoch 9834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:16,968 - INFO - [diffusion][Epoch 9835] Epoch 9836/12000
2024-11-05 04:44:21,070 - INFO - [diffusion][Epoch 9835] diffusion training Loss: 0.05019200686365366
2024-11-05 04:44:21,072 - INFO - [diffusion][Epoch 9835] diffusion learning rate: 0.001
2024-11-05 04:44:21,074 - INFO - [diffusion][Epoch 9835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:21,075 - INFO - [diffusion][Epoch 9836] Epoch 9837/12000
2024-11-05 04:44:25,186 - INFO - [diffusion][Epoch 9836] diffusion training Loss: 0.05508668161928654
2024-11-05 04:44:25,188 - INFO - [diffusion][Epoch 9836] diffusion learning rate: 0.001
2024-11-05 04:44:25,190 - INFO - [diffusion][Epoch 9836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:25,191 - INFO - [diffusion][Epoch 9837] Epoch 9838/12000
2024-11-05 04:44:29,271 - INFO - [diffusion][Epoch 9837] diffusion training Loss: 0.04708295501768589
2024-11-05 04:44:29,273 - INFO - [diffusion][Epoch 9837] diffusion learning rate: 0.001
2024-11-05 04:44:29,274 - INFO - [diffusion][Epoch 9837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:29,276 - INFO - [diffusion][Epoch 9838] Epoch 9839/12000
2024-11-05 04:44:33,304 - INFO - [diffusion][Epoch 9838] diffusion training Loss: 0.04730757884681225
2024-11-05 04:44:33,308 - INFO - [diffusion][Epoch 9838] diffusion learning rate: 0.001
2024-11-05 04:44:33,310 - INFO - [diffusion][Epoch 9838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:33,311 - INFO - [diffusion][Epoch 9839] Epoch 9840/12000
2024-11-05 04:44:37,297 - INFO - [diffusion][Epoch 9839] diffusion training Loss: 0.04999072290956974
2024-11-05 04:44:37,299 - INFO - [diffusion][Epoch 9839] diffusion learning rate: 0.001
2024-11-05 04:44:37,301 - INFO - [diffusion][Epoch 9839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:37,302 - INFO - [diffusion][Epoch 9840] Epoch 9841/12000
2024-11-05 04:44:41,389 - INFO - [diffusion][Epoch 9840] diffusion training Loss: 0.053300537168979645
2024-11-05 04:44:41,391 - INFO - [diffusion][Epoch 9840] diffusion learning rate: 0.001
2024-11-05 04:44:41,392 - INFO - [diffusion][Epoch 9840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:41,394 - INFO - [diffusion][Epoch 9841] Epoch 9842/12000
2024-11-05 04:44:45,521 - INFO - [diffusion][Epoch 9841] diffusion training Loss: 0.05268663167953491
2024-11-05 04:44:45,523 - INFO - [diffusion][Epoch 9841] diffusion learning rate: 0.001
2024-11-05 04:44:45,525 - INFO - [diffusion][Epoch 9841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:45,526 - INFO - [diffusion][Epoch 9842] Epoch 9843/12000
2024-11-05 04:44:49,622 - INFO - [diffusion][Epoch 9842] diffusion training Loss: 0.052828557789325714
2024-11-05 04:44:49,624 - INFO - [diffusion][Epoch 9842] diffusion learning rate: 0.001
2024-11-05 04:44:49,626 - INFO - [diffusion][Epoch 9842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:49,627 - INFO - [diffusion][Epoch 9843] Epoch 9844/12000
2024-11-05 04:44:53,729 - INFO - [diffusion][Epoch 9843] diffusion training Loss: 0.047066654078662395
2024-11-05 04:44:53,731 - INFO - [diffusion][Epoch 9843] diffusion learning rate: 0.001
2024-11-05 04:44:53,733 - INFO - [diffusion][Epoch 9843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:53,734 - INFO - [diffusion][Epoch 9844] Epoch 9845/12000
2024-11-05 04:44:57,793 - INFO - [diffusion][Epoch 9844] diffusion training Loss: 0.04762809816747904
2024-11-05 04:44:57,794 - INFO - [diffusion][Epoch 9844] diffusion learning rate: 0.001
2024-11-05 04:44:57,796 - INFO - [diffusion][Epoch 9844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:44:57,797 - INFO - [diffusion][Epoch 9845] Epoch 9846/12000
2024-11-05 04:45:01,947 - INFO - [diffusion][Epoch 9845] diffusion training Loss: 0.05902442429214716
2024-11-05 04:45:01,949 - INFO - [diffusion][Epoch 9845] diffusion learning rate: 0.001
2024-11-05 04:45:01,951 - INFO - [diffusion][Epoch 9845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:01,952 - INFO - [diffusion][Epoch 9846] Epoch 9847/12000
2024-11-05 04:45:06,067 - INFO - [diffusion][Epoch 9846] diffusion training Loss: 0.052846509963274
2024-11-05 04:45:06,069 - INFO - [diffusion][Epoch 9846] diffusion learning rate: 0.001
2024-11-05 04:45:06,071 - INFO - [diffusion][Epoch 9846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:06,072 - INFO - [diffusion][Epoch 9847] Epoch 9848/12000
2024-11-05 04:45:10,145 - INFO - [diffusion][Epoch 9847] diffusion training Loss: 0.051232676953077316
2024-11-05 04:45:10,147 - INFO - [diffusion][Epoch 9847] diffusion learning rate: 0.001
2024-11-05 04:45:10,149 - INFO - [diffusion][Epoch 9847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:10,150 - INFO - [diffusion][Epoch 9848] Epoch 9849/12000
2024-11-05 04:45:14,316 - INFO - [diffusion][Epoch 9848] diffusion training Loss: 0.05607928242534399
2024-11-05 04:45:14,318 - INFO - [diffusion][Epoch 9848] diffusion learning rate: 0.001
2024-11-05 04:45:14,320 - INFO - [diffusion][Epoch 9848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:14,322 - INFO - [diffusion][Epoch 9849] Epoch 9850/12000
2024-11-05 04:45:18,427 - INFO - [diffusion][Epoch 9849] diffusion training Loss: 0.04774769488722086
2024-11-05 04:45:18,429 - INFO - [diffusion][Epoch 9849] diffusion learning rate: 0.001
2024-11-05 04:45:18,431 - INFO - [diffusion][Epoch 9849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:18,432 - INFO - [diffusion][Epoch 9850] Epoch 9851/12000
2024-11-05 04:45:22,551 - INFO - [diffusion][Epoch 9850] diffusion training Loss: 0.05183079466223717
2024-11-05 04:45:22,553 - INFO - [diffusion][Epoch 9850] diffusion learning rate: 0.001
2024-11-05 04:45:22,555 - INFO - [diffusion][Epoch 9850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:22,556 - INFO - [diffusion][Epoch 9851] Epoch 9852/12000
2024-11-05 04:45:26,675 - INFO - [diffusion][Epoch 9851] diffusion training Loss: 0.048283929005265236
2024-11-05 04:45:26,676 - INFO - [diffusion][Epoch 9851] diffusion learning rate: 0.001
2024-11-05 04:45:26,679 - INFO - [diffusion][Epoch 9851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:26,680 - INFO - [diffusion][Epoch 9852] Epoch 9853/12000
2024-11-05 04:45:30,749 - INFO - [diffusion][Epoch 9852] diffusion training Loss: 0.053781197406351566
2024-11-05 04:45:30,751 - INFO - [diffusion][Epoch 9852] diffusion learning rate: 0.001
2024-11-05 04:45:30,753 - INFO - [diffusion][Epoch 9852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:30,754 - INFO - [diffusion][Epoch 9853] Epoch 9854/12000
2024-11-05 04:45:34,903 - INFO - [diffusion][Epoch 9853] diffusion training Loss: 0.04998780973255634
2024-11-05 04:45:34,906 - INFO - [diffusion][Epoch 9853] diffusion learning rate: 0.001
2024-11-05 04:45:34,907 - INFO - [diffusion][Epoch 9853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:34,909 - INFO - [diffusion][Epoch 9854] Epoch 9855/12000
2024-11-05 04:45:38,931 - INFO - [diffusion][Epoch 9854] diffusion training Loss: 0.04832281917333603
2024-11-05 04:45:38,933 - INFO - [diffusion][Epoch 9854] diffusion learning rate: 0.001
2024-11-05 04:45:38,935 - INFO - [diffusion][Epoch 9854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:38,936 - INFO - [diffusion][Epoch 9855] Epoch 9856/12000
2024-11-05 04:45:43,017 - INFO - [diffusion][Epoch 9855] diffusion training Loss: 0.047513799741864204
2024-11-05 04:45:43,019 - INFO - [diffusion][Epoch 9855] diffusion learning rate: 0.001
2024-11-05 04:45:43,021 - INFO - [diffusion][Epoch 9855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:43,022 - INFO - [diffusion][Epoch 9856] Epoch 9857/12000
2024-11-05 04:45:47,077 - INFO - [diffusion][Epoch 9856] diffusion training Loss: 0.051156939938664436
2024-11-05 04:45:47,079 - INFO - [diffusion][Epoch 9856] diffusion learning rate: 0.001
2024-11-05 04:45:47,081 - INFO - [diffusion][Epoch 9856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:47,082 - INFO - [diffusion][Epoch 9857] Epoch 9858/12000
2024-11-05 04:45:51,161 - INFO - [diffusion][Epoch 9857] diffusion training Loss: 0.049608832225203514
2024-11-05 04:45:51,163 - INFO - [diffusion][Epoch 9857] diffusion learning rate: 0.001
2024-11-05 04:45:51,165 - INFO - [diffusion][Epoch 9857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:51,166 - INFO - [diffusion][Epoch 9858] Epoch 9859/12000
2024-11-05 04:45:55,282 - INFO - [diffusion][Epoch 9858] diffusion training Loss: 0.04812123626470566
2024-11-05 04:45:55,284 - INFO - [diffusion][Epoch 9858] diffusion learning rate: 0.001
2024-11-05 04:45:55,286 - INFO - [diffusion][Epoch 9858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:55,287 - INFO - [diffusion][Epoch 9859] Epoch 9860/12000
2024-11-05 04:45:59,324 - INFO - [diffusion][Epoch 9859] diffusion training Loss: 0.04470420442521572
2024-11-05 04:45:59,326 - INFO - [diffusion][Epoch 9859] diffusion learning rate: 0.001
2024-11-05 04:45:59,328 - INFO - [diffusion][Epoch 9859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:45:59,329 - INFO - [diffusion][Epoch 9860] Epoch 9861/12000
2024-11-05 04:46:03,457 - INFO - [diffusion][Epoch 9860] diffusion training Loss: 0.051864877343177795
2024-11-05 04:46:03,460 - INFO - [diffusion][Epoch 9860] diffusion learning rate: 0.001
2024-11-05 04:46:03,463 - INFO - [diffusion][Epoch 9860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:03,465 - INFO - [diffusion][Epoch 9861] Epoch 9862/12000
2024-11-05 04:46:07,341 - INFO - [diffusion][Epoch 9861] diffusion training Loss: 0.049465435557067394
2024-11-05 04:46:07,343 - INFO - [diffusion][Epoch 9861] diffusion learning rate: 0.001
2024-11-05 04:46:07,345 - INFO - [diffusion][Epoch 9861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:07,346 - INFO - [diffusion][Epoch 9862] Epoch 9863/12000
2024-11-05 04:46:11,458 - INFO - [diffusion][Epoch 9862] diffusion training Loss: 0.045470773242414
2024-11-05 04:46:11,460 - INFO - [diffusion][Epoch 9862] diffusion learning rate: 0.001
2024-11-05 04:46:11,462 - INFO - [diffusion][Epoch 9862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:11,463 - INFO - [diffusion][Epoch 9863] Epoch 9864/12000
2024-11-05 04:46:15,565 - INFO - [diffusion][Epoch 9863] diffusion training Loss: 0.04373341705650091
2024-11-05 04:46:15,566 - INFO - [diffusion][Epoch 9863] diffusion learning rate: 0.001
2024-11-05 04:46:15,568 - INFO - [diffusion][Epoch 9863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:15,569 - INFO - [diffusion][Epoch 9864] Epoch 9865/12000
2024-11-05 04:46:19,668 - INFO - [diffusion][Epoch 9864] diffusion training Loss: 0.047950600273907185
2024-11-05 04:46:19,670 - INFO - [diffusion][Epoch 9864] diffusion learning rate: 0.001
2024-11-05 04:46:19,671 - INFO - [diffusion][Epoch 9864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:19,673 - INFO - [diffusion][Epoch 9865] Epoch 9866/12000
2024-11-05 04:46:23,847 - INFO - [diffusion][Epoch 9865] diffusion training Loss: 0.047596768476068974
2024-11-05 04:46:23,849 - INFO - [diffusion][Epoch 9865] diffusion learning rate: 0.001
2024-11-05 04:46:23,850 - INFO - [diffusion][Epoch 9865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:23,852 - INFO - [diffusion][Epoch 9866] Epoch 9867/12000
2024-11-05 04:46:28,123 - INFO - [diffusion][Epoch 9866] diffusion training Loss: 0.04988976661115885
2024-11-05 04:46:28,126 - INFO - [diffusion][Epoch 9866] diffusion learning rate: 0.001
2024-11-05 04:46:28,128 - INFO - [diffusion][Epoch 9866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:28,129 - INFO - [diffusion][Epoch 9867] Epoch 9868/12000
2024-11-05 04:46:32,184 - INFO - [diffusion][Epoch 9867] diffusion training Loss: 0.04605583380907774
2024-11-05 04:46:32,187 - INFO - [diffusion][Epoch 9867] diffusion learning rate: 0.001
2024-11-05 04:46:32,189 - INFO - [diffusion][Epoch 9867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:32,190 - INFO - [diffusion][Epoch 9868] Epoch 9869/12000
2024-11-05 04:46:36,266 - INFO - [diffusion][Epoch 9868] diffusion training Loss: 0.04466200340539217
2024-11-05 04:46:36,270 - INFO - [diffusion][Epoch 9868] diffusion learning rate: 0.001
2024-11-05 04:46:36,272 - INFO - [diffusion][Epoch 9868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:36,273 - INFO - [diffusion][Epoch 9869] Epoch 9870/12000
2024-11-05 04:46:40,496 - INFO - [diffusion][Epoch 9869] diffusion training Loss: 0.046107701025903225
2024-11-05 04:46:40,498 - INFO - [diffusion][Epoch 9869] diffusion learning rate: 0.001
2024-11-05 04:46:40,533 - INFO - [diffusion][Epoch 9869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:40,534 - INFO - [diffusion][Epoch 9870] Epoch 9871/12000
2024-11-05 04:46:44,626 - INFO - [diffusion][Epoch 9870] diffusion training Loss: 0.05529418122023344
2024-11-05 04:46:44,628 - INFO - [diffusion][Epoch 9870] diffusion learning rate: 0.001
2024-11-05 04:46:44,629 - INFO - [diffusion][Epoch 9870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:44,631 - INFO - [diffusion][Epoch 9871] Epoch 9872/12000
2024-11-05 04:46:48,687 - INFO - [diffusion][Epoch 9871] diffusion training Loss: 0.054114608094096184
2024-11-05 04:46:48,689 - INFO - [diffusion][Epoch 9871] diffusion learning rate: 0.001
2024-11-05 04:46:48,691 - INFO - [diffusion][Epoch 9871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:48,692 - INFO - [diffusion][Epoch 9872] Epoch 9873/12000
2024-11-05 04:46:52,772 - INFO - [diffusion][Epoch 9872] diffusion training Loss: 0.05174513440579176
2024-11-05 04:46:52,774 - INFO - [diffusion][Epoch 9872] diffusion learning rate: 0.001
2024-11-05 04:46:52,776 - INFO - [diffusion][Epoch 9872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:52,778 - INFO - [diffusion][Epoch 9873] Epoch 9874/12000
2024-11-05 04:46:56,874 - INFO - [diffusion][Epoch 9873] diffusion training Loss: 0.04897081479430199
2024-11-05 04:46:56,876 - INFO - [diffusion][Epoch 9873] diffusion learning rate: 0.001
2024-11-05 04:46:56,878 - INFO - [diffusion][Epoch 9873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:46:56,879 - INFO - [diffusion][Epoch 9874] Epoch 9875/12000
2024-11-05 04:47:01,073 - INFO - [diffusion][Epoch 9874] diffusion training Loss: 0.05490211024880409
2024-11-05 04:47:01,075 - INFO - [diffusion][Epoch 9874] diffusion learning rate: 0.001
2024-11-05 04:47:01,076 - INFO - [diffusion][Epoch 9874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:01,078 - INFO - [diffusion][Epoch 9875] Epoch 9876/12000
2024-11-05 04:47:05,169 - INFO - [diffusion][Epoch 9875] diffusion training Loss: 0.05200502183288336
2024-11-05 04:47:05,171 - INFO - [diffusion][Epoch 9875] diffusion learning rate: 0.001
2024-11-05 04:47:05,173 - INFO - [diffusion][Epoch 9875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:05,174 - INFO - [diffusion][Epoch 9876] Epoch 9877/12000
2024-11-05 04:47:09,092 - INFO - [diffusion][Epoch 9876] diffusion training Loss: 0.05283436365425587
2024-11-05 04:47:09,094 - INFO - [diffusion][Epoch 9876] diffusion learning rate: 0.001
2024-11-05 04:47:09,096 - INFO - [diffusion][Epoch 9876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:09,097 - INFO - [diffusion][Epoch 9877] Epoch 9878/12000
2024-11-05 04:47:13,103 - INFO - [diffusion][Epoch 9877] diffusion training Loss: 0.05070068035274744
2024-11-05 04:47:13,105 - INFO - [diffusion][Epoch 9877] diffusion learning rate: 0.001
2024-11-05 04:47:13,106 - INFO - [diffusion][Epoch 9877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:13,108 - INFO - [diffusion][Epoch 9878] Epoch 9879/12000
2024-11-05 04:47:17,223 - INFO - [diffusion][Epoch 9878] diffusion training Loss: 0.04415525309741497
2024-11-05 04:47:17,225 - INFO - [diffusion][Epoch 9878] diffusion learning rate: 0.001
2024-11-05 04:47:17,227 - INFO - [diffusion][Epoch 9878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:17,228 - INFO - [diffusion][Epoch 9879] Epoch 9880/12000
2024-11-05 04:47:21,311 - INFO - [diffusion][Epoch 9879] diffusion training Loss: 0.049594092182815075
2024-11-05 04:47:21,313 - INFO - [diffusion][Epoch 9879] diffusion learning rate: 0.001
2024-11-05 04:47:21,315 - INFO - [diffusion][Epoch 9879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:21,316 - INFO - [diffusion][Epoch 9880] Epoch 9881/12000
2024-11-05 04:47:25,398 - INFO - [diffusion][Epoch 9880] diffusion training Loss: 0.05317264888435602
2024-11-05 04:47:25,400 - INFO - [diffusion][Epoch 9880] diffusion learning rate: 0.001
2024-11-05 04:47:25,402 - INFO - [diffusion][Epoch 9880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:25,403 - INFO - [diffusion][Epoch 9881] Epoch 9882/12000
2024-11-05 04:47:29,486 - INFO - [diffusion][Epoch 9881] diffusion training Loss: 0.045493338257074356
2024-11-05 04:47:29,488 - INFO - [diffusion][Epoch 9881] diffusion learning rate: 0.001
2024-11-05 04:47:29,490 - INFO - [diffusion][Epoch 9881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:29,491 - INFO - [diffusion][Epoch 9882] Epoch 9883/12000
2024-11-05 04:47:33,630 - INFO - [diffusion][Epoch 9882] diffusion training Loss: 0.049232992343604565
2024-11-05 04:47:33,632 - INFO - [diffusion][Epoch 9882] diffusion learning rate: 0.001
2024-11-05 04:47:33,634 - INFO - [diffusion][Epoch 9882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:33,635 - INFO - [diffusion][Epoch 9883] Epoch 9884/12000
2024-11-05 04:47:37,638 - INFO - [diffusion][Epoch 9883] diffusion training Loss: 0.04959738999605179
2024-11-05 04:47:37,641 - INFO - [diffusion][Epoch 9883] diffusion learning rate: 0.001
2024-11-05 04:47:37,642 - INFO - [diffusion][Epoch 9883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:37,643 - INFO - [diffusion][Epoch 9884] Epoch 9885/12000
2024-11-05 04:47:41,831 - INFO - [diffusion][Epoch 9884] diffusion training Loss: 0.0472064008936286
2024-11-05 04:47:41,833 - INFO - [diffusion][Epoch 9884] diffusion learning rate: 0.001
2024-11-05 04:47:41,835 - INFO - [diffusion][Epoch 9884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:41,837 - INFO - [diffusion][Epoch 9885] Epoch 9886/12000
2024-11-05 04:47:45,949 - INFO - [diffusion][Epoch 9885] diffusion training Loss: 0.05095571093261242
2024-11-05 04:47:45,951 - INFO - [diffusion][Epoch 9885] diffusion learning rate: 0.001
2024-11-05 04:47:45,953 - INFO - [diffusion][Epoch 9885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:45,954 - INFO - [diffusion][Epoch 9886] Epoch 9887/12000
2024-11-05 04:47:49,985 - INFO - [diffusion][Epoch 9886] diffusion training Loss: 0.04593362472951412
2024-11-05 04:47:49,987 - INFO - [diffusion][Epoch 9886] diffusion learning rate: 0.001
2024-11-05 04:47:49,990 - INFO - [diffusion][Epoch 9886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:49,991 - INFO - [diffusion][Epoch 9887] Epoch 9888/12000
2024-11-05 04:47:54,036 - INFO - [diffusion][Epoch 9887] diffusion training Loss: 0.04946222063153982
2024-11-05 04:47:54,038 - INFO - [diffusion][Epoch 9887] diffusion learning rate: 0.001
2024-11-05 04:47:54,040 - INFO - [diffusion][Epoch 9887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:54,041 - INFO - [diffusion][Epoch 9888] Epoch 9889/12000
2024-11-05 04:47:58,179 - INFO - [diffusion][Epoch 9888] diffusion training Loss: 0.05198635160923004
2024-11-05 04:47:58,181 - INFO - [diffusion][Epoch 9888] diffusion learning rate: 0.001
2024-11-05 04:47:58,183 - INFO - [diffusion][Epoch 9888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:47:58,184 - INFO - [diffusion][Epoch 9889] Epoch 9890/12000
2024-11-05 04:48:02,281 - INFO - [diffusion][Epoch 9889] diffusion training Loss: 0.05036642123013735
2024-11-05 04:48:02,287 - INFO - [diffusion][Epoch 9889] diffusion learning rate: 0.001
2024-11-05 04:48:02,288 - INFO - [diffusion][Epoch 9889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:02,290 - INFO - [diffusion][Epoch 9890] Epoch 9891/12000
2024-11-05 04:48:06,736 - INFO - [diffusion][Epoch 9890] diffusion training Loss: 0.05071780178695917
2024-11-05 04:48:06,738 - INFO - [diffusion][Epoch 9890] diffusion learning rate: 0.001
2024-11-05 04:48:06,740 - INFO - [diffusion][Epoch 9890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:06,741 - INFO - [diffusion][Epoch 9891] Epoch 9892/12000
2024-11-05 04:48:10,898 - INFO - [diffusion][Epoch 9891] diffusion training Loss: 0.049051111564040184
2024-11-05 04:48:10,900 - INFO - [diffusion][Epoch 9891] diffusion learning rate: 0.001
2024-11-05 04:48:10,902 - INFO - [diffusion][Epoch 9891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:10,904 - INFO - [diffusion][Epoch 9892] Epoch 9893/12000
2024-11-05 04:48:15,171 - INFO - [diffusion][Epoch 9892] diffusion training Loss: 0.046553248539566994
2024-11-05 04:48:15,173 - INFO - [diffusion][Epoch 9892] diffusion learning rate: 0.001
2024-11-05 04:48:15,176 - INFO - [diffusion][Epoch 9892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:15,177 - INFO - [diffusion][Epoch 9893] Epoch 9894/12000
2024-11-05 04:48:19,232 - INFO - [diffusion][Epoch 9893] diffusion training Loss: 0.05069552268832922
2024-11-05 04:48:19,234 - INFO - [diffusion][Epoch 9893] diffusion learning rate: 0.001
2024-11-05 04:48:19,235 - INFO - [diffusion][Epoch 9893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:19,237 - INFO - [diffusion][Epoch 9894] Epoch 9895/12000
2024-11-05 04:48:23,317 - INFO - [diffusion][Epoch 9894] diffusion training Loss: 0.04544412903487682
2024-11-05 04:48:23,319 - INFO - [diffusion][Epoch 9894] diffusion learning rate: 0.001
2024-11-05 04:48:23,321 - INFO - [diffusion][Epoch 9894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:23,323 - INFO - [diffusion][Epoch 9895] Epoch 9896/12000
2024-11-05 04:48:27,533 - INFO - [diffusion][Epoch 9895] diffusion training Loss: 0.052687049843370914
2024-11-05 04:48:27,535 - INFO - [diffusion][Epoch 9895] diffusion learning rate: 0.001
2024-11-05 04:48:27,536 - INFO - [diffusion][Epoch 9895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:27,538 - INFO - [diffusion][Epoch 9896] Epoch 9897/12000
2024-11-05 04:48:31,627 - INFO - [diffusion][Epoch 9896] diffusion training Loss: 0.05206125322729349
2024-11-05 04:48:31,629 - INFO - [diffusion][Epoch 9896] diffusion learning rate: 0.001
2024-11-05 04:48:31,631 - INFO - [diffusion][Epoch 9896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:31,632 - INFO - [diffusion][Epoch 9897] Epoch 9898/12000
2024-11-05 04:48:35,768 - INFO - [diffusion][Epoch 9897] diffusion training Loss: 0.04755111038684845
2024-11-05 04:48:35,770 - INFO - [diffusion][Epoch 9897] diffusion learning rate: 0.001
2024-11-05 04:48:35,772 - INFO - [diffusion][Epoch 9897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:35,773 - INFO - [diffusion][Epoch 9898] Epoch 9899/12000
2024-11-05 04:48:39,838 - INFO - [diffusion][Epoch 9898] diffusion training Loss: 0.05456872470676899
2024-11-05 04:48:39,840 - INFO - [diffusion][Epoch 9898] diffusion learning rate: 0.001
2024-11-05 04:48:39,842 - INFO - [diffusion][Epoch 9898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:39,843 - INFO - [diffusion][Epoch 9899] Epoch 9900/12000
2024-11-05 04:48:43,831 - INFO - [diffusion][Epoch 9899] diffusion training Loss: 0.05150277726352215
2024-11-05 04:48:43,833 - INFO - [diffusion][Epoch 9899] diffusion learning rate: 0.001
2024-11-05 04:48:43,835 - INFO - [diffusion][Epoch 9899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:43,836 - INFO - [diffusion][Epoch 9900] Epoch 9901/12000
2024-11-05 04:48:47,907 - INFO - [diffusion][Epoch 9900] diffusion training Loss: 0.04995104484260082
2024-11-05 04:48:47,910 - INFO - [diffusion][Epoch 9900] diffusion learning rate: 0.001
2024-11-05 04:48:47,912 - INFO - [diffusion][Epoch 9900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:47,913 - INFO - [diffusion][Epoch 9901] Epoch 9902/12000
2024-11-05 04:48:52,010 - INFO - [diffusion][Epoch 9901] diffusion training Loss: 0.05235772114247084
2024-11-05 04:48:52,012 - INFO - [diffusion][Epoch 9901] diffusion learning rate: 0.001
2024-11-05 04:48:52,014 - INFO - [diffusion][Epoch 9901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:52,016 - INFO - [diffusion][Epoch 9902] Epoch 9903/12000
2024-11-05 04:48:56,125 - INFO - [diffusion][Epoch 9902] diffusion training Loss: 0.04812981095165014
2024-11-05 04:48:56,129 - INFO - [diffusion][Epoch 9902] diffusion learning rate: 0.001
2024-11-05 04:48:56,161 - INFO - [diffusion][Epoch 9902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:48:56,162 - INFO - [diffusion][Epoch 9903] Epoch 9904/12000
2024-11-05 04:49:00,187 - INFO - [diffusion][Epoch 9903] diffusion training Loss: 0.05093846283853054
2024-11-05 04:49:00,190 - INFO - [diffusion][Epoch 9903] diffusion learning rate: 0.001
2024-11-05 04:49:00,191 - INFO - [diffusion][Epoch 9903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:00,193 - INFO - [diffusion][Epoch 9904] Epoch 9905/12000
2024-11-05 04:49:04,596 - INFO - [diffusion][Epoch 9904] diffusion training Loss: 0.050359489396214485
2024-11-05 04:49:04,597 - INFO - [diffusion][Epoch 9904] diffusion learning rate: 0.001
2024-11-05 04:49:04,599 - INFO - [diffusion][Epoch 9904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:04,600 - INFO - [diffusion][Epoch 9905] Epoch 9906/12000
2024-11-05 04:49:08,722 - INFO - [diffusion][Epoch 9905] diffusion training Loss: 0.049352725967764854
2024-11-05 04:49:08,724 - INFO - [diffusion][Epoch 9905] diffusion learning rate: 0.001
2024-11-05 04:49:08,725 - INFO - [diffusion][Epoch 9905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:08,727 - INFO - [diffusion][Epoch 9906] Epoch 9907/12000
2024-11-05 04:49:12,832 - INFO - [diffusion][Epoch 9906] diffusion training Loss: 0.048695625737309456
2024-11-05 04:49:12,834 - INFO - [diffusion][Epoch 9906] diffusion learning rate: 0.001
2024-11-05 04:49:12,836 - INFO - [diffusion][Epoch 9906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:12,837 - INFO - [diffusion][Epoch 9907] Epoch 9908/12000
2024-11-05 04:49:16,952 - INFO - [diffusion][Epoch 9907] diffusion training Loss: 0.04696098901331425
2024-11-05 04:49:16,954 - INFO - [diffusion][Epoch 9907] diffusion learning rate: 0.001
2024-11-05 04:49:16,955 - INFO - [diffusion][Epoch 9907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:16,957 - INFO - [diffusion][Epoch 9908] Epoch 9909/12000
2024-11-05 04:49:21,080 - INFO - [diffusion][Epoch 9908] diffusion training Loss: 0.05024055950343609
2024-11-05 04:49:21,082 - INFO - [diffusion][Epoch 9908] diffusion learning rate: 0.001
2024-11-05 04:49:21,084 - INFO - [diffusion][Epoch 9908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:21,085 - INFO - [diffusion][Epoch 9909] Epoch 9910/12000
2024-11-05 04:49:25,251 - INFO - [diffusion][Epoch 9909] diffusion training Loss: 0.05191054195165634
2024-11-05 04:49:25,253 - INFO - [diffusion][Epoch 9909] diffusion learning rate: 0.001
2024-11-05 04:49:25,255 - INFO - [diffusion][Epoch 9909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:25,256 - INFO - [diffusion][Epoch 9910] Epoch 9911/12000
2024-11-05 04:49:29,328 - INFO - [diffusion][Epoch 9910] diffusion training Loss: 0.04990438278764486
2024-11-05 04:49:29,330 - INFO - [diffusion][Epoch 9910] diffusion learning rate: 0.001
2024-11-05 04:49:29,372 - INFO - [diffusion][Epoch 9910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:29,373 - INFO - [diffusion][Epoch 9911] Epoch 9912/12000
2024-11-05 04:49:33,459 - INFO - [diffusion][Epoch 9911] diffusion training Loss: 0.05307433567941189
2024-11-05 04:49:33,461 - INFO - [diffusion][Epoch 9911] diffusion learning rate: 0.001
2024-11-05 04:49:33,463 - INFO - [diffusion][Epoch 9911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:33,464 - INFO - [diffusion][Epoch 9912] Epoch 9913/12000
2024-11-05 04:49:37,491 - INFO - [diffusion][Epoch 9912] diffusion training Loss: 0.04967690631747246
2024-11-05 04:49:37,493 - INFO - [diffusion][Epoch 9912] diffusion learning rate: 0.001
2024-11-05 04:49:37,495 - INFO - [diffusion][Epoch 9912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:37,496 - INFO - [diffusion][Epoch 9913] Epoch 9914/12000
2024-11-05 04:49:41,560 - INFO - [diffusion][Epoch 9913] diffusion training Loss: 0.04862617515027523
2024-11-05 04:49:41,562 - INFO - [diffusion][Epoch 9913] diffusion learning rate: 0.001
2024-11-05 04:49:41,564 - INFO - [diffusion][Epoch 9913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:41,565 - INFO - [diffusion][Epoch 9914] Epoch 9915/12000
2024-11-05 04:49:45,681 - INFO - [diffusion][Epoch 9914] diffusion training Loss: 0.05608842708170414
2024-11-05 04:49:45,683 - INFO - [diffusion][Epoch 9914] diffusion learning rate: 0.001
2024-11-05 04:49:45,721 - INFO - [diffusion][Epoch 9914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:45,722 - INFO - [diffusion][Epoch 9915] Epoch 9916/12000
2024-11-05 04:49:49,792 - INFO - [diffusion][Epoch 9915] diffusion training Loss: 0.050894638523459435
2024-11-05 04:49:49,794 - INFO - [diffusion][Epoch 9915] diffusion learning rate: 0.001
2024-11-05 04:49:49,795 - INFO - [diffusion][Epoch 9915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:49,796 - INFO - [diffusion][Epoch 9916] Epoch 9917/12000
2024-11-05 04:49:53,903 - INFO - [diffusion][Epoch 9916] diffusion training Loss: 0.050242189317941666
2024-11-05 04:49:53,905 - INFO - [diffusion][Epoch 9916] diffusion learning rate: 0.001
2024-11-05 04:49:53,907 - INFO - [diffusion][Epoch 9916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:53,908 - INFO - [diffusion][Epoch 9917] Epoch 9918/12000
2024-11-05 04:49:57,979 - INFO - [diffusion][Epoch 9917] diffusion training Loss: 0.05044065602123737
2024-11-05 04:49:57,981 - INFO - [diffusion][Epoch 9917] diffusion learning rate: 0.001
2024-11-05 04:49:57,983 - INFO - [diffusion][Epoch 9917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:49:57,984 - INFO - [diffusion][Epoch 9918] Epoch 9919/12000
2024-11-05 04:50:02,057 - INFO - [diffusion][Epoch 9918] diffusion training Loss: 0.04934034775942564
2024-11-05 04:50:02,059 - INFO - [diffusion][Epoch 9918] diffusion learning rate: 0.001
2024-11-05 04:50:02,061 - INFO - [diffusion][Epoch 9918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:02,062 - INFO - [diffusion][Epoch 9919] Epoch 9920/12000
2024-11-05 04:50:06,155 - INFO - [diffusion][Epoch 9919] diffusion training Loss: 0.05773771647363901
2024-11-05 04:50:06,157 - INFO - [diffusion][Epoch 9919] diffusion learning rate: 0.001
2024-11-05 04:50:06,159 - INFO - [diffusion][Epoch 9919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:06,160 - INFO - [diffusion][Epoch 9920] Epoch 9921/12000
2024-11-05 04:50:10,252 - INFO - [diffusion][Epoch 9920] diffusion training Loss: 0.05028850585222244
2024-11-05 04:50:10,254 - INFO - [diffusion][Epoch 9920] diffusion learning rate: 0.001
2024-11-05 04:50:10,255 - INFO - [diffusion][Epoch 9920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:10,257 - INFO - [diffusion][Epoch 9921] Epoch 9922/12000
2024-11-05 04:50:14,367 - INFO - [diffusion][Epoch 9921] diffusion training Loss: 0.050468078814446926
2024-11-05 04:50:14,369 - INFO - [diffusion][Epoch 9921] diffusion learning rate: 0.001
2024-11-05 04:50:14,371 - INFO - [diffusion][Epoch 9921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:14,372 - INFO - [diffusion][Epoch 9922] Epoch 9923/12000
2024-11-05 04:50:18,597 - INFO - [diffusion][Epoch 9922] diffusion training Loss: 0.05443073250353336
2024-11-05 04:50:18,598 - INFO - [diffusion][Epoch 9922] diffusion learning rate: 0.001
2024-11-05 04:50:18,600 - INFO - [diffusion][Epoch 9922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:18,601 - INFO - [diffusion][Epoch 9923] Epoch 9924/12000
2024-11-05 04:50:23,544 - INFO - [diffusion][Epoch 9923] diffusion training Loss: 0.052838098257780075
2024-11-05 04:50:23,546 - INFO - [diffusion][Epoch 9923] diffusion learning rate: 0.001
2024-11-05 04:50:23,548 - INFO - [diffusion][Epoch 9923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:23,549 - INFO - [diffusion][Epoch 9924] Epoch 9925/12000
2024-11-05 04:50:27,817 - INFO - [diffusion][Epoch 9924] diffusion training Loss: 0.0549236461520195
2024-11-05 04:50:27,819 - INFO - [diffusion][Epoch 9924] diffusion learning rate: 0.001
2024-11-05 04:50:27,821 - INFO - [diffusion][Epoch 9924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:27,822 - INFO - [diffusion][Epoch 9925] Epoch 9926/12000
2024-11-05 04:50:31,926 - INFO - [diffusion][Epoch 9925] diffusion training Loss: 0.053627450950443745
2024-11-05 04:50:31,928 - INFO - [diffusion][Epoch 9925] diffusion learning rate: 0.001
2024-11-05 04:50:31,929 - INFO - [diffusion][Epoch 9925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:31,931 - INFO - [diffusion][Epoch 9926] Epoch 9927/12000
2024-11-05 04:50:35,875 - INFO - [diffusion][Epoch 9926] diffusion training Loss: 0.05432176776230335
2024-11-05 04:50:35,877 - INFO - [diffusion][Epoch 9926] diffusion learning rate: 0.001
2024-11-05 04:50:35,878 - INFO - [diffusion][Epoch 9926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:35,880 - INFO - [diffusion][Epoch 9927] Epoch 9928/12000
2024-11-05 04:50:40,021 - INFO - [diffusion][Epoch 9927] diffusion training Loss: 0.0525641068816185
2024-11-05 04:50:40,023 - INFO - [diffusion][Epoch 9927] diffusion learning rate: 0.001
2024-11-05 04:50:40,025 - INFO - [diffusion][Epoch 9927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:40,028 - INFO - [diffusion][Epoch 9928] Epoch 9929/12000
2024-11-05 04:50:44,106 - INFO - [diffusion][Epoch 9928] diffusion training Loss: 0.04824580531567335
2024-11-05 04:50:44,109 - INFO - [diffusion][Epoch 9928] diffusion learning rate: 0.001
2024-11-05 04:50:44,110 - INFO - [diffusion][Epoch 9928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:44,111 - INFO - [diffusion][Epoch 9929] Epoch 9930/12000
2024-11-05 04:50:48,175 - INFO - [diffusion][Epoch 9929] diffusion training Loss: 0.05131226032972336
2024-11-05 04:50:48,177 - INFO - [diffusion][Epoch 9929] diffusion learning rate: 0.001
2024-11-05 04:50:48,179 - INFO - [diffusion][Epoch 9929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:48,181 - INFO - [diffusion][Epoch 9930] Epoch 9931/12000
2024-11-05 04:50:52,051 - INFO - [diffusion][Epoch 9930] diffusion training Loss: 0.053220234811306
2024-11-05 04:50:52,053 - INFO - [diffusion][Epoch 9930] diffusion learning rate: 0.001
2024-11-05 04:50:52,055 - INFO - [diffusion][Epoch 9930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:52,056 - INFO - [diffusion][Epoch 9931] Epoch 9932/12000
2024-11-05 04:50:56,103 - INFO - [diffusion][Epoch 9931] diffusion training Loss: 0.0490622092038393
2024-11-05 04:50:56,106 - INFO - [diffusion][Epoch 9931] diffusion learning rate: 0.001
2024-11-05 04:50:56,108 - INFO - [diffusion][Epoch 9931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:50:56,110 - INFO - [diffusion][Epoch 9932] Epoch 9933/12000
2024-11-05 04:51:00,173 - INFO - [diffusion][Epoch 9932] diffusion training Loss: 0.049022919498384
2024-11-05 04:51:00,176 - INFO - [diffusion][Epoch 9932] diffusion learning rate: 0.001
2024-11-05 04:51:00,178 - INFO - [diffusion][Epoch 9932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:00,179 - INFO - [diffusion][Epoch 9933] Epoch 9934/12000
2024-11-05 04:51:04,259 - INFO - [diffusion][Epoch 9933] diffusion training Loss: 0.04744091164320707
2024-11-05 04:51:04,261 - INFO - [diffusion][Epoch 9933] diffusion learning rate: 0.001
2024-11-05 04:51:04,263 - INFO - [diffusion][Epoch 9933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:04,264 - INFO - [diffusion][Epoch 9934] Epoch 9935/12000
2024-11-05 04:51:08,257 - INFO - [diffusion][Epoch 9934] diffusion training Loss: 0.050473904237151146
2024-11-05 04:51:08,260 - INFO - [diffusion][Epoch 9934] diffusion learning rate: 0.001
2024-11-05 04:51:08,262 - INFO - [diffusion][Epoch 9934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:08,263 - INFO - [diffusion][Epoch 9935] Epoch 9936/12000
2024-11-05 04:51:12,298 - INFO - [diffusion][Epoch 9935] diffusion training Loss: 0.047442952170968056
2024-11-05 04:51:12,300 - INFO - [diffusion][Epoch 9935] diffusion learning rate: 0.001
2024-11-05 04:51:12,302 - INFO - [diffusion][Epoch 9935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:12,304 - INFO - [diffusion][Epoch 9936] Epoch 9937/12000
2024-11-05 04:51:16,530 - INFO - [diffusion][Epoch 9936] diffusion training Loss: 0.05056642275303602
2024-11-05 04:51:16,532 - INFO - [diffusion][Epoch 9936] diffusion learning rate: 0.001
2024-11-05 04:51:16,534 - INFO - [diffusion][Epoch 9936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:16,535 - INFO - [diffusion][Epoch 9937] Epoch 9938/12000
2024-11-05 04:51:20,650 - INFO - [diffusion][Epoch 9937] diffusion training Loss: 0.04612405598163605
2024-11-05 04:51:20,652 - INFO - [diffusion][Epoch 9937] diffusion learning rate: 0.001
2024-11-05 04:51:20,654 - INFO - [diffusion][Epoch 9937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:20,655 - INFO - [diffusion][Epoch 9938] Epoch 9939/12000
2024-11-05 04:51:24,722 - INFO - [diffusion][Epoch 9938] diffusion training Loss: 0.049647348932921886
2024-11-05 04:51:24,724 - INFO - [diffusion][Epoch 9938] diffusion learning rate: 0.001
2024-11-05 04:51:24,726 - INFO - [diffusion][Epoch 9938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:24,727 - INFO - [diffusion][Epoch 9939] Epoch 9940/12000
2024-11-05 04:51:28,772 - INFO - [diffusion][Epoch 9939] diffusion training Loss: 0.04568670876324177
2024-11-05 04:51:28,774 - INFO - [diffusion][Epoch 9939] diffusion learning rate: 0.001
2024-11-05 04:51:28,776 - INFO - [diffusion][Epoch 9939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:28,777 - INFO - [diffusion][Epoch 9940] Epoch 9941/12000
2024-11-05 04:51:32,877 - INFO - [diffusion][Epoch 9940] diffusion training Loss: 0.05562275182455778
2024-11-05 04:51:32,879 - INFO - [diffusion][Epoch 9940] diffusion learning rate: 0.001
2024-11-05 04:51:32,881 - INFO - [diffusion][Epoch 9940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:32,882 - INFO - [diffusion][Epoch 9941] Epoch 9942/12000
2024-11-05 04:51:36,959 - INFO - [diffusion][Epoch 9941] diffusion training Loss: 0.049426302313804626
2024-11-05 04:51:36,961 - INFO - [diffusion][Epoch 9941] diffusion learning rate: 0.001
2024-11-05 04:51:36,962 - INFO - [diffusion][Epoch 9941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:36,964 - INFO - [diffusion][Epoch 9942] Epoch 9943/12000
2024-11-05 04:51:41,013 - INFO - [diffusion][Epoch 9942] diffusion training Loss: 0.05023143347352743
2024-11-05 04:51:41,017 - INFO - [diffusion][Epoch 9942] diffusion learning rate: 0.001
2024-11-05 04:51:41,019 - INFO - [diffusion][Epoch 9942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:41,021 - INFO - [diffusion][Epoch 9943] Epoch 9944/12000
2024-11-05 04:51:45,143 - INFO - [diffusion][Epoch 9943] diffusion training Loss: 0.04936476144939661
2024-11-05 04:51:45,146 - INFO - [diffusion][Epoch 9943] diffusion learning rate: 0.001
2024-11-05 04:51:45,148 - INFO - [diffusion][Epoch 9943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:45,149 - INFO - [diffusion][Epoch 9944] Epoch 9945/12000
2024-11-05 04:51:49,445 - INFO - [diffusion][Epoch 9944] diffusion training Loss: 0.04755164124071598
2024-11-05 04:51:49,447 - INFO - [diffusion][Epoch 9944] diffusion learning rate: 0.001
2024-11-05 04:51:49,450 - INFO - [diffusion][Epoch 9944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:49,451 - INFO - [diffusion][Epoch 9945] Epoch 9946/12000
2024-11-05 04:51:53,428 - INFO - [diffusion][Epoch 9945] diffusion training Loss: 0.04863064270466566
2024-11-05 04:51:53,431 - INFO - [diffusion][Epoch 9945] diffusion learning rate: 0.001
2024-11-05 04:51:53,433 - INFO - [diffusion][Epoch 9945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:53,434 - INFO - [diffusion][Epoch 9946] Epoch 9947/12000
2024-11-05 04:51:57,636 - INFO - [diffusion][Epoch 9946] diffusion training Loss: 0.05111384857445955
2024-11-05 04:51:57,639 - INFO - [diffusion][Epoch 9946] diffusion learning rate: 0.001
2024-11-05 04:51:57,641 - INFO - [diffusion][Epoch 9946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:51:57,643 - INFO - [diffusion][Epoch 9947] Epoch 9948/12000
2024-11-05 04:52:01,807 - INFO - [diffusion][Epoch 9947] diffusion training Loss: 0.04870143346488476
2024-11-05 04:52:01,810 - INFO - [diffusion][Epoch 9947] diffusion learning rate: 0.001
2024-11-05 04:52:01,811 - INFO - [diffusion][Epoch 9947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:01,813 - INFO - [diffusion][Epoch 9948] Epoch 9949/12000
2024-11-05 04:52:05,948 - INFO - [diffusion][Epoch 9948] diffusion training Loss: 0.04759659431874752
2024-11-05 04:52:05,951 - INFO - [diffusion][Epoch 9948] diffusion learning rate: 0.001
2024-11-05 04:52:05,953 - INFO - [diffusion][Epoch 9948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:05,954 - INFO - [diffusion][Epoch 9949] Epoch 9950/12000
2024-11-05 04:52:09,684 - INFO - [diffusion][Epoch 9949] diffusion training Loss: 0.05395408347249031
2024-11-05 04:52:09,686 - INFO - [diffusion][Epoch 9949] diffusion learning rate: 0.001
2024-11-05 04:52:09,688 - INFO - [diffusion][Epoch 9949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:09,689 - INFO - [diffusion][Epoch 9950] Epoch 9951/12000
2024-11-05 04:52:13,629 - INFO - [diffusion][Epoch 9950] diffusion training Loss: 0.04929372947663069
2024-11-05 04:52:13,631 - INFO - [diffusion][Epoch 9950] diffusion learning rate: 0.001
2024-11-05 04:52:13,633 - INFO - [diffusion][Epoch 9950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:13,634 - INFO - [diffusion][Epoch 9951] Epoch 9952/12000
2024-11-05 04:52:17,674 - INFO - [diffusion][Epoch 9951] diffusion training Loss: 0.05074639059603214
2024-11-05 04:52:17,676 - INFO - [diffusion][Epoch 9951] diffusion learning rate: 0.001
2024-11-05 04:52:17,678 - INFO - [diffusion][Epoch 9951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:17,679 - INFO - [diffusion][Epoch 9952] Epoch 9953/12000
2024-11-05 04:52:21,765 - INFO - [diffusion][Epoch 9952] diffusion training Loss: 0.0501614473760128
2024-11-05 04:52:21,767 - INFO - [diffusion][Epoch 9952] diffusion learning rate: 0.001
2024-11-05 04:52:21,769 - INFO - [diffusion][Epoch 9952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:21,771 - INFO - [diffusion][Epoch 9953] Epoch 9954/12000
2024-11-05 04:52:25,974 - INFO - [diffusion][Epoch 9953] diffusion training Loss: 0.04898054711520672
2024-11-05 04:52:25,976 - INFO - [diffusion][Epoch 9953] diffusion learning rate: 0.001
2024-11-05 04:52:25,977 - INFO - [diffusion][Epoch 9953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:25,979 - INFO - [diffusion][Epoch 9954] Epoch 9955/12000
2024-11-05 04:52:30,157 - INFO - [diffusion][Epoch 9954] diffusion training Loss: 0.050828855484724045
2024-11-05 04:52:30,159 - INFO - [diffusion][Epoch 9954] diffusion learning rate: 0.001
2024-11-05 04:52:30,160 - INFO - [diffusion][Epoch 9954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:30,162 - INFO - [diffusion][Epoch 9955] Epoch 9956/12000
2024-11-05 04:52:34,260 - INFO - [diffusion][Epoch 9955] diffusion training Loss: 0.05148223228752613
2024-11-05 04:52:34,262 - INFO - [diffusion][Epoch 9955] diffusion learning rate: 0.001
2024-11-05 04:52:34,264 - INFO - [diffusion][Epoch 9955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:34,265 - INFO - [diffusion][Epoch 9956] Epoch 9957/12000
2024-11-05 04:52:38,364 - INFO - [diffusion][Epoch 9956] diffusion training Loss: 0.049660010263323784
2024-11-05 04:52:38,366 - INFO - [diffusion][Epoch 9956] diffusion learning rate: 0.001
2024-11-05 04:52:38,367 - INFO - [diffusion][Epoch 9956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:38,368 - INFO - [diffusion][Epoch 9957] Epoch 9958/12000
2024-11-05 04:52:42,424 - INFO - [diffusion][Epoch 9957] diffusion training Loss: 0.04910894576460123
2024-11-05 04:52:42,426 - INFO - [diffusion][Epoch 9957] diffusion learning rate: 0.001
2024-11-05 04:52:42,428 - INFO - [diffusion][Epoch 9957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:42,430 - INFO - [diffusion][Epoch 9958] Epoch 9959/12000
2024-11-05 04:52:46,531 - INFO - [diffusion][Epoch 9958] diffusion training Loss: 0.05072680953890085
2024-11-05 04:52:46,533 - INFO - [diffusion][Epoch 9958] diffusion learning rate: 0.001
2024-11-05 04:52:46,535 - INFO - [diffusion][Epoch 9958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:46,536 - INFO - [diffusion][Epoch 9959] Epoch 9960/12000
2024-11-05 04:52:50,627 - INFO - [diffusion][Epoch 9959] diffusion training Loss: 0.04669502843171358
2024-11-05 04:52:50,629 - INFO - [diffusion][Epoch 9959] diffusion learning rate: 0.001
2024-11-05 04:52:50,631 - INFO - [diffusion][Epoch 9959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:50,632 - INFO - [diffusion][Epoch 9960] Epoch 9961/12000
2024-11-05 04:52:54,712 - INFO - [diffusion][Epoch 9960] diffusion training Loss: 0.050383931025862694
2024-11-05 04:52:54,714 - INFO - [diffusion][Epoch 9960] diffusion learning rate: 0.001
2024-11-05 04:52:54,715 - INFO - [diffusion][Epoch 9960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:54,717 - INFO - [diffusion][Epoch 9961] Epoch 9962/12000
2024-11-05 04:52:58,836 - INFO - [diffusion][Epoch 9961] diffusion training Loss: 0.052920044399797916
2024-11-05 04:52:58,838 - INFO - [diffusion][Epoch 9961] diffusion learning rate: 0.001
2024-11-05 04:52:58,840 - INFO - [diffusion][Epoch 9961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:52:58,841 - INFO - [diffusion][Epoch 9962] Epoch 9963/12000
2024-11-05 04:53:02,936 - INFO - [diffusion][Epoch 9962] diffusion training Loss: 0.04926113039255142
2024-11-05 04:53:02,938 - INFO - [diffusion][Epoch 9962] diffusion learning rate: 0.001
2024-11-05 04:53:02,940 - INFO - [diffusion][Epoch 9962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:02,941 - INFO - [diffusion][Epoch 9963] Epoch 9964/12000
2024-11-05 04:53:07,020 - INFO - [diffusion][Epoch 9963] diffusion training Loss: 0.0497706001624465
2024-11-05 04:53:07,022 - INFO - [diffusion][Epoch 9963] diffusion learning rate: 0.001
2024-11-05 04:53:07,024 - INFO - [diffusion][Epoch 9963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:07,025 - INFO - [diffusion][Epoch 9964] Epoch 9965/12000
2024-11-05 04:53:11,744 - INFO - [diffusion][Epoch 9964] diffusion training Loss: 0.05238579772412777
2024-11-05 04:53:11,746 - INFO - [diffusion][Epoch 9964] diffusion learning rate: 0.001
2024-11-05 04:53:11,748 - INFO - [diffusion][Epoch 9964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:11,749 - INFO - [diffusion][Epoch 9965] Epoch 9966/12000
2024-11-05 04:53:15,815 - INFO - [diffusion][Epoch 9965] diffusion training Loss: 0.04747341480106115
2024-11-05 04:53:15,817 - INFO - [diffusion][Epoch 9965] diffusion learning rate: 0.001
2024-11-05 04:53:15,819 - INFO - [diffusion][Epoch 9965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:15,820 - INFO - [diffusion][Epoch 9966] Epoch 9967/12000
2024-11-05 04:53:19,927 - INFO - [diffusion][Epoch 9966] diffusion training Loss: 0.05399689171463251
2024-11-05 04:53:19,929 - INFO - [diffusion][Epoch 9966] diffusion learning rate: 0.001
2024-11-05 04:53:19,931 - INFO - [diffusion][Epoch 9966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:19,932 - INFO - [diffusion][Epoch 9967] Epoch 9968/12000
2024-11-05 04:53:23,878 - INFO - [diffusion][Epoch 9967] diffusion training Loss: 0.05339513998478651
2024-11-05 04:53:23,879 - INFO - [diffusion][Epoch 9967] diffusion learning rate: 0.001
2024-11-05 04:53:23,881 - INFO - [diffusion][Epoch 9967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:23,882 - INFO - [diffusion][Epoch 9968] Epoch 9969/12000
2024-11-05 04:53:27,954 - INFO - [diffusion][Epoch 9968] diffusion training Loss: 0.048551905900239944
2024-11-05 04:53:27,956 - INFO - [diffusion][Epoch 9968] diffusion learning rate: 0.001
2024-11-05 04:53:27,958 - INFO - [diffusion][Epoch 9968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:27,959 - INFO - [diffusion][Epoch 9969] Epoch 9970/12000
2024-11-05 04:53:32,029 - INFO - [diffusion][Epoch 9969] diffusion training Loss: 0.04685165826231241
2024-11-05 04:53:32,031 - INFO - [diffusion][Epoch 9969] diffusion learning rate: 0.001
2024-11-05 04:53:32,058 - INFO - [diffusion][Epoch 9969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:32,059 - INFO - [diffusion][Epoch 9970] Epoch 9971/12000
2024-11-05 04:53:36,166 - INFO - [diffusion][Epoch 9970] diffusion training Loss: 0.04904237762093544
2024-11-05 04:53:36,168 - INFO - [diffusion][Epoch 9970] diffusion learning rate: 0.001
2024-11-05 04:53:36,170 - INFO - [diffusion][Epoch 9970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:36,171 - INFO - [diffusion][Epoch 9971] Epoch 9972/12000
2024-11-05 04:53:40,257 - INFO - [diffusion][Epoch 9971] diffusion training Loss: 0.04424766357988119
2024-11-05 04:53:40,259 - INFO - [diffusion][Epoch 9971] diffusion learning rate: 0.001
2024-11-05 04:53:40,261 - INFO - [diffusion][Epoch 9971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:40,262 - INFO - [diffusion][Epoch 9972] Epoch 9973/12000
2024-11-05 04:53:44,214 - INFO - [diffusion][Epoch 9972] diffusion training Loss: 0.046790944412350655
2024-11-05 04:53:44,216 - INFO - [diffusion][Epoch 9972] diffusion learning rate: 0.001
2024-11-05 04:53:44,218 - INFO - [diffusion][Epoch 9972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:44,220 - INFO - [diffusion][Epoch 9973] Epoch 9974/12000
2024-11-05 04:53:48,288 - INFO - [diffusion][Epoch 9973] diffusion training Loss: 0.04701204318553209
2024-11-05 04:53:48,291 - INFO - [diffusion][Epoch 9973] diffusion learning rate: 0.001
2024-11-05 04:53:48,293 - INFO - [diffusion][Epoch 9973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:48,294 - INFO - [diffusion][Epoch 9974] Epoch 9975/12000
2024-11-05 04:53:52,168 - INFO - [diffusion][Epoch 9974] diffusion training Loss: 0.051503680646419525
2024-11-05 04:53:52,170 - INFO - [diffusion][Epoch 9974] diffusion learning rate: 0.001
2024-11-05 04:53:52,171 - INFO - [diffusion][Epoch 9974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:52,172 - INFO - [diffusion][Epoch 9975] Epoch 9976/12000
2024-11-05 04:53:56,193 - INFO - [diffusion][Epoch 9975] diffusion training Loss: 0.048053015023469925
2024-11-05 04:53:56,195 - INFO - [diffusion][Epoch 9975] diffusion learning rate: 0.001
2024-11-05 04:53:56,196 - INFO - [diffusion][Epoch 9975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:53:56,198 - INFO - [diffusion][Epoch 9976] Epoch 9977/12000
2024-11-05 04:54:00,285 - INFO - [diffusion][Epoch 9976] diffusion training Loss: 0.05028852540999651
2024-11-05 04:54:00,287 - INFO - [diffusion][Epoch 9976] diffusion learning rate: 0.001
2024-11-05 04:54:00,289 - INFO - [diffusion][Epoch 9976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:00,290 - INFO - [diffusion][Epoch 9977] Epoch 9978/12000
2024-11-05 04:54:04,384 - INFO - [diffusion][Epoch 9977] diffusion training Loss: 0.04647845309227705
2024-11-05 04:54:04,386 - INFO - [diffusion][Epoch 9977] diffusion learning rate: 0.001
2024-11-05 04:54:04,388 - INFO - [diffusion][Epoch 9977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:04,389 - INFO - [diffusion][Epoch 9978] Epoch 9979/12000
2024-11-05 04:54:08,458 - INFO - [diffusion][Epoch 9978] diffusion training Loss: 0.05244350805878639
2024-11-05 04:54:08,460 - INFO - [diffusion][Epoch 9978] diffusion learning rate: 0.001
2024-11-05 04:54:08,461 - INFO - [diffusion][Epoch 9978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:08,462 - INFO - [diffusion][Epoch 9979] Epoch 9980/12000
2024-11-05 04:54:12,559 - INFO - [diffusion][Epoch 9979] diffusion training Loss: 0.04942604247480631
2024-11-05 04:54:12,561 - INFO - [diffusion][Epoch 9979] diffusion learning rate: 0.001
2024-11-05 04:54:12,563 - INFO - [diffusion][Epoch 9979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:12,564 - INFO - [diffusion][Epoch 9980] Epoch 9981/12000
2024-11-05 04:54:16,659 - INFO - [diffusion][Epoch 9980] diffusion training Loss: 0.051603762432932854
2024-11-05 04:54:16,661 - INFO - [diffusion][Epoch 9980] diffusion learning rate: 0.001
2024-11-05 04:54:16,663 - INFO - [diffusion][Epoch 9980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:16,664 - INFO - [diffusion][Epoch 9981] Epoch 9982/12000
2024-11-05 04:54:20,620 - INFO - [diffusion][Epoch 9981] diffusion training Loss: 0.05512021202594042
2024-11-05 04:54:20,622 - INFO - [diffusion][Epoch 9981] diffusion learning rate: 0.001
2024-11-05 04:54:20,624 - INFO - [diffusion][Epoch 9981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:20,625 - INFO - [diffusion][Epoch 9982] Epoch 9983/12000
2024-11-05 04:54:24,584 - INFO - [diffusion][Epoch 9982] diffusion training Loss: 0.04644120577722788
2024-11-05 04:54:24,586 - INFO - [diffusion][Epoch 9982] diffusion learning rate: 0.001
2024-11-05 04:54:24,588 - INFO - [diffusion][Epoch 9982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:24,589 - INFO - [diffusion][Epoch 9983] Epoch 9984/12000
2024-11-05 04:54:28,674 - INFO - [diffusion][Epoch 9983] diffusion training Loss: 0.05532206408679485
2024-11-05 04:54:28,676 - INFO - [diffusion][Epoch 9983] diffusion learning rate: 0.001
2024-11-05 04:54:28,678 - INFO - [diffusion][Epoch 9983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:28,679 - INFO - [diffusion][Epoch 9984] Epoch 9985/12000
2024-11-05 04:54:33,260 - INFO - [diffusion][Epoch 9984] diffusion training Loss: 0.05270425695925951
2024-11-05 04:54:33,262 - INFO - [diffusion][Epoch 9984] diffusion learning rate: 0.001
2024-11-05 04:54:33,263 - INFO - [diffusion][Epoch 9984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:33,265 - INFO - [diffusion][Epoch 9985] Epoch 9986/12000
2024-11-05 04:54:37,372 - INFO - [diffusion][Epoch 9985] diffusion training Loss: 0.04786287620663643
2024-11-05 04:54:37,374 - INFO - [diffusion][Epoch 9985] diffusion learning rate: 0.001
2024-11-05 04:54:37,400 - INFO - [diffusion][Epoch 9985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:37,402 - INFO - [diffusion][Epoch 9986] Epoch 9987/12000
2024-11-05 04:54:41,374 - INFO - [diffusion][Epoch 9986] diffusion training Loss: 0.05059381481260061
2024-11-05 04:54:41,376 - INFO - [diffusion][Epoch 9986] diffusion learning rate: 0.001
2024-11-05 04:54:41,378 - INFO - [diffusion][Epoch 9986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:41,379 - INFO - [diffusion][Epoch 9987] Epoch 9988/12000
2024-11-05 04:54:45,452 - INFO - [diffusion][Epoch 9987] diffusion training Loss: 0.047095710411667824
2024-11-05 04:54:45,454 - INFO - [diffusion][Epoch 9987] diffusion learning rate: 0.001
2024-11-05 04:54:45,456 - INFO - [diffusion][Epoch 9987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:45,457 - INFO - [diffusion][Epoch 9988] Epoch 9989/12000
2024-11-05 04:54:49,540 - INFO - [diffusion][Epoch 9988] diffusion training Loss: 0.04898825567215681
2024-11-05 04:54:49,543 - INFO - [diffusion][Epoch 9988] diffusion learning rate: 0.001
2024-11-05 04:54:49,545 - INFO - [diffusion][Epoch 9988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:49,546 - INFO - [diffusion][Epoch 9989] Epoch 9990/12000
2024-11-05 04:54:53,608 - INFO - [diffusion][Epoch 9989] diffusion training Loss: 0.05051956977695227
2024-11-05 04:54:53,610 - INFO - [diffusion][Epoch 9989] diffusion learning rate: 0.001
2024-11-05 04:54:53,612 - INFO - [diffusion][Epoch 9989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:53,613 - INFO - [diffusion][Epoch 9990] Epoch 9991/12000
2024-11-05 04:54:57,673 - INFO - [diffusion][Epoch 9990] diffusion training Loss: 0.054375775158405304
2024-11-05 04:54:57,675 - INFO - [diffusion][Epoch 9990] diffusion learning rate: 0.001
2024-11-05 04:54:57,677 - INFO - [diffusion][Epoch 9990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:54:57,678 - INFO - [diffusion][Epoch 9991] Epoch 9992/12000
2024-11-05 04:55:01,558 - INFO - [diffusion][Epoch 9991] diffusion training Loss: 0.054365137591958046
2024-11-05 04:55:01,560 - INFO - [diffusion][Epoch 9991] diffusion learning rate: 0.001
2024-11-05 04:55:01,562 - INFO - [diffusion][Epoch 9991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:01,563 - INFO - [diffusion][Epoch 9992] Epoch 9993/12000
2024-11-05 04:55:05,630 - INFO - [diffusion][Epoch 9992] diffusion training Loss: 0.04999012593179941
2024-11-05 04:55:05,632 - INFO - [diffusion][Epoch 9992] diffusion learning rate: 0.001
2024-11-05 04:55:05,633 - INFO - [diffusion][Epoch 9992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:05,635 - INFO - [diffusion][Epoch 9993] Epoch 9994/12000
2024-11-05 04:55:09,774 - INFO - [diffusion][Epoch 9993] diffusion training Loss: 0.05233578663319349
2024-11-05 04:55:09,776 - INFO - [diffusion][Epoch 9993] diffusion learning rate: 0.001
2024-11-05 04:55:09,778 - INFO - [diffusion][Epoch 9993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:09,779 - INFO - [diffusion][Epoch 9994] Epoch 9995/12000
2024-11-05 04:55:13,862 - INFO - [diffusion][Epoch 9994] diffusion training Loss: 0.046704753302037716
2024-11-05 04:55:13,864 - INFO - [diffusion][Epoch 9994] diffusion learning rate: 0.001
2024-11-05 04:55:13,865 - INFO - [diffusion][Epoch 9994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:13,866 - INFO - [diffusion][Epoch 9995] Epoch 9996/12000
2024-11-05 04:55:17,867 - INFO - [diffusion][Epoch 9995] diffusion training Loss: 0.04965343978255987
2024-11-05 04:55:17,869 - INFO - [diffusion][Epoch 9995] diffusion learning rate: 0.001
2024-11-05 04:55:17,871 - INFO - [diffusion][Epoch 9995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:17,872 - INFO - [diffusion][Epoch 9996] Epoch 9997/12000
2024-11-05 04:55:21,868 - INFO - [diffusion][Epoch 9996] diffusion training Loss: 0.04967175889760256
2024-11-05 04:55:21,870 - INFO - [diffusion][Epoch 9996] diffusion learning rate: 0.001
2024-11-05 04:55:21,872 - INFO - [diffusion][Epoch 9996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:21,873 - INFO - [diffusion][Epoch 9997] Epoch 9998/12000
2024-11-05 04:55:25,898 - INFO - [diffusion][Epoch 9997] diffusion training Loss: 0.050619613379240036
2024-11-05 04:55:25,900 - INFO - [diffusion][Epoch 9997] diffusion learning rate: 0.001
2024-11-05 04:55:25,902 - INFO - [diffusion][Epoch 9997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:25,903 - INFO - [diffusion][Epoch 9998] Epoch 9999/12000
2024-11-05 04:55:30,014 - INFO - [diffusion][Epoch 9998] diffusion training Loss: 0.05689387395977974
2024-11-05 04:55:30,016 - INFO - [diffusion][Epoch 9998] diffusion learning rate: 0.001
2024-11-05 04:55:30,018 - INFO - [diffusion][Epoch 9998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:30,019 - INFO - [diffusion][Epoch 9999] Epoch 10000/12000
2024-11-05 04:55:34,379 - INFO - [diffusion][Epoch 9999] diffusion training Loss: 0.04983746446669102
2024-11-05 04:55:34,381 - INFO - [diffusion][Epoch 9999] diffusion learning rate: 0.001
2024-11-05 04:55:34,575 - INFO - [diffusion][Epoch 9999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:34,576 - INFO - [diffusion][Epoch 10000] Epoch 10001/12000
2024-11-05 04:55:38,680 - INFO - [diffusion][Epoch 10000] diffusion training Loss: 0.047895535826683044
2024-11-05 04:55:38,682 - INFO - [diffusion][Epoch 10000] diffusion learning rate: 0.001
2024-11-05 04:55:38,684 - INFO - [diffusion][Epoch 10000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:38,686 - INFO - [diffusion][Epoch 10001] Epoch 10002/12000
2024-11-05 04:55:42,691 - INFO - [diffusion][Epoch 10001] diffusion training Loss: 0.05275382660329342
2024-11-05 04:55:42,693 - INFO - [diffusion][Epoch 10001] diffusion learning rate: 0.001
2024-11-05 04:55:42,695 - INFO - [diffusion][Epoch 10001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:42,697 - INFO - [diffusion][Epoch 10002] Epoch 10003/12000
2024-11-05 04:55:46,790 - INFO - [diffusion][Epoch 10002] diffusion training Loss: 0.05302041955292225
2024-11-05 04:55:46,792 - INFO - [diffusion][Epoch 10002] diffusion learning rate: 0.001
2024-11-05 04:55:46,794 - INFO - [diffusion][Epoch 10002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:46,795 - INFO - [diffusion][Epoch 10003] Epoch 10004/12000
2024-11-05 04:55:50,845 - INFO - [diffusion][Epoch 10003] diffusion training Loss: 0.04984303191304207
2024-11-05 04:55:50,849 - INFO - [diffusion][Epoch 10003] diffusion learning rate: 0.001
2024-11-05 04:55:50,850 - INFO - [diffusion][Epoch 10003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:50,852 - INFO - [diffusion][Epoch 10004] Epoch 10005/12000
2024-11-05 04:55:55,118 - INFO - [diffusion][Epoch 10004] diffusion training Loss: 0.04902202636003494
2024-11-05 04:55:55,120 - INFO - [diffusion][Epoch 10004] diffusion learning rate: 0.001
2024-11-05 04:55:55,122 - INFO - [diffusion][Epoch 10004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:55,123 - INFO - [diffusion][Epoch 10005] Epoch 10006/12000
2024-11-05 04:55:59,210 - INFO - [diffusion][Epoch 10005] diffusion training Loss: 0.05281548481434584
2024-11-05 04:55:59,212 - INFO - [diffusion][Epoch 10005] diffusion learning rate: 0.001
2024-11-05 04:55:59,226 - INFO - [diffusion][Epoch 10005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:55:59,227 - INFO - [diffusion][Epoch 10006] Epoch 10007/12000
2024-11-05 04:56:03,283 - INFO - [diffusion][Epoch 10006] diffusion training Loss: 0.0493059866130352
2024-11-05 04:56:03,285 - INFO - [diffusion][Epoch 10006] diffusion learning rate: 0.001
2024-11-05 04:56:03,287 - INFO - [diffusion][Epoch 10006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:03,288 - INFO - [diffusion][Epoch 10007] Epoch 10008/12000
2024-11-05 04:56:07,364 - INFO - [diffusion][Epoch 10007] diffusion training Loss: 0.050944034941494465
2024-11-05 04:56:07,366 - INFO - [diffusion][Epoch 10007] diffusion learning rate: 0.001
2024-11-05 04:56:07,368 - INFO - [diffusion][Epoch 10007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:07,369 - INFO - [diffusion][Epoch 10008] Epoch 10009/12000
2024-11-05 04:56:11,404 - INFO - [diffusion][Epoch 10008] diffusion training Loss: 0.054734205827116966
2024-11-05 04:56:11,406 - INFO - [diffusion][Epoch 10008] diffusion learning rate: 0.001
2024-11-05 04:56:11,408 - INFO - [diffusion][Epoch 10008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:11,409 - INFO - [diffusion][Epoch 10009] Epoch 10010/12000
2024-11-05 04:56:15,537 - INFO - [diffusion][Epoch 10009] diffusion training Loss: 0.048832885921001434
2024-11-05 04:56:15,539 - INFO - [diffusion][Epoch 10009] diffusion learning rate: 0.001
2024-11-05 04:56:15,565 - INFO - [diffusion][Epoch 10009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:15,567 - INFO - [diffusion][Epoch 10010] Epoch 10011/12000
2024-11-05 04:56:19,538 - INFO - [diffusion][Epoch 10010] diffusion training Loss: 0.05038381926715374
2024-11-05 04:56:19,540 - INFO - [diffusion][Epoch 10010] diffusion learning rate: 0.001
2024-11-05 04:56:19,542 - INFO - [diffusion][Epoch 10010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:19,543 - INFO - [diffusion][Epoch 10011] Epoch 10012/12000
2024-11-05 04:56:23,620 - INFO - [diffusion][Epoch 10011] diffusion training Loss: 0.05463376268744469
2024-11-05 04:56:23,622 - INFO - [diffusion][Epoch 10011] diffusion learning rate: 0.001
2024-11-05 04:56:23,624 - INFO - [diffusion][Epoch 10011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:23,625 - INFO - [diffusion][Epoch 10012] Epoch 10013/12000
2024-11-05 04:56:27,699 - INFO - [diffusion][Epoch 10012] diffusion training Loss: 0.05575265642255545
2024-11-05 04:56:27,701 - INFO - [diffusion][Epoch 10012] diffusion learning rate: 0.001
2024-11-05 04:56:27,703 - INFO - [diffusion][Epoch 10012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:27,704 - INFO - [diffusion][Epoch 10013] Epoch 10014/12000
2024-11-05 04:56:31,578 - INFO - [diffusion][Epoch 10013] diffusion training Loss: 0.04873630777001381
2024-11-05 04:56:31,580 - INFO - [diffusion][Epoch 10013] diffusion learning rate: 0.001
2024-11-05 04:56:31,581 - INFO - [diffusion][Epoch 10013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:31,582 - INFO - [diffusion][Epoch 10014] Epoch 10015/12000
2024-11-05 04:56:35,547 - INFO - [diffusion][Epoch 10014] diffusion training Loss: 0.047974071465432644
2024-11-05 04:56:35,549 - INFO - [diffusion][Epoch 10014] diffusion learning rate: 0.001
2024-11-05 04:56:35,551 - INFO - [diffusion][Epoch 10014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:35,552 - INFO - [diffusion][Epoch 10015] Epoch 10016/12000
2024-11-05 04:56:39,679 - INFO - [diffusion][Epoch 10015] diffusion training Loss: 0.045932344160974026
2024-11-05 04:56:39,681 - INFO - [diffusion][Epoch 10015] diffusion learning rate: 0.001
2024-11-05 04:56:39,683 - INFO - [diffusion][Epoch 10015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:39,684 - INFO - [diffusion][Epoch 10016] Epoch 10017/12000
2024-11-05 04:56:43,722 - INFO - [diffusion][Epoch 10016] diffusion training Loss: 0.0458090640604496
2024-11-05 04:56:43,724 - INFO - [diffusion][Epoch 10016] diffusion learning rate: 0.001
2024-11-05 04:56:43,725 - INFO - [diffusion][Epoch 10016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:43,727 - INFO - [diffusion][Epoch 10017] Epoch 10018/12000
2024-11-05 04:56:47,812 - INFO - [diffusion][Epoch 10017] diffusion training Loss: 0.043292789719998837
2024-11-05 04:56:47,814 - INFO - [diffusion][Epoch 10017] diffusion learning rate: 0.001
2024-11-05 04:56:47,816 - INFO - [diffusion][Epoch 10017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:47,818 - INFO - [diffusion][Epoch 10018] Epoch 10019/12000
2024-11-05 04:56:51,980 - INFO - [diffusion][Epoch 10018] diffusion training Loss: 0.05578135885298252
2024-11-05 04:56:51,982 - INFO - [diffusion][Epoch 10018] diffusion learning rate: 0.001
2024-11-05 04:56:51,984 - INFO - [diffusion][Epoch 10018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:51,986 - INFO - [diffusion][Epoch 10019] Epoch 10020/12000
2024-11-05 04:56:56,071 - INFO - [diffusion][Epoch 10019] diffusion training Loss: 0.05005059018731117
2024-11-05 04:56:56,073 - INFO - [diffusion][Epoch 10019] diffusion learning rate: 0.001
2024-11-05 04:56:56,075 - INFO - [diffusion][Epoch 10019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:56:56,076 - INFO - [diffusion][Epoch 10020] Epoch 10021/12000
2024-11-05 04:57:00,189 - INFO - [diffusion][Epoch 10020] diffusion training Loss: 0.050193026661872864
2024-11-05 04:57:00,191 - INFO - [diffusion][Epoch 10020] diffusion learning rate: 0.001
2024-11-05 04:57:00,193 - INFO - [diffusion][Epoch 10020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:00,194 - INFO - [diffusion][Epoch 10021] Epoch 10022/12000
2024-11-05 04:57:04,249 - INFO - [diffusion][Epoch 10021] diffusion training Loss: 0.04816865734755993
2024-11-05 04:57:04,251 - INFO - [diffusion][Epoch 10021] diffusion learning rate: 0.001
2024-11-05 04:57:04,253 - INFO - [diffusion][Epoch 10021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:04,254 - INFO - [diffusion][Epoch 10022] Epoch 10023/12000
2024-11-05 04:57:08,356 - INFO - [diffusion][Epoch 10022] diffusion training Loss: 0.04904269054532051
2024-11-05 04:57:08,359 - INFO - [diffusion][Epoch 10022] diffusion learning rate: 0.001
2024-11-05 04:57:08,360 - INFO - [diffusion][Epoch 10022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:08,362 - INFO - [diffusion][Epoch 10023] Epoch 10024/12000
2024-11-05 04:57:12,399 - INFO - [diffusion][Epoch 10023] diffusion training Loss: 0.048350740224123
2024-11-05 04:57:12,401 - INFO - [diffusion][Epoch 10023] diffusion learning rate: 0.001
2024-11-05 04:57:12,419 - INFO - [diffusion][Epoch 10023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:12,420 - INFO - [diffusion][Epoch 10024] Epoch 10025/12000
2024-11-05 04:57:16,792 - INFO - [diffusion][Epoch 10024] diffusion training Loss: 0.04629611782729626
2024-11-05 04:57:16,794 - INFO - [diffusion][Epoch 10024] diffusion learning rate: 0.001
2024-11-05 04:57:16,795 - INFO - [diffusion][Epoch 10024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:16,797 - INFO - [diffusion][Epoch 10025] Epoch 10026/12000
2024-11-05 04:57:20,939 - INFO - [diffusion][Epoch 10025] diffusion training Loss: 0.04801715910434723
2024-11-05 04:57:20,941 - INFO - [diffusion][Epoch 10025] diffusion learning rate: 0.001
2024-11-05 04:57:20,943 - INFO - [diffusion][Epoch 10025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:20,944 - INFO - [diffusion][Epoch 10026] Epoch 10027/12000
2024-11-05 04:57:25,020 - INFO - [diffusion][Epoch 10026] diffusion training Loss: 0.048051406629383564
2024-11-05 04:57:25,022 - INFO - [diffusion][Epoch 10026] diffusion learning rate: 0.001
2024-11-05 04:57:25,024 - INFO - [diffusion][Epoch 10026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:25,026 - INFO - [diffusion][Epoch 10027] Epoch 10028/12000
2024-11-05 04:57:29,147 - INFO - [diffusion][Epoch 10027] diffusion training Loss: 0.04678614065051079
2024-11-05 04:57:29,149 - INFO - [diffusion][Epoch 10027] diffusion learning rate: 0.001
2024-11-05 04:57:29,150 - INFO - [diffusion][Epoch 10027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:29,152 - INFO - [diffusion][Epoch 10028] Epoch 10029/12000
2024-11-05 04:57:33,315 - INFO - [diffusion][Epoch 10028] diffusion training Loss: 0.04941761866211891
2024-11-05 04:57:33,317 - INFO - [diffusion][Epoch 10028] diffusion learning rate: 0.001
2024-11-05 04:57:33,343 - INFO - [diffusion][Epoch 10028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:33,345 - INFO - [diffusion][Epoch 10029] Epoch 10030/12000
2024-11-05 04:57:37,576 - INFO - [diffusion][Epoch 10029] diffusion training Loss: 0.05192040465772152
2024-11-05 04:57:37,578 - INFO - [diffusion][Epoch 10029] diffusion learning rate: 0.001
2024-11-05 04:57:37,580 - INFO - [diffusion][Epoch 10029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:37,581 - INFO - [diffusion][Epoch 10030] Epoch 10031/12000
2024-11-05 04:57:41,854 - INFO - [diffusion][Epoch 10030] diffusion training Loss: 0.04509750474244356
2024-11-05 04:57:41,856 - INFO - [diffusion][Epoch 10030] diffusion learning rate: 0.001
2024-11-05 04:57:41,858 - INFO - [diffusion][Epoch 10030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:41,860 - INFO - [diffusion][Epoch 10031] Epoch 10032/12000
2024-11-05 04:57:45,997 - INFO - [diffusion][Epoch 10031] diffusion training Loss: 0.04730846453458071
2024-11-05 04:57:46,000 - INFO - [diffusion][Epoch 10031] diffusion learning rate: 0.001
2024-11-05 04:57:46,002 - INFO - [diffusion][Epoch 10031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:46,003 - INFO - [diffusion][Epoch 10032] Epoch 10033/12000
2024-11-05 04:57:50,197 - INFO - [diffusion][Epoch 10032] diffusion training Loss: 0.047432330437004566
2024-11-05 04:57:50,224 - INFO - [diffusion][Epoch 10032] diffusion learning rate: 0.001
2024-11-05 04:57:50,226 - INFO - [diffusion][Epoch 10032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:50,228 - INFO - [diffusion][Epoch 10033] Epoch 10034/12000
2024-11-05 04:57:54,334 - INFO - [diffusion][Epoch 10033] diffusion training Loss: 0.04743485152721405
2024-11-05 04:57:54,337 - INFO - [diffusion][Epoch 10033] diffusion learning rate: 0.001
2024-11-05 04:57:54,339 - INFO - [diffusion][Epoch 10033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:54,340 - INFO - [diffusion][Epoch 10034] Epoch 10035/12000
2024-11-05 04:57:58,439 - INFO - [diffusion][Epoch 10034] diffusion training Loss: 0.04770359396934509
2024-11-05 04:57:58,441 - INFO - [diffusion][Epoch 10034] diffusion learning rate: 0.001
2024-11-05 04:57:58,442 - INFO - [diffusion][Epoch 10034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:57:58,444 - INFO - [diffusion][Epoch 10035] Epoch 10036/12000
2024-11-05 04:58:02,540 - INFO - [diffusion][Epoch 10035] diffusion training Loss: 0.050596194341778755
2024-11-05 04:58:02,542 - INFO - [diffusion][Epoch 10035] diffusion learning rate: 0.001
2024-11-05 04:58:02,543 - INFO - [diffusion][Epoch 10035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:02,545 - INFO - [diffusion][Epoch 10036] Epoch 10037/12000
2024-11-05 04:58:06,636 - INFO - [diffusion][Epoch 10036] diffusion training Loss: 0.050280279479920864
2024-11-05 04:58:06,638 - INFO - [diffusion][Epoch 10036] diffusion learning rate: 0.001
2024-11-05 04:58:06,640 - INFO - [diffusion][Epoch 10036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:06,641 - INFO - [diffusion][Epoch 10037] Epoch 10038/12000
2024-11-05 04:58:10,746 - INFO - [diffusion][Epoch 10037] diffusion training Loss: 0.04872002173215151
2024-11-05 04:58:10,748 - INFO - [diffusion][Epoch 10037] diffusion learning rate: 0.001
2024-11-05 04:58:10,750 - INFO - [diffusion][Epoch 10037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:10,751 - INFO - [diffusion][Epoch 10038] Epoch 10039/12000
2024-11-05 04:58:14,858 - INFO - [diffusion][Epoch 10038] diffusion training Loss: 0.05115603841841221
2024-11-05 04:58:14,859 - INFO - [diffusion][Epoch 10038] diffusion learning rate: 0.001
2024-11-05 04:58:14,861 - INFO - [diffusion][Epoch 10038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:14,862 - INFO - [diffusion][Epoch 10039] Epoch 10040/12000
2024-11-05 04:58:19,044 - INFO - [diffusion][Epoch 10039] diffusion training Loss: 0.04978583659976721
2024-11-05 04:58:19,046 - INFO - [diffusion][Epoch 10039] diffusion learning rate: 0.001
2024-11-05 04:58:19,047 - INFO - [diffusion][Epoch 10039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:19,049 - INFO - [diffusion][Epoch 10040] Epoch 10041/12000
2024-11-05 04:58:23,037 - INFO - [diffusion][Epoch 10040] diffusion training Loss: 0.05342685617506504
2024-11-05 04:58:23,039 - INFO - [diffusion][Epoch 10040] diffusion learning rate: 0.001
2024-11-05 04:58:23,040 - INFO - [diffusion][Epoch 10040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:23,042 - INFO - [diffusion][Epoch 10041] Epoch 10042/12000
2024-11-05 04:58:26,998 - INFO - [diffusion][Epoch 10041] diffusion training Loss: 0.04910576622933149
2024-11-05 04:58:27,000 - INFO - [diffusion][Epoch 10041] diffusion learning rate: 0.001
2024-11-05 04:58:27,002 - INFO - [diffusion][Epoch 10041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:27,003 - INFO - [diffusion][Epoch 10042] Epoch 10043/12000
2024-11-05 04:58:31,023 - INFO - [diffusion][Epoch 10042] diffusion training Loss: 0.05312393605709076
2024-11-05 04:58:31,025 - INFO - [diffusion][Epoch 10042] diffusion learning rate: 0.001
2024-11-05 04:58:31,027 - INFO - [diffusion][Epoch 10042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:31,028 - INFO - [diffusion][Epoch 10043] Epoch 10044/12000
2024-11-05 04:58:35,073 - INFO - [diffusion][Epoch 10043] diffusion training Loss: 0.04785619489848614
2024-11-05 04:58:35,075 - INFO - [diffusion][Epoch 10043] diffusion learning rate: 0.001
2024-11-05 04:58:35,076 - INFO - [diffusion][Epoch 10043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:35,078 - INFO - [diffusion][Epoch 10044] Epoch 10045/12000
2024-11-05 04:58:39,138 - INFO - [diffusion][Epoch 10044] diffusion training Loss: 0.049377862364053726
2024-11-05 04:58:39,139 - INFO - [diffusion][Epoch 10044] diffusion learning rate: 0.001
2024-11-05 04:58:39,141 - INFO - [diffusion][Epoch 10044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:39,142 - INFO - [diffusion][Epoch 10045] Epoch 10046/12000
2024-11-05 04:58:43,229 - INFO - [diffusion][Epoch 10045] diffusion training Loss: 0.04618317633867264
2024-11-05 04:58:43,231 - INFO - [diffusion][Epoch 10045] diffusion learning rate: 0.001
2024-11-05 04:58:43,233 - INFO - [diffusion][Epoch 10045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:43,234 - INFO - [diffusion][Epoch 10046] Epoch 10047/12000
2024-11-05 04:58:47,281 - INFO - [diffusion][Epoch 10046] diffusion training Loss: 0.05233873799443245
2024-11-05 04:58:47,283 - INFO - [diffusion][Epoch 10046] diffusion learning rate: 0.001
2024-11-05 04:58:47,285 - INFO - [diffusion][Epoch 10046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:47,286 - INFO - [diffusion][Epoch 10047] Epoch 10048/12000
2024-11-05 04:58:51,270 - INFO - [diffusion][Epoch 10047] diffusion training Loss: 0.04477715864777565
2024-11-05 04:58:51,272 - INFO - [diffusion][Epoch 10047] diffusion learning rate: 0.001
2024-11-05 04:58:51,274 - INFO - [diffusion][Epoch 10047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:51,275 - INFO - [diffusion][Epoch 10048] Epoch 10049/12000
2024-11-05 04:58:55,257 - INFO - [diffusion][Epoch 10048] diffusion training Loss: 0.050650191493332386
2024-11-05 04:58:55,260 - INFO - [diffusion][Epoch 10048] diffusion learning rate: 0.001
2024-11-05 04:58:55,262 - INFO - [diffusion][Epoch 10048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:55,263 - INFO - [diffusion][Epoch 10049] Epoch 10050/12000
2024-11-05 04:58:59,393 - INFO - [diffusion][Epoch 10049] diffusion training Loss: 0.050233323127031326
2024-11-05 04:58:59,395 - INFO - [diffusion][Epoch 10049] diffusion learning rate: 0.001
2024-11-05 04:58:59,397 - INFO - [diffusion][Epoch 10049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:58:59,399 - INFO - [diffusion][Epoch 10050] Epoch 10051/12000
2024-11-05 04:59:03,500 - INFO - [diffusion][Epoch 10050] diffusion training Loss: 0.044509598053991795
2024-11-05 04:59:03,502 - INFO - [diffusion][Epoch 10050] diffusion learning rate: 0.001
2024-11-05 04:59:03,504 - INFO - [diffusion][Epoch 10050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:03,505 - INFO - [diffusion][Epoch 10051] Epoch 10052/12000
2024-11-05 04:59:07,529 - INFO - [diffusion][Epoch 10051] diffusion training Loss: 0.055482182651758194
2024-11-05 04:59:07,531 - INFO - [diffusion][Epoch 10051] diffusion learning rate: 0.001
2024-11-05 04:59:07,533 - INFO - [diffusion][Epoch 10051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:07,534 - INFO - [diffusion][Epoch 10052] Epoch 10053/12000
2024-11-05 04:59:11,710 - INFO - [diffusion][Epoch 10052] diffusion training Loss: 0.05152235459536314
2024-11-05 04:59:11,712 - INFO - [diffusion][Epoch 10052] diffusion learning rate: 0.001
2024-11-05 04:59:11,741 - INFO - [diffusion][Epoch 10052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:11,743 - INFO - [diffusion][Epoch 10053] Epoch 10054/12000
2024-11-05 04:59:16,008 - INFO - [diffusion][Epoch 10053] diffusion training Loss: 0.0519609022885561
2024-11-05 04:59:16,011 - INFO - [diffusion][Epoch 10053] diffusion learning rate: 0.001
2024-11-05 04:59:16,013 - INFO - [diffusion][Epoch 10053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:16,014 - INFO - [diffusion][Epoch 10054] Epoch 10055/12000
2024-11-05 04:59:20,370 - INFO - [diffusion][Epoch 10054] diffusion training Loss: 0.05304563045501709
2024-11-05 04:59:20,372 - INFO - [diffusion][Epoch 10054] diffusion learning rate: 0.001
2024-11-05 04:59:20,374 - INFO - [diffusion][Epoch 10054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:20,376 - INFO - [diffusion][Epoch 10055] Epoch 10056/12000
2024-11-05 04:59:24,489 - INFO - [diffusion][Epoch 10055] diffusion training Loss: 0.04843174386769533
2024-11-05 04:59:24,491 - INFO - [diffusion][Epoch 10055] diffusion learning rate: 0.001
2024-11-05 04:59:24,493 - INFO - [diffusion][Epoch 10055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:24,494 - INFO - [diffusion][Epoch 10056] Epoch 10057/12000
2024-11-05 04:59:28,407 - INFO - [diffusion][Epoch 10056] diffusion training Loss: 0.046223390847444534
2024-11-05 04:59:28,410 - INFO - [diffusion][Epoch 10056] diffusion learning rate: 0.001
2024-11-05 04:59:28,412 - INFO - [diffusion][Epoch 10056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:28,413 - INFO - [diffusion][Epoch 10057] Epoch 10058/12000
2024-11-05 04:59:32,339 - INFO - [diffusion][Epoch 10057] diffusion training Loss: 0.04513134807348251
2024-11-05 04:59:32,341 - INFO - [diffusion][Epoch 10057] diffusion learning rate: 0.001
2024-11-05 04:59:32,344 - INFO - [diffusion][Epoch 10057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:32,345 - INFO - [diffusion][Epoch 10058] Epoch 10059/12000
2024-11-05 04:59:36,438 - INFO - [diffusion][Epoch 10058] diffusion training Loss: 0.0498468978330493
2024-11-05 04:59:36,440 - INFO - [diffusion][Epoch 10058] diffusion learning rate: 0.001
2024-11-05 04:59:36,441 - INFO - [diffusion][Epoch 10058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:36,442 - INFO - [diffusion][Epoch 10059] Epoch 10060/12000
2024-11-05 04:59:40,532 - INFO - [diffusion][Epoch 10059] diffusion training Loss: 0.04737165290862322
2024-11-05 04:59:40,534 - INFO - [diffusion][Epoch 10059] diffusion learning rate: 0.001
2024-11-05 04:59:40,536 - INFO - [diffusion][Epoch 10059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:40,537 - INFO - [diffusion][Epoch 10060] Epoch 10061/12000
2024-11-05 04:59:44,811 - INFO - [diffusion][Epoch 10060] diffusion training Loss: 0.04690849129110575
2024-11-05 04:59:44,813 - INFO - [diffusion][Epoch 10060] diffusion learning rate: 0.001
2024-11-05 04:59:44,814 - INFO - [diffusion][Epoch 10060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:44,816 - INFO - [diffusion][Epoch 10061] Epoch 10062/12000
2024-11-05 04:59:48,952 - INFO - [diffusion][Epoch 10061] diffusion training Loss: 0.049971397034823895
2024-11-05 04:59:48,955 - INFO - [diffusion][Epoch 10061] diffusion learning rate: 0.001
2024-11-05 04:59:48,956 - INFO - [diffusion][Epoch 10061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:48,958 - INFO - [diffusion][Epoch 10062] Epoch 10063/12000
2024-11-05 04:59:53,016 - INFO - [diffusion][Epoch 10062] diffusion training Loss: 0.0465629743412137
2024-11-05 04:59:53,018 - INFO - [diffusion][Epoch 10062] diffusion learning rate: 0.001
2024-11-05 04:59:53,020 - INFO - [diffusion][Epoch 10062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:53,022 - INFO - [diffusion][Epoch 10063] Epoch 10064/12000
2024-11-05 04:59:57,140 - INFO - [diffusion][Epoch 10063] diffusion training Loss: 0.052781347185373306
2024-11-05 04:59:57,143 - INFO - [diffusion][Epoch 10063] diffusion learning rate: 0.001
2024-11-05 04:59:57,145 - INFO - [diffusion][Epoch 10063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 04:59:57,147 - INFO - [diffusion][Epoch 10064] Epoch 10065/12000
2024-11-05 05:00:01,405 - INFO - [diffusion][Epoch 10064] diffusion training Loss: 0.047951399348676205
2024-11-05 05:00:01,407 - INFO - [diffusion][Epoch 10064] diffusion learning rate: 0.001
2024-11-05 05:00:01,409 - INFO - [diffusion][Epoch 10064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:01,410 - INFO - [diffusion][Epoch 10065] Epoch 10066/12000
2024-11-05 05:00:06,082 - INFO - [diffusion][Epoch 10065] diffusion training Loss: 0.04769839718937874
2024-11-05 05:00:06,084 - INFO - [diffusion][Epoch 10065] diffusion learning rate: 0.001
2024-11-05 05:00:06,086 - INFO - [diffusion][Epoch 10065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:06,087 - INFO - [diffusion][Epoch 10066] Epoch 10067/12000
2024-11-05 05:00:10,209 - INFO - [diffusion][Epoch 10066] diffusion training Loss: 0.04857051186263561
2024-11-05 05:00:10,211 - INFO - [diffusion][Epoch 10066] diffusion learning rate: 0.001
2024-11-05 05:00:10,213 - INFO - [diffusion][Epoch 10066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:10,214 - INFO - [diffusion][Epoch 10067] Epoch 10068/12000
2024-11-05 05:00:14,318 - INFO - [diffusion][Epoch 10067] diffusion training Loss: 0.04795248247683048
2024-11-05 05:00:14,320 - INFO - [diffusion][Epoch 10067] diffusion learning rate: 0.001
2024-11-05 05:00:14,340 - INFO - [diffusion][Epoch 10067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:14,341 - INFO - [diffusion][Epoch 10068] Epoch 10069/12000
2024-11-05 05:00:18,391 - INFO - [diffusion][Epoch 10068] diffusion training Loss: 0.047052619978785515
2024-11-05 05:00:18,393 - INFO - [diffusion][Epoch 10068] diffusion learning rate: 0.001
2024-11-05 05:00:18,395 - INFO - [diffusion][Epoch 10068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:18,396 - INFO - [diffusion][Epoch 10069] Epoch 10070/12000
2024-11-05 05:00:22,400 - INFO - [diffusion][Epoch 10069] diffusion training Loss: 0.047168418765068054
2024-11-05 05:00:22,402 - INFO - [diffusion][Epoch 10069] diffusion learning rate: 0.001
2024-11-05 05:00:22,404 - INFO - [diffusion][Epoch 10069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:22,406 - INFO - [diffusion][Epoch 10070] Epoch 10071/12000
2024-11-05 05:00:26,495 - INFO - [diffusion][Epoch 10070] diffusion training Loss: 0.05065550655126572
2024-11-05 05:00:26,497 - INFO - [diffusion][Epoch 10070] diffusion learning rate: 0.001
2024-11-05 05:00:26,499 - INFO - [diffusion][Epoch 10070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:26,500 - INFO - [diffusion][Epoch 10071] Epoch 10072/12000
2024-11-05 05:00:30,580 - INFO - [diffusion][Epoch 10071] diffusion training Loss: 0.04930734820663929
2024-11-05 05:00:30,581 - INFO - [diffusion][Epoch 10071] diffusion learning rate: 0.001
2024-11-05 05:00:30,583 - INFO - [diffusion][Epoch 10071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:30,585 - INFO - [diffusion][Epoch 10072] Epoch 10073/12000
2024-11-05 05:00:34,617 - INFO - [diffusion][Epoch 10072] diffusion training Loss: 0.04886717163026333
2024-11-05 05:00:34,619 - INFO - [diffusion][Epoch 10072] diffusion learning rate: 0.001
2024-11-05 05:00:34,621 - INFO - [diffusion][Epoch 10072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:34,623 - INFO - [diffusion][Epoch 10073] Epoch 10074/12000
2024-11-05 05:00:38,809 - INFO - [diffusion][Epoch 10073] diffusion training Loss: 0.05022581201046705
2024-11-05 05:00:38,811 - INFO - [diffusion][Epoch 10073] diffusion learning rate: 0.001
2024-11-05 05:00:38,813 - INFO - [diffusion][Epoch 10073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:38,815 - INFO - [diffusion][Epoch 10074] Epoch 10075/12000
2024-11-05 05:00:42,958 - INFO - [diffusion][Epoch 10074] diffusion training Loss: 0.04395656753331423
2024-11-05 05:00:42,961 - INFO - [diffusion][Epoch 10074] diffusion learning rate: 0.001
2024-11-05 05:00:42,962 - INFO - [diffusion][Epoch 10074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:42,964 - INFO - [diffusion][Epoch 10075] Epoch 10076/12000
2024-11-05 05:00:47,081 - INFO - [diffusion][Epoch 10075] diffusion training Loss: 0.045985796488821507
2024-11-05 05:00:47,083 - INFO - [diffusion][Epoch 10075] diffusion learning rate: 0.001
2024-11-05 05:00:47,085 - INFO - [diffusion][Epoch 10075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:47,086 - INFO - [diffusion][Epoch 10076] Epoch 10077/12000
2024-11-05 05:00:51,338 - INFO - [diffusion][Epoch 10076] diffusion training Loss: 0.05102933757007122
2024-11-05 05:00:51,340 - INFO - [diffusion][Epoch 10076] diffusion learning rate: 0.001
2024-11-05 05:00:51,345 - INFO - [diffusion][Epoch 10076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:51,347 - INFO - [diffusion][Epoch 10077] Epoch 10078/12000
2024-11-05 05:00:55,539 - INFO - [diffusion][Epoch 10077] diffusion training Loss: 0.050914086401462555
2024-11-05 05:00:55,543 - INFO - [diffusion][Epoch 10077] diffusion learning rate: 0.001
2024-11-05 05:00:55,544 - INFO - [diffusion][Epoch 10077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:55,546 - INFO - [diffusion][Epoch 10078] Epoch 10079/12000
2024-11-05 05:00:59,680 - INFO - [diffusion][Epoch 10078] diffusion training Loss: 0.053946685045957565
2024-11-05 05:00:59,683 - INFO - [diffusion][Epoch 10078] diffusion learning rate: 0.001
2024-11-05 05:00:59,685 - INFO - [diffusion][Epoch 10078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:00:59,686 - INFO - [diffusion][Epoch 10079] Epoch 10080/12000
2024-11-05 05:01:03,854 - INFO - [diffusion][Epoch 10079] diffusion training Loss: 0.04588643554598093
2024-11-05 05:01:03,856 - INFO - [diffusion][Epoch 10079] diffusion learning rate: 0.001
2024-11-05 05:01:03,858 - INFO - [diffusion][Epoch 10079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:03,860 - INFO - [diffusion][Epoch 10080] Epoch 10081/12000
2024-11-05 05:01:07,962 - INFO - [diffusion][Epoch 10080] diffusion training Loss: 0.04874248243868351
2024-11-05 05:01:07,964 - INFO - [diffusion][Epoch 10080] diffusion learning rate: 0.001
2024-11-05 05:01:07,968 - INFO - [diffusion][Epoch 10080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:07,969 - INFO - [diffusion][Epoch 10081] Epoch 10082/12000
2024-11-05 05:01:12,013 - INFO - [diffusion][Epoch 10081] diffusion training Loss: 0.044799298979341984
2024-11-05 05:01:12,015 - INFO - [diffusion][Epoch 10081] diffusion learning rate: 0.001
2024-11-05 05:01:12,017 - INFO - [diffusion][Epoch 10081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:12,018 - INFO - [diffusion][Epoch 10082] Epoch 10083/12000
2024-11-05 05:01:15,967 - INFO - [diffusion][Epoch 10082] diffusion training Loss: 0.051290543749928474
2024-11-05 05:01:15,969 - INFO - [diffusion][Epoch 10082] diffusion learning rate: 0.001
2024-11-05 05:01:15,972 - INFO - [diffusion][Epoch 10082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:15,973 - INFO - [diffusion][Epoch 10083] Epoch 10084/12000
2024-11-05 05:01:20,030 - INFO - [diffusion][Epoch 10083] diffusion training Loss: 0.04902214650064707
2024-11-05 05:01:20,031 - INFO - [diffusion][Epoch 10083] diffusion learning rate: 0.001
2024-11-05 05:01:20,077 - INFO - [diffusion][Epoch 10083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:20,079 - INFO - [diffusion][Epoch 10084] Epoch 10085/12000
2024-11-05 05:01:24,158 - INFO - [diffusion][Epoch 10084] diffusion training Loss: 0.0473015746101737
2024-11-05 05:01:24,160 - INFO - [diffusion][Epoch 10084] diffusion learning rate: 0.001
2024-11-05 05:01:24,162 - INFO - [diffusion][Epoch 10084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:24,163 - INFO - [diffusion][Epoch 10085] Epoch 10086/12000
2024-11-05 05:01:28,302 - INFO - [diffusion][Epoch 10085] diffusion training Loss: 0.04965306166559458
2024-11-05 05:01:28,304 - INFO - [diffusion][Epoch 10085] diffusion learning rate: 0.001
2024-11-05 05:01:28,306 - INFO - [diffusion][Epoch 10085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:28,307 - INFO - [diffusion][Epoch 10086] Epoch 10087/12000
2024-11-05 05:01:32,968 - INFO - [diffusion][Epoch 10086] diffusion training Loss: 0.048086377792060375
2024-11-05 05:01:32,970 - INFO - [diffusion][Epoch 10086] diffusion learning rate: 0.001
2024-11-05 05:01:32,972 - INFO - [diffusion][Epoch 10086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:32,974 - INFO - [diffusion][Epoch 10087] Epoch 10088/12000
2024-11-05 05:01:37,167 - INFO - [diffusion][Epoch 10087] diffusion training Loss: 0.05429319757968187
2024-11-05 05:01:37,169 - INFO - [diffusion][Epoch 10087] diffusion learning rate: 0.001
2024-11-05 05:01:37,170 - INFO - [diffusion][Epoch 10087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:37,172 - INFO - [diffusion][Epoch 10088] Epoch 10089/12000
2024-11-05 05:01:41,209 - INFO - [diffusion][Epoch 10088] diffusion training Loss: 0.05178798548877239
2024-11-05 05:01:41,210 - INFO - [diffusion][Epoch 10088] diffusion learning rate: 0.001
2024-11-05 05:01:41,212 - INFO - [diffusion][Epoch 10088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:41,214 - INFO - [diffusion][Epoch 10089] Epoch 10090/12000
2024-11-05 05:01:45,267 - INFO - [diffusion][Epoch 10089] diffusion training Loss: 0.05409666709601879
2024-11-05 05:01:45,269 - INFO - [diffusion][Epoch 10089] diffusion learning rate: 0.001
2024-11-05 05:01:45,271 - INFO - [diffusion][Epoch 10089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:45,272 - INFO - [diffusion][Epoch 10090] Epoch 10091/12000
2024-11-05 05:01:49,463 - INFO - [diffusion][Epoch 10090] diffusion training Loss: 0.04590865038335323
2024-11-05 05:01:49,465 - INFO - [diffusion][Epoch 10090] diffusion learning rate: 0.001
2024-11-05 05:01:49,467 - INFO - [diffusion][Epoch 10090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:49,468 - INFO - [diffusion][Epoch 10091] Epoch 10092/12000
2024-11-05 05:01:53,532 - INFO - [diffusion][Epoch 10091] diffusion training Loss: 0.047057412564754486
2024-11-05 05:01:53,534 - INFO - [diffusion][Epoch 10091] diffusion learning rate: 0.001
2024-11-05 05:01:53,536 - INFO - [diffusion][Epoch 10091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:53,537 - INFO - [diffusion][Epoch 10092] Epoch 10093/12000
2024-11-05 05:01:57,632 - INFO - [diffusion][Epoch 10092] diffusion training Loss: 0.047979310154914856
2024-11-05 05:01:57,634 - INFO - [diffusion][Epoch 10092] diffusion learning rate: 0.001
2024-11-05 05:01:57,635 - INFO - [diffusion][Epoch 10092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:01:57,636 - INFO - [diffusion][Epoch 10093] Epoch 10094/12000
2024-11-05 05:02:01,716 - INFO - [diffusion][Epoch 10093] diffusion training Loss: 0.049829659052193165
2024-11-05 05:02:01,718 - INFO - [diffusion][Epoch 10093] diffusion learning rate: 0.001
2024-11-05 05:02:01,720 - INFO - [diffusion][Epoch 10093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:01,721 - INFO - [diffusion][Epoch 10094] Epoch 10095/12000
2024-11-05 05:02:05,819 - INFO - [diffusion][Epoch 10094] diffusion training Loss: 0.049573274329304695
2024-11-05 05:02:05,821 - INFO - [diffusion][Epoch 10094] diffusion learning rate: 0.001
2024-11-05 05:02:05,823 - INFO - [diffusion][Epoch 10094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:05,825 - INFO - [diffusion][Epoch 10095] Epoch 10096/12000
2024-11-05 05:02:09,910 - INFO - [diffusion][Epoch 10095] diffusion training Loss: 0.0498826690018177
2024-11-05 05:02:09,913 - INFO - [diffusion][Epoch 10095] diffusion learning rate: 0.001
2024-11-05 05:02:09,915 - INFO - [diffusion][Epoch 10095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:09,916 - INFO - [diffusion][Epoch 10096] Epoch 10097/12000
2024-11-05 05:02:13,970 - INFO - [diffusion][Epoch 10096] diffusion training Loss: 0.04731957521289587
2024-11-05 05:02:13,972 - INFO - [diffusion][Epoch 10096] diffusion learning rate: 0.001
2024-11-05 05:02:13,974 - INFO - [diffusion][Epoch 10096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:13,975 - INFO - [diffusion][Epoch 10097] Epoch 10098/12000
2024-11-05 05:02:18,051 - INFO - [diffusion][Epoch 10097] diffusion training Loss: 0.051253185607492924
2024-11-05 05:02:18,053 - INFO - [diffusion][Epoch 10097] diffusion learning rate: 0.001
2024-11-05 05:02:18,055 - INFO - [diffusion][Epoch 10097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:18,056 - INFO - [diffusion][Epoch 10098] Epoch 10099/12000
2024-11-05 05:02:22,119 - INFO - [diffusion][Epoch 10098] diffusion training Loss: 0.051088571548461914
2024-11-05 05:02:22,121 - INFO - [diffusion][Epoch 10098] diffusion learning rate: 0.001
2024-11-05 05:02:22,123 - INFO - [diffusion][Epoch 10098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:22,125 - INFO - [diffusion][Epoch 10099] Epoch 10100/12000
2024-11-05 05:02:26,297 - INFO - [diffusion][Epoch 10099] diffusion training Loss: 0.04789675027132034
2024-11-05 05:02:26,299 - INFO - [diffusion][Epoch 10099] diffusion learning rate: 0.001
2024-11-05 05:02:26,300 - INFO - [diffusion][Epoch 10099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:26,302 - INFO - [diffusion][Epoch 10100] Epoch 10101/12000
2024-11-05 05:02:30,527 - INFO - [diffusion][Epoch 10100] diffusion training Loss: 0.04684085492044687
2024-11-05 05:02:30,529 - INFO - [diffusion][Epoch 10100] diffusion learning rate: 0.001
2024-11-05 05:02:30,531 - INFO - [diffusion][Epoch 10100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:30,532 - INFO - [diffusion][Epoch 10101] Epoch 10102/12000
2024-11-05 05:02:34,652 - INFO - [diffusion][Epoch 10101] diffusion training Loss: 0.05589732155203819
2024-11-05 05:02:34,654 - INFO - [diffusion][Epoch 10101] diffusion learning rate: 0.001
2024-11-05 05:02:34,656 - INFO - [diffusion][Epoch 10101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:34,657 - INFO - [diffusion][Epoch 10102] Epoch 10103/12000
2024-11-05 05:02:38,721 - INFO - [diffusion][Epoch 10102] diffusion training Loss: 0.05290211644023657
2024-11-05 05:02:38,723 - INFO - [diffusion][Epoch 10102] diffusion learning rate: 0.001
2024-11-05 05:02:38,724 - INFO - [diffusion][Epoch 10102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:38,726 - INFO - [diffusion][Epoch 10103] Epoch 10104/12000
2024-11-05 05:02:42,927 - INFO - [diffusion][Epoch 10103] diffusion training Loss: 0.05233069323003292
2024-11-05 05:02:42,929 - INFO - [diffusion][Epoch 10103] diffusion learning rate: 0.001
2024-11-05 05:02:42,931 - INFO - [diffusion][Epoch 10103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:42,932 - INFO - [diffusion][Epoch 10104] Epoch 10105/12000
2024-11-05 05:02:47,029 - INFO - [diffusion][Epoch 10104] diffusion training Loss: 0.04703333880752325
2024-11-05 05:02:47,031 - INFO - [diffusion][Epoch 10104] diffusion learning rate: 0.001
2024-11-05 05:02:47,033 - INFO - [diffusion][Epoch 10104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:47,034 - INFO - [diffusion][Epoch 10105] Epoch 10106/12000
2024-11-05 05:02:51,136 - INFO - [diffusion][Epoch 10105] diffusion training Loss: 0.04764371830970049
2024-11-05 05:02:51,138 - INFO - [diffusion][Epoch 10105] diffusion learning rate: 0.001
2024-11-05 05:02:51,140 - INFO - [diffusion][Epoch 10105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:51,141 - INFO - [diffusion][Epoch 10106] Epoch 10107/12000
2024-11-05 05:02:55,821 - INFO - [diffusion][Epoch 10106] diffusion training Loss: 0.049429453909397125
2024-11-05 05:02:55,823 - INFO - [diffusion][Epoch 10106] diffusion learning rate: 0.001
2024-11-05 05:02:55,825 - INFO - [diffusion][Epoch 10106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:55,826 - INFO - [diffusion][Epoch 10107] Epoch 10108/12000
2024-11-05 05:02:59,942 - INFO - [diffusion][Epoch 10107] diffusion training Loss: 0.049772705882787704
2024-11-05 05:02:59,944 - INFO - [diffusion][Epoch 10107] diffusion learning rate: 0.001
2024-11-05 05:02:59,952 - INFO - [diffusion][Epoch 10107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:02:59,953 - INFO - [diffusion][Epoch 10108] Epoch 10109/12000
2024-11-05 05:03:04,055 - INFO - [diffusion][Epoch 10108] diffusion training Loss: 0.04513950366526842
2024-11-05 05:03:04,057 - INFO - [diffusion][Epoch 10108] diffusion learning rate: 0.001
2024-11-05 05:03:04,059 - INFO - [diffusion][Epoch 10108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:04,060 - INFO - [diffusion][Epoch 10109] Epoch 10110/12000
2024-11-05 05:03:08,230 - INFO - [diffusion][Epoch 10109] diffusion training Loss: 0.04750174377113581
2024-11-05 05:03:08,232 - INFO - [diffusion][Epoch 10109] diffusion learning rate: 0.001
2024-11-05 05:03:08,233 - INFO - [diffusion][Epoch 10109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:08,235 - INFO - [diffusion][Epoch 10110] Epoch 10111/12000
2024-11-05 05:03:12,348 - INFO - [diffusion][Epoch 10110] diffusion training Loss: 0.048519009724259377
2024-11-05 05:03:12,414 - INFO - [diffusion][Epoch 10110] diffusion learning rate: 0.001
2024-11-05 05:03:12,416 - INFO - [diffusion][Epoch 10110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:12,417 - INFO - [diffusion][Epoch 10111] Epoch 10112/12000
2024-11-05 05:03:16,511 - INFO - [diffusion][Epoch 10111] diffusion training Loss: 0.05196969863027334
2024-11-05 05:03:16,513 - INFO - [diffusion][Epoch 10111] diffusion learning rate: 0.001
2024-11-05 05:03:16,515 - INFO - [diffusion][Epoch 10111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:16,516 - INFO - [diffusion][Epoch 10112] Epoch 10113/12000
2024-11-05 05:03:20,790 - INFO - [diffusion][Epoch 10112] diffusion training Loss: 0.04575273673981428
2024-11-05 05:03:20,792 - INFO - [diffusion][Epoch 10112] diffusion learning rate: 0.001
2024-11-05 05:03:20,794 - INFO - [diffusion][Epoch 10112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:20,795 - INFO - [diffusion][Epoch 10113] Epoch 10114/12000
2024-11-05 05:03:24,907 - INFO - [diffusion][Epoch 10113] diffusion training Loss: 0.0483651515096426
2024-11-05 05:03:24,909 - INFO - [diffusion][Epoch 10113] diffusion learning rate: 0.001
2024-11-05 05:03:24,910 - INFO - [diffusion][Epoch 10113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:24,912 - INFO - [diffusion][Epoch 10114] Epoch 10115/12000
2024-11-05 05:03:29,030 - INFO - [diffusion][Epoch 10114] diffusion training Loss: 0.05132121779024601
2024-11-05 05:03:29,031 - INFO - [diffusion][Epoch 10114] diffusion learning rate: 0.001
2024-11-05 05:03:29,033 - INFO - [diffusion][Epoch 10114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:29,034 - INFO - [diffusion][Epoch 10115] Epoch 10116/12000
2024-11-05 05:03:32,973 - INFO - [diffusion][Epoch 10115] diffusion training Loss: 0.0480222599580884
2024-11-05 05:03:32,975 - INFO - [diffusion][Epoch 10115] diffusion learning rate: 0.001
2024-11-05 05:03:32,977 - INFO - [diffusion][Epoch 10115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:32,978 - INFO - [diffusion][Epoch 10116] Epoch 10117/12000
2024-11-05 05:03:36,828 - INFO - [diffusion][Epoch 10116] diffusion training Loss: 0.04672563076019287
2024-11-05 05:03:36,830 - INFO - [diffusion][Epoch 10116] diffusion learning rate: 0.001
2024-11-05 05:03:36,832 - INFO - [diffusion][Epoch 10116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:36,833 - INFO - [diffusion][Epoch 10117] Epoch 10118/12000
2024-11-05 05:03:40,989 - INFO - [diffusion][Epoch 10117] diffusion training Loss: 0.05146355181932449
2024-11-05 05:03:40,990 - INFO - [diffusion][Epoch 10117] diffusion learning rate: 0.001
2024-11-05 05:03:40,992 - INFO - [diffusion][Epoch 10117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:40,993 - INFO - [diffusion][Epoch 10118] Epoch 10119/12000
2024-11-05 05:03:45,120 - INFO - [diffusion][Epoch 10118] diffusion training Loss: 0.05258969683200121
2024-11-05 05:03:45,122 - INFO - [diffusion][Epoch 10118] diffusion learning rate: 0.001
2024-11-05 05:03:45,124 - INFO - [diffusion][Epoch 10118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:45,125 - INFO - [diffusion][Epoch 10119] Epoch 10120/12000
2024-11-05 05:03:49,191 - INFO - [diffusion][Epoch 10119] diffusion training Loss: 0.052426946349442005
2024-11-05 05:03:49,193 - INFO - [diffusion][Epoch 10119] diffusion learning rate: 0.001
2024-11-05 05:03:49,195 - INFO - [diffusion][Epoch 10119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:49,196 - INFO - [diffusion][Epoch 10120] Epoch 10121/12000
2024-11-05 05:03:53,430 - INFO - [diffusion][Epoch 10120] diffusion training Loss: 0.04734952561557293
2024-11-05 05:03:53,432 - INFO - [diffusion][Epoch 10120] diffusion learning rate: 0.001
2024-11-05 05:03:53,464 - INFO - [diffusion][Epoch 10120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:53,465 - INFO - [diffusion][Epoch 10121] Epoch 10122/12000
2024-11-05 05:03:57,623 - INFO - [diffusion][Epoch 10121] diffusion training Loss: 0.05122779402881861
2024-11-05 05:03:57,625 - INFO - [diffusion][Epoch 10121] diffusion learning rate: 0.001
2024-11-05 05:03:57,627 - INFO - [diffusion][Epoch 10121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:03:57,629 - INFO - [diffusion][Epoch 10122] Epoch 10123/12000
2024-11-05 05:04:01,846 - INFO - [diffusion][Epoch 10122] diffusion training Loss: 0.05057779420167208
2024-11-05 05:04:01,848 - INFO - [diffusion][Epoch 10122] diffusion learning rate: 0.001
2024-11-05 05:04:01,849 - INFO - [diffusion][Epoch 10122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:01,851 - INFO - [diffusion][Epoch 10123] Epoch 10124/12000
2024-11-05 05:04:06,011 - INFO - [diffusion][Epoch 10123] diffusion training Loss: 0.04817618615925312
2024-11-05 05:04:06,013 - INFO - [diffusion][Epoch 10123] diffusion learning rate: 0.001
2024-11-05 05:04:06,015 - INFO - [diffusion][Epoch 10123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:06,016 - INFO - [diffusion][Epoch 10124] Epoch 10125/12000
2024-11-05 05:04:10,190 - INFO - [diffusion][Epoch 10124] diffusion training Loss: 0.04729902185499668
2024-11-05 05:04:10,192 - INFO - [diffusion][Epoch 10124] diffusion learning rate: 0.001
2024-11-05 05:04:10,195 - INFO - [diffusion][Epoch 10124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:10,196 - INFO - [diffusion][Epoch 10125] Epoch 10126/12000
2024-11-05 05:04:14,355 - INFO - [diffusion][Epoch 10125] diffusion training Loss: 0.04872520826756954
2024-11-05 05:04:14,357 - INFO - [diffusion][Epoch 10125] diffusion learning rate: 0.001
2024-11-05 05:04:14,360 - INFO - [diffusion][Epoch 10125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:14,362 - INFO - [diffusion][Epoch 10126] Epoch 10127/12000
2024-11-05 05:04:18,445 - INFO - [diffusion][Epoch 10126] diffusion training Loss: 0.04953446052968502
2024-11-05 05:04:18,453 - INFO - [diffusion][Epoch 10126] diffusion learning rate: 0.001
2024-11-05 05:04:18,455 - INFO - [diffusion][Epoch 10126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:18,456 - INFO - [diffusion][Epoch 10127] Epoch 10128/12000
2024-11-05 05:04:22,675 - INFO - [diffusion][Epoch 10127] diffusion training Loss: 0.05661103129386902
2024-11-05 05:04:22,677 - INFO - [diffusion][Epoch 10127] diffusion learning rate: 0.001
2024-11-05 05:04:22,679 - INFO - [diffusion][Epoch 10127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:22,680 - INFO - [diffusion][Epoch 10128] Epoch 10129/12000
2024-11-05 05:04:26,846 - INFO - [diffusion][Epoch 10128] diffusion training Loss: 0.048366282135248184
2024-11-05 05:04:26,848 - INFO - [diffusion][Epoch 10128] diffusion learning rate: 0.001
2024-11-05 05:04:26,850 - INFO - [diffusion][Epoch 10128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:26,851 - INFO - [diffusion][Epoch 10129] Epoch 10130/12000
2024-11-05 05:04:30,955 - INFO - [diffusion][Epoch 10129] diffusion training Loss: 0.05004718340933323
2024-11-05 05:04:30,957 - INFO - [diffusion][Epoch 10129] diffusion learning rate: 0.001
2024-11-05 05:04:30,959 - INFO - [diffusion][Epoch 10129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:30,960 - INFO - [diffusion][Epoch 10130] Epoch 10131/12000
2024-11-05 05:04:35,045 - INFO - [diffusion][Epoch 10130] diffusion training Loss: 0.05096343159675598
2024-11-05 05:04:35,047 - INFO - [diffusion][Epoch 10130] diffusion learning rate: 0.001
2024-11-05 05:04:35,049 - INFO - [diffusion][Epoch 10130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:35,052 - INFO - [diffusion][Epoch 10131] Epoch 10132/12000
2024-11-05 05:04:39,071 - INFO - [diffusion][Epoch 10131] diffusion training Loss: 0.04447327181696892
2024-11-05 05:04:39,073 - INFO - [diffusion][Epoch 10131] diffusion learning rate: 0.001
2024-11-05 05:04:39,074 - INFO - [diffusion][Epoch 10131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:39,076 - INFO - [diffusion][Epoch 10132] Epoch 10133/12000
2024-11-05 05:04:43,238 - INFO - [diffusion][Epoch 10132] diffusion training Loss: 0.05239696986973286
2024-11-05 05:04:43,240 - INFO - [diffusion][Epoch 10132] diffusion learning rate: 0.001
2024-11-05 05:04:43,242 - INFO - [diffusion][Epoch 10132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:43,243 - INFO - [diffusion][Epoch 10133] Epoch 10134/12000
2024-11-05 05:04:47,361 - INFO - [diffusion][Epoch 10133] diffusion training Loss: 0.0461922362446785
2024-11-05 05:04:47,363 - INFO - [diffusion][Epoch 10133] diffusion learning rate: 0.001
2024-11-05 05:04:47,365 - INFO - [diffusion][Epoch 10133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:47,366 - INFO - [diffusion][Epoch 10134] Epoch 10135/12000
2024-11-05 05:04:51,493 - INFO - [diffusion][Epoch 10134] diffusion training Loss: 0.05122931953519583
2024-11-05 05:04:51,495 - INFO - [diffusion][Epoch 10134] diffusion learning rate: 0.001
2024-11-05 05:04:51,496 - INFO - [diffusion][Epoch 10134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:51,498 - INFO - [diffusion][Epoch 10135] Epoch 10136/12000
2024-11-05 05:04:55,689 - INFO - [diffusion][Epoch 10135] diffusion training Loss: 0.050175610929727554
2024-11-05 05:04:55,691 - INFO - [diffusion][Epoch 10135] diffusion learning rate: 0.001
2024-11-05 05:04:55,693 - INFO - [diffusion][Epoch 10135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:55,694 - INFO - [diffusion][Epoch 10136] Epoch 10137/12000
2024-11-05 05:04:59,806 - INFO - [diffusion][Epoch 10136] diffusion training Loss: 0.04690372198820114
2024-11-05 05:04:59,808 - INFO - [diffusion][Epoch 10136] diffusion learning rate: 0.001
2024-11-05 05:04:59,810 - INFO - [diffusion][Epoch 10136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:04:59,811 - INFO - [diffusion][Epoch 10137] Epoch 10138/12000
2024-11-05 05:05:03,923 - INFO - [diffusion][Epoch 10137] diffusion training Loss: 0.049107663333415985
2024-11-05 05:05:03,925 - INFO - [diffusion][Epoch 10137] diffusion learning rate: 0.001
2024-11-05 05:05:03,926 - INFO - [diffusion][Epoch 10137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:03,927 - INFO - [diffusion][Epoch 10138] Epoch 10139/12000
2024-11-05 05:05:08,017 - INFO - [diffusion][Epoch 10138] diffusion training Loss: 0.04889002628624439
2024-11-05 05:05:08,020 - INFO - [diffusion][Epoch 10138] diffusion learning rate: 0.001
2024-11-05 05:05:08,022 - INFO - [diffusion][Epoch 10138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:08,024 - INFO - [diffusion][Epoch 10139] Epoch 10140/12000
2024-11-05 05:05:12,184 - INFO - [diffusion][Epoch 10139] diffusion training Loss: 0.05361352674663067
2024-11-05 05:05:12,303 - INFO - [diffusion][Epoch 10139] diffusion learning rate: 0.001
2024-11-05 05:05:12,304 - INFO - [diffusion][Epoch 10139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:12,306 - INFO - [diffusion][Epoch 10140] Epoch 10141/12000
2024-11-05 05:05:16,407 - INFO - [diffusion][Epoch 10140] diffusion training Loss: 0.04504450224339962
2024-11-05 05:05:16,409 - INFO - [diffusion][Epoch 10140] diffusion learning rate: 0.001
2024-11-05 05:05:16,411 - INFO - [diffusion][Epoch 10140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:16,412 - INFO - [diffusion][Epoch 10141] Epoch 10142/12000
2024-11-05 05:05:20,435 - INFO - [diffusion][Epoch 10141] diffusion training Loss: 0.043685777112841606
2024-11-05 05:05:20,437 - INFO - [diffusion][Epoch 10141] diffusion learning rate: 0.001
2024-11-05 05:05:20,438 - INFO - [diffusion][Epoch 10141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:20,440 - INFO - [diffusion][Epoch 10142] Epoch 10143/12000
2024-11-05 05:05:24,317 - INFO - [diffusion][Epoch 10142] diffusion training Loss: 0.05087508074939251
2024-11-05 05:05:24,319 - INFO - [diffusion][Epoch 10142] diffusion learning rate: 0.001
2024-11-05 05:05:24,321 - INFO - [diffusion][Epoch 10142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:24,323 - INFO - [diffusion][Epoch 10143] Epoch 10144/12000
2024-11-05 05:05:28,411 - INFO - [diffusion][Epoch 10143] diffusion training Loss: 0.05729696340858936
2024-11-05 05:05:28,412 - INFO - [diffusion][Epoch 10143] diffusion learning rate: 0.001
2024-11-05 05:05:28,414 - INFO - [diffusion][Epoch 10143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:28,415 - INFO - [diffusion][Epoch 10144] Epoch 10145/12000
2024-11-05 05:05:32,385 - INFO - [diffusion][Epoch 10144] diffusion training Loss: 0.04936063941568136
2024-11-05 05:05:32,387 - INFO - [diffusion][Epoch 10144] diffusion learning rate: 0.001
2024-11-05 05:05:32,389 - INFO - [diffusion][Epoch 10144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:32,390 - INFO - [diffusion][Epoch 10145] Epoch 10146/12000
2024-11-05 05:05:36,984 - INFO - [diffusion][Epoch 10145] diffusion training Loss: 0.04686596430838108
2024-11-05 05:05:36,987 - INFO - [diffusion][Epoch 10145] diffusion learning rate: 0.001
2024-11-05 05:05:36,989 - INFO - [diffusion][Epoch 10145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:36,990 - INFO - [diffusion][Epoch 10146] Epoch 10147/12000
2024-11-05 05:05:41,318 - INFO - [diffusion][Epoch 10146] diffusion training Loss: 0.041814892552793026
2024-11-05 05:05:41,321 - INFO - [diffusion][Epoch 10146] diffusion learning rate: 0.001
2024-11-05 05:05:41,323 - INFO - [diffusion][Epoch 10146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:41,325 - INFO - [diffusion][Epoch 10147] Epoch 10148/12000
2024-11-05 05:05:45,432 - INFO - [diffusion][Epoch 10147] diffusion training Loss: 0.04916870966553688
2024-11-05 05:05:45,434 - INFO - [diffusion][Epoch 10147] diffusion learning rate: 0.001
2024-11-05 05:05:45,436 - INFO - [diffusion][Epoch 10147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:45,437 - INFO - [diffusion][Epoch 10148] Epoch 10149/12000
2024-11-05 05:05:49,429 - INFO - [diffusion][Epoch 10148] diffusion training Loss: 0.046738903038203716
2024-11-05 05:05:49,431 - INFO - [diffusion][Epoch 10148] diffusion learning rate: 0.001
2024-11-05 05:05:49,433 - INFO - [diffusion][Epoch 10148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:49,434 - INFO - [diffusion][Epoch 10149] Epoch 10150/12000
2024-11-05 05:05:53,556 - INFO - [diffusion][Epoch 10149] diffusion training Loss: 0.049733023159205914
2024-11-05 05:05:53,559 - INFO - [diffusion][Epoch 10149] diffusion learning rate: 0.001
2024-11-05 05:05:53,561 - INFO - [diffusion][Epoch 10149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:53,562 - INFO - [diffusion][Epoch 10150] Epoch 10151/12000
2024-11-05 05:05:57,578 - INFO - [diffusion][Epoch 10150] diffusion training Loss: 0.04856492765247822
2024-11-05 05:05:57,580 - INFO - [diffusion][Epoch 10150] diffusion learning rate: 0.001
2024-11-05 05:05:57,582 - INFO - [diffusion][Epoch 10150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:05:57,583 - INFO - [diffusion][Epoch 10151] Epoch 10152/12000
2024-11-05 05:06:01,702 - INFO - [diffusion][Epoch 10151] diffusion training Loss: 0.05127058271318674
2024-11-05 05:06:01,704 - INFO - [diffusion][Epoch 10151] diffusion learning rate: 0.001
2024-11-05 05:06:01,706 - INFO - [diffusion][Epoch 10151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:01,707 - INFO - [diffusion][Epoch 10152] Epoch 10153/12000
2024-11-05 05:06:05,807 - INFO - [diffusion][Epoch 10152] diffusion training Loss: 0.04650995880365372
2024-11-05 05:06:05,809 - INFO - [diffusion][Epoch 10152] diffusion learning rate: 0.001
2024-11-05 05:06:05,811 - INFO - [diffusion][Epoch 10152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:05,812 - INFO - [diffusion][Epoch 10153] Epoch 10154/12000
2024-11-05 05:06:09,902 - INFO - [diffusion][Epoch 10153] diffusion training Loss: 0.047258537262678146
2024-11-05 05:06:09,905 - INFO - [diffusion][Epoch 10153] diffusion learning rate: 0.001
2024-11-05 05:06:09,907 - INFO - [diffusion][Epoch 10153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:09,908 - INFO - [diffusion][Epoch 10154] Epoch 10155/12000
2024-11-05 05:06:14,066 - INFO - [diffusion][Epoch 10154] diffusion training Loss: 0.04899872653186321
2024-11-05 05:06:14,069 - INFO - [diffusion][Epoch 10154] diffusion learning rate: 0.001
2024-11-05 05:06:14,088 - INFO - [diffusion][Epoch 10154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:14,089 - INFO - [diffusion][Epoch 10155] Epoch 10156/12000
2024-11-05 05:06:18,210 - INFO - [diffusion][Epoch 10155] diffusion training Loss: 0.04863847326487303
2024-11-05 05:06:18,212 - INFO - [diffusion][Epoch 10155] diffusion learning rate: 0.001
2024-11-05 05:06:18,214 - INFO - [diffusion][Epoch 10155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:18,215 - INFO - [diffusion][Epoch 10156] Epoch 10157/12000
2024-11-05 05:06:22,321 - INFO - [diffusion][Epoch 10156] diffusion training Loss: 0.044708991423249245
2024-11-05 05:06:22,323 - INFO - [diffusion][Epoch 10156] diffusion learning rate: 0.001
2024-11-05 05:06:22,324 - INFO - [diffusion][Epoch 10156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:22,326 - INFO - [diffusion][Epoch 10157] Epoch 10158/12000
2024-11-05 05:06:26,427 - INFO - [diffusion][Epoch 10157] diffusion training Loss: 0.047044288367033005
2024-11-05 05:06:26,429 - INFO - [diffusion][Epoch 10157] diffusion learning rate: 0.001
2024-11-05 05:06:26,431 - INFO - [diffusion][Epoch 10157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:26,432 - INFO - [diffusion][Epoch 10158] Epoch 10159/12000
2024-11-05 05:06:30,529 - INFO - [diffusion][Epoch 10158] diffusion training Loss: 0.04532498400658369
2024-11-05 05:06:30,531 - INFO - [diffusion][Epoch 10158] diffusion learning rate: 0.001
2024-11-05 05:06:30,533 - INFO - [diffusion][Epoch 10158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:30,534 - INFO - [diffusion][Epoch 10159] Epoch 10160/12000
2024-11-05 05:06:34,614 - INFO - [diffusion][Epoch 10159] diffusion training Loss: 0.04789603129029274
2024-11-05 05:06:34,617 - INFO - [diffusion][Epoch 10159] diffusion learning rate: 0.001
2024-11-05 05:06:34,618 - INFO - [diffusion][Epoch 10159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:34,620 - INFO - [diffusion][Epoch 10160] Epoch 10161/12000
2024-11-05 05:06:38,743 - INFO - [diffusion][Epoch 10160] diffusion training Loss: 0.053875209763646126
2024-11-05 05:06:38,745 - INFO - [diffusion][Epoch 10160] diffusion learning rate: 0.001
2024-11-05 05:06:38,747 - INFO - [diffusion][Epoch 10160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:38,748 - INFO - [diffusion][Epoch 10161] Epoch 10162/12000
2024-11-05 05:06:42,943 - INFO - [diffusion][Epoch 10161] diffusion training Loss: 0.04935918468981981
2024-11-05 05:06:42,944 - INFO - [diffusion][Epoch 10161] diffusion learning rate: 0.001
2024-11-05 05:06:42,946 - INFO - [diffusion][Epoch 10161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:42,947 - INFO - [diffusion][Epoch 10162] Epoch 10163/12000
2024-11-05 05:06:47,060 - INFO - [diffusion][Epoch 10162] diffusion training Loss: 0.046618870459496975
2024-11-05 05:06:47,062 - INFO - [diffusion][Epoch 10162] diffusion learning rate: 0.001
2024-11-05 05:06:47,063 - INFO - [diffusion][Epoch 10162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:47,065 - INFO - [diffusion][Epoch 10163] Epoch 10164/12000
2024-11-05 05:06:51,153 - INFO - [diffusion][Epoch 10163] diffusion training Loss: 0.04479427356272936
2024-11-05 05:06:51,155 - INFO - [diffusion][Epoch 10163] diffusion learning rate: 0.001
2024-11-05 05:06:51,157 - INFO - [diffusion][Epoch 10163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:51,158 - INFO - [diffusion][Epoch 10164] Epoch 10165/12000
2024-11-05 05:06:55,287 - INFO - [diffusion][Epoch 10164] diffusion training Loss: 0.049701339565217495
2024-11-05 05:06:55,289 - INFO - [diffusion][Epoch 10164] diffusion learning rate: 0.001
2024-11-05 05:06:55,290 - INFO - [diffusion][Epoch 10164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:55,292 - INFO - [diffusion][Epoch 10165] Epoch 10166/12000
2024-11-05 05:06:59,306 - INFO - [diffusion][Epoch 10165] diffusion training Loss: 0.053371280431747437
2024-11-05 05:06:59,308 - INFO - [diffusion][Epoch 10165] diffusion learning rate: 0.001
2024-11-05 05:06:59,309 - INFO - [diffusion][Epoch 10165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:06:59,311 - INFO - [diffusion][Epoch 10166] Epoch 10167/12000
2024-11-05 05:07:03,353 - INFO - [diffusion][Epoch 10166] diffusion training Loss: 0.051011654548346996
2024-11-05 05:07:03,355 - INFO - [diffusion][Epoch 10166] diffusion learning rate: 0.001
2024-11-05 05:07:03,356 - INFO - [diffusion][Epoch 10166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:03,358 - INFO - [diffusion][Epoch 10167] Epoch 10168/12000
2024-11-05 05:07:07,925 - INFO - [diffusion][Epoch 10167] diffusion training Loss: 0.047373308800160885
2024-11-05 05:07:07,928 - INFO - [diffusion][Epoch 10167] diffusion learning rate: 0.001
2024-11-05 05:07:07,930 - INFO - [diffusion][Epoch 10167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:07,932 - INFO - [diffusion][Epoch 10168] Epoch 10169/12000
2024-11-05 05:07:11,891 - INFO - [diffusion][Epoch 10168] diffusion training Loss: 0.04701262153685093
2024-11-05 05:07:11,894 - INFO - [diffusion][Epoch 10168] diffusion learning rate: 0.001
2024-11-05 05:07:11,895 - INFO - [diffusion][Epoch 10168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:11,897 - INFO - [diffusion][Epoch 10169] Epoch 10170/12000
2024-11-05 05:07:15,960 - INFO - [diffusion][Epoch 10169] diffusion training Loss: 0.0497240973636508
2024-11-05 05:07:15,962 - INFO - [diffusion][Epoch 10169] diffusion learning rate: 0.001
2024-11-05 05:07:15,963 - INFO - [diffusion][Epoch 10169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:15,965 - INFO - [diffusion][Epoch 10170] Epoch 10171/12000
2024-11-05 05:07:20,103 - INFO - [diffusion][Epoch 10170] diffusion training Loss: 0.049161188304424286
2024-11-05 05:07:20,105 - INFO - [diffusion][Epoch 10170] diffusion learning rate: 0.001
2024-11-05 05:07:20,107 - INFO - [diffusion][Epoch 10170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:20,108 - INFO - [diffusion][Epoch 10171] Epoch 10172/12000
2024-11-05 05:07:24,247 - INFO - [diffusion][Epoch 10171] diffusion training Loss: 0.05252372194081545
2024-11-05 05:07:24,248 - INFO - [diffusion][Epoch 10171] diffusion learning rate: 0.001
2024-11-05 05:07:24,250 - INFO - [diffusion][Epoch 10171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:24,252 - INFO - [diffusion][Epoch 10172] Epoch 10173/12000
2024-11-05 05:07:28,368 - INFO - [diffusion][Epoch 10172] diffusion training Loss: 0.05142680462449789
2024-11-05 05:07:28,370 - INFO - [diffusion][Epoch 10172] diffusion learning rate: 0.001
2024-11-05 05:07:28,398 - INFO - [diffusion][Epoch 10172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:28,399 - INFO - [diffusion][Epoch 10173] Epoch 10174/12000
2024-11-05 05:07:32,547 - INFO - [diffusion][Epoch 10173] diffusion training Loss: 0.04761670995503664
2024-11-05 05:07:32,549 - INFO - [diffusion][Epoch 10173] diffusion learning rate: 0.001
2024-11-05 05:07:32,551 - INFO - [diffusion][Epoch 10173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:32,552 - INFO - [diffusion][Epoch 10174] Epoch 10175/12000
2024-11-05 05:07:36,667 - INFO - [diffusion][Epoch 10174] diffusion training Loss: 0.04530798923224211
2024-11-05 05:07:36,669 - INFO - [diffusion][Epoch 10174] diffusion learning rate: 0.001
2024-11-05 05:07:36,671 - INFO - [diffusion][Epoch 10174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:36,673 - INFO - [diffusion][Epoch 10175] Epoch 10176/12000
2024-11-05 05:07:40,653 - INFO - [diffusion][Epoch 10175] diffusion training Loss: 0.045645191334187984
2024-11-05 05:07:40,655 - INFO - [diffusion][Epoch 10175] diffusion learning rate: 0.001
2024-11-05 05:07:40,684 - INFO - [diffusion][Epoch 10175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:40,685 - INFO - [diffusion][Epoch 10176] Epoch 10177/12000
2024-11-05 05:07:44,716 - INFO - [diffusion][Epoch 10176] diffusion training Loss: 0.0563404681161046
2024-11-05 05:07:44,718 - INFO - [diffusion][Epoch 10176] diffusion learning rate: 0.001
2024-11-05 05:07:44,719 - INFO - [diffusion][Epoch 10176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:44,721 - INFO - [diffusion][Epoch 10177] Epoch 10178/12000
2024-11-05 05:07:48,803 - INFO - [diffusion][Epoch 10177] diffusion training Loss: 0.04820001404732466
2024-11-05 05:07:48,805 - INFO - [diffusion][Epoch 10177] diffusion learning rate: 0.001
2024-11-05 05:07:48,807 - INFO - [diffusion][Epoch 10177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:48,808 - INFO - [diffusion][Epoch 10178] Epoch 10179/12000
2024-11-05 05:07:52,966 - INFO - [diffusion][Epoch 10178] diffusion training Loss: 0.04891676642000675
2024-11-05 05:07:52,969 - INFO - [diffusion][Epoch 10178] diffusion learning rate: 0.001
2024-11-05 05:07:53,030 - INFO - [diffusion][Epoch 10178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:53,031 - INFO - [diffusion][Epoch 10179] Epoch 10180/12000
2024-11-05 05:07:57,122 - INFO - [diffusion][Epoch 10179] diffusion training Loss: 0.04834007937461138
2024-11-05 05:07:57,124 - INFO - [diffusion][Epoch 10179] diffusion learning rate: 0.001
2024-11-05 05:07:57,126 - INFO - [diffusion][Epoch 10179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:07:57,127 - INFO - [diffusion][Epoch 10180] Epoch 10181/12000
2024-11-05 05:08:01,273 - INFO - [diffusion][Epoch 10180] diffusion training Loss: 0.05209263227880001
2024-11-05 05:08:01,275 - INFO - [diffusion][Epoch 10180] diffusion learning rate: 0.001
2024-11-05 05:08:01,277 - INFO - [diffusion][Epoch 10180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:01,278 - INFO - [diffusion][Epoch 10181] Epoch 10182/12000
2024-11-05 05:08:05,400 - INFO - [diffusion][Epoch 10181] diffusion training Loss: 0.05395641457289457
2024-11-05 05:08:05,402 - INFO - [diffusion][Epoch 10181] diffusion learning rate: 0.001
2024-11-05 05:08:05,404 - INFO - [diffusion][Epoch 10181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:05,405 - INFO - [diffusion][Epoch 10182] Epoch 10183/12000
2024-11-05 05:08:09,513 - INFO - [diffusion][Epoch 10182] diffusion training Loss: 0.04870734456926584
2024-11-05 05:08:09,515 - INFO - [diffusion][Epoch 10182] diffusion learning rate: 0.001
2024-11-05 05:08:09,541 - INFO - [diffusion][Epoch 10182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:09,543 - INFO - [diffusion][Epoch 10183] Epoch 10184/12000
2024-11-05 05:08:13,612 - INFO - [diffusion][Epoch 10183] diffusion training Loss: 0.050166478380560875
2024-11-05 05:08:13,615 - INFO - [diffusion][Epoch 10183] diffusion learning rate: 0.001
2024-11-05 05:08:13,617 - INFO - [diffusion][Epoch 10183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:13,618 - INFO - [diffusion][Epoch 10184] Epoch 10185/12000
2024-11-05 05:08:17,721 - INFO - [diffusion][Epoch 10184] diffusion training Loss: 0.04909227788448334
2024-11-05 05:08:17,723 - INFO - [diffusion][Epoch 10184] diffusion learning rate: 0.001
2024-11-05 05:08:17,724 - INFO - [diffusion][Epoch 10184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:17,726 - INFO - [diffusion][Epoch 10185] Epoch 10186/12000
2024-11-05 05:08:21,668 - INFO - [diffusion][Epoch 10185] diffusion training Loss: 0.047988759353756905
2024-11-05 05:08:21,670 - INFO - [diffusion][Epoch 10185] diffusion learning rate: 0.001
2024-11-05 05:08:21,671 - INFO - [diffusion][Epoch 10185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:21,673 - INFO - [diffusion][Epoch 10186] Epoch 10187/12000
2024-11-05 05:08:25,600 - INFO - [diffusion][Epoch 10186] diffusion training Loss: 0.046311164274811745
2024-11-05 05:08:25,602 - INFO - [diffusion][Epoch 10186] diffusion learning rate: 0.001
2024-11-05 05:08:25,634 - INFO - [diffusion][Epoch 10186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:25,635 - INFO - [diffusion][Epoch 10187] Epoch 10188/12000
2024-11-05 05:08:29,996 - INFO - [diffusion][Epoch 10187] diffusion training Loss: 0.04669913835823536
2024-11-05 05:08:29,998 - INFO - [diffusion][Epoch 10187] diffusion learning rate: 0.001
2024-11-05 05:08:29,999 - INFO - [diffusion][Epoch 10187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:30,001 - INFO - [diffusion][Epoch 10188] Epoch 10189/12000
2024-11-05 05:08:34,189 - INFO - [diffusion][Epoch 10188] diffusion training Loss: 0.04501200560480356
2024-11-05 05:08:34,190 - INFO - [diffusion][Epoch 10188] diffusion learning rate: 0.001
2024-11-05 05:08:34,192 - INFO - [diffusion][Epoch 10188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:34,194 - INFO - [diffusion][Epoch 10189] Epoch 10190/12000
2024-11-05 05:08:38,303 - INFO - [diffusion][Epoch 10189] diffusion training Loss: 0.04686236102133989
2024-11-05 05:08:38,305 - INFO - [diffusion][Epoch 10189] diffusion learning rate: 0.001
2024-11-05 05:08:38,307 - INFO - [diffusion][Epoch 10189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:38,308 - INFO - [diffusion][Epoch 10190] Epoch 10191/12000
2024-11-05 05:08:42,378 - INFO - [diffusion][Epoch 10190] diffusion training Loss: 0.0485335448756814
2024-11-05 05:08:42,380 - INFO - [diffusion][Epoch 10190] diffusion learning rate: 0.001
2024-11-05 05:08:42,381 - INFO - [diffusion][Epoch 10190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:42,383 - INFO - [diffusion][Epoch 10191] Epoch 10192/12000
2024-11-05 05:08:46,355 - INFO - [diffusion][Epoch 10191] diffusion training Loss: 0.04526505991816521
2024-11-05 05:08:46,357 - INFO - [diffusion][Epoch 10191] diffusion learning rate: 0.001
2024-11-05 05:08:46,359 - INFO - [diffusion][Epoch 10191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:46,360 - INFO - [diffusion][Epoch 10192] Epoch 10193/12000
2024-11-05 05:08:50,417 - INFO - [diffusion][Epoch 10192] diffusion training Loss: 0.04711948987096548
2024-11-05 05:08:50,419 - INFO - [diffusion][Epoch 10192] diffusion learning rate: 0.001
2024-11-05 05:08:50,421 - INFO - [diffusion][Epoch 10192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:50,422 - INFO - [diffusion][Epoch 10193] Epoch 10194/12000
2024-11-05 05:08:54,483 - INFO - [diffusion][Epoch 10193] diffusion training Loss: 0.04788169916719198
2024-11-05 05:08:54,485 - INFO - [diffusion][Epoch 10193] diffusion learning rate: 0.001
2024-11-05 05:08:54,486 - INFO - [diffusion][Epoch 10193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:54,488 - INFO - [diffusion][Epoch 10194] Epoch 10195/12000
2024-11-05 05:08:58,574 - INFO - [diffusion][Epoch 10194] diffusion training Loss: 0.04568769969046116
2024-11-05 05:08:58,576 - INFO - [diffusion][Epoch 10194] diffusion learning rate: 0.001
2024-11-05 05:08:58,578 - INFO - [diffusion][Epoch 10194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:08:58,579 - INFO - [diffusion][Epoch 10195] Epoch 10196/12000
2024-11-05 05:09:02,679 - INFO - [diffusion][Epoch 10195] diffusion training Loss: 0.04367897566407919
2024-11-05 05:09:02,682 - INFO - [diffusion][Epoch 10195] diffusion learning rate: 0.001
2024-11-05 05:09:02,684 - INFO - [diffusion][Epoch 10195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:02,685 - INFO - [diffusion][Epoch 10196] Epoch 10197/12000
2024-11-05 05:09:06,797 - INFO - [diffusion][Epoch 10196] diffusion training Loss: 0.05266478192061186
2024-11-05 05:09:06,799 - INFO - [diffusion][Epoch 10196] diffusion learning rate: 0.001
2024-11-05 05:09:06,801 - INFO - [diffusion][Epoch 10196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:06,802 - INFO - [diffusion][Epoch 10197] Epoch 10198/12000
2024-11-05 05:09:10,899 - INFO - [diffusion][Epoch 10197] diffusion training Loss: 0.049077519215643406
2024-11-05 05:09:10,901 - INFO - [diffusion][Epoch 10197] diffusion learning rate: 0.001
2024-11-05 05:09:10,903 - INFO - [diffusion][Epoch 10197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:10,904 - INFO - [diffusion][Epoch 10198] Epoch 10199/12000
2024-11-05 05:09:15,007 - INFO - [diffusion][Epoch 10198] diffusion training Loss: 0.04740206338465214
2024-11-05 05:09:15,011 - INFO - [diffusion][Epoch 10198] diffusion learning rate: 0.001
2024-11-05 05:09:15,013 - INFO - [diffusion][Epoch 10198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:15,014 - INFO - [diffusion][Epoch 10199] Epoch 10200/12000
2024-11-05 05:09:19,158 - INFO - [diffusion][Epoch 10199] diffusion training Loss: 0.04611192084848881
2024-11-05 05:09:19,160 - INFO - [diffusion][Epoch 10199] diffusion learning rate: 0.001
2024-11-05 05:09:19,162 - INFO - [diffusion][Epoch 10199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:19,163 - INFO - [diffusion][Epoch 10200] Epoch 10201/12000
2024-11-05 05:09:23,264 - INFO - [diffusion][Epoch 10200] diffusion training Loss: 0.046872769482433796
2024-11-05 05:09:23,266 - INFO - [diffusion][Epoch 10200] diffusion learning rate: 0.001
2024-11-05 05:09:23,268 - INFO - [diffusion][Epoch 10200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:23,269 - INFO - [diffusion][Epoch 10201] Epoch 10202/12000
2024-11-05 05:09:27,347 - INFO - [diffusion][Epoch 10201] diffusion training Loss: 0.04774333070963621
2024-11-05 05:09:27,349 - INFO - [diffusion][Epoch 10201] diffusion learning rate: 0.001
2024-11-05 05:09:27,398 - INFO - [diffusion][Epoch 10201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:27,399 - INFO - [diffusion][Epoch 10202] Epoch 10203/12000
2024-11-05 05:09:31,551 - INFO - [diffusion][Epoch 10202] diffusion training Loss: 0.055900245904922485
2024-11-05 05:09:31,554 - INFO - [diffusion][Epoch 10202] diffusion learning rate: 0.001
2024-11-05 05:09:31,555 - INFO - [diffusion][Epoch 10202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:31,557 - INFO - [diffusion][Epoch 10203] Epoch 10204/12000
2024-11-05 05:09:35,507 - INFO - [diffusion][Epoch 10203] diffusion training Loss: 0.04818349704146385
2024-11-05 05:09:35,509 - INFO - [diffusion][Epoch 10203] diffusion learning rate: 0.001
2024-11-05 05:09:35,511 - INFO - [diffusion][Epoch 10203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:35,512 - INFO - [diffusion][Epoch 10204] Epoch 10205/12000
2024-11-05 05:09:39,632 - INFO - [diffusion][Epoch 10204] diffusion training Loss: 0.04477943293750286
2024-11-05 05:09:39,634 - INFO - [diffusion][Epoch 10204] diffusion learning rate: 0.001
2024-11-05 05:09:39,636 - INFO - [diffusion][Epoch 10204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:39,637 - INFO - [diffusion][Epoch 10205] Epoch 10206/12000
2024-11-05 05:09:43,853 - INFO - [diffusion][Epoch 10205] diffusion training Loss: 0.051752385683357716
2024-11-05 05:09:43,855 - INFO - [diffusion][Epoch 10205] diffusion learning rate: 0.001
2024-11-05 05:09:43,857 - INFO - [diffusion][Epoch 10205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:43,858 - INFO - [diffusion][Epoch 10206] Epoch 10207/12000
2024-11-05 05:09:48,005 - INFO - [diffusion][Epoch 10206] diffusion training Loss: 0.05329131614416838
2024-11-05 05:09:48,007 - INFO - [diffusion][Epoch 10206] diffusion learning rate: 0.001
2024-11-05 05:09:48,009 - INFO - [diffusion][Epoch 10206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:48,010 - INFO - [diffusion][Epoch 10207] Epoch 10208/12000
2024-11-05 05:09:52,291 - INFO - [diffusion][Epoch 10207] diffusion training Loss: 0.0449701976031065
2024-11-05 05:09:52,293 - INFO - [diffusion][Epoch 10207] diffusion learning rate: 0.001
2024-11-05 05:09:52,294 - INFO - [diffusion][Epoch 10207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:52,296 - INFO - [diffusion][Epoch 10208] Epoch 10209/12000
2024-11-05 05:09:56,395 - INFO - [diffusion][Epoch 10208] diffusion training Loss: 0.050034974701702595
2024-11-05 05:09:56,397 - INFO - [diffusion][Epoch 10208] diffusion learning rate: 0.001
2024-11-05 05:09:56,398 - INFO - [diffusion][Epoch 10208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:09:56,400 - INFO - [diffusion][Epoch 10209] Epoch 10210/12000
2024-11-05 05:10:00,483 - INFO - [diffusion][Epoch 10209] diffusion training Loss: 0.053467980585992336
2024-11-05 05:10:00,485 - INFO - [diffusion][Epoch 10209] diffusion learning rate: 0.001
2024-11-05 05:10:00,487 - INFO - [diffusion][Epoch 10209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:00,488 - INFO - [diffusion][Epoch 10210] Epoch 10211/12000
2024-11-05 05:10:04,561 - INFO - [diffusion][Epoch 10210] diffusion training Loss: 0.04629097692668438
2024-11-05 05:10:04,563 - INFO - [diffusion][Epoch 10210] diffusion learning rate: 0.001
2024-11-05 05:10:04,565 - INFO - [diffusion][Epoch 10210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:04,567 - INFO - [diffusion][Epoch 10211] Epoch 10212/12000
2024-11-05 05:10:08,672 - INFO - [diffusion][Epoch 10211] diffusion training Loss: 0.04625265672802925
2024-11-05 05:10:08,674 - INFO - [diffusion][Epoch 10211] diffusion learning rate: 0.001
2024-11-05 05:10:08,676 - INFO - [diffusion][Epoch 10211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:08,677 - INFO - [diffusion][Epoch 10212] Epoch 10213/12000
2024-11-05 05:10:12,744 - INFO - [diffusion][Epoch 10212] diffusion training Loss: 0.054910799488425255
2024-11-05 05:10:12,746 - INFO - [diffusion][Epoch 10212] diffusion learning rate: 0.001
2024-11-05 05:10:12,747 - INFO - [diffusion][Epoch 10212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:12,748 - INFO - [diffusion][Epoch 10213] Epoch 10214/12000
2024-11-05 05:10:16,832 - INFO - [diffusion][Epoch 10213] diffusion training Loss: 0.04876147210597992
2024-11-05 05:10:16,836 - INFO - [diffusion][Epoch 10213] diffusion learning rate: 0.001
2024-11-05 05:10:16,863 - INFO - [diffusion][Epoch 10213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:16,864 - INFO - [diffusion][Epoch 10214] Epoch 10215/12000
2024-11-05 05:10:20,970 - INFO - [diffusion][Epoch 10214] diffusion training Loss: 0.04900837503373623
2024-11-05 05:10:20,972 - INFO - [diffusion][Epoch 10214] diffusion learning rate: 0.001
2024-11-05 05:10:20,974 - INFO - [diffusion][Epoch 10214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:20,975 - INFO - [diffusion][Epoch 10215] Epoch 10216/12000
2024-11-05 05:10:25,062 - INFO - [diffusion][Epoch 10215] diffusion training Loss: 0.04430865030735731
2024-11-05 05:10:25,064 - INFO - [diffusion][Epoch 10215] diffusion learning rate: 0.001
2024-11-05 05:10:25,066 - INFO - [diffusion][Epoch 10215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:25,067 - INFO - [diffusion][Epoch 10216] Epoch 10217/12000
2024-11-05 05:10:29,137 - INFO - [diffusion][Epoch 10216] diffusion training Loss: 0.05183748994022608
2024-11-05 05:10:29,139 - INFO - [diffusion][Epoch 10216] diffusion learning rate: 0.001
2024-11-05 05:10:29,140 - INFO - [diffusion][Epoch 10216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:29,142 - INFO - [diffusion][Epoch 10217] Epoch 10218/12000
2024-11-05 05:10:33,208 - INFO - [diffusion][Epoch 10217] diffusion training Loss: 0.048283448442816734
2024-11-05 05:10:33,210 - INFO - [diffusion][Epoch 10217] diffusion learning rate: 0.001
2024-11-05 05:10:33,211 - INFO - [diffusion][Epoch 10217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:33,212 - INFO - [diffusion][Epoch 10218] Epoch 10219/12000
2024-11-05 05:10:37,269 - INFO - [diffusion][Epoch 10218] diffusion training Loss: 0.049027890898287296
2024-11-05 05:10:37,272 - INFO - [diffusion][Epoch 10218] diffusion learning rate: 0.001
2024-11-05 05:10:37,274 - INFO - [diffusion][Epoch 10218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:37,276 - INFO - [diffusion][Epoch 10219] Epoch 10220/12000
2024-11-05 05:10:41,417 - INFO - [diffusion][Epoch 10219] diffusion training Loss: 0.05250734183937311
2024-11-05 05:10:41,419 - INFO - [diffusion][Epoch 10219] diffusion learning rate: 0.001
2024-11-05 05:10:41,421 - INFO - [diffusion][Epoch 10219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:41,422 - INFO - [diffusion][Epoch 10220] Epoch 10221/12000
2024-11-05 05:10:45,525 - INFO - [diffusion][Epoch 10220] diffusion training Loss: 0.0496593089774251
2024-11-05 05:10:45,527 - INFO - [diffusion][Epoch 10220] diffusion learning rate: 0.001
2024-11-05 05:10:45,529 - INFO - [diffusion][Epoch 10220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:45,530 - INFO - [diffusion][Epoch 10221] Epoch 10222/12000
2024-11-05 05:10:49,644 - INFO - [diffusion][Epoch 10221] diffusion training Loss: 0.04679093696177006
2024-11-05 05:10:49,646 - INFO - [diffusion][Epoch 10221] diffusion learning rate: 0.001
2024-11-05 05:10:49,648 - INFO - [diffusion][Epoch 10221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:49,649 - INFO - [diffusion][Epoch 10222] Epoch 10223/12000
2024-11-05 05:10:53,756 - INFO - [diffusion][Epoch 10222] diffusion training Loss: 0.05095583572983742
2024-11-05 05:10:53,758 - INFO - [diffusion][Epoch 10222] diffusion learning rate: 0.001
2024-11-05 05:10:53,760 - INFO - [diffusion][Epoch 10222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:53,761 - INFO - [diffusion][Epoch 10223] Epoch 10224/12000
2024-11-05 05:10:57,855 - INFO - [diffusion][Epoch 10223] diffusion training Loss: 0.05514480732381344
2024-11-05 05:10:57,857 - INFO - [diffusion][Epoch 10223] diffusion learning rate: 0.001
2024-11-05 05:10:57,859 - INFO - [diffusion][Epoch 10223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:10:57,860 - INFO - [diffusion][Epoch 10224] Epoch 10225/12000
2024-11-05 05:11:01,927 - INFO - [diffusion][Epoch 10224] diffusion training Loss: 0.046013508923351765
2024-11-05 05:11:01,929 - INFO - [diffusion][Epoch 10224] diffusion learning rate: 0.001
2024-11-05 05:11:01,933 - INFO - [diffusion][Epoch 10224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:01,934 - INFO - [diffusion][Epoch 10225] Epoch 10226/12000
2024-11-05 05:11:06,021 - INFO - [diffusion][Epoch 10225] diffusion training Loss: 0.05042647663503885
2024-11-05 05:11:06,023 - INFO - [diffusion][Epoch 10225] diffusion learning rate: 0.001
2024-11-05 05:11:06,024 - INFO - [diffusion][Epoch 10225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:06,028 - INFO - [diffusion][Epoch 10226] Epoch 10227/12000
2024-11-05 05:11:10,016 - INFO - [diffusion][Epoch 10226] diffusion training Loss: 0.04988633655011654
2024-11-05 05:11:10,018 - INFO - [diffusion][Epoch 10226] diffusion learning rate: 0.001
2024-11-05 05:11:10,020 - INFO - [diffusion][Epoch 10226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:10,021 - INFO - [diffusion][Epoch 10227] Epoch 10228/12000
2024-11-05 05:11:14,096 - INFO - [diffusion][Epoch 10227] diffusion training Loss: 0.051301924511790276
2024-11-05 05:11:14,098 - INFO - [diffusion][Epoch 10227] diffusion learning rate: 0.001
2024-11-05 05:11:14,100 - INFO - [diffusion][Epoch 10227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:14,101 - INFO - [diffusion][Epoch 10228] Epoch 10229/12000
2024-11-05 05:11:18,380 - INFO - [diffusion][Epoch 10228] diffusion training Loss: 0.043751465156674385
2024-11-05 05:11:18,384 - INFO - [diffusion][Epoch 10228] diffusion learning rate: 0.001
2024-11-05 05:11:18,385 - INFO - [diffusion][Epoch 10228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:18,387 - INFO - [diffusion][Epoch 10229] Epoch 10230/12000
2024-11-05 05:11:22,467 - INFO - [diffusion][Epoch 10229] diffusion training Loss: 0.046024080365896225
2024-11-05 05:11:22,469 - INFO - [diffusion][Epoch 10229] diffusion learning rate: 0.001
2024-11-05 05:11:22,471 - INFO - [diffusion][Epoch 10229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:22,472 - INFO - [diffusion][Epoch 10230] Epoch 10231/12000
2024-11-05 05:11:26,540 - INFO - [diffusion][Epoch 10230] diffusion training Loss: 0.05003973934799433
2024-11-05 05:11:26,542 - INFO - [diffusion][Epoch 10230] diffusion learning rate: 0.001
2024-11-05 05:11:26,545 - INFO - [diffusion][Epoch 10230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:26,546 - INFO - [diffusion][Epoch 10231] Epoch 10232/12000
2024-11-05 05:11:30,657 - INFO - [diffusion][Epoch 10231] diffusion training Loss: 0.04665691778063774
2024-11-05 05:11:30,700 - INFO - [diffusion][Epoch 10231] diffusion learning rate: 0.001
2024-11-05 05:11:30,702 - INFO - [diffusion][Epoch 10231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:30,703 - INFO - [diffusion][Epoch 10232] Epoch 10233/12000
2024-11-05 05:11:34,641 - INFO - [diffusion][Epoch 10232] diffusion training Loss: 0.05396149028092623
2024-11-05 05:11:34,642 - INFO - [diffusion][Epoch 10232] diffusion learning rate: 0.001
2024-11-05 05:11:34,644 - INFO - [diffusion][Epoch 10232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:34,645 - INFO - [diffusion][Epoch 10233] Epoch 10234/12000
2024-11-05 05:11:38,771 - INFO - [diffusion][Epoch 10233] diffusion training Loss: 0.05518763605505228
2024-11-05 05:11:38,773 - INFO - [diffusion][Epoch 10233] diffusion learning rate: 0.001
2024-11-05 05:11:38,800 - INFO - [diffusion][Epoch 10233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:38,801 - INFO - [diffusion][Epoch 10234] Epoch 10235/12000
2024-11-05 05:11:42,908 - INFO - [diffusion][Epoch 10234] diffusion training Loss: 0.04995761066675186
2024-11-05 05:11:42,910 - INFO - [diffusion][Epoch 10234] diffusion learning rate: 0.001
2024-11-05 05:11:42,912 - INFO - [diffusion][Epoch 10234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:42,913 - INFO - [diffusion][Epoch 10235] Epoch 10236/12000
2024-11-05 05:11:47,048 - INFO - [diffusion][Epoch 10235] diffusion training Loss: 0.04841814935207367
2024-11-05 05:11:47,050 - INFO - [diffusion][Epoch 10235] diffusion learning rate: 0.001
2024-11-05 05:11:47,052 - INFO - [diffusion][Epoch 10235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:47,053 - INFO - [diffusion][Epoch 10236] Epoch 10237/12000
2024-11-05 05:11:51,158 - INFO - [diffusion][Epoch 10236] diffusion training Loss: 0.05808091349899769
2024-11-05 05:11:51,160 - INFO - [diffusion][Epoch 10236] diffusion learning rate: 0.001
2024-11-05 05:11:51,161 - INFO - [diffusion][Epoch 10236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:51,163 - INFO - [diffusion][Epoch 10237] Epoch 10238/12000
2024-11-05 05:11:55,256 - INFO - [diffusion][Epoch 10237] diffusion training Loss: 0.05059854593127966
2024-11-05 05:11:55,258 - INFO - [diffusion][Epoch 10237] diffusion learning rate: 0.001
2024-11-05 05:11:55,260 - INFO - [diffusion][Epoch 10237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:55,261 - INFO - [diffusion][Epoch 10238] Epoch 10239/12000
2024-11-05 05:11:59,370 - INFO - [diffusion][Epoch 10238] diffusion training Loss: 0.05051775183528662
2024-11-05 05:11:59,372 - INFO - [diffusion][Epoch 10238] diffusion learning rate: 0.001
2024-11-05 05:11:59,373 - INFO - [diffusion][Epoch 10238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:11:59,375 - INFO - [diffusion][Epoch 10239] Epoch 10240/12000
2024-11-05 05:12:03,350 - INFO - [diffusion][Epoch 10239] diffusion training Loss: 0.05059951636940241
2024-11-05 05:12:03,353 - INFO - [diffusion][Epoch 10239] diffusion learning rate: 0.001
2024-11-05 05:12:03,355 - INFO - [diffusion][Epoch 10239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:03,357 - INFO - [diffusion][Epoch 10240] Epoch 10241/12000
2024-11-05 05:12:07,231 - INFO - [diffusion][Epoch 10240] diffusion training Loss: 0.05220731906592846
2024-11-05 05:12:07,234 - INFO - [diffusion][Epoch 10240] diffusion learning rate: 0.001
2024-11-05 05:12:07,284 - INFO - [diffusion][Epoch 10240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:07,285 - INFO - [diffusion][Epoch 10241] Epoch 10242/12000
2024-11-05 05:12:11,345 - INFO - [diffusion][Epoch 10241] diffusion training Loss: 0.05499557312577963
2024-11-05 05:12:11,347 - INFO - [diffusion][Epoch 10241] diffusion learning rate: 0.001
2024-11-05 05:12:11,348 - INFO - [diffusion][Epoch 10241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:11,350 - INFO - [diffusion][Epoch 10242] Epoch 10243/12000
2024-11-05 05:12:15,308 - INFO - [diffusion][Epoch 10242] diffusion training Loss: 0.04571560584008694
2024-11-05 05:12:15,310 - INFO - [diffusion][Epoch 10242] diffusion learning rate: 0.001
2024-11-05 05:12:15,312 - INFO - [diffusion][Epoch 10242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:15,314 - INFO - [diffusion][Epoch 10243] Epoch 10244/12000
2024-11-05 05:12:19,425 - INFO - [diffusion][Epoch 10243] diffusion training Loss: 0.05185752920806408
2024-11-05 05:12:19,428 - INFO - [diffusion][Epoch 10243] diffusion learning rate: 0.001
2024-11-05 05:12:19,430 - INFO - [diffusion][Epoch 10243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:19,432 - INFO - [diffusion][Epoch 10244] Epoch 10245/12000
2024-11-05 05:12:23,521 - INFO - [diffusion][Epoch 10244] diffusion training Loss: 0.053216999396681786
2024-11-05 05:12:23,523 - INFO - [diffusion][Epoch 10244] diffusion learning rate: 0.001
2024-11-05 05:12:23,524 - INFO - [diffusion][Epoch 10244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:23,526 - INFO - [diffusion][Epoch 10245] Epoch 10246/12000
2024-11-05 05:12:27,613 - INFO - [diffusion][Epoch 10245] diffusion training Loss: 0.05146600119769573
2024-11-05 05:12:27,615 - INFO - [diffusion][Epoch 10245] diffusion learning rate: 0.001
2024-11-05 05:12:27,617 - INFO - [diffusion][Epoch 10245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:27,618 - INFO - [diffusion][Epoch 10246] Epoch 10247/12000
2024-11-05 05:12:31,752 - INFO - [diffusion][Epoch 10246] diffusion training Loss: 0.045779693871736526
2024-11-05 05:12:31,754 - INFO - [diffusion][Epoch 10246] diffusion learning rate: 0.001
2024-11-05 05:12:31,756 - INFO - [diffusion][Epoch 10246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:31,757 - INFO - [diffusion][Epoch 10247] Epoch 10248/12000
2024-11-05 05:12:35,827 - INFO - [diffusion][Epoch 10247] diffusion training Loss: 0.048417930491268635
2024-11-05 05:12:35,829 - INFO - [diffusion][Epoch 10247] diffusion learning rate: 0.001
2024-11-05 05:12:35,831 - INFO - [diffusion][Epoch 10247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:35,832 - INFO - [diffusion][Epoch 10248] Epoch 10249/12000
2024-11-05 05:12:39,935 - INFO - [diffusion][Epoch 10248] diffusion training Loss: 0.04592003487050533
2024-11-05 05:12:39,937 - INFO - [diffusion][Epoch 10248] diffusion learning rate: 0.001
2024-11-05 05:12:39,938 - INFO - [diffusion][Epoch 10248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:39,940 - INFO - [diffusion][Epoch 10249] Epoch 10250/12000
2024-11-05 05:12:44,078 - INFO - [diffusion][Epoch 10249] diffusion training Loss: 0.0467632869258523
2024-11-05 05:12:44,080 - INFO - [diffusion][Epoch 10249] diffusion learning rate: 0.001
2024-11-05 05:12:44,082 - INFO - [diffusion][Epoch 10249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:44,083 - INFO - [diffusion][Epoch 10250] Epoch 10251/12000
2024-11-05 05:12:48,190 - INFO - [diffusion][Epoch 10250] diffusion training Loss: 0.05380316637456417
2024-11-05 05:12:48,192 - INFO - [diffusion][Epoch 10250] diffusion learning rate: 0.001
2024-11-05 05:12:48,194 - INFO - [diffusion][Epoch 10250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:48,195 - INFO - [diffusion][Epoch 10251] Epoch 10252/12000
2024-11-05 05:12:52,298 - INFO - [diffusion][Epoch 10251] diffusion training Loss: 0.04839140735566616
2024-11-05 05:12:52,300 - INFO - [diffusion][Epoch 10251] diffusion learning rate: 0.001
2024-11-05 05:12:52,302 - INFO - [diffusion][Epoch 10251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:52,303 - INFO - [diffusion][Epoch 10252] Epoch 10253/12000
2024-11-05 05:12:56,414 - INFO - [diffusion][Epoch 10252] diffusion training Loss: 0.04524332471191883
2024-11-05 05:12:56,416 - INFO - [diffusion][Epoch 10252] diffusion learning rate: 0.001
2024-11-05 05:12:56,418 - INFO - [diffusion][Epoch 10252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:12:56,419 - INFO - [diffusion][Epoch 10253] Epoch 10254/12000
2024-11-05 05:13:00,521 - INFO - [diffusion][Epoch 10253] diffusion training Loss: 0.04958980530500412
2024-11-05 05:13:00,523 - INFO - [diffusion][Epoch 10253] diffusion learning rate: 0.001
2024-11-05 05:13:00,550 - INFO - [diffusion][Epoch 10253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:00,552 - INFO - [diffusion][Epoch 10254] Epoch 10255/12000
2024-11-05 05:13:04,637 - INFO - [diffusion][Epoch 10254] diffusion training Loss: 0.04985837545245886
2024-11-05 05:13:04,639 - INFO - [diffusion][Epoch 10254] diffusion learning rate: 0.001
2024-11-05 05:13:04,640 - INFO - [diffusion][Epoch 10254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:04,642 - INFO - [diffusion][Epoch 10255] Epoch 10256/12000
2024-11-05 05:13:08,745 - INFO - [diffusion][Epoch 10255] diffusion training Loss: 0.04759438429027796
2024-11-05 05:13:08,747 - INFO - [diffusion][Epoch 10255] diffusion learning rate: 0.001
2024-11-05 05:13:08,749 - INFO - [diffusion][Epoch 10255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:08,750 - INFO - [diffusion][Epoch 10256] Epoch 10257/12000
2024-11-05 05:13:12,715 - INFO - [diffusion][Epoch 10256] diffusion training Loss: 0.05132416170090437
2024-11-05 05:13:12,716 - INFO - [diffusion][Epoch 10256] diffusion learning rate: 0.001
2024-11-05 05:13:12,718 - INFO - [diffusion][Epoch 10256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:12,719 - INFO - [diffusion][Epoch 10257] Epoch 10258/12000
2024-11-05 05:13:16,832 - INFO - [diffusion][Epoch 10257] diffusion training Loss: 0.045479923486709595
2024-11-05 05:13:16,834 - INFO - [diffusion][Epoch 10257] diffusion learning rate: 0.001
2024-11-05 05:13:16,835 - INFO - [diffusion][Epoch 10257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:16,837 - INFO - [diffusion][Epoch 10258] Epoch 10259/12000
2024-11-05 05:13:20,971 - INFO - [diffusion][Epoch 10258] diffusion training Loss: 0.0468674898147583
2024-11-05 05:13:20,975 - INFO - [diffusion][Epoch 10258] diffusion learning rate: 0.001
2024-11-05 05:13:20,977 - INFO - [diffusion][Epoch 10258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:20,978 - INFO - [diffusion][Epoch 10259] Epoch 10260/12000
2024-11-05 05:13:25,095 - INFO - [diffusion][Epoch 10259] diffusion training Loss: 0.04606284759938717
2024-11-05 05:13:25,097 - INFO - [diffusion][Epoch 10259] diffusion learning rate: 0.001
2024-11-05 05:13:25,126 - INFO - [diffusion][Epoch 10259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:25,127 - INFO - [diffusion][Epoch 10260] Epoch 10261/12000
2024-11-05 05:13:29,204 - INFO - [diffusion][Epoch 10260] diffusion training Loss: 0.049590046517550945
2024-11-05 05:13:29,206 - INFO - [diffusion][Epoch 10260] diffusion learning rate: 0.001
2024-11-05 05:13:29,207 - INFO - [diffusion][Epoch 10260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:29,208 - INFO - [diffusion][Epoch 10261] Epoch 10262/12000
2024-11-05 05:13:33,301 - INFO - [diffusion][Epoch 10261] diffusion training Loss: 0.04981997422873974
2024-11-05 05:13:33,303 - INFO - [diffusion][Epoch 10261] diffusion learning rate: 0.001
2024-11-05 05:13:33,305 - INFO - [diffusion][Epoch 10261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:33,306 - INFO - [diffusion][Epoch 10262] Epoch 10263/12000
2024-11-05 05:13:37,420 - INFO - [diffusion][Epoch 10262] diffusion training Loss: 0.0478921914473176
2024-11-05 05:13:37,422 - INFO - [diffusion][Epoch 10262] diffusion learning rate: 0.001
2024-11-05 05:13:37,424 - INFO - [diffusion][Epoch 10262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:37,425 - INFO - [diffusion][Epoch 10263] Epoch 10264/12000
2024-11-05 05:13:41,494 - INFO - [diffusion][Epoch 10263] diffusion training Loss: 0.049758956767618656
2024-11-05 05:13:41,496 - INFO - [diffusion][Epoch 10263] diffusion learning rate: 0.001
2024-11-05 05:13:41,498 - INFO - [diffusion][Epoch 10263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:41,499 - INFO - [diffusion][Epoch 10264] Epoch 10265/12000
2024-11-05 05:13:45,537 - INFO - [diffusion][Epoch 10264] diffusion training Loss: 0.048049732111394405
2024-11-05 05:13:45,539 - INFO - [diffusion][Epoch 10264] diffusion learning rate: 0.001
2024-11-05 05:13:45,541 - INFO - [diffusion][Epoch 10264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:45,542 - INFO - [diffusion][Epoch 10265] Epoch 10266/12000
2024-11-05 05:13:49,653 - INFO - [diffusion][Epoch 10265] diffusion training Loss: 0.04821728356182575
2024-11-05 05:13:49,655 - INFO - [diffusion][Epoch 10265] diffusion learning rate: 0.001
2024-11-05 05:13:49,657 - INFO - [diffusion][Epoch 10265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:49,658 - INFO - [diffusion][Epoch 10266] Epoch 10267/12000
2024-11-05 05:13:53,621 - INFO - [diffusion][Epoch 10266] diffusion training Loss: 0.04795774072408676
2024-11-05 05:13:53,623 - INFO - [diffusion][Epoch 10266] diffusion learning rate: 0.001
2024-11-05 05:13:53,625 - INFO - [diffusion][Epoch 10266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:53,626 - INFO - [diffusion][Epoch 10267] Epoch 10268/12000
2024-11-05 05:13:57,729 - INFO - [diffusion][Epoch 10267] diffusion training Loss: 0.04960537701845169
2024-11-05 05:13:57,731 - INFO - [diffusion][Epoch 10267] diffusion learning rate: 0.001
2024-11-05 05:13:57,733 - INFO - [diffusion][Epoch 10267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:13:57,735 - INFO - [diffusion][Epoch 10268] Epoch 10269/12000
2024-11-05 05:14:01,807 - INFO - [diffusion][Epoch 10268] diffusion training Loss: 0.05227783787995577
2024-11-05 05:14:01,809 - INFO - [diffusion][Epoch 10268] diffusion learning rate: 0.001
2024-11-05 05:14:01,811 - INFO - [diffusion][Epoch 10268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:01,812 - INFO - [diffusion][Epoch 10269] Epoch 10270/12000
2024-11-05 05:14:05,808 - INFO - [diffusion][Epoch 10269] diffusion training Loss: 0.050132003612816334
2024-11-05 05:14:05,810 - INFO - [diffusion][Epoch 10269] diffusion learning rate: 0.001
2024-11-05 05:14:05,812 - INFO - [diffusion][Epoch 10269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:05,813 - INFO - [diffusion][Epoch 10270] Epoch 10271/12000
2024-11-05 05:14:10,020 - INFO - [diffusion][Epoch 10270] diffusion training Loss: 0.05419251974672079
2024-11-05 05:14:10,022 - INFO - [diffusion][Epoch 10270] diffusion learning rate: 0.001
2024-11-05 05:14:10,024 - INFO - [diffusion][Epoch 10270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:10,025 - INFO - [diffusion][Epoch 10271] Epoch 10272/12000
2024-11-05 05:14:14,130 - INFO - [diffusion][Epoch 10271] diffusion training Loss: 0.04933390486985445
2024-11-05 05:14:14,132 - INFO - [diffusion][Epoch 10271] diffusion learning rate: 0.001
2024-11-05 05:14:14,134 - INFO - [diffusion][Epoch 10271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:14,135 - INFO - [diffusion][Epoch 10272] Epoch 10273/12000
2024-11-05 05:14:18,249 - INFO - [diffusion][Epoch 10272] diffusion training Loss: 0.057510693557560444
2024-11-05 05:14:18,251 - INFO - [diffusion][Epoch 10272] diffusion learning rate: 0.001
2024-11-05 05:14:18,253 - INFO - [diffusion][Epoch 10272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:18,254 - INFO - [diffusion][Epoch 10273] Epoch 10274/12000
2024-11-05 05:14:22,378 - INFO - [diffusion][Epoch 10273] diffusion training Loss: 0.050504762679338455
2024-11-05 05:14:22,382 - INFO - [diffusion][Epoch 10273] diffusion learning rate: 0.001
2024-11-05 05:14:22,384 - INFO - [diffusion][Epoch 10273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:22,385 - INFO - [diffusion][Epoch 10274] Epoch 10275/12000
2024-11-05 05:14:26,481 - INFO - [diffusion][Epoch 10274] diffusion training Loss: 0.05278604198247194
2024-11-05 05:14:26,483 - INFO - [diffusion][Epoch 10274] diffusion learning rate: 0.001
2024-11-05 05:14:26,485 - INFO - [diffusion][Epoch 10274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:26,486 - INFO - [diffusion][Epoch 10275] Epoch 10276/12000
2024-11-05 05:14:30,602 - INFO - [diffusion][Epoch 10275] diffusion training Loss: 0.047481756657361984
2024-11-05 05:14:30,604 - INFO - [diffusion][Epoch 10275] diffusion learning rate: 0.001
2024-11-05 05:14:30,606 - INFO - [diffusion][Epoch 10275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:30,607 - INFO - [diffusion][Epoch 10276] Epoch 10277/12000
2024-11-05 05:14:34,699 - INFO - [diffusion][Epoch 10276] diffusion training Loss: 0.049413672648370266
2024-11-05 05:14:34,700 - INFO - [diffusion][Epoch 10276] diffusion learning rate: 0.001
2024-11-05 05:14:34,702 - INFO - [diffusion][Epoch 10276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:34,704 - INFO - [diffusion][Epoch 10277] Epoch 10278/12000
2024-11-05 05:14:38,788 - INFO - [diffusion][Epoch 10277] diffusion training Loss: 0.04839729890227318
2024-11-05 05:14:38,790 - INFO - [diffusion][Epoch 10277] diffusion learning rate: 0.001
2024-11-05 05:14:38,792 - INFO - [diffusion][Epoch 10277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:38,793 - INFO - [diffusion][Epoch 10278] Epoch 10279/12000
2024-11-05 05:14:42,841 - INFO - [diffusion][Epoch 10278] diffusion training Loss: 0.052022174932062626
2024-11-05 05:14:42,843 - INFO - [diffusion][Epoch 10278] diffusion learning rate: 0.001
2024-11-05 05:14:42,845 - INFO - [diffusion][Epoch 10278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:42,846 - INFO - [diffusion][Epoch 10279] Epoch 10280/12000
2024-11-05 05:14:46,937 - INFO - [diffusion][Epoch 10279] diffusion training Loss: 0.048025296069681644
2024-11-05 05:14:46,939 - INFO - [diffusion][Epoch 10279] diffusion learning rate: 0.001
2024-11-05 05:14:46,941 - INFO - [diffusion][Epoch 10279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:46,942 - INFO - [diffusion][Epoch 10280] Epoch 10281/12000
2024-11-05 05:14:50,971 - INFO - [diffusion][Epoch 10280] diffusion training Loss: 0.05139409750699997
2024-11-05 05:14:50,973 - INFO - [diffusion][Epoch 10280] diffusion learning rate: 0.001
2024-11-05 05:14:50,976 - INFO - [diffusion][Epoch 10280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:50,977 - INFO - [diffusion][Epoch 10281] Epoch 10282/12000
2024-11-05 05:14:55,072 - INFO - [diffusion][Epoch 10281] diffusion training Loss: 0.04870371334254742
2024-11-05 05:14:55,074 - INFO - [diffusion][Epoch 10281] diffusion learning rate: 0.001
2024-11-05 05:14:55,075 - INFO - [diffusion][Epoch 10281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:55,077 - INFO - [diffusion][Epoch 10282] Epoch 10283/12000
2024-11-05 05:14:59,182 - INFO - [diffusion][Epoch 10282] diffusion training Loss: 0.04754212684929371
2024-11-05 05:14:59,184 - INFO - [diffusion][Epoch 10282] diffusion learning rate: 0.001
2024-11-05 05:14:59,185 - INFO - [diffusion][Epoch 10282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:14:59,187 - INFO - [diffusion][Epoch 10283] Epoch 10284/12000
2024-11-05 05:15:03,275 - INFO - [diffusion][Epoch 10283] diffusion training Loss: 0.05252981651574373
2024-11-05 05:15:03,276 - INFO - [diffusion][Epoch 10283] diffusion learning rate: 0.001
2024-11-05 05:15:03,278 - INFO - [diffusion][Epoch 10283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:03,279 - INFO - [diffusion][Epoch 10284] Epoch 10285/12000
2024-11-05 05:15:07,370 - INFO - [diffusion][Epoch 10284] diffusion training Loss: 0.04433407075703144
2024-11-05 05:15:07,371 - INFO - [diffusion][Epoch 10284] diffusion learning rate: 0.001
2024-11-05 05:15:07,373 - INFO - [diffusion][Epoch 10284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:07,374 - INFO - [diffusion][Epoch 10285] Epoch 10286/12000
2024-11-05 05:15:11,467 - INFO - [diffusion][Epoch 10285] diffusion training Loss: 0.046717578545212746
2024-11-05 05:15:11,469 - INFO - [diffusion][Epoch 10285] diffusion learning rate: 0.001
2024-11-05 05:15:11,471 - INFO - [diffusion][Epoch 10285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:11,472 - INFO - [diffusion][Epoch 10286] Epoch 10287/12000
2024-11-05 05:15:15,333 - INFO - [diffusion][Epoch 10286] diffusion training Loss: 0.04540952015668154
2024-11-05 05:15:15,334 - INFO - [diffusion][Epoch 10286] diffusion learning rate: 0.001
2024-11-05 05:15:15,336 - INFO - [diffusion][Epoch 10286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:15,337 - INFO - [diffusion][Epoch 10287] Epoch 10288/12000
2024-11-05 05:15:19,445 - INFO - [diffusion][Epoch 10287] diffusion training Loss: 0.04850297886878252
2024-11-05 05:15:19,447 - INFO - [diffusion][Epoch 10287] diffusion learning rate: 0.001
2024-11-05 05:15:19,449 - INFO - [diffusion][Epoch 10287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:19,450 - INFO - [diffusion][Epoch 10288] Epoch 10289/12000
2024-11-05 05:15:23,387 - INFO - [diffusion][Epoch 10288] diffusion training Loss: 0.04366866685450077
2024-11-05 05:15:23,391 - INFO - [diffusion][Epoch 10288] diffusion learning rate: 0.001
2024-11-05 05:15:23,418 - INFO - [diffusion][Epoch 10288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:23,419 - INFO - [diffusion][Epoch 10289] Epoch 10290/12000
2024-11-05 05:15:27,401 - INFO - [diffusion][Epoch 10289] diffusion training Loss: 0.050486719235777855
2024-11-05 05:15:27,404 - INFO - [diffusion][Epoch 10289] diffusion learning rate: 0.001
2024-11-05 05:15:27,405 - INFO - [diffusion][Epoch 10289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:27,406 - INFO - [diffusion][Epoch 10290] Epoch 10291/12000
2024-11-05 05:15:31,345 - INFO - [diffusion][Epoch 10290] diffusion training Loss: 0.04531350638717413
2024-11-05 05:15:31,347 - INFO - [diffusion][Epoch 10290] diffusion learning rate: 0.001
2024-11-05 05:15:31,349 - INFO - [diffusion][Epoch 10290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:31,351 - INFO - [diffusion][Epoch 10291] Epoch 10292/12000
2024-11-05 05:15:36,039 - INFO - [diffusion][Epoch 10291] diffusion training Loss: 0.04452127870172262
2024-11-05 05:15:36,040 - INFO - [diffusion][Epoch 10291] diffusion learning rate: 0.001
2024-11-05 05:15:36,042 - INFO - [diffusion][Epoch 10291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:36,044 - INFO - [diffusion][Epoch 10292] Epoch 10293/12000
2024-11-05 05:15:40,173 - INFO - [diffusion][Epoch 10292] diffusion training Loss: 0.04720696061849594
2024-11-05 05:15:40,175 - INFO - [diffusion][Epoch 10292] diffusion learning rate: 0.001
2024-11-05 05:15:40,177 - INFO - [diffusion][Epoch 10292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:40,178 - INFO - [diffusion][Epoch 10293] Epoch 10294/12000
2024-11-05 05:15:44,254 - INFO - [diffusion][Epoch 10293] diffusion training Loss: 0.0524595994502306
2024-11-05 05:15:44,256 - INFO - [diffusion][Epoch 10293] diffusion learning rate: 0.001
2024-11-05 05:15:44,258 - INFO - [diffusion][Epoch 10293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:44,259 - INFO - [diffusion][Epoch 10294] Epoch 10295/12000
2024-11-05 05:15:48,369 - INFO - [diffusion][Epoch 10294] diffusion training Loss: 0.04945947974920273
2024-11-05 05:15:48,371 - INFO - [diffusion][Epoch 10294] diffusion learning rate: 0.001
2024-11-05 05:15:48,373 - INFO - [diffusion][Epoch 10294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:48,374 - INFO - [diffusion][Epoch 10295] Epoch 10296/12000
2024-11-05 05:15:52,508 - INFO - [diffusion][Epoch 10295] diffusion training Loss: 0.048974888399243355
2024-11-05 05:15:52,510 - INFO - [diffusion][Epoch 10295] diffusion learning rate: 0.001
2024-11-05 05:15:52,512 - INFO - [diffusion][Epoch 10295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:52,513 - INFO - [diffusion][Epoch 10296] Epoch 10297/12000
2024-11-05 05:15:56,592 - INFO - [diffusion][Epoch 10296] diffusion training Loss: 0.05056026764214039
2024-11-05 05:15:56,594 - INFO - [diffusion][Epoch 10296] diffusion learning rate: 0.001
2024-11-05 05:15:56,596 - INFO - [diffusion][Epoch 10296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:15:56,598 - INFO - [diffusion][Epoch 10297] Epoch 10298/12000
2024-11-05 05:16:00,753 - INFO - [diffusion][Epoch 10297] diffusion training Loss: 0.0508820740506053
2024-11-05 05:16:00,755 - INFO - [diffusion][Epoch 10297] diffusion learning rate: 0.001
2024-11-05 05:16:00,757 - INFO - [diffusion][Epoch 10297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:00,758 - INFO - [diffusion][Epoch 10298] Epoch 10299/12000
2024-11-05 05:16:04,827 - INFO - [diffusion][Epoch 10298] diffusion training Loss: 0.056729093194007874
2024-11-05 05:16:04,829 - INFO - [diffusion][Epoch 10298] diffusion learning rate: 0.001
2024-11-05 05:16:04,831 - INFO - [diffusion][Epoch 10298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:04,832 - INFO - [diffusion][Epoch 10299] Epoch 10300/12000
2024-11-05 05:16:08,912 - INFO - [diffusion][Epoch 10299] diffusion training Loss: 0.047245933674275875
2024-11-05 05:16:08,914 - INFO - [diffusion][Epoch 10299] diffusion learning rate: 0.001
2024-11-05 05:16:08,916 - INFO - [diffusion][Epoch 10299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:08,917 - INFO - [diffusion][Epoch 10300] Epoch 10301/12000
2024-11-05 05:16:13,019 - INFO - [diffusion][Epoch 10300] diffusion training Loss: 0.04649735242128372
2024-11-05 05:16:13,021 - INFO - [diffusion][Epoch 10300] diffusion learning rate: 0.001
2024-11-05 05:16:13,023 - INFO - [diffusion][Epoch 10300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:13,024 - INFO - [diffusion][Epoch 10301] Epoch 10302/12000
2024-11-05 05:16:17,082 - INFO - [diffusion][Epoch 10301] diffusion training Loss: 0.05436840560287237
2024-11-05 05:16:17,084 - INFO - [diffusion][Epoch 10301] diffusion learning rate: 0.001
2024-11-05 05:16:17,086 - INFO - [diffusion][Epoch 10301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:17,087 - INFO - [diffusion][Epoch 10302] Epoch 10303/12000
2024-11-05 05:16:21,180 - INFO - [diffusion][Epoch 10302] diffusion training Loss: 0.053275263868272305
2024-11-05 05:16:21,182 - INFO - [diffusion][Epoch 10302] diffusion learning rate: 0.001
2024-11-05 05:16:21,184 - INFO - [diffusion][Epoch 10302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:21,185 - INFO - [diffusion][Epoch 10303] Epoch 10304/12000
2024-11-05 05:16:25,333 - INFO - [diffusion][Epoch 10303] diffusion training Loss: 0.04866472538560629
2024-11-05 05:16:25,337 - INFO - [diffusion][Epoch 10303] diffusion learning rate: 0.001
2024-11-05 05:16:25,339 - INFO - [diffusion][Epoch 10303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:25,340 - INFO - [diffusion][Epoch 10304] Epoch 10305/12000
2024-11-05 05:16:29,434 - INFO - [diffusion][Epoch 10304] diffusion training Loss: 0.04955322667956352
2024-11-05 05:16:29,436 - INFO - [diffusion][Epoch 10304] diffusion learning rate: 0.001
2024-11-05 05:16:29,438 - INFO - [diffusion][Epoch 10304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:29,439 - INFO - [diffusion][Epoch 10305] Epoch 10306/12000
2024-11-05 05:16:33,514 - INFO - [diffusion][Epoch 10305] diffusion training Loss: 0.0490997014567256
2024-11-05 05:16:33,518 - INFO - [diffusion][Epoch 10305] diffusion learning rate: 0.001
2024-11-05 05:16:33,519 - INFO - [diffusion][Epoch 10305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:33,521 - INFO - [diffusion][Epoch 10306] Epoch 10307/12000
2024-11-05 05:16:37,615 - INFO - [diffusion][Epoch 10306] diffusion training Loss: 0.04753896780312061
2024-11-05 05:16:37,617 - INFO - [diffusion][Epoch 10306] diffusion learning rate: 0.001
2024-11-05 05:16:37,619 - INFO - [diffusion][Epoch 10306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:37,620 - INFO - [diffusion][Epoch 10307] Epoch 10308/12000
2024-11-05 05:16:41,726 - INFO - [diffusion][Epoch 10307] diffusion training Loss: 0.046460689045488834
2024-11-05 05:16:41,728 - INFO - [diffusion][Epoch 10307] diffusion learning rate: 0.001
2024-11-05 05:16:41,730 - INFO - [diffusion][Epoch 10307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:41,731 - INFO - [diffusion][Epoch 10308] Epoch 10309/12000
2024-11-05 05:16:45,849 - INFO - [diffusion][Epoch 10308] diffusion training Loss: 0.050448400899767876
2024-11-05 05:16:45,852 - INFO - [diffusion][Epoch 10308] diffusion learning rate: 0.001
2024-11-05 05:16:45,853 - INFO - [diffusion][Epoch 10308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:45,855 - INFO - [diffusion][Epoch 10309] Epoch 10310/12000
2024-11-05 05:16:49,956 - INFO - [diffusion][Epoch 10309] diffusion training Loss: 0.04930800013244152
2024-11-05 05:16:49,958 - INFO - [diffusion][Epoch 10309] diffusion learning rate: 0.001
2024-11-05 05:16:49,960 - INFO - [diffusion][Epoch 10309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:49,961 - INFO - [diffusion][Epoch 10310] Epoch 10311/12000
2024-11-05 05:16:54,148 - INFO - [diffusion][Epoch 10310] diffusion training Loss: 0.0519541846588254
2024-11-05 05:16:54,150 - INFO - [diffusion][Epoch 10310] diffusion learning rate: 0.001
2024-11-05 05:16:54,151 - INFO - [diffusion][Epoch 10310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:54,153 - INFO - [diffusion][Epoch 10311] Epoch 10312/12000
2024-11-05 05:16:58,135 - INFO - [diffusion][Epoch 10311] diffusion training Loss: 0.048557513393461704
2024-11-05 05:16:58,137 - INFO - [diffusion][Epoch 10311] diffusion learning rate: 0.001
2024-11-05 05:16:58,165 - INFO - [diffusion][Epoch 10311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:16:58,167 - INFO - [diffusion][Epoch 10312] Epoch 10313/12000
2024-11-05 05:17:02,113 - INFO - [diffusion][Epoch 10312] diffusion training Loss: 0.047817242331802845
2024-11-05 05:17:02,115 - INFO - [diffusion][Epoch 10312] diffusion learning rate: 0.001
2024-11-05 05:17:02,117 - INFO - [diffusion][Epoch 10312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:02,118 - INFO - [diffusion][Epoch 10313] Epoch 10314/12000
2024-11-05 05:17:06,178 - INFO - [diffusion][Epoch 10313] diffusion training Loss: 0.04927160870283842
2024-11-05 05:17:06,180 - INFO - [diffusion][Epoch 10313] diffusion learning rate: 0.001
2024-11-05 05:17:06,182 - INFO - [diffusion][Epoch 10313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:06,183 - INFO - [diffusion][Epoch 10314] Epoch 10315/12000
2024-11-05 05:17:10,271 - INFO - [diffusion][Epoch 10314] diffusion training Loss: 0.04304324556142092
2024-11-05 05:17:10,273 - INFO - [diffusion][Epoch 10314] diffusion learning rate: 0.001
2024-11-05 05:17:10,275 - INFO - [diffusion][Epoch 10314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:10,276 - INFO - [diffusion][Epoch 10315] Epoch 10316/12000
2024-11-05 05:17:14,379 - INFO - [diffusion][Epoch 10315] diffusion training Loss: 0.047644298523664474
2024-11-05 05:17:14,381 - INFO - [diffusion][Epoch 10315] diffusion learning rate: 0.001
2024-11-05 05:17:14,383 - INFO - [diffusion][Epoch 10315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:14,384 - INFO - [diffusion][Epoch 10316] Epoch 10317/12000
2024-11-05 05:17:18,479 - INFO - [diffusion][Epoch 10316] diffusion training Loss: 0.04991605877876282
2024-11-05 05:17:18,481 - INFO - [diffusion][Epoch 10316] diffusion learning rate: 0.001
2024-11-05 05:17:18,482 - INFO - [diffusion][Epoch 10316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:18,483 - INFO - [diffusion][Epoch 10317] Epoch 10318/12000
2024-11-05 05:17:22,609 - INFO - [diffusion][Epoch 10317] diffusion training Loss: 0.05420960392802954
2024-11-05 05:17:22,610 - INFO - [diffusion][Epoch 10317] diffusion learning rate: 0.001
2024-11-05 05:17:22,612 - INFO - [diffusion][Epoch 10317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:22,613 - INFO - [diffusion][Epoch 10318] Epoch 10319/12000
2024-11-05 05:17:26,759 - INFO - [diffusion][Epoch 10318] diffusion training Loss: 0.04871886223554611
2024-11-05 05:17:26,763 - INFO - [diffusion][Epoch 10318] diffusion learning rate: 0.001
2024-11-05 05:17:26,765 - INFO - [diffusion][Epoch 10318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:26,766 - INFO - [diffusion][Epoch 10319] Epoch 10320/12000
2024-11-05 05:17:30,873 - INFO - [diffusion][Epoch 10319] diffusion training Loss: 0.048459749668836594
2024-11-05 05:17:30,875 - INFO - [diffusion][Epoch 10319] diffusion learning rate: 0.001
2024-11-05 05:17:30,877 - INFO - [diffusion][Epoch 10319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:30,878 - INFO - [diffusion][Epoch 10320] Epoch 10321/12000
2024-11-05 05:17:35,002 - INFO - [diffusion][Epoch 10320] diffusion training Loss: 0.04857207182794809
2024-11-05 05:17:35,004 - INFO - [diffusion][Epoch 10320] diffusion learning rate: 0.001
2024-11-05 05:17:35,006 - INFO - [diffusion][Epoch 10320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:35,008 - INFO - [diffusion][Epoch 10321] Epoch 10322/12000
2024-11-05 05:17:39,142 - INFO - [diffusion][Epoch 10321] diffusion training Loss: 0.05128421448171139
2024-11-05 05:17:39,144 - INFO - [diffusion][Epoch 10321] diffusion learning rate: 0.001
2024-11-05 05:17:39,146 - INFO - [diffusion][Epoch 10321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:39,147 - INFO - [diffusion][Epoch 10322] Epoch 10323/12000
2024-11-05 05:17:43,234 - INFO - [diffusion][Epoch 10322] diffusion training Loss: 0.04681966174393892
2024-11-05 05:17:43,236 - INFO - [diffusion][Epoch 10322] diffusion learning rate: 0.001
2024-11-05 05:17:43,238 - INFO - [diffusion][Epoch 10322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:43,239 - INFO - [diffusion][Epoch 10323] Epoch 10324/12000
2024-11-05 05:17:47,372 - INFO - [diffusion][Epoch 10323] diffusion training Loss: 0.05116328410804272
2024-11-05 05:17:47,374 - INFO - [diffusion][Epoch 10323] diffusion learning rate: 0.001
2024-11-05 05:17:47,376 - INFO - [diffusion][Epoch 10323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:47,377 - INFO - [diffusion][Epoch 10324] Epoch 10325/12000
2024-11-05 05:17:51,495 - INFO - [diffusion][Epoch 10324] diffusion training Loss: 0.04827854596078396
2024-11-05 05:17:51,497 - INFO - [diffusion][Epoch 10324] diffusion learning rate: 0.001
2024-11-05 05:17:51,499 - INFO - [diffusion][Epoch 10324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:51,500 - INFO - [diffusion][Epoch 10325] Epoch 10326/12000
2024-11-05 05:17:55,545 - INFO - [diffusion][Epoch 10325] diffusion training Loss: 0.05235147196799517
2024-11-05 05:17:55,547 - INFO - [diffusion][Epoch 10325] diffusion learning rate: 0.001
2024-11-05 05:17:55,549 - INFO - [diffusion][Epoch 10325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:55,550 - INFO - [diffusion][Epoch 10326] Epoch 10327/12000
2024-11-05 05:17:59,678 - INFO - [diffusion][Epoch 10326] diffusion training Loss: 0.045955054461956024
2024-11-05 05:17:59,680 - INFO - [diffusion][Epoch 10326] diffusion learning rate: 0.001
2024-11-05 05:17:59,682 - INFO - [diffusion][Epoch 10326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:17:59,684 - INFO - [diffusion][Epoch 10327] Epoch 10328/12000
2024-11-05 05:18:03,753 - INFO - [diffusion][Epoch 10327] diffusion training Loss: 0.048653929494321346
2024-11-05 05:18:03,776 - INFO - [diffusion][Epoch 10327] diffusion learning rate: 0.001
2024-11-05 05:18:03,778 - INFO - [diffusion][Epoch 10327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:03,779 - INFO - [diffusion][Epoch 10328] Epoch 10329/12000
2024-11-05 05:18:07,874 - INFO - [diffusion][Epoch 10328] diffusion training Loss: 0.049305543303489685
2024-11-05 05:18:07,876 - INFO - [diffusion][Epoch 10328] diffusion learning rate: 0.001
2024-11-05 05:18:07,884 - INFO - [diffusion][Epoch 10328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:07,885 - INFO - [diffusion][Epoch 10329] Epoch 10330/12000
2024-11-05 05:18:12,002 - INFO - [diffusion][Epoch 10329] diffusion training Loss: 0.052980903536081314
2024-11-05 05:18:12,004 - INFO - [diffusion][Epoch 10329] diffusion learning rate: 0.001
2024-11-05 05:18:12,006 - INFO - [diffusion][Epoch 10329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:12,007 - INFO - [diffusion][Epoch 10330] Epoch 10331/12000
2024-11-05 05:18:16,215 - INFO - [diffusion][Epoch 10330] diffusion training Loss: 0.048204438760876656
2024-11-05 05:18:16,217 - INFO - [diffusion][Epoch 10330] diffusion learning rate: 0.001
2024-11-05 05:18:16,245 - INFO - [diffusion][Epoch 10330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:16,247 - INFO - [diffusion][Epoch 10331] Epoch 10332/12000
2024-11-05 05:18:20,366 - INFO - [diffusion][Epoch 10331] diffusion training Loss: 0.04752245917916298
2024-11-05 05:18:20,368 - INFO - [diffusion][Epoch 10331] diffusion learning rate: 0.001
2024-11-05 05:18:20,370 - INFO - [diffusion][Epoch 10331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:20,371 - INFO - [diffusion][Epoch 10332] Epoch 10333/12000
2024-11-05 05:18:24,484 - INFO - [diffusion][Epoch 10332] diffusion training Loss: 0.04886201675981283
2024-11-05 05:18:24,486 - INFO - [diffusion][Epoch 10332] diffusion learning rate: 0.001
2024-11-05 05:18:24,487 - INFO - [diffusion][Epoch 10332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:24,489 - INFO - [diffusion][Epoch 10333] Epoch 10334/12000
2024-11-05 05:18:28,583 - INFO - [diffusion][Epoch 10333] diffusion training Loss: 0.04481056425720453
2024-11-05 05:18:28,587 - INFO - [diffusion][Epoch 10333] diffusion learning rate: 0.001
2024-11-05 05:18:28,588 - INFO - [diffusion][Epoch 10333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:28,590 - INFO - [diffusion][Epoch 10334] Epoch 10335/12000
2024-11-05 05:18:32,719 - INFO - [diffusion][Epoch 10334] diffusion training Loss: 0.04874244146049023
2024-11-05 05:18:32,720 - INFO - [diffusion][Epoch 10334] diffusion learning rate: 0.001
2024-11-05 05:18:32,722 - INFO - [diffusion][Epoch 10334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:32,723 - INFO - [diffusion][Epoch 10335] Epoch 10336/12000
2024-11-05 05:18:36,800 - INFO - [diffusion][Epoch 10335] diffusion training Loss: 0.05589088145643473
2024-11-05 05:18:36,803 - INFO - [diffusion][Epoch 10335] diffusion learning rate: 0.001
2024-11-05 05:18:36,804 - INFO - [diffusion][Epoch 10335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:36,806 - INFO - [diffusion][Epoch 10336] Epoch 10337/12000
2024-11-05 05:18:40,902 - INFO - [diffusion][Epoch 10336] diffusion training Loss: 0.049347516149282455
2024-11-05 05:18:40,904 - INFO - [diffusion][Epoch 10336] diffusion learning rate: 0.001
2024-11-05 05:18:40,906 - INFO - [diffusion][Epoch 10336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:40,907 - INFO - [diffusion][Epoch 10337] Epoch 10338/12000
2024-11-05 05:18:45,028 - INFO - [diffusion][Epoch 10337] diffusion training Loss: 0.04911976307630539
2024-11-05 05:18:45,030 - INFO - [diffusion][Epoch 10337] diffusion learning rate: 0.001
2024-11-05 05:18:45,032 - INFO - [diffusion][Epoch 10337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:45,033 - INFO - [diffusion][Epoch 10338] Epoch 10339/12000
2024-11-05 05:18:49,113 - INFO - [diffusion][Epoch 10338] diffusion training Loss: 0.047471147030591965
2024-11-05 05:18:49,115 - INFO - [diffusion][Epoch 10338] diffusion learning rate: 0.001
2024-11-05 05:18:49,117 - INFO - [diffusion][Epoch 10338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:49,118 - INFO - [diffusion][Epoch 10339] Epoch 10340/12000
2024-11-05 05:18:53,284 - INFO - [diffusion][Epoch 10339] diffusion training Loss: 0.04871921520680189
2024-11-05 05:18:53,286 - INFO - [diffusion][Epoch 10339] diffusion learning rate: 0.001
2024-11-05 05:18:53,288 - INFO - [diffusion][Epoch 10339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:53,289 - INFO - [diffusion][Epoch 10340] Epoch 10341/12000
2024-11-05 05:18:57,379 - INFO - [diffusion][Epoch 10340] diffusion training Loss: 0.04951277654618025
2024-11-05 05:18:57,381 - INFO - [diffusion][Epoch 10340] diffusion learning rate: 0.001
2024-11-05 05:18:57,383 - INFO - [diffusion][Epoch 10340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:18:57,385 - INFO - [diffusion][Epoch 10341] Epoch 10342/12000
2024-11-05 05:19:01,511 - INFO - [diffusion][Epoch 10341] diffusion training Loss: 0.0505291735753417
2024-11-05 05:19:01,513 - INFO - [diffusion][Epoch 10341] diffusion learning rate: 0.001
2024-11-05 05:19:01,514 - INFO - [diffusion][Epoch 10341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:01,516 - INFO - [diffusion][Epoch 10342] Epoch 10343/12000
2024-11-05 05:19:05,637 - INFO - [diffusion][Epoch 10342] diffusion training Loss: 0.054140063002705574
2024-11-05 05:19:05,639 - INFO - [diffusion][Epoch 10342] diffusion learning rate: 0.001
2024-11-05 05:19:05,641 - INFO - [diffusion][Epoch 10342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:05,642 - INFO - [diffusion][Epoch 10343] Epoch 10344/12000
2024-11-05 05:19:09,743 - INFO - [diffusion][Epoch 10343] diffusion training Loss: 0.04668715503066778
2024-11-05 05:19:09,745 - INFO - [diffusion][Epoch 10343] diffusion learning rate: 0.001
2024-11-05 05:19:09,746 - INFO - [diffusion][Epoch 10343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:09,748 - INFO - [diffusion][Epoch 10344] Epoch 10345/12000
2024-11-05 05:19:13,868 - INFO - [diffusion][Epoch 10344] diffusion training Loss: 0.049728075973689556
2024-11-05 05:19:13,870 - INFO - [diffusion][Epoch 10344] diffusion learning rate: 0.001
2024-11-05 05:19:13,872 - INFO - [diffusion][Epoch 10344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:13,874 - INFO - [diffusion][Epoch 10345] Epoch 10346/12000
2024-11-05 05:19:17,984 - INFO - [diffusion][Epoch 10345] diffusion training Loss: 0.04968172963708639
2024-11-05 05:19:17,986 - INFO - [diffusion][Epoch 10345] diffusion learning rate: 0.001
2024-11-05 05:19:17,988 - INFO - [diffusion][Epoch 10345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:17,989 - INFO - [diffusion][Epoch 10346] Epoch 10347/12000
2024-11-05 05:19:22,085 - INFO - [diffusion][Epoch 10346] diffusion training Loss: 0.046719741076231
2024-11-05 05:19:22,087 - INFO - [diffusion][Epoch 10346] diffusion learning rate: 0.001
2024-11-05 05:19:22,089 - INFO - [diffusion][Epoch 10346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:22,090 - INFO - [diffusion][Epoch 10347] Epoch 10348/12000
2024-11-05 05:19:26,189 - INFO - [diffusion][Epoch 10347] diffusion training Loss: 0.05378345586359501
2024-11-05 05:19:26,191 - INFO - [diffusion][Epoch 10347] diffusion learning rate: 0.001
2024-11-05 05:19:26,192 - INFO - [diffusion][Epoch 10347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:26,194 - INFO - [diffusion][Epoch 10348] Epoch 10349/12000
2024-11-05 05:19:30,249 - INFO - [diffusion][Epoch 10348] diffusion training Loss: 0.0475333109498024
2024-11-05 05:19:30,253 - INFO - [diffusion][Epoch 10348] diffusion learning rate: 0.001
2024-11-05 05:19:30,255 - INFO - [diffusion][Epoch 10348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:30,256 - INFO - [diffusion][Epoch 10349] Epoch 10350/12000
2024-11-05 05:19:34,339 - INFO - [diffusion][Epoch 10349] diffusion training Loss: 0.04548235237598419
2024-11-05 05:19:34,341 - INFO - [diffusion][Epoch 10349] diffusion learning rate: 0.001
2024-11-05 05:19:34,343 - INFO - [diffusion][Epoch 10349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:34,344 - INFO - [diffusion][Epoch 10350] Epoch 10351/12000
2024-11-05 05:19:38,479 - INFO - [diffusion][Epoch 10350] diffusion training Loss: 0.05570768378674984
2024-11-05 05:19:38,481 - INFO - [diffusion][Epoch 10350] diffusion learning rate: 0.001
2024-11-05 05:19:38,483 - INFO - [diffusion][Epoch 10350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:38,484 - INFO - [diffusion][Epoch 10351] Epoch 10352/12000
2024-11-05 05:19:42,793 - INFO - [diffusion][Epoch 10351] diffusion training Loss: 0.05138026550412178
2024-11-05 05:19:42,794 - INFO - [diffusion][Epoch 10351] diffusion learning rate: 0.001
2024-11-05 05:19:42,796 - INFO - [diffusion][Epoch 10351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:42,797 - INFO - [diffusion][Epoch 10352] Epoch 10353/12000
2024-11-05 05:19:46,920 - INFO - [diffusion][Epoch 10352] diffusion training Loss: 0.05030736234039068
2024-11-05 05:19:46,922 - INFO - [diffusion][Epoch 10352] diffusion learning rate: 0.001
2024-11-05 05:19:46,924 - INFO - [diffusion][Epoch 10352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:46,925 - INFO - [diffusion][Epoch 10353] Epoch 10354/12000
2024-11-05 05:19:51,029 - INFO - [diffusion][Epoch 10353] diffusion training Loss: 0.046293714083731174
2024-11-05 05:19:51,031 - INFO - [diffusion][Epoch 10353] diffusion learning rate: 0.001
2024-11-05 05:19:51,033 - INFO - [diffusion][Epoch 10353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:51,034 - INFO - [diffusion][Epoch 10354] Epoch 10355/12000
2024-11-05 05:19:55,153 - INFO - [diffusion][Epoch 10354] diffusion training Loss: 0.046369774267077446
2024-11-05 05:19:55,155 - INFO - [diffusion][Epoch 10354] diffusion learning rate: 0.001
2024-11-05 05:19:55,156 - INFO - [diffusion][Epoch 10354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:55,158 - INFO - [diffusion][Epoch 10355] Epoch 10356/12000
2024-11-05 05:19:59,209 - INFO - [diffusion][Epoch 10355] diffusion training Loss: 0.047123853117227554
2024-11-05 05:19:59,211 - INFO - [diffusion][Epoch 10355] diffusion learning rate: 0.001
2024-11-05 05:19:59,213 - INFO - [diffusion][Epoch 10355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:19:59,214 - INFO - [diffusion][Epoch 10356] Epoch 10357/12000
2024-11-05 05:20:03,369 - INFO - [diffusion][Epoch 10356] diffusion training Loss: 0.04896703641861677
2024-11-05 05:20:03,371 - INFO - [diffusion][Epoch 10356] diffusion learning rate: 0.001
2024-11-05 05:20:03,373 - INFO - [diffusion][Epoch 10356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:03,374 - INFO - [diffusion][Epoch 10357] Epoch 10358/12000
2024-11-05 05:20:07,469 - INFO - [diffusion][Epoch 10357] diffusion training Loss: 0.04646019823849201
2024-11-05 05:20:07,471 - INFO - [diffusion][Epoch 10357] diffusion learning rate: 0.001
2024-11-05 05:20:07,473 - INFO - [diffusion][Epoch 10357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:07,475 - INFO - [diffusion][Epoch 10358] Epoch 10359/12000
2024-11-05 05:20:11,567 - INFO - [diffusion][Epoch 10358] diffusion training Loss: 0.0518783163279295
2024-11-05 05:20:11,569 - INFO - [diffusion][Epoch 10358] diffusion learning rate: 0.001
2024-11-05 05:20:11,571 - INFO - [diffusion][Epoch 10358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:11,572 - INFO - [diffusion][Epoch 10359] Epoch 10360/12000
2024-11-05 05:20:15,712 - INFO - [diffusion][Epoch 10359] diffusion training Loss: 0.04788181558251381
2024-11-05 05:20:15,714 - INFO - [diffusion][Epoch 10359] diffusion learning rate: 0.001
2024-11-05 05:20:15,717 - INFO - [diffusion][Epoch 10359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:15,718 - INFO - [diffusion][Epoch 10360] Epoch 10361/12000
2024-11-05 05:20:19,670 - INFO - [diffusion][Epoch 10360] diffusion training Loss: 0.05128535255789757
2024-11-05 05:20:19,672 - INFO - [diffusion][Epoch 10360] diffusion learning rate: 0.001
2024-11-05 05:20:19,710 - INFO - [diffusion][Epoch 10360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:19,711 - INFO - [diffusion][Epoch 10361] Epoch 10362/12000
2024-11-05 05:20:23,780 - INFO - [diffusion][Epoch 10361] diffusion training Loss: 0.05159578286111355
2024-11-05 05:20:23,782 - INFO - [diffusion][Epoch 10361] diffusion learning rate: 0.001
2024-11-05 05:20:23,784 - INFO - [diffusion][Epoch 10361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:23,785 - INFO - [diffusion][Epoch 10362] Epoch 10363/12000
2024-11-05 05:20:27,883 - INFO - [diffusion][Epoch 10362] diffusion training Loss: 0.049323505721986294
2024-11-05 05:20:27,885 - INFO - [diffusion][Epoch 10362] diffusion learning rate: 0.001
2024-11-05 05:20:27,887 - INFO - [diffusion][Epoch 10362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:27,888 - INFO - [diffusion][Epoch 10363] Epoch 10364/12000
2024-11-05 05:20:31,982 - INFO - [diffusion][Epoch 10363] diffusion training Loss: 0.05111771076917648
2024-11-05 05:20:31,986 - INFO - [diffusion][Epoch 10363] diffusion learning rate: 0.001
2024-11-05 05:20:31,988 - INFO - [diffusion][Epoch 10363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:31,989 - INFO - [diffusion][Epoch 10364] Epoch 10365/12000
2024-11-05 05:20:36,055 - INFO - [diffusion][Epoch 10364] diffusion training Loss: 0.04809658229351044
2024-11-05 05:20:36,057 - INFO - [diffusion][Epoch 10364] diffusion learning rate: 0.001
2024-11-05 05:20:36,058 - INFO - [diffusion][Epoch 10364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:36,060 - INFO - [diffusion][Epoch 10365] Epoch 10366/12000
2024-11-05 05:20:40,160 - INFO - [diffusion][Epoch 10365] diffusion training Loss: 0.052687070332467556
2024-11-05 05:20:40,161 - INFO - [diffusion][Epoch 10365] diffusion learning rate: 0.001
2024-11-05 05:20:40,163 - INFO - [diffusion][Epoch 10365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:40,164 - INFO - [diffusion][Epoch 10366] Epoch 10367/12000
2024-11-05 05:20:44,251 - INFO - [diffusion][Epoch 10366] diffusion training Loss: 0.05463430844247341
2024-11-05 05:20:44,252 - INFO - [diffusion][Epoch 10366] diffusion learning rate: 0.001
2024-11-05 05:20:44,254 - INFO - [diffusion][Epoch 10366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:44,255 - INFO - [diffusion][Epoch 10367] Epoch 10368/12000
2024-11-05 05:20:48,340 - INFO - [diffusion][Epoch 10367] diffusion training Loss: 0.04761071875691414
2024-11-05 05:20:48,342 - INFO - [diffusion][Epoch 10367] diffusion learning rate: 0.001
2024-11-05 05:20:48,344 - INFO - [diffusion][Epoch 10367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:48,345 - INFO - [diffusion][Epoch 10368] Epoch 10369/12000
2024-11-05 05:20:52,423 - INFO - [diffusion][Epoch 10368] diffusion training Loss: 0.05208685249090195
2024-11-05 05:20:52,424 - INFO - [diffusion][Epoch 10368] diffusion learning rate: 0.001
2024-11-05 05:20:52,426 - INFO - [diffusion][Epoch 10368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:52,428 - INFO - [diffusion][Epoch 10369] Epoch 10370/12000
2024-11-05 05:20:56,494 - INFO - [diffusion][Epoch 10369] diffusion training Loss: 0.04721737187355757
2024-11-05 05:20:56,496 - INFO - [diffusion][Epoch 10369] diffusion learning rate: 0.001
2024-11-05 05:20:56,498 - INFO - [diffusion][Epoch 10369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:20:56,499 - INFO - [diffusion][Epoch 10370] Epoch 10371/12000
2024-11-05 05:21:00,576 - INFO - [diffusion][Epoch 10370] diffusion training Loss: 0.04908236674964428
2024-11-05 05:21:00,578 - INFO - [diffusion][Epoch 10370] diffusion learning rate: 0.001
2024-11-05 05:21:00,579 - INFO - [diffusion][Epoch 10370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:00,581 - INFO - [diffusion][Epoch 10371] Epoch 10372/12000
2024-11-05 05:21:04,628 - INFO - [diffusion][Epoch 10371] diffusion training Loss: 0.04737686738371849
2024-11-05 05:21:04,630 - INFO - [diffusion][Epoch 10371] diffusion learning rate: 0.001
2024-11-05 05:21:04,632 - INFO - [diffusion][Epoch 10371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:04,633 - INFO - [diffusion][Epoch 10372] Epoch 10373/12000
2024-11-05 05:21:08,735 - INFO - [diffusion][Epoch 10372] diffusion training Loss: 0.046249100007116795
2024-11-05 05:21:08,737 - INFO - [diffusion][Epoch 10372] diffusion learning rate: 0.001
2024-11-05 05:21:08,739 - INFO - [diffusion][Epoch 10372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:08,740 - INFO - [diffusion][Epoch 10373] Epoch 10374/12000
2024-11-05 05:21:12,850 - INFO - [diffusion][Epoch 10373] diffusion training Loss: 0.0513801546767354
2024-11-05 05:21:12,852 - INFO - [diffusion][Epoch 10373] diffusion learning rate: 0.001
2024-11-05 05:21:12,853 - INFO - [diffusion][Epoch 10373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:12,855 - INFO - [diffusion][Epoch 10374] Epoch 10375/12000
2024-11-05 05:21:16,820 - INFO - [diffusion][Epoch 10374] diffusion training Loss: 0.05107195768505335
2024-11-05 05:21:16,824 - INFO - [diffusion][Epoch 10374] diffusion learning rate: 0.001
2024-11-05 05:21:16,826 - INFO - [diffusion][Epoch 10374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:16,827 - INFO - [diffusion][Epoch 10375] Epoch 10376/12000
2024-11-05 05:21:20,858 - INFO - [diffusion][Epoch 10375] diffusion training Loss: 0.04902402311563492
2024-11-05 05:21:20,860 - INFO - [diffusion][Epoch 10375] diffusion learning rate: 0.001
2024-11-05 05:21:20,862 - INFO - [diffusion][Epoch 10375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:20,863 - INFO - [diffusion][Epoch 10376] Epoch 10377/12000
2024-11-05 05:21:24,936 - INFO - [diffusion][Epoch 10376] diffusion training Loss: 0.047466378659009933
2024-11-05 05:21:24,938 - INFO - [diffusion][Epoch 10376] diffusion learning rate: 0.001
2024-11-05 05:21:24,940 - INFO - [diffusion][Epoch 10376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:24,941 - INFO - [diffusion][Epoch 10377] Epoch 10378/12000
2024-11-05 05:21:29,022 - INFO - [diffusion][Epoch 10377] diffusion training Loss: 0.048259176313877106
2024-11-05 05:21:29,024 - INFO - [diffusion][Epoch 10377] diffusion learning rate: 0.001
2024-11-05 05:21:29,026 - INFO - [diffusion][Epoch 10377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:29,027 - INFO - [diffusion][Epoch 10378] Epoch 10379/12000
2024-11-05 05:21:33,114 - INFO - [diffusion][Epoch 10378] diffusion training Loss: 0.05378646403551102
2024-11-05 05:21:33,117 - INFO - [diffusion][Epoch 10378] diffusion learning rate: 0.001
2024-11-05 05:21:33,119 - INFO - [diffusion][Epoch 10378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:33,120 - INFO - [diffusion][Epoch 10379] Epoch 10380/12000
2024-11-05 05:21:37,214 - INFO - [diffusion][Epoch 10379] diffusion training Loss: 0.04773730970919132
2024-11-05 05:21:37,216 - INFO - [diffusion][Epoch 10379] diffusion learning rate: 0.001
2024-11-05 05:21:37,217 - INFO - [diffusion][Epoch 10379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:37,219 - INFO - [diffusion][Epoch 10380] Epoch 10381/12000
2024-11-05 05:21:41,581 - INFO - [diffusion][Epoch 10380] diffusion training Loss: 0.052060363814234734
2024-11-05 05:21:41,583 - INFO - [diffusion][Epoch 10380] diffusion learning rate: 0.001
2024-11-05 05:21:41,585 - INFO - [diffusion][Epoch 10380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:41,587 - INFO - [diffusion][Epoch 10381] Epoch 10382/12000
2024-11-05 05:21:45,642 - INFO - [diffusion][Epoch 10381] diffusion training Loss: 0.050074946135282516
2024-11-05 05:21:45,644 - INFO - [diffusion][Epoch 10381] diffusion learning rate: 0.001
2024-11-05 05:21:45,645 - INFO - [diffusion][Epoch 10381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:45,647 - INFO - [diffusion][Epoch 10382] Epoch 10383/12000
2024-11-05 05:21:49,761 - INFO - [diffusion][Epoch 10382] diffusion training Loss: 0.046555157750844955
2024-11-05 05:21:49,763 - INFO - [diffusion][Epoch 10382] diffusion learning rate: 0.001
2024-11-05 05:21:49,765 - INFO - [diffusion][Epoch 10382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:49,766 - INFO - [diffusion][Epoch 10383] Epoch 10384/12000
2024-11-05 05:21:53,818 - INFO - [diffusion][Epoch 10383] diffusion training Loss: 0.046378692612051964
2024-11-05 05:21:53,820 - INFO - [diffusion][Epoch 10383] diffusion learning rate: 0.001
2024-11-05 05:21:53,821 - INFO - [diffusion][Epoch 10383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:53,823 - INFO - [diffusion][Epoch 10384] Epoch 10385/12000
2024-11-05 05:21:57,900 - INFO - [diffusion][Epoch 10384] diffusion training Loss: 0.05116346850991249
2024-11-05 05:21:57,902 - INFO - [diffusion][Epoch 10384] diffusion learning rate: 0.001
2024-11-05 05:21:57,904 - INFO - [diffusion][Epoch 10384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:21:57,905 - INFO - [diffusion][Epoch 10385] Epoch 10386/12000
2024-11-05 05:22:02,010 - INFO - [diffusion][Epoch 10385] diffusion training Loss: 0.04520224500447512
2024-11-05 05:22:02,012 - INFO - [diffusion][Epoch 10385] diffusion learning rate: 0.001
2024-11-05 05:22:02,014 - INFO - [diffusion][Epoch 10385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:02,015 - INFO - [diffusion][Epoch 10386] Epoch 10387/12000
2024-11-05 05:22:06,128 - INFO - [diffusion][Epoch 10386] diffusion training Loss: 0.045030455105006695
2024-11-05 05:22:06,130 - INFO - [diffusion][Epoch 10386] diffusion learning rate: 0.001
2024-11-05 05:22:06,157 - INFO - [diffusion][Epoch 10386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:06,158 - INFO - [diffusion][Epoch 10387] Epoch 10388/12000
2024-11-05 05:22:10,249 - INFO - [diffusion][Epoch 10387] diffusion training Loss: 0.04462626110762358
2024-11-05 05:22:10,251 - INFO - [diffusion][Epoch 10387] diffusion learning rate: 0.001
2024-11-05 05:22:10,252 - INFO - [diffusion][Epoch 10387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:10,254 - INFO - [diffusion][Epoch 10388] Epoch 10389/12000
2024-11-05 05:22:14,354 - INFO - [diffusion][Epoch 10388] diffusion training Loss: 0.050266873091459274
2024-11-05 05:22:14,356 - INFO - [diffusion][Epoch 10388] diffusion learning rate: 0.001
2024-11-05 05:22:14,357 - INFO - [diffusion][Epoch 10388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:14,359 - INFO - [diffusion][Epoch 10389] Epoch 10390/12000
2024-11-05 05:22:18,456 - INFO - [diffusion][Epoch 10389] diffusion training Loss: 0.043453103862702847
2024-11-05 05:22:18,458 - INFO - [diffusion][Epoch 10389] diffusion learning rate: 0.001
2024-11-05 05:22:18,460 - INFO - [diffusion][Epoch 10389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:18,462 - INFO - [diffusion][Epoch 10390] Epoch 10391/12000
2024-11-05 05:22:22,540 - INFO - [diffusion][Epoch 10390] diffusion training Loss: 0.04813607223331928
2024-11-05 05:22:22,542 - INFO - [diffusion][Epoch 10390] diffusion learning rate: 0.001
2024-11-05 05:22:22,544 - INFO - [diffusion][Epoch 10390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:22,546 - INFO - [diffusion][Epoch 10391] Epoch 10392/12000
2024-11-05 05:22:26,635 - INFO - [diffusion][Epoch 10391] diffusion training Loss: 0.05053108558058739
2024-11-05 05:22:26,637 - INFO - [diffusion][Epoch 10391] diffusion learning rate: 0.001
2024-11-05 05:22:26,639 - INFO - [diffusion][Epoch 10391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:26,640 - INFO - [diffusion][Epoch 10392] Epoch 10393/12000
2024-11-05 05:22:30,735 - INFO - [diffusion][Epoch 10392] diffusion training Loss: 0.04698597360402346
2024-11-05 05:22:30,737 - INFO - [diffusion][Epoch 10392] diffusion learning rate: 0.001
2024-11-05 05:22:30,738 - INFO - [diffusion][Epoch 10392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:30,740 - INFO - [diffusion][Epoch 10393] Epoch 10394/12000
2024-11-05 05:22:35,054 - INFO - [diffusion][Epoch 10393] diffusion training Loss: 0.05009303707629442
2024-11-05 05:22:35,057 - INFO - [diffusion][Epoch 10393] diffusion learning rate: 0.001
2024-11-05 05:22:35,059 - INFO - [diffusion][Epoch 10393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:35,060 - INFO - [diffusion][Epoch 10394] Epoch 10395/12000
2024-11-05 05:22:39,143 - INFO - [diffusion][Epoch 10394] diffusion training Loss: 0.04663261491805315
2024-11-05 05:22:39,145 - INFO - [diffusion][Epoch 10394] diffusion learning rate: 0.001
2024-11-05 05:22:39,147 - INFO - [diffusion][Epoch 10394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:39,148 - INFO - [diffusion][Epoch 10395] Epoch 10396/12000
2024-11-05 05:22:43,235 - INFO - [diffusion][Epoch 10395] diffusion training Loss: 0.0477680591866374
2024-11-05 05:22:43,237 - INFO - [diffusion][Epoch 10395] diffusion learning rate: 0.001
2024-11-05 05:22:43,238 - INFO - [diffusion][Epoch 10395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:43,240 - INFO - [diffusion][Epoch 10396] Epoch 10397/12000
2024-11-05 05:22:47,297 - INFO - [diffusion][Epoch 10396] diffusion training Loss: 0.05265078693628311
2024-11-05 05:22:47,299 - INFO - [diffusion][Epoch 10396] diffusion learning rate: 0.001
2024-11-05 05:22:47,301 - INFO - [diffusion][Epoch 10396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:47,303 - INFO - [diffusion][Epoch 10397] Epoch 10398/12000
2024-11-05 05:22:51,334 - INFO - [diffusion][Epoch 10397] diffusion training Loss: 0.04490692354738712
2024-11-05 05:22:51,336 - INFO - [diffusion][Epoch 10397] diffusion learning rate: 0.001
2024-11-05 05:22:51,338 - INFO - [diffusion][Epoch 10397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:51,340 - INFO - [diffusion][Epoch 10398] Epoch 10399/12000
2024-11-05 05:22:55,213 - INFO - [diffusion][Epoch 10398] diffusion training Loss: 0.054435200057923794
2024-11-05 05:22:55,217 - INFO - [diffusion][Epoch 10398] diffusion learning rate: 0.001
2024-11-05 05:22:55,220 - INFO - [diffusion][Epoch 10398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:55,222 - INFO - [diffusion][Epoch 10399] Epoch 10400/12000
2024-11-05 05:22:59,318 - INFO - [diffusion][Epoch 10399] diffusion training Loss: 0.044866629876196384
2024-11-05 05:22:59,320 - INFO - [diffusion][Epoch 10399] diffusion learning rate: 0.001
2024-11-05 05:22:59,322 - INFO - [diffusion][Epoch 10399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:22:59,323 - INFO - [diffusion][Epoch 10400] Epoch 10401/12000
2024-11-05 05:23:03,417 - INFO - [diffusion][Epoch 10400] diffusion training Loss: 0.049479580484330654
2024-11-05 05:23:03,419 - INFO - [diffusion][Epoch 10400] diffusion learning rate: 0.001
2024-11-05 05:23:03,421 - INFO - [diffusion][Epoch 10400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:03,423 - INFO - [diffusion][Epoch 10401] Epoch 10402/12000
2024-11-05 05:23:07,512 - INFO - [diffusion][Epoch 10401] diffusion training Loss: 0.05173935741186142
2024-11-05 05:23:07,514 - INFO - [diffusion][Epoch 10401] diffusion learning rate: 0.001
2024-11-05 05:23:07,516 - INFO - [diffusion][Epoch 10401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:07,517 - INFO - [diffusion][Epoch 10402] Epoch 10403/12000
2024-11-05 05:23:11,629 - INFO - [diffusion][Epoch 10402] diffusion training Loss: 0.050017799250781536
2024-11-05 05:23:11,631 - INFO - [diffusion][Epoch 10402] diffusion learning rate: 0.001
2024-11-05 05:23:11,633 - INFO - [diffusion][Epoch 10402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:11,634 - INFO - [diffusion][Epoch 10403] Epoch 10404/12000
2024-11-05 05:23:15,734 - INFO - [diffusion][Epoch 10403] diffusion training Loss: 0.0541431549936533
2024-11-05 05:23:15,736 - INFO - [diffusion][Epoch 10403] diffusion learning rate: 0.001
2024-11-05 05:23:15,738 - INFO - [diffusion][Epoch 10403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:15,739 - INFO - [diffusion][Epoch 10404] Epoch 10405/12000
2024-11-05 05:23:19,811 - INFO - [diffusion][Epoch 10404] diffusion training Loss: 0.04790295474231243
2024-11-05 05:23:19,814 - INFO - [diffusion][Epoch 10404] diffusion learning rate: 0.001
2024-11-05 05:23:19,816 - INFO - [diffusion][Epoch 10404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:19,817 - INFO - [diffusion][Epoch 10405] Epoch 10406/12000
2024-11-05 05:23:23,951 - INFO - [diffusion][Epoch 10405] diffusion training Loss: 0.05159103777259588
2024-11-05 05:23:23,952 - INFO - [diffusion][Epoch 10405] diffusion learning rate: 0.001
2024-11-05 05:23:23,954 - INFO - [diffusion][Epoch 10405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:23,955 - INFO - [diffusion][Epoch 10406] Epoch 10407/12000
2024-11-05 05:23:28,010 - INFO - [diffusion][Epoch 10406] diffusion training Loss: 0.04796729050576687
2024-11-05 05:23:28,012 - INFO - [diffusion][Epoch 10406] diffusion learning rate: 0.001
2024-11-05 05:23:28,014 - INFO - [diffusion][Epoch 10406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:28,015 - INFO - [diffusion][Epoch 10407] Epoch 10408/12000
2024-11-05 05:23:32,070 - INFO - [diffusion][Epoch 10407] diffusion training Loss: 0.052200084552168846
2024-11-05 05:23:32,072 - INFO - [diffusion][Epoch 10407] diffusion learning rate: 0.001
2024-11-05 05:23:32,074 - INFO - [diffusion][Epoch 10407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:32,076 - INFO - [diffusion][Epoch 10408] Epoch 10409/12000
2024-11-05 05:23:36,138 - INFO - [diffusion][Epoch 10408] diffusion training Loss: 0.04936855658888817
2024-11-05 05:23:36,142 - INFO - [diffusion][Epoch 10408] diffusion learning rate: 0.001
2024-11-05 05:23:36,144 - INFO - [diffusion][Epoch 10408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:36,145 - INFO - [diffusion][Epoch 10409] Epoch 10410/12000
2024-11-05 05:23:40,212 - INFO - [diffusion][Epoch 10409] diffusion training Loss: 0.047940876334905624
2024-11-05 05:23:40,214 - INFO - [diffusion][Epoch 10409] diffusion learning rate: 0.001
2024-11-05 05:23:40,216 - INFO - [diffusion][Epoch 10409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:40,217 - INFO - [diffusion][Epoch 10410] Epoch 10411/12000
2024-11-05 05:23:44,293 - INFO - [diffusion][Epoch 10410] diffusion training Loss: 0.04636364243924618
2024-11-05 05:23:44,295 - INFO - [diffusion][Epoch 10410] diffusion learning rate: 0.001
2024-11-05 05:23:44,297 - INFO - [diffusion][Epoch 10410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:44,298 - INFO - [diffusion][Epoch 10411] Epoch 10412/12000
2024-11-05 05:23:48,404 - INFO - [diffusion][Epoch 10411] diffusion training Loss: 0.047272724099457264
2024-11-05 05:23:48,406 - INFO - [diffusion][Epoch 10411] diffusion learning rate: 0.001
2024-11-05 05:23:48,408 - INFO - [diffusion][Epoch 10411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:48,409 - INFO - [diffusion][Epoch 10412] Epoch 10413/12000
2024-11-05 05:23:52,700 - INFO - [diffusion][Epoch 10412] diffusion training Loss: 0.04771795030683279
2024-11-05 05:23:52,702 - INFO - [diffusion][Epoch 10412] diffusion learning rate: 0.001
2024-11-05 05:23:52,704 - INFO - [diffusion][Epoch 10412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:52,705 - INFO - [diffusion][Epoch 10413] Epoch 10414/12000
2024-11-05 05:23:56,766 - INFO - [diffusion][Epoch 10413] diffusion training Loss: 0.04585431516170502
2024-11-05 05:23:56,768 - INFO - [diffusion][Epoch 10413] diffusion learning rate: 0.001
2024-11-05 05:23:56,770 - INFO - [diffusion][Epoch 10413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:23:56,771 - INFO - [diffusion][Epoch 10414] Epoch 10415/12000
2024-11-05 05:24:00,826 - INFO - [diffusion][Epoch 10414] diffusion training Loss: 0.04713236354291439
2024-11-05 05:24:00,828 - INFO - [diffusion][Epoch 10414] diffusion learning rate: 0.001
2024-11-05 05:24:00,830 - INFO - [diffusion][Epoch 10414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:00,831 - INFO - [diffusion][Epoch 10415] Epoch 10416/12000
2024-11-05 05:24:04,917 - INFO - [diffusion][Epoch 10415] diffusion training Loss: 0.047141666524112225
2024-11-05 05:24:04,919 - INFO - [diffusion][Epoch 10415] diffusion learning rate: 0.001
2024-11-05 05:24:04,921 - INFO - [diffusion][Epoch 10415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:04,922 - INFO - [diffusion][Epoch 10416] Epoch 10417/12000
2024-11-05 05:24:08,989 - INFO - [diffusion][Epoch 10416] diffusion training Loss: 0.04532578308135271
2024-11-05 05:24:08,991 - INFO - [diffusion][Epoch 10416] diffusion learning rate: 0.001
2024-11-05 05:24:08,993 - INFO - [diffusion][Epoch 10416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:08,994 - INFO - [diffusion][Epoch 10417] Epoch 10418/12000
2024-11-05 05:24:13,092 - INFO - [diffusion][Epoch 10417] diffusion training Loss: 0.049205210991203785
2024-11-05 05:24:13,182 - INFO - [diffusion][Epoch 10417] diffusion learning rate: 0.001
2024-11-05 05:24:13,184 - INFO - [diffusion][Epoch 10417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:13,185 - INFO - [diffusion][Epoch 10418] Epoch 10419/12000
2024-11-05 05:24:17,272 - INFO - [diffusion][Epoch 10418] diffusion training Loss: 0.050467029213905334
2024-11-05 05:24:17,274 - INFO - [diffusion][Epoch 10418] diffusion learning rate: 0.001
2024-11-05 05:24:17,276 - INFO - [diffusion][Epoch 10418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:17,277 - INFO - [diffusion][Epoch 10419] Epoch 10420/12000
2024-11-05 05:24:21,372 - INFO - [diffusion][Epoch 10419] diffusion training Loss: 0.05006104428321123
2024-11-05 05:24:21,374 - INFO - [diffusion][Epoch 10419] diffusion learning rate: 0.001
2024-11-05 05:24:21,376 - INFO - [diffusion][Epoch 10419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:21,377 - INFO - [diffusion][Epoch 10420] Epoch 10421/12000
2024-11-05 05:24:25,469 - INFO - [diffusion][Epoch 10420] diffusion training Loss: 0.04516003653407097
2024-11-05 05:24:25,471 - INFO - [diffusion][Epoch 10420] diffusion learning rate: 0.001
2024-11-05 05:24:25,472 - INFO - [diffusion][Epoch 10420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:25,474 - INFO - [diffusion][Epoch 10421] Epoch 10422/12000
2024-11-05 05:24:29,506 - INFO - [diffusion][Epoch 10421] diffusion training Loss: 0.04455544054508209
2024-11-05 05:24:29,508 - INFO - [diffusion][Epoch 10421] diffusion learning rate: 0.001
2024-11-05 05:24:29,509 - INFO - [diffusion][Epoch 10421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:29,511 - INFO - [diffusion][Epoch 10422] Epoch 10423/12000
2024-11-05 05:24:33,609 - INFO - [diffusion][Epoch 10422] diffusion training Loss: 0.04708579555153847
2024-11-05 05:24:33,611 - INFO - [diffusion][Epoch 10422] diffusion learning rate: 0.001
2024-11-05 05:24:33,613 - INFO - [diffusion][Epoch 10422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:33,614 - INFO - [diffusion][Epoch 10423] Epoch 10424/12000
2024-11-05 05:24:37,690 - INFO - [diffusion][Epoch 10423] diffusion training Loss: 0.05234407540410757
2024-11-05 05:24:37,694 - INFO - [diffusion][Epoch 10423] diffusion learning rate: 0.001
2024-11-05 05:24:37,696 - INFO - [diffusion][Epoch 10423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:37,699 - INFO - [diffusion][Epoch 10424] Epoch 10425/12000
2024-11-05 05:24:41,762 - INFO - [diffusion][Epoch 10424] diffusion training Loss: 0.04986138083040714
2024-11-05 05:24:41,764 - INFO - [diffusion][Epoch 10424] diffusion learning rate: 0.001
2024-11-05 05:24:41,766 - INFO - [diffusion][Epoch 10424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:41,767 - INFO - [diffusion][Epoch 10425] Epoch 10426/12000
2024-11-05 05:24:45,878 - INFO - [diffusion][Epoch 10425] diffusion training Loss: 0.04642513580620289
2024-11-05 05:24:45,880 - INFO - [diffusion][Epoch 10425] diffusion learning rate: 0.001
2024-11-05 05:24:45,882 - INFO - [diffusion][Epoch 10425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:45,883 - INFO - [diffusion][Epoch 10426] Epoch 10427/12000
2024-11-05 05:24:49,848 - INFO - [diffusion][Epoch 10426] diffusion training Loss: 0.04907162953168154
2024-11-05 05:24:49,850 - INFO - [diffusion][Epoch 10426] diffusion learning rate: 0.001
2024-11-05 05:24:49,852 - INFO - [diffusion][Epoch 10426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:49,853 - INFO - [diffusion][Epoch 10427] Epoch 10428/12000
2024-11-05 05:24:53,913 - INFO - [diffusion][Epoch 10427] diffusion training Loss: 0.04847289901226759
2024-11-05 05:24:53,915 - INFO - [diffusion][Epoch 10427] diffusion learning rate: 0.001
2024-11-05 05:24:53,916 - INFO - [diffusion][Epoch 10427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:53,918 - INFO - [diffusion][Epoch 10428] Epoch 10429/12000
2024-11-05 05:24:58,005 - INFO - [diffusion][Epoch 10428] diffusion training Loss: 0.04591135587543249
2024-11-05 05:24:58,007 - INFO - [diffusion][Epoch 10428] diffusion learning rate: 0.001
2024-11-05 05:24:58,009 - INFO - [diffusion][Epoch 10428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:24:58,010 - INFO - [diffusion][Epoch 10429] Epoch 10430/12000
2024-11-05 05:25:02,075 - INFO - [diffusion][Epoch 10429] diffusion training Loss: 0.0482745636254549
2024-11-05 05:25:02,076 - INFO - [diffusion][Epoch 10429] diffusion learning rate: 0.001
2024-11-05 05:25:02,078 - INFO - [diffusion][Epoch 10429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:02,079 - INFO - [diffusion][Epoch 10430] Epoch 10431/12000
2024-11-05 05:25:06,172 - INFO - [diffusion][Epoch 10430] diffusion training Loss: 0.04724991787225008
2024-11-05 05:25:06,174 - INFO - [diffusion][Epoch 10430] diffusion learning rate: 0.001
2024-11-05 05:25:06,176 - INFO - [diffusion][Epoch 10430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:06,178 - INFO - [diffusion][Epoch 10431] Epoch 10432/12000
2024-11-05 05:25:10,309 - INFO - [diffusion][Epoch 10431] diffusion training Loss: 0.04790610074996948
2024-11-05 05:25:10,311 - INFO - [diffusion][Epoch 10431] diffusion learning rate: 0.001
2024-11-05 05:25:10,312 - INFO - [diffusion][Epoch 10431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:10,314 - INFO - [diffusion][Epoch 10432] Epoch 10433/12000
2024-11-05 05:25:14,718 - INFO - [diffusion][Epoch 10432] diffusion training Loss: 0.04758589807897806
2024-11-05 05:25:14,720 - INFO - [diffusion][Epoch 10432] diffusion learning rate: 0.001
2024-11-05 05:25:14,722 - INFO - [diffusion][Epoch 10432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:14,723 - INFO - [diffusion][Epoch 10433] Epoch 10434/12000
2024-11-05 05:25:18,794 - INFO - [diffusion][Epoch 10433] diffusion training Loss: 0.051454718224704266
2024-11-05 05:25:18,796 - INFO - [diffusion][Epoch 10433] diffusion learning rate: 0.001
2024-11-05 05:25:18,798 - INFO - [diffusion][Epoch 10433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:18,799 - INFO - [diffusion][Epoch 10434] Epoch 10435/12000
2024-11-05 05:25:22,993 - INFO - [diffusion][Epoch 10434] diffusion training Loss: 0.04904473014175892
2024-11-05 05:25:22,995 - INFO - [diffusion][Epoch 10434] diffusion learning rate: 0.001
2024-11-05 05:25:22,997 - INFO - [diffusion][Epoch 10434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:22,999 - INFO - [diffusion][Epoch 10435] Epoch 10436/12000
2024-11-05 05:25:27,143 - INFO - [diffusion][Epoch 10435] diffusion training Loss: 0.050521877594292164
2024-11-05 05:25:27,144 - INFO - [diffusion][Epoch 10435] diffusion learning rate: 0.001
2024-11-05 05:25:27,146 - INFO - [diffusion][Epoch 10435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:27,147 - INFO - [diffusion][Epoch 10436] Epoch 10437/12000
2024-11-05 05:25:31,263 - INFO - [diffusion][Epoch 10436] diffusion training Loss: 0.0500048091635108
2024-11-05 05:25:31,265 - INFO - [diffusion][Epoch 10436] diffusion learning rate: 0.001
2024-11-05 05:25:31,267 - INFO - [diffusion][Epoch 10436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:31,269 - INFO - [diffusion][Epoch 10437] Epoch 10438/12000
2024-11-05 05:25:35,395 - INFO - [diffusion][Epoch 10437] diffusion training Loss: 0.0514316838234663
2024-11-05 05:25:35,397 - INFO - [diffusion][Epoch 10437] diffusion learning rate: 0.001
2024-11-05 05:25:35,399 - INFO - [diffusion][Epoch 10437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:35,400 - INFO - [diffusion][Epoch 10438] Epoch 10439/12000
2024-11-05 05:25:39,477 - INFO - [diffusion][Epoch 10438] diffusion training Loss: 0.04535242076963186
2024-11-05 05:25:39,480 - INFO - [diffusion][Epoch 10438] diffusion learning rate: 0.001
2024-11-05 05:25:39,482 - INFO - [diffusion][Epoch 10438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:39,484 - INFO - [diffusion][Epoch 10439] Epoch 10440/12000
2024-11-05 05:25:43,565 - INFO - [diffusion][Epoch 10439] diffusion training Loss: 0.04449087614193559
2024-11-05 05:25:43,567 - INFO - [diffusion][Epoch 10439] diffusion learning rate: 0.001
2024-11-05 05:25:43,568 - INFO - [diffusion][Epoch 10439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:43,570 - INFO - [diffusion][Epoch 10440] Epoch 10441/12000
2024-11-05 05:25:47,702 - INFO - [diffusion][Epoch 10440] diffusion training Loss: 0.05303306691348553
2024-11-05 05:25:47,704 - INFO - [diffusion][Epoch 10440] diffusion learning rate: 0.001
2024-11-05 05:25:47,705 - INFO - [diffusion][Epoch 10440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:47,707 - INFO - [diffusion][Epoch 10441] Epoch 10442/12000
2024-11-05 05:25:51,801 - INFO - [diffusion][Epoch 10441] diffusion training Loss: 0.043636882677674294
2024-11-05 05:25:51,803 - INFO - [diffusion][Epoch 10441] diffusion learning rate: 0.001
2024-11-05 05:25:51,805 - INFO - [diffusion][Epoch 10441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:51,806 - INFO - [diffusion][Epoch 10442] Epoch 10443/12000
2024-11-05 05:25:55,899 - INFO - [diffusion][Epoch 10442] diffusion training Loss: 0.050587612204253674
2024-11-05 05:25:55,900 - INFO - [diffusion][Epoch 10442] diffusion learning rate: 0.001
2024-11-05 05:25:55,928 - INFO - [diffusion][Epoch 10442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:25:55,929 - INFO - [diffusion][Epoch 10443] Epoch 10444/12000
2024-11-05 05:26:00,034 - INFO - [diffusion][Epoch 10443] diffusion training Loss: 0.05048801936209202
2024-11-05 05:26:00,036 - INFO - [diffusion][Epoch 10443] diffusion learning rate: 0.001
2024-11-05 05:26:00,038 - INFO - [diffusion][Epoch 10443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:00,039 - INFO - [diffusion][Epoch 10444] Epoch 10445/12000
2024-11-05 05:26:04,160 - INFO - [diffusion][Epoch 10444] diffusion training Loss: 0.04706067219376564
2024-11-05 05:26:04,162 - INFO - [diffusion][Epoch 10444] diffusion learning rate: 0.001
2024-11-05 05:26:04,164 - INFO - [diffusion][Epoch 10444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:04,165 - INFO - [diffusion][Epoch 10445] Epoch 10446/12000
2024-11-05 05:26:08,293 - INFO - [diffusion][Epoch 10445] diffusion training Loss: 0.04677358642220497
2024-11-05 05:26:08,295 - INFO - [diffusion][Epoch 10445] diffusion learning rate: 0.001
2024-11-05 05:26:08,297 - INFO - [diffusion][Epoch 10445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:08,298 - INFO - [diffusion][Epoch 10446] Epoch 10447/12000
2024-11-05 05:26:12,414 - INFO - [diffusion][Epoch 10446] diffusion training Loss: 0.049663837999105453
2024-11-05 05:26:12,416 - INFO - [diffusion][Epoch 10446] diffusion learning rate: 0.001
2024-11-05 05:26:12,418 - INFO - [diffusion][Epoch 10446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:12,419 - INFO - [diffusion][Epoch 10447] Epoch 10448/12000
2024-11-05 05:26:16,553 - INFO - [diffusion][Epoch 10447] diffusion training Loss: 0.04655889701098204
2024-11-05 05:26:16,556 - INFO - [diffusion][Epoch 10447] diffusion learning rate: 0.001
2024-11-05 05:26:16,557 - INFO - [diffusion][Epoch 10447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:16,559 - INFO - [diffusion][Epoch 10448] Epoch 10449/12000
2024-11-05 05:26:20,669 - INFO - [diffusion][Epoch 10448] diffusion training Loss: 0.05216402839869261
2024-11-05 05:26:20,670 - INFO - [diffusion][Epoch 10448] diffusion learning rate: 0.001
2024-11-05 05:26:20,672 - INFO - [diffusion][Epoch 10448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:20,673 - INFO - [diffusion][Epoch 10449] Epoch 10450/12000
2024-11-05 05:26:24,754 - INFO - [diffusion][Epoch 10449] diffusion training Loss: 0.04361061006784439
2024-11-05 05:26:24,756 - INFO - [diffusion][Epoch 10449] diffusion learning rate: 0.001
2024-11-05 05:26:24,758 - INFO - [diffusion][Epoch 10449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:24,759 - INFO - [diffusion][Epoch 10450] Epoch 10451/12000
2024-11-05 05:26:28,863 - INFO - [diffusion][Epoch 10450] diffusion training Loss: 0.04802447929978371
2024-11-05 05:26:28,865 - INFO - [diffusion][Epoch 10450] diffusion learning rate: 0.001
2024-11-05 05:26:28,885 - INFO - [diffusion][Epoch 10450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:28,886 - INFO - [diffusion][Epoch 10451] Epoch 10452/12000
2024-11-05 05:26:32,986 - INFO - [diffusion][Epoch 10451] diffusion training Loss: 0.04712132178246975
2024-11-05 05:26:32,988 - INFO - [diffusion][Epoch 10451] diffusion learning rate: 0.001
2024-11-05 05:26:32,990 - INFO - [diffusion][Epoch 10451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:32,991 - INFO - [diffusion][Epoch 10452] Epoch 10453/12000
2024-11-05 05:26:37,099 - INFO - [diffusion][Epoch 10452] diffusion training Loss: 0.05411417409777641
2024-11-05 05:26:37,101 - INFO - [diffusion][Epoch 10452] diffusion learning rate: 0.001
2024-11-05 05:26:37,102 - INFO - [diffusion][Epoch 10452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:37,103 - INFO - [diffusion][Epoch 10453] Epoch 10454/12000
2024-11-05 05:26:41,249 - INFO - [diffusion][Epoch 10453] diffusion training Loss: 0.05099212285131216
2024-11-05 05:26:41,252 - INFO - [diffusion][Epoch 10453] diffusion learning rate: 0.001
2024-11-05 05:26:41,254 - INFO - [diffusion][Epoch 10453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:41,255 - INFO - [diffusion][Epoch 10454] Epoch 10455/12000
2024-11-05 05:26:45,347 - INFO - [diffusion][Epoch 10454] diffusion training Loss: 0.050848472863435745
2024-11-05 05:26:45,349 - INFO - [diffusion][Epoch 10454] diffusion learning rate: 0.001
2024-11-05 05:26:45,351 - INFO - [diffusion][Epoch 10454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:45,352 - INFO - [diffusion][Epoch 10455] Epoch 10456/12000
2024-11-05 05:26:49,437 - INFO - [diffusion][Epoch 10455] diffusion training Loss: 0.0530503960326314
2024-11-05 05:26:49,439 - INFO - [diffusion][Epoch 10455] diffusion learning rate: 0.001
2024-11-05 05:26:49,441 - INFO - [diffusion][Epoch 10455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:49,442 - INFO - [diffusion][Epoch 10456] Epoch 10457/12000
2024-11-05 05:26:53,538 - INFO - [diffusion][Epoch 10456] diffusion training Loss: 0.05141525063663721
2024-11-05 05:26:53,540 - INFO - [diffusion][Epoch 10456] diffusion learning rate: 0.001
2024-11-05 05:26:53,542 - INFO - [diffusion][Epoch 10456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:53,543 - INFO - [diffusion][Epoch 10457] Epoch 10458/12000
2024-11-05 05:26:57,623 - INFO - [diffusion][Epoch 10457] diffusion training Loss: 0.04774294886738062
2024-11-05 05:26:57,625 - INFO - [diffusion][Epoch 10457] diffusion learning rate: 0.001
2024-11-05 05:26:57,652 - INFO - [diffusion][Epoch 10457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:26:57,653 - INFO - [diffusion][Epoch 10458] Epoch 10459/12000
2024-11-05 05:27:01,772 - INFO - [diffusion][Epoch 10458] diffusion training Loss: 0.05251121986657381
2024-11-05 05:27:01,774 - INFO - [diffusion][Epoch 10458] diffusion learning rate: 0.001
2024-11-05 05:27:01,775 - INFO - [diffusion][Epoch 10458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:01,777 - INFO - [diffusion][Epoch 10459] Epoch 10460/12000
2024-11-05 05:27:05,839 - INFO - [diffusion][Epoch 10459] diffusion training Loss: 0.0486880149692297
2024-11-05 05:27:05,841 - INFO - [diffusion][Epoch 10459] diffusion learning rate: 0.001
2024-11-05 05:27:05,843 - INFO - [diffusion][Epoch 10459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:05,844 - INFO - [diffusion][Epoch 10460] Epoch 10461/12000
2024-11-05 05:27:09,929 - INFO - [diffusion][Epoch 10460] diffusion training Loss: 0.044691817834973335
2024-11-05 05:27:09,931 - INFO - [diffusion][Epoch 10460] diffusion learning rate: 0.001
2024-11-05 05:27:09,958 - INFO - [diffusion][Epoch 10460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:09,959 - INFO - [diffusion][Epoch 10461] Epoch 10462/12000
2024-11-05 05:27:14,050 - INFO - [diffusion][Epoch 10461] diffusion training Loss: 0.04779417812824249
2024-11-05 05:27:14,052 - INFO - [diffusion][Epoch 10461] diffusion learning rate: 0.001
2024-11-05 05:27:14,054 - INFO - [diffusion][Epoch 10461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:14,055 - INFO - [diffusion][Epoch 10462] Epoch 10463/12000
2024-11-05 05:27:18,207 - INFO - [diffusion][Epoch 10462] diffusion training Loss: 0.04794139973819256
2024-11-05 05:27:18,209 - INFO - [diffusion][Epoch 10462] diffusion learning rate: 0.001
2024-11-05 05:27:18,211 - INFO - [diffusion][Epoch 10462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:18,212 - INFO - [diffusion][Epoch 10463] Epoch 10464/12000
2024-11-05 05:27:22,384 - INFO - [diffusion][Epoch 10463] diffusion training Loss: 0.04767918214201927
2024-11-05 05:27:22,386 - INFO - [diffusion][Epoch 10463] diffusion learning rate: 0.001
2024-11-05 05:27:22,388 - INFO - [diffusion][Epoch 10463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:22,389 - INFO - [diffusion][Epoch 10464] Epoch 10465/12000
2024-11-05 05:27:26,429 - INFO - [diffusion][Epoch 10464] diffusion training Loss: 0.0459298575296998
2024-11-05 05:27:26,431 - INFO - [diffusion][Epoch 10464] diffusion learning rate: 0.001
2024-11-05 05:27:26,432 - INFO - [diffusion][Epoch 10464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:26,434 - INFO - [diffusion][Epoch 10465] Epoch 10466/12000
2024-11-05 05:27:30,461 - INFO - [diffusion][Epoch 10465] diffusion training Loss: 0.04333687946200371
2024-11-05 05:27:30,463 - INFO - [diffusion][Epoch 10465] diffusion learning rate: 0.001
2024-11-05 05:27:30,465 - INFO - [diffusion][Epoch 10465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:30,466 - INFO - [diffusion][Epoch 10466] Epoch 10467/12000
2024-11-05 05:27:34,560 - INFO - [diffusion][Epoch 10466] diffusion training Loss: 0.05133245512843132
2024-11-05 05:27:34,562 - INFO - [diffusion][Epoch 10466] diffusion learning rate: 0.001
2024-11-05 05:27:34,564 - INFO - [diffusion][Epoch 10466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:34,565 - INFO - [diffusion][Epoch 10467] Epoch 10468/12000
2024-11-05 05:27:38,653 - INFO - [diffusion][Epoch 10467] diffusion training Loss: 0.049446335062384605
2024-11-05 05:27:38,655 - INFO - [diffusion][Epoch 10467] diffusion learning rate: 0.001
2024-11-05 05:27:38,657 - INFO - [diffusion][Epoch 10467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:38,658 - INFO - [diffusion][Epoch 10468] Epoch 10469/12000
2024-11-05 05:27:42,749 - INFO - [diffusion][Epoch 10468] diffusion training Loss: 0.04853840917348862
2024-11-05 05:27:42,753 - INFO - [diffusion][Epoch 10468] diffusion learning rate: 0.001
2024-11-05 05:27:42,754 - INFO - [diffusion][Epoch 10468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:42,755 - INFO - [diffusion][Epoch 10469] Epoch 10470/12000
2024-11-05 05:27:46,694 - INFO - [diffusion][Epoch 10469] diffusion training Loss: 0.05099070258438587
2024-11-05 05:27:46,696 - INFO - [diffusion][Epoch 10469] diffusion learning rate: 0.001
2024-11-05 05:27:46,724 - INFO - [diffusion][Epoch 10469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:46,725 - INFO - [diffusion][Epoch 10470] Epoch 10471/12000
2024-11-05 05:27:50,823 - INFO - [diffusion][Epoch 10470] diffusion training Loss: 0.04762324597686529
2024-11-05 05:27:50,825 - INFO - [diffusion][Epoch 10470] diffusion learning rate: 0.001
2024-11-05 05:27:50,826 - INFO - [diffusion][Epoch 10470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:50,828 - INFO - [diffusion][Epoch 10471] Epoch 10472/12000
2024-11-05 05:27:54,956 - INFO - [diffusion][Epoch 10471] diffusion training Loss: 0.046817585825920105
2024-11-05 05:27:54,958 - INFO - [diffusion][Epoch 10471] diffusion learning rate: 0.001
2024-11-05 05:27:54,960 - INFO - [diffusion][Epoch 10471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:54,961 - INFO - [diffusion][Epoch 10472] Epoch 10473/12000
2024-11-05 05:27:59,059 - INFO - [diffusion][Epoch 10472] diffusion training Loss: 0.047164843417704105
2024-11-05 05:27:59,061 - INFO - [diffusion][Epoch 10472] diffusion learning rate: 0.001
2024-11-05 05:27:59,062 - INFO - [diffusion][Epoch 10472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:27:59,064 - INFO - [diffusion][Epoch 10473] Epoch 10474/12000
2024-11-05 05:28:03,625 - INFO - [diffusion][Epoch 10473] diffusion training Loss: 0.0521460585296154
2024-11-05 05:28:03,627 - INFO - [diffusion][Epoch 10473] diffusion learning rate: 0.001
2024-11-05 05:28:03,629 - INFO - [diffusion][Epoch 10473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:03,630 - INFO - [diffusion][Epoch 10474] Epoch 10475/12000
2024-11-05 05:28:07,751 - INFO - [diffusion][Epoch 10474] diffusion training Loss: 0.05025036633014679
2024-11-05 05:28:07,753 - INFO - [diffusion][Epoch 10474] diffusion learning rate: 0.001
2024-11-05 05:28:07,754 - INFO - [diffusion][Epoch 10474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:07,756 - INFO - [diffusion][Epoch 10475] Epoch 10476/12000
2024-11-05 05:28:11,836 - INFO - [diffusion][Epoch 10475] diffusion training Loss: 0.051676527597010136
2024-11-05 05:28:11,838 - INFO - [diffusion][Epoch 10475] diffusion learning rate: 0.001
2024-11-05 05:28:11,840 - INFO - [diffusion][Epoch 10475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:11,841 - INFO - [diffusion][Epoch 10476] Epoch 10477/12000
2024-11-05 05:28:15,998 - INFO - [diffusion][Epoch 10476] diffusion training Loss: 0.05066596157848835
2024-11-05 05:28:16,000 - INFO - [diffusion][Epoch 10476] diffusion learning rate: 0.001
2024-11-05 05:28:16,028 - INFO - [diffusion][Epoch 10476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:16,029 - INFO - [diffusion][Epoch 10477] Epoch 10478/12000
2024-11-05 05:28:20,174 - INFO - [diffusion][Epoch 10477] diffusion training Loss: 0.05566265620291233
2024-11-05 05:28:20,176 - INFO - [diffusion][Epoch 10477] diffusion learning rate: 0.001
2024-11-05 05:28:20,178 - INFO - [diffusion][Epoch 10477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:20,179 - INFO - [diffusion][Epoch 10478] Epoch 10479/12000
2024-11-05 05:28:24,269 - INFO - [diffusion][Epoch 10478] diffusion training Loss: 0.05193126481026411
2024-11-05 05:28:24,271 - INFO - [diffusion][Epoch 10478] diffusion learning rate: 0.001
2024-11-05 05:28:24,273 - INFO - [diffusion][Epoch 10478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:24,274 - INFO - [diffusion][Epoch 10479] Epoch 10480/12000
2024-11-05 05:28:28,346 - INFO - [diffusion][Epoch 10479] diffusion training Loss: 0.04503932688385248
2024-11-05 05:28:28,347 - INFO - [diffusion][Epoch 10479] diffusion learning rate: 0.001
2024-11-05 05:28:28,349 - INFO - [diffusion][Epoch 10479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:28,350 - INFO - [diffusion][Epoch 10480] Epoch 10481/12000
2024-11-05 05:28:32,306 - INFO - [diffusion][Epoch 10480] diffusion training Loss: 0.04822626896202564
2024-11-05 05:28:32,309 - INFO - [diffusion][Epoch 10480] diffusion learning rate: 0.001
2024-11-05 05:28:32,311 - INFO - [diffusion][Epoch 10480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:32,313 - INFO - [diffusion][Epoch 10481] Epoch 10482/12000
2024-11-05 05:28:36,404 - INFO - [diffusion][Epoch 10481] diffusion training Loss: 0.051767329685389996
2024-11-05 05:28:36,406 - INFO - [diffusion][Epoch 10481] diffusion learning rate: 0.001
2024-11-05 05:28:36,408 - INFO - [diffusion][Epoch 10481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:36,409 - INFO - [diffusion][Epoch 10482] Epoch 10483/12000
2024-11-05 05:28:40,558 - INFO - [diffusion][Epoch 10482] diffusion training Loss: 0.04585914220660925
2024-11-05 05:28:40,560 - INFO - [diffusion][Epoch 10482] diffusion learning rate: 0.001
2024-11-05 05:28:40,561 - INFO - [diffusion][Epoch 10482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:40,563 - INFO - [diffusion][Epoch 10483] Epoch 10484/12000
2024-11-05 05:28:44,550 - INFO - [diffusion][Epoch 10483] diffusion training Loss: 0.046124232932925224
2024-11-05 05:28:44,553 - INFO - [diffusion][Epoch 10483] diffusion learning rate: 0.001
2024-11-05 05:28:44,555 - INFO - [diffusion][Epoch 10483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:44,556 - INFO - [diffusion][Epoch 10484] Epoch 10485/12000
2024-11-05 05:28:48,403 - INFO - [diffusion][Epoch 10484] diffusion training Loss: 0.050941454246640205
2024-11-05 05:28:48,405 - INFO - [diffusion][Epoch 10484] diffusion learning rate: 0.001
2024-11-05 05:28:48,408 - INFO - [diffusion][Epoch 10484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:48,409 - INFO - [diffusion][Epoch 10485] Epoch 10486/12000
2024-11-05 05:28:52,501 - INFO - [diffusion][Epoch 10485] diffusion training Loss: 0.05603659711778164
2024-11-05 05:28:52,503 - INFO - [diffusion][Epoch 10485] diffusion learning rate: 0.001
2024-11-05 05:28:52,532 - INFO - [diffusion][Epoch 10485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:52,533 - INFO - [diffusion][Epoch 10486] Epoch 10487/12000
2024-11-05 05:28:56,632 - INFO - [diffusion][Epoch 10486] diffusion training Loss: 0.05187690909951925
2024-11-05 05:28:56,634 - INFO - [diffusion][Epoch 10486] diffusion learning rate: 0.001
2024-11-05 05:28:56,635 - INFO - [diffusion][Epoch 10486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:28:56,637 - INFO - [diffusion][Epoch 10487] Epoch 10488/12000
2024-11-05 05:29:00,601 - INFO - [diffusion][Epoch 10487] diffusion training Loss: 0.044475773349404335
2024-11-05 05:29:00,603 - INFO - [diffusion][Epoch 10487] diffusion learning rate: 0.001
2024-11-05 05:29:00,605 - INFO - [diffusion][Epoch 10487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:00,606 - INFO - [diffusion][Epoch 10488] Epoch 10489/12000
2024-11-05 05:29:04,612 - INFO - [diffusion][Epoch 10488] diffusion training Loss: 0.044733413495123386
2024-11-05 05:29:04,614 - INFO - [diffusion][Epoch 10488] diffusion learning rate: 0.001
2024-11-05 05:29:04,616 - INFO - [diffusion][Epoch 10488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:04,617 - INFO - [diffusion][Epoch 10489] Epoch 10490/12000
2024-11-05 05:29:08,648 - INFO - [diffusion][Epoch 10489] diffusion training Loss: 0.047478366643190384
2024-11-05 05:29:08,650 - INFO - [diffusion][Epoch 10489] diffusion learning rate: 0.001
2024-11-05 05:29:08,651 - INFO - [diffusion][Epoch 10489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:08,652 - INFO - [diffusion][Epoch 10490] Epoch 10491/12000
2024-11-05 05:29:12,729 - INFO - [diffusion][Epoch 10490] diffusion training Loss: 0.04934060666710138
2024-11-05 05:29:12,731 - INFO - [diffusion][Epoch 10490] diffusion learning rate: 0.001
2024-11-05 05:29:12,733 - INFO - [diffusion][Epoch 10490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:12,734 - INFO - [diffusion][Epoch 10491] Epoch 10492/12000
2024-11-05 05:29:16,820 - INFO - [diffusion][Epoch 10491] diffusion training Loss: 0.049230633303523064
2024-11-05 05:29:16,822 - INFO - [diffusion][Epoch 10491] diffusion learning rate: 0.001
2024-11-05 05:29:16,824 - INFO - [diffusion][Epoch 10491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:16,825 - INFO - [diffusion][Epoch 10492] Epoch 10493/12000
2024-11-05 05:29:20,912 - INFO - [diffusion][Epoch 10492] diffusion training Loss: 0.049425335600972176
2024-11-05 05:29:20,914 - INFO - [diffusion][Epoch 10492] diffusion learning rate: 0.001
2024-11-05 05:29:20,941 - INFO - [diffusion][Epoch 10492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:20,942 - INFO - [diffusion][Epoch 10493] Epoch 10494/12000
2024-11-05 05:29:25,136 - INFO - [diffusion][Epoch 10493] diffusion training Loss: 0.04852306004613638
2024-11-05 05:29:25,138 - INFO - [diffusion][Epoch 10493] diffusion learning rate: 0.001
2024-11-05 05:29:25,140 - INFO - [diffusion][Epoch 10493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:25,142 - INFO - [diffusion][Epoch 10494] Epoch 10495/12000
2024-11-05 05:29:29,254 - INFO - [diffusion][Epoch 10494] diffusion training Loss: 0.04728440009057522
2024-11-05 05:29:29,256 - INFO - [diffusion][Epoch 10494] diffusion learning rate: 0.001
2024-11-05 05:29:29,258 - INFO - [diffusion][Epoch 10494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:29,259 - INFO - [diffusion][Epoch 10495] Epoch 10496/12000
2024-11-05 05:29:33,368 - INFO - [diffusion][Epoch 10495] diffusion training Loss: 0.05166400596499443
2024-11-05 05:29:33,370 - INFO - [diffusion][Epoch 10495] diffusion learning rate: 0.001
2024-11-05 05:29:33,372 - INFO - [diffusion][Epoch 10495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:33,373 - INFO - [diffusion][Epoch 10496] Epoch 10497/12000
2024-11-05 05:29:37,480 - INFO - [diffusion][Epoch 10496] diffusion training Loss: 0.04542674031108618
2024-11-05 05:29:37,482 - INFO - [diffusion][Epoch 10496] diffusion learning rate: 0.001
2024-11-05 05:29:37,484 - INFO - [diffusion][Epoch 10496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:37,485 - INFO - [diffusion][Epoch 10497] Epoch 10498/12000
2024-11-05 05:29:41,588 - INFO - [diffusion][Epoch 10497] diffusion training Loss: 0.04983754735440016
2024-11-05 05:29:41,590 - INFO - [diffusion][Epoch 10497] diffusion learning rate: 0.001
2024-11-05 05:29:41,592 - INFO - [diffusion][Epoch 10497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:41,593 - INFO - [diffusion][Epoch 10498] Epoch 10499/12000
2024-11-05 05:29:45,697 - INFO - [diffusion][Epoch 10498] diffusion training Loss: 0.04599664360284805
2024-11-05 05:29:45,701 - INFO - [diffusion][Epoch 10498] diffusion learning rate: 0.001
2024-11-05 05:29:45,702 - INFO - [diffusion][Epoch 10498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:45,703 - INFO - [diffusion][Epoch 10499] Epoch 10500/12000
2024-11-05 05:29:49,807 - INFO - [diffusion][Epoch 10499] diffusion training Loss: 0.047378089278936386
2024-11-05 05:29:49,809 - INFO - [diffusion][Epoch 10499] diffusion learning rate: 0.001
2024-11-05 05:29:49,837 - INFO - [diffusion][Epoch 10499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:49,838 - INFO - [diffusion][Epoch 10500] Epoch 10501/12000
2024-11-05 05:29:53,921 - INFO - [diffusion][Epoch 10500] diffusion training Loss: 0.0502617284655571
2024-11-05 05:29:53,924 - INFO - [diffusion][Epoch 10500] diffusion learning rate: 0.001
2024-11-05 05:29:53,925 - INFO - [diffusion][Epoch 10500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:53,927 - INFO - [diffusion][Epoch 10501] Epoch 10502/12000
2024-11-05 05:29:58,010 - INFO - [diffusion][Epoch 10501] diffusion training Loss: 0.04738913383334875
2024-11-05 05:29:58,012 - INFO - [diffusion][Epoch 10501] diffusion learning rate: 0.001
2024-11-05 05:29:58,014 - INFO - [diffusion][Epoch 10501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:29:58,015 - INFO - [diffusion][Epoch 10502] Epoch 10503/12000
2024-11-05 05:30:02,125 - INFO - [diffusion][Epoch 10502] diffusion training Loss: 0.046388378366827965
2024-11-05 05:30:02,127 - INFO - [diffusion][Epoch 10502] diffusion learning rate: 0.001
2024-11-05 05:30:02,129 - INFO - [diffusion][Epoch 10502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:02,130 - INFO - [diffusion][Epoch 10503] Epoch 10504/12000
2024-11-05 05:30:06,213 - INFO - [diffusion][Epoch 10503] diffusion training Loss: 0.05302936863154173
2024-11-05 05:30:06,215 - INFO - [diffusion][Epoch 10503] diffusion learning rate: 0.001
2024-11-05 05:30:06,217 - INFO - [diffusion][Epoch 10503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:06,218 - INFO - [diffusion][Epoch 10504] Epoch 10505/12000
2024-11-05 05:30:10,358 - INFO - [diffusion][Epoch 10504] diffusion training Loss: 0.0455887746065855
2024-11-05 05:30:10,360 - INFO - [diffusion][Epoch 10504] diffusion learning rate: 0.001
2024-11-05 05:30:10,361 - INFO - [diffusion][Epoch 10504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:10,363 - INFO - [diffusion][Epoch 10505] Epoch 10506/12000
2024-11-05 05:30:14,492 - INFO - [diffusion][Epoch 10505] diffusion training Loss: 0.04524541925638914
2024-11-05 05:30:14,494 - INFO - [diffusion][Epoch 10505] diffusion learning rate: 0.001
2024-11-05 05:30:14,495 - INFO - [diffusion][Epoch 10505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:14,496 - INFO - [diffusion][Epoch 10506] Epoch 10507/12000
2024-11-05 05:30:18,589 - INFO - [diffusion][Epoch 10506] diffusion training Loss: 0.047293021343648434
2024-11-05 05:30:18,591 - INFO - [diffusion][Epoch 10506] diffusion learning rate: 0.001
2024-11-05 05:30:18,593 - INFO - [diffusion][Epoch 10506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:18,594 - INFO - [diffusion][Epoch 10507] Epoch 10508/12000
2024-11-05 05:30:22,694 - INFO - [diffusion][Epoch 10507] diffusion training Loss: 0.04629534017294645
2024-11-05 05:30:22,696 - INFO - [diffusion][Epoch 10507] diffusion learning rate: 0.001
2024-11-05 05:30:22,698 - INFO - [diffusion][Epoch 10507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:22,699 - INFO - [diffusion][Epoch 10508] Epoch 10509/12000
2024-11-05 05:30:26,798 - INFO - [diffusion][Epoch 10508] diffusion training Loss: 0.04822424706071615
2024-11-05 05:30:26,800 - INFO - [diffusion][Epoch 10508] diffusion learning rate: 0.001
2024-11-05 05:30:26,802 - INFO - [diffusion][Epoch 10508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:26,803 - INFO - [diffusion][Epoch 10509] Epoch 10510/12000
2024-11-05 05:30:30,774 - INFO - [diffusion][Epoch 10509] diffusion training Loss: 0.05010468140244484
2024-11-05 05:30:30,776 - INFO - [diffusion][Epoch 10509] diffusion learning rate: 0.001
2024-11-05 05:30:30,803 - INFO - [diffusion][Epoch 10509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:30,804 - INFO - [diffusion][Epoch 10510] Epoch 10511/12000
2024-11-05 05:30:34,896 - INFO - [diffusion][Epoch 10510] diffusion training Loss: 0.04259931389242411
2024-11-05 05:30:34,898 - INFO - [diffusion][Epoch 10510] diffusion learning rate: 0.001
2024-11-05 05:30:34,900 - INFO - [diffusion][Epoch 10510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:34,901 - INFO - [diffusion][Epoch 10511] Epoch 10512/12000
2024-11-05 05:30:38,968 - INFO - [diffusion][Epoch 10511] diffusion training Loss: 0.049427902325987816
2024-11-05 05:30:38,970 - INFO - [diffusion][Epoch 10511] diffusion learning rate: 0.001
2024-11-05 05:30:38,972 - INFO - [diffusion][Epoch 10511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:38,973 - INFO - [diffusion][Epoch 10512] Epoch 10513/12000
2024-11-05 05:30:43,087 - INFO - [diffusion][Epoch 10512] diffusion training Loss: 0.05248590651899576
2024-11-05 05:30:43,089 - INFO - [diffusion][Epoch 10512] diffusion learning rate: 0.001
2024-11-05 05:30:43,090 - INFO - [diffusion][Epoch 10512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:43,091 - INFO - [diffusion][Epoch 10513] Epoch 10514/12000
2024-11-05 05:30:47,215 - INFO - [diffusion][Epoch 10513] diffusion training Loss: 0.04752385523170233
2024-11-05 05:30:47,218 - INFO - [diffusion][Epoch 10513] diffusion learning rate: 0.001
2024-11-05 05:30:47,220 - INFO - [diffusion][Epoch 10513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:47,222 - INFO - [diffusion][Epoch 10514] Epoch 10515/12000
2024-11-05 05:30:51,311 - INFO - [diffusion][Epoch 10514] diffusion training Loss: 0.04650460369884968
2024-11-05 05:30:51,312 - INFO - [diffusion][Epoch 10514] diffusion learning rate: 0.001
2024-11-05 05:30:51,314 - INFO - [diffusion][Epoch 10514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:51,315 - INFO - [diffusion][Epoch 10515] Epoch 10516/12000
2024-11-05 05:30:55,924 - INFO - [diffusion][Epoch 10515] diffusion training Loss: 0.0453455476090312
2024-11-05 05:30:55,926 - INFO - [diffusion][Epoch 10515] diffusion learning rate: 0.001
2024-11-05 05:30:55,928 - INFO - [diffusion][Epoch 10515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:30:55,929 - INFO - [diffusion][Epoch 10516] Epoch 10517/12000
2024-11-05 05:31:00,029 - INFO - [diffusion][Epoch 10516] diffusion training Loss: 0.047256129793822765
2024-11-05 05:31:00,031 - INFO - [diffusion][Epoch 10516] diffusion learning rate: 0.001
2024-11-05 05:31:00,033 - INFO - [diffusion][Epoch 10516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:00,034 - INFO - [diffusion][Epoch 10517] Epoch 10518/12000
2024-11-05 05:31:04,133 - INFO - [diffusion][Epoch 10517] diffusion training Loss: 0.05291198566555977
2024-11-05 05:31:04,135 - INFO - [diffusion][Epoch 10517] diffusion learning rate: 0.001
2024-11-05 05:31:04,137 - INFO - [diffusion][Epoch 10517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:04,138 - INFO - [diffusion][Epoch 10518] Epoch 10519/12000
2024-11-05 05:31:08,243 - INFO - [diffusion][Epoch 10518] diffusion training Loss: 0.047474125400185585
2024-11-05 05:31:08,245 - INFO - [diffusion][Epoch 10518] diffusion learning rate: 0.001
2024-11-05 05:31:08,247 - INFO - [diffusion][Epoch 10518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:08,248 - INFO - [diffusion][Epoch 10519] Epoch 10520/12000
2024-11-05 05:31:12,310 - INFO - [diffusion][Epoch 10519] diffusion training Loss: 0.050706964917480946
2024-11-05 05:31:12,311 - INFO - [diffusion][Epoch 10519] diffusion learning rate: 0.001
2024-11-05 05:31:12,313 - INFO - [diffusion][Epoch 10519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:12,315 - INFO - [diffusion][Epoch 10520] Epoch 10521/12000
2024-11-05 05:31:16,400 - INFO - [diffusion][Epoch 10520] diffusion training Loss: 0.0470086894929409
2024-11-05 05:31:16,402 - INFO - [diffusion][Epoch 10520] diffusion learning rate: 0.001
2024-11-05 05:31:16,404 - INFO - [diffusion][Epoch 10520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:16,405 - INFO - [diffusion][Epoch 10521] Epoch 10522/12000
2024-11-05 05:31:20,506 - INFO - [diffusion][Epoch 10521] diffusion training Loss: 0.04926914069801569
2024-11-05 05:31:20,508 - INFO - [diffusion][Epoch 10521] diffusion learning rate: 0.001
2024-11-05 05:31:20,510 - INFO - [diffusion][Epoch 10521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:20,511 - INFO - [diffusion][Epoch 10522] Epoch 10523/12000
2024-11-05 05:31:24,560 - INFO - [diffusion][Epoch 10522] diffusion training Loss: 0.05253107380121946
2024-11-05 05:31:24,562 - INFO - [diffusion][Epoch 10522] diffusion learning rate: 0.001
2024-11-05 05:31:24,564 - INFO - [diffusion][Epoch 10522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:24,565 - INFO - [diffusion][Epoch 10523] Epoch 10524/12000
2024-11-05 05:31:28,559 - INFO - [diffusion][Epoch 10523] diffusion training Loss: 0.04914127755910158
2024-11-05 05:31:28,561 - INFO - [diffusion][Epoch 10523] diffusion learning rate: 0.001
2024-11-05 05:31:28,563 - INFO - [diffusion][Epoch 10523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:28,564 - INFO - [diffusion][Epoch 10524] Epoch 10525/12000
2024-11-05 05:31:32,672 - INFO - [diffusion][Epoch 10524] diffusion training Loss: 0.04889771807938814
2024-11-05 05:31:32,674 - INFO - [diffusion][Epoch 10524] diffusion learning rate: 0.001
2024-11-05 05:31:32,676 - INFO - [diffusion][Epoch 10524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:32,677 - INFO - [diffusion][Epoch 10525] Epoch 10526/12000
2024-11-05 05:31:36,790 - INFO - [diffusion][Epoch 10525] diffusion training Loss: 0.050011287443339825
2024-11-05 05:31:36,792 - INFO - [diffusion][Epoch 10525] diffusion learning rate: 0.001
2024-11-05 05:31:36,794 - INFO - [diffusion][Epoch 10525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:36,795 - INFO - [diffusion][Epoch 10526] Epoch 10527/12000
2024-11-05 05:31:40,858 - INFO - [diffusion][Epoch 10526] diffusion training Loss: 0.04752608574926853
2024-11-05 05:31:40,860 - INFO - [diffusion][Epoch 10526] diffusion learning rate: 0.001
2024-11-05 05:31:40,861 - INFO - [diffusion][Epoch 10526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:40,863 - INFO - [diffusion][Epoch 10527] Epoch 10528/12000
2024-11-05 05:31:44,956 - INFO - [diffusion][Epoch 10527] diffusion training Loss: 0.046555713750422
2024-11-05 05:31:44,958 - INFO - [diffusion][Epoch 10527] diffusion learning rate: 0.001
2024-11-05 05:31:45,000 - INFO - [diffusion][Epoch 10527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:45,002 - INFO - [diffusion][Epoch 10528] Epoch 10529/12000
2024-11-05 05:31:49,104 - INFO - [diffusion][Epoch 10528] diffusion training Loss: 0.048010287806391716
2024-11-05 05:31:49,107 - INFO - [diffusion][Epoch 10528] diffusion learning rate: 0.001
2024-11-05 05:31:49,109 - INFO - [diffusion][Epoch 10528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:49,111 - INFO - [diffusion][Epoch 10529] Epoch 10530/12000
2024-11-05 05:31:53,180 - INFO - [diffusion][Epoch 10529] diffusion training Loss: 0.053175462409853935
2024-11-05 05:31:53,182 - INFO - [diffusion][Epoch 10529] diffusion learning rate: 0.001
2024-11-05 05:31:53,184 - INFO - [diffusion][Epoch 10529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:53,185 - INFO - [diffusion][Epoch 10530] Epoch 10531/12000
2024-11-05 05:31:57,301 - INFO - [diffusion][Epoch 10530] diffusion training Loss: 0.04629879631102085
2024-11-05 05:31:57,303 - INFO - [diffusion][Epoch 10530] diffusion learning rate: 0.001
2024-11-05 05:31:57,306 - INFO - [diffusion][Epoch 10530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:31:57,307 - INFO - [diffusion][Epoch 10531] Epoch 10532/12000
2024-11-05 05:32:01,317 - INFO - [diffusion][Epoch 10531] diffusion training Loss: 0.05119841452687979
2024-11-05 05:32:01,319 - INFO - [diffusion][Epoch 10531] diffusion learning rate: 0.001
2024-11-05 05:32:01,321 - INFO - [diffusion][Epoch 10531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:01,323 - INFO - [diffusion][Epoch 10532] Epoch 10533/12000
2024-11-05 05:32:05,443 - INFO - [diffusion][Epoch 10532] diffusion training Loss: 0.04690906498581171
2024-11-05 05:32:05,445 - INFO - [diffusion][Epoch 10532] diffusion learning rate: 0.001
2024-11-05 05:32:05,447 - INFO - [diffusion][Epoch 10532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:05,448 - INFO - [diffusion][Epoch 10533] Epoch 10534/12000
2024-11-05 05:32:09,551 - INFO - [diffusion][Epoch 10533] diffusion training Loss: 0.04793867468833923
2024-11-05 05:32:09,553 - INFO - [diffusion][Epoch 10533] diffusion learning rate: 0.001
2024-11-05 05:32:09,555 - INFO - [diffusion][Epoch 10533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:09,556 - INFO - [diffusion][Epoch 10534] Epoch 10535/12000
2024-11-05 05:32:13,661 - INFO - [diffusion][Epoch 10534] diffusion training Loss: 0.04572162311524153
2024-11-05 05:32:13,663 - INFO - [diffusion][Epoch 10534] diffusion learning rate: 0.001
2024-11-05 05:32:13,691 - INFO - [diffusion][Epoch 10534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:13,692 - INFO - [diffusion][Epoch 10535] Epoch 10536/12000
2024-11-05 05:32:18,359 - INFO - [diffusion][Epoch 10535] diffusion training Loss: 0.04297718312591314
2024-11-05 05:32:18,361 - INFO - [diffusion][Epoch 10535] diffusion learning rate: 0.001
2024-11-05 05:32:18,363 - INFO - [diffusion][Epoch 10535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:18,364 - INFO - [diffusion][Epoch 10536] Epoch 10537/12000
2024-11-05 05:32:22,452 - INFO - [diffusion][Epoch 10536] diffusion training Loss: 0.04565173014998436
2024-11-05 05:32:22,453 - INFO - [diffusion][Epoch 10536] diffusion learning rate: 0.001
2024-11-05 05:32:22,455 - INFO - [diffusion][Epoch 10536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:22,457 - INFO - [diffusion][Epoch 10537] Epoch 10538/12000
2024-11-05 05:32:26,359 - INFO - [diffusion][Epoch 10537] diffusion training Loss: 0.0460563488304615
2024-11-05 05:32:26,361 - INFO - [diffusion][Epoch 10537] diffusion learning rate: 0.001
2024-11-05 05:32:26,364 - INFO - [diffusion][Epoch 10537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:26,365 - INFO - [diffusion][Epoch 10538] Epoch 10539/12000
2024-11-05 05:32:30,464 - INFO - [diffusion][Epoch 10538] diffusion training Loss: 0.04868654813617468
2024-11-05 05:32:30,466 - INFO - [diffusion][Epoch 10538] diffusion learning rate: 0.001
2024-11-05 05:32:30,468 - INFO - [diffusion][Epoch 10538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:30,469 - INFO - [diffusion][Epoch 10539] Epoch 10540/12000
2024-11-05 05:32:34,561 - INFO - [diffusion][Epoch 10539] diffusion training Loss: 0.04825246799737215
2024-11-05 05:32:34,563 - INFO - [diffusion][Epoch 10539] diffusion learning rate: 0.001
2024-11-05 05:32:34,565 - INFO - [diffusion][Epoch 10539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:34,566 - INFO - [diffusion][Epoch 10540] Epoch 10541/12000
2024-11-05 05:32:38,692 - INFO - [diffusion][Epoch 10540] diffusion training Loss: 0.05059844721108675
2024-11-05 05:32:38,694 - INFO - [diffusion][Epoch 10540] diffusion learning rate: 0.001
2024-11-05 05:32:38,696 - INFO - [diffusion][Epoch 10540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:38,697 - INFO - [diffusion][Epoch 10541] Epoch 10542/12000
2024-11-05 05:32:42,766 - INFO - [diffusion][Epoch 10541] diffusion training Loss: 0.04856346268206835
2024-11-05 05:32:42,768 - INFO - [diffusion][Epoch 10541] diffusion learning rate: 0.001
2024-11-05 05:32:42,770 - INFO - [diffusion][Epoch 10541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:42,771 - INFO - [diffusion][Epoch 10542] Epoch 10543/12000
2024-11-05 05:32:46,839 - INFO - [diffusion][Epoch 10542] diffusion training Loss: 0.04150646645575762
2024-11-05 05:32:46,841 - INFO - [diffusion][Epoch 10542] diffusion learning rate: 0.001
2024-11-05 05:32:46,843 - INFO - [diffusion][Epoch 10542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:46,844 - INFO - [diffusion][Epoch 10543] Epoch 10544/12000
2024-11-05 05:32:50,952 - INFO - [diffusion][Epoch 10543] diffusion training Loss: 0.04672328382730484
2024-11-05 05:32:50,955 - INFO - [diffusion][Epoch 10543] diffusion learning rate: 0.001
2024-11-05 05:32:50,957 - INFO - [diffusion][Epoch 10543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:50,959 - INFO - [diffusion][Epoch 10544] Epoch 10545/12000
2024-11-05 05:32:55,073 - INFO - [diffusion][Epoch 10544] diffusion training Loss: 0.04756637290120125
2024-11-05 05:32:55,075 - INFO - [diffusion][Epoch 10544] diffusion learning rate: 0.001
2024-11-05 05:32:55,076 - INFO - [diffusion][Epoch 10544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:55,077 - INFO - [diffusion][Epoch 10545] Epoch 10546/12000
2024-11-05 05:32:59,169 - INFO - [diffusion][Epoch 10545] diffusion training Loss: 0.048264067620038986
2024-11-05 05:32:59,171 - INFO - [diffusion][Epoch 10545] diffusion learning rate: 0.001
2024-11-05 05:32:59,173 - INFO - [diffusion][Epoch 10545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:32:59,174 - INFO - [diffusion][Epoch 10546] Epoch 10547/12000
2024-11-05 05:33:03,274 - INFO - [diffusion][Epoch 10546] diffusion training Loss: 0.05038764514029026
2024-11-05 05:33:03,277 - INFO - [diffusion][Epoch 10546] diffusion learning rate: 0.001
2024-11-05 05:33:03,278 - INFO - [diffusion][Epoch 10546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:03,280 - INFO - [diffusion][Epoch 10547] Epoch 10548/12000
2024-11-05 05:33:07,314 - INFO - [diffusion][Epoch 10547] diffusion training Loss: 0.05332976020872593
2024-11-05 05:33:07,316 - INFO - [diffusion][Epoch 10547] diffusion learning rate: 0.001
2024-11-05 05:33:07,318 - INFO - [diffusion][Epoch 10547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:07,319 - INFO - [diffusion][Epoch 10548] Epoch 10549/12000
2024-11-05 05:33:11,447 - INFO - [diffusion][Epoch 10548] diffusion training Loss: 0.052599016577005386
2024-11-05 05:33:11,449 - INFO - [diffusion][Epoch 10548] diffusion learning rate: 0.001
2024-11-05 05:33:11,451 - INFO - [diffusion][Epoch 10548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:11,452 - INFO - [diffusion][Epoch 10549] Epoch 10550/12000
2024-11-05 05:33:15,514 - INFO - [diffusion][Epoch 10549] diffusion training Loss: 0.04982369393110275
2024-11-05 05:33:15,516 - INFO - [diffusion][Epoch 10549] diffusion learning rate: 0.001
2024-11-05 05:33:15,518 - INFO - [diffusion][Epoch 10549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:15,519 - INFO - [diffusion][Epoch 10550] Epoch 10551/12000
2024-11-05 05:33:19,620 - INFO - [diffusion][Epoch 10550] diffusion training Loss: 0.048536673188209534
2024-11-05 05:33:19,622 - INFO - [diffusion][Epoch 10550] diffusion learning rate: 0.001
2024-11-05 05:33:19,624 - INFO - [diffusion][Epoch 10550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:19,625 - INFO - [diffusion][Epoch 10551] Epoch 10552/12000
2024-11-05 05:33:23,717 - INFO - [diffusion][Epoch 10551] diffusion training Loss: 0.04788447543978691
2024-11-05 05:33:23,719 - INFO - [diffusion][Epoch 10551] diffusion learning rate: 0.001
2024-11-05 05:33:23,721 - INFO - [diffusion][Epoch 10551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:23,722 - INFO - [diffusion][Epoch 10552] Epoch 10553/12000
2024-11-05 05:33:27,818 - INFO - [diffusion][Epoch 10552] diffusion training Loss: 0.04724607523530722
2024-11-05 05:33:27,820 - INFO - [diffusion][Epoch 10552] diffusion learning rate: 0.001
2024-11-05 05:33:27,822 - INFO - [diffusion][Epoch 10552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:27,823 - INFO - [diffusion][Epoch 10553] Epoch 10554/12000
2024-11-05 05:33:31,884 - INFO - [diffusion][Epoch 10553] diffusion training Loss: 0.04488092940300703
2024-11-05 05:33:31,886 - INFO - [diffusion][Epoch 10553] diffusion learning rate: 0.001
2024-11-05 05:33:31,888 - INFO - [diffusion][Epoch 10553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:31,889 - INFO - [diffusion][Epoch 10554] Epoch 10555/12000
2024-11-05 05:33:36,222 - INFO - [diffusion][Epoch 10554] diffusion training Loss: 0.04515319038182497
2024-11-05 05:33:36,224 - INFO - [diffusion][Epoch 10554] diffusion learning rate: 0.001
2024-11-05 05:33:36,226 - INFO - [diffusion][Epoch 10554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:36,227 - INFO - [diffusion][Epoch 10555] Epoch 10556/12000
2024-11-05 05:33:40,314 - INFO - [diffusion][Epoch 10555] diffusion training Loss: 0.04864412546157837
2024-11-05 05:33:40,315 - INFO - [diffusion][Epoch 10555] diffusion learning rate: 0.001
2024-11-05 05:33:40,317 - INFO - [diffusion][Epoch 10555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:40,319 - INFO - [diffusion][Epoch 10556] Epoch 10557/12000
2024-11-05 05:33:44,393 - INFO - [diffusion][Epoch 10556] diffusion training Loss: 0.04824592638760805
2024-11-05 05:33:44,395 - INFO - [diffusion][Epoch 10556] diffusion learning rate: 0.001
2024-11-05 05:33:44,396 - INFO - [diffusion][Epoch 10556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:44,397 - INFO - [diffusion][Epoch 10557] Epoch 10558/12000
2024-11-05 05:33:48,474 - INFO - [diffusion][Epoch 10557] diffusion training Loss: 0.04730885662138462
2024-11-05 05:33:48,476 - INFO - [diffusion][Epoch 10557] diffusion learning rate: 0.001
2024-11-05 05:33:48,478 - INFO - [diffusion][Epoch 10557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:48,479 - INFO - [diffusion][Epoch 10558] Epoch 10559/12000
2024-11-05 05:33:52,606 - INFO - [diffusion][Epoch 10558] diffusion training Loss: 0.051304133608937263
2024-11-05 05:33:52,610 - INFO - [diffusion][Epoch 10558] diffusion learning rate: 0.001
2024-11-05 05:33:52,612 - INFO - [diffusion][Epoch 10558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:52,613 - INFO - [diffusion][Epoch 10559] Epoch 10560/12000
2024-11-05 05:33:56,687 - INFO - [diffusion][Epoch 10559] diffusion training Loss: 0.04721117299050093
2024-11-05 05:33:56,689 - INFO - [diffusion][Epoch 10559] diffusion learning rate: 0.001
2024-11-05 05:33:56,690 - INFO - [diffusion][Epoch 10559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:33:56,692 - INFO - [diffusion][Epoch 10560] Epoch 10561/12000
2024-11-05 05:34:00,755 - INFO - [diffusion][Epoch 10560] diffusion training Loss: 0.05004510097205639
2024-11-05 05:34:00,757 - INFO - [diffusion][Epoch 10560] diffusion learning rate: 0.001
2024-11-05 05:34:00,759 - INFO - [diffusion][Epoch 10560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:00,760 - INFO - [diffusion][Epoch 10561] Epoch 10562/12000
2024-11-05 05:34:04,860 - INFO - [diffusion][Epoch 10561] diffusion training Loss: 0.04949030466377735
2024-11-05 05:34:04,862 - INFO - [diffusion][Epoch 10561] diffusion learning rate: 0.001
2024-11-05 05:34:04,864 - INFO - [diffusion][Epoch 10561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:04,865 - INFO - [diffusion][Epoch 10562] Epoch 10563/12000
2024-11-05 05:34:08,975 - INFO - [diffusion][Epoch 10562] diffusion training Loss: 0.05290234740823507
2024-11-05 05:34:09,095 - INFO - [diffusion][Epoch 10562] diffusion learning rate: 0.001
2024-11-05 05:34:09,161 - INFO - [diffusion][Epoch 10562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:09,162 - INFO - [diffusion][Epoch 10563] Epoch 10564/12000
2024-11-05 05:34:13,257 - INFO - [diffusion][Epoch 10563] diffusion training Loss: 0.04855111055076122
2024-11-05 05:34:13,258 - INFO - [diffusion][Epoch 10563] diffusion learning rate: 0.001
2024-11-05 05:34:13,261 - INFO - [diffusion][Epoch 10563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:13,262 - INFO - [diffusion][Epoch 10564] Epoch 10565/12000
2024-11-05 05:34:17,374 - INFO - [diffusion][Epoch 10564] diffusion training Loss: 0.04870228283107281
2024-11-05 05:34:17,376 - INFO - [diffusion][Epoch 10564] diffusion learning rate: 0.001
2024-11-05 05:34:17,378 - INFO - [diffusion][Epoch 10564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:17,379 - INFO - [diffusion][Epoch 10565] Epoch 10566/12000
2024-11-05 05:34:21,511 - INFO - [diffusion][Epoch 10565] diffusion training Loss: 0.04503384046256542
2024-11-05 05:34:21,513 - INFO - [diffusion][Epoch 10565] diffusion learning rate: 0.001
2024-11-05 05:34:21,514 - INFO - [diffusion][Epoch 10565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:21,516 - INFO - [diffusion][Epoch 10566] Epoch 10567/12000
2024-11-05 05:34:25,543 - INFO - [diffusion][Epoch 10566] diffusion training Loss: 0.044804297387599945
2024-11-05 05:34:25,545 - INFO - [diffusion][Epoch 10566] diffusion learning rate: 0.001
2024-11-05 05:34:25,547 - INFO - [diffusion][Epoch 10566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:25,548 - INFO - [diffusion][Epoch 10567] Epoch 10568/12000
2024-11-05 05:34:29,660 - INFO - [diffusion][Epoch 10567] diffusion training Loss: 0.05595443770289421
2024-11-05 05:34:29,662 - INFO - [diffusion][Epoch 10567] diffusion learning rate: 0.001
2024-11-05 05:34:29,664 - INFO - [diffusion][Epoch 10567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:29,665 - INFO - [diffusion][Epoch 10568] Epoch 10569/12000
2024-11-05 05:34:33,784 - INFO - [diffusion][Epoch 10568] diffusion training Loss: 0.05065411888062954
2024-11-05 05:34:33,785 - INFO - [diffusion][Epoch 10568] diffusion learning rate: 0.001
2024-11-05 05:34:33,787 - INFO - [diffusion][Epoch 10568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:33,789 - INFO - [diffusion][Epoch 10569] Epoch 10570/12000
2024-11-05 05:34:37,880 - INFO - [diffusion][Epoch 10569] diffusion training Loss: 0.05282426252961159
2024-11-05 05:34:37,882 - INFO - [diffusion][Epoch 10569] diffusion learning rate: 0.001
2024-11-05 05:34:37,884 - INFO - [diffusion][Epoch 10569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:37,885 - INFO - [diffusion][Epoch 10570] Epoch 10571/12000
2024-11-05 05:34:42,032 - INFO - [diffusion][Epoch 10570] diffusion training Loss: 0.0479766633361578
2024-11-05 05:34:42,034 - INFO - [diffusion][Epoch 10570] diffusion learning rate: 0.001
2024-11-05 05:34:42,036 - INFO - [diffusion][Epoch 10570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:42,037 - INFO - [diffusion][Epoch 10571] Epoch 10572/12000
2024-11-05 05:34:46,128 - INFO - [diffusion][Epoch 10571] diffusion training Loss: 0.050524717196822166
2024-11-05 05:34:46,130 - INFO - [diffusion][Epoch 10571] diffusion learning rate: 0.001
2024-11-05 05:34:46,132 - INFO - [diffusion][Epoch 10571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:46,133 - INFO - [diffusion][Epoch 10572] Epoch 10573/12000
2024-11-05 05:34:50,238 - INFO - [diffusion][Epoch 10572] diffusion training Loss: 0.046307976357638836
2024-11-05 05:34:50,240 - INFO - [diffusion][Epoch 10572] diffusion learning rate: 0.001
2024-11-05 05:34:50,242 - INFO - [diffusion][Epoch 10572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:50,243 - INFO - [diffusion][Epoch 10573] Epoch 10574/12000
2024-11-05 05:34:54,350 - INFO - [diffusion][Epoch 10573] diffusion training Loss: 0.04551130626350641
2024-11-05 05:34:54,354 - INFO - [diffusion][Epoch 10573] diffusion learning rate: 0.001
2024-11-05 05:34:54,355 - INFO - [diffusion][Epoch 10573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:54,357 - INFO - [diffusion][Epoch 10574] Epoch 10575/12000
2024-11-05 05:34:58,567 - INFO - [diffusion][Epoch 10574] diffusion training Loss: 0.05054248031228781
2024-11-05 05:34:58,569 - INFO - [diffusion][Epoch 10574] diffusion learning rate: 0.001
2024-11-05 05:34:58,571 - INFO - [diffusion][Epoch 10574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:34:58,572 - INFO - [diffusion][Epoch 10575] Epoch 10576/12000
2024-11-05 05:35:02,662 - INFO - [diffusion][Epoch 10575] diffusion training Loss: 0.0512541513890028
2024-11-05 05:35:02,664 - INFO - [diffusion][Epoch 10575] diffusion learning rate: 0.001
2024-11-05 05:35:02,666 - INFO - [diffusion][Epoch 10575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:02,667 - INFO - [diffusion][Epoch 10576] Epoch 10577/12000
2024-11-05 05:35:06,730 - INFO - [diffusion][Epoch 10576] diffusion training Loss: 0.0423206789419055
2024-11-05 05:35:06,732 - INFO - [diffusion][Epoch 10576] diffusion learning rate: 0.001
2024-11-05 05:35:06,733 - INFO - [diffusion][Epoch 10576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:06,735 - INFO - [diffusion][Epoch 10577] Epoch 10578/12000
2024-11-05 05:35:10,859 - INFO - [diffusion][Epoch 10577] diffusion training Loss: 0.04879274405539036
2024-11-05 05:35:10,861 - INFO - [diffusion][Epoch 10577] diffusion learning rate: 0.001
2024-11-05 05:35:10,862 - INFO - [diffusion][Epoch 10577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:10,864 - INFO - [diffusion][Epoch 10578] Epoch 10579/12000
2024-11-05 05:35:14,988 - INFO - [diffusion][Epoch 10578] diffusion training Loss: 0.04987764451652765
2024-11-05 05:35:14,990 - INFO - [diffusion][Epoch 10578] diffusion learning rate: 0.001
2024-11-05 05:35:14,991 - INFO - [diffusion][Epoch 10578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:14,993 - INFO - [diffusion][Epoch 10579] Epoch 10580/12000
2024-11-05 05:35:19,083 - INFO - [diffusion][Epoch 10579] diffusion training Loss: 0.04874911718070507
2024-11-05 05:35:19,085 - INFO - [diffusion][Epoch 10579] diffusion learning rate: 0.001
2024-11-05 05:35:19,086 - INFO - [diffusion][Epoch 10579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:19,088 - INFO - [diffusion][Epoch 10580] Epoch 10581/12000
2024-11-05 05:35:23,181 - INFO - [diffusion][Epoch 10580] diffusion training Loss: 0.046950957737863064
2024-11-05 05:35:23,183 - INFO - [diffusion][Epoch 10580] diffusion learning rate: 0.001
2024-11-05 05:35:23,185 - INFO - [diffusion][Epoch 10580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:23,187 - INFO - [diffusion][Epoch 10581] Epoch 10582/12000
2024-11-05 05:35:27,219 - INFO - [diffusion][Epoch 10581] diffusion training Loss: 0.051095292903482914
2024-11-05 05:35:27,221 - INFO - [diffusion][Epoch 10581] diffusion learning rate: 0.001
2024-11-05 05:35:27,223 - INFO - [diffusion][Epoch 10581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:27,225 - INFO - [diffusion][Epoch 10582] Epoch 10583/12000
2024-11-05 05:35:31,373 - INFO - [diffusion][Epoch 10582] diffusion training Loss: 0.04942111298441887
2024-11-05 05:35:31,375 - INFO - [diffusion][Epoch 10582] diffusion learning rate: 0.001
2024-11-05 05:35:31,377 - INFO - [diffusion][Epoch 10582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:31,378 - INFO - [diffusion][Epoch 10583] Epoch 10584/12000
2024-11-05 05:35:35,505 - INFO - [diffusion][Epoch 10583] diffusion training Loss: 0.0475967638194561
2024-11-05 05:35:35,507 - INFO - [diffusion][Epoch 10583] diffusion learning rate: 0.001
2024-11-05 05:35:35,508 - INFO - [diffusion][Epoch 10583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:35,510 - INFO - [diffusion][Epoch 10584] Epoch 10585/12000
2024-11-05 05:35:39,643 - INFO - [diffusion][Epoch 10584] diffusion training Loss: 0.04722151532769203
2024-11-05 05:35:39,645 - INFO - [diffusion][Epoch 10584] diffusion learning rate: 0.001
2024-11-05 05:35:39,671 - INFO - [diffusion][Epoch 10584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:39,673 - INFO - [diffusion][Epoch 10585] Epoch 10586/12000
2024-11-05 05:35:43,764 - INFO - [diffusion][Epoch 10585] diffusion training Loss: 0.047813559882342815
2024-11-05 05:35:43,767 - INFO - [diffusion][Epoch 10585] diffusion learning rate: 0.001
2024-11-05 05:35:43,768 - INFO - [diffusion][Epoch 10585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:43,770 - INFO - [diffusion][Epoch 10586] Epoch 10587/12000
2024-11-05 05:35:47,874 - INFO - [diffusion][Epoch 10586] diffusion training Loss: 0.04800091218203306
2024-11-05 05:35:47,876 - INFO - [diffusion][Epoch 10586] diffusion learning rate: 0.001
2024-11-05 05:35:47,878 - INFO - [diffusion][Epoch 10586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:47,879 - INFO - [diffusion][Epoch 10587] Epoch 10588/12000
2024-11-05 05:35:51,989 - INFO - [diffusion][Epoch 10587] diffusion training Loss: 0.047263496555387974
2024-11-05 05:35:51,991 - INFO - [diffusion][Epoch 10587] diffusion learning rate: 0.001
2024-11-05 05:35:51,993 - INFO - [diffusion][Epoch 10587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:51,994 - INFO - [diffusion][Epoch 10588] Epoch 10589/12000
2024-11-05 05:35:56,092 - INFO - [diffusion][Epoch 10588] diffusion training Loss: 0.05006828997284174
2024-11-05 05:35:56,096 - INFO - [diffusion][Epoch 10588] diffusion learning rate: 0.001
2024-11-05 05:35:56,097 - INFO - [diffusion][Epoch 10588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:35:56,099 - INFO - [diffusion][Epoch 10589] Epoch 10590/12000
2024-11-05 05:36:00,256 - INFO - [diffusion][Epoch 10589] diffusion training Loss: 0.051296986639499664
2024-11-05 05:36:00,258 - INFO - [diffusion][Epoch 10589] diffusion learning rate: 0.001
2024-11-05 05:36:00,260 - INFO - [diffusion][Epoch 10589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:00,261 - INFO - [diffusion][Epoch 10590] Epoch 10591/12000
2024-11-05 05:36:04,326 - INFO - [diffusion][Epoch 10590] diffusion training Loss: 0.046352154575288296
2024-11-05 05:36:04,328 - INFO - [diffusion][Epoch 10590] diffusion learning rate: 0.001
2024-11-05 05:36:04,330 - INFO - [diffusion][Epoch 10590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:04,331 - INFO - [diffusion][Epoch 10591] Epoch 10592/12000
2024-11-05 05:36:08,429 - INFO - [diffusion][Epoch 10591] diffusion training Loss: 0.048994140699505806
2024-11-05 05:36:08,431 - INFO - [diffusion][Epoch 10591] diffusion learning rate: 0.001
2024-11-05 05:36:08,432 - INFO - [diffusion][Epoch 10591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:08,434 - INFO - [diffusion][Epoch 10592] Epoch 10593/12000
2024-11-05 05:36:12,514 - INFO - [diffusion][Epoch 10592] diffusion training Loss: 0.05027594696730375
2024-11-05 05:36:12,516 - INFO - [diffusion][Epoch 10592] diffusion learning rate: 0.001
2024-11-05 05:36:12,555 - INFO - [diffusion][Epoch 10592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:12,556 - INFO - [diffusion][Epoch 10593] Epoch 10594/12000
2024-11-05 05:36:16,666 - INFO - [diffusion][Epoch 10593] diffusion training Loss: 0.04995868355035782
2024-11-05 05:36:16,667 - INFO - [diffusion][Epoch 10593] diffusion learning rate: 0.001
2024-11-05 05:36:16,669 - INFO - [diffusion][Epoch 10593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:16,670 - INFO - [diffusion][Epoch 10594] Epoch 10595/12000
2024-11-05 05:36:20,987 - INFO - [diffusion][Epoch 10594] diffusion training Loss: 0.04715157765895128
2024-11-05 05:36:20,989 - INFO - [diffusion][Epoch 10594] diffusion learning rate: 0.001
2024-11-05 05:36:20,991 - INFO - [diffusion][Epoch 10594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:20,992 - INFO - [diffusion][Epoch 10595] Epoch 10596/12000
2024-11-05 05:36:25,097 - INFO - [diffusion][Epoch 10595] diffusion training Loss: 0.05050914827734232
2024-11-05 05:36:25,099 - INFO - [diffusion][Epoch 10595] diffusion learning rate: 0.001
2024-11-05 05:36:25,101 - INFO - [diffusion][Epoch 10595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:25,102 - INFO - [diffusion][Epoch 10596] Epoch 10597/12000
2024-11-05 05:36:29,205 - INFO - [diffusion][Epoch 10596] diffusion training Loss: 0.04639836959540844
2024-11-05 05:36:29,207 - INFO - [diffusion][Epoch 10596] diffusion learning rate: 0.001
2024-11-05 05:36:29,209 - INFO - [diffusion][Epoch 10596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:29,210 - INFO - [diffusion][Epoch 10597] Epoch 10598/12000
2024-11-05 05:36:33,195 - INFO - [diffusion][Epoch 10597] diffusion training Loss: 0.05183213483542204
2024-11-05 05:36:33,197 - INFO - [diffusion][Epoch 10597] diffusion learning rate: 0.001
2024-11-05 05:36:33,199 - INFO - [diffusion][Epoch 10597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:33,201 - INFO - [diffusion][Epoch 10598] Epoch 10599/12000
2024-11-05 05:36:37,227 - INFO - [diffusion][Epoch 10598] diffusion training Loss: 0.04070423264056444
2024-11-05 05:36:37,229 - INFO - [diffusion][Epoch 10598] diffusion learning rate: 0.001
2024-11-05 05:36:37,231 - INFO - [diffusion][Epoch 10598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:37,233 - INFO - [diffusion][Epoch 10599] Epoch 10600/12000
2024-11-05 05:36:41,283 - INFO - [diffusion][Epoch 10599] diffusion training Loss: 0.05021478049457073
2024-11-05 05:36:41,285 - INFO - [diffusion][Epoch 10599] diffusion learning rate: 0.001
2024-11-05 05:36:41,286 - INFO - [diffusion][Epoch 10599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:41,288 - INFO - [diffusion][Epoch 10600] Epoch 10601/12000
2024-11-05 05:36:45,446 - INFO - [diffusion][Epoch 10600] diffusion training Loss: 0.04639197513461113
2024-11-05 05:36:45,449 - INFO - [diffusion][Epoch 10600] diffusion learning rate: 0.001
2024-11-05 05:36:45,451 - INFO - [diffusion][Epoch 10600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:45,452 - INFO - [diffusion][Epoch 10601] Epoch 10602/12000
2024-11-05 05:36:49,595 - INFO - [diffusion][Epoch 10601] diffusion training Loss: 0.04540253151208162
2024-11-05 05:36:49,597 - INFO - [diffusion][Epoch 10601] diffusion learning rate: 0.001
2024-11-05 05:36:49,599 - INFO - [diffusion][Epoch 10601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:49,600 - INFO - [diffusion][Epoch 10602] Epoch 10603/12000
2024-11-05 05:36:53,709 - INFO - [diffusion][Epoch 10602] diffusion training Loss: 0.048664407804608345
2024-11-05 05:36:53,711 - INFO - [diffusion][Epoch 10602] diffusion learning rate: 0.001
2024-11-05 05:36:53,712 - INFO - [diffusion][Epoch 10602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:53,714 - INFO - [diffusion][Epoch 10603] Epoch 10604/12000
2024-11-05 05:36:57,763 - INFO - [diffusion][Epoch 10603] diffusion training Loss: 0.049411152489483356
2024-11-05 05:36:57,767 - INFO - [diffusion][Epoch 10603] diffusion learning rate: 0.001
2024-11-05 05:36:57,768 - INFO - [diffusion][Epoch 10603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:36:57,770 - INFO - [diffusion][Epoch 10604] Epoch 10605/12000
2024-11-05 05:37:01,875 - INFO - [diffusion][Epoch 10604] diffusion training Loss: 0.04722970724105835
2024-11-05 05:37:01,877 - INFO - [diffusion][Epoch 10604] diffusion learning rate: 0.001
2024-11-05 05:37:01,917 - INFO - [diffusion][Epoch 10604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:01,919 - INFO - [diffusion][Epoch 10605] Epoch 10606/12000
2024-11-05 05:37:06,055 - INFO - [diffusion][Epoch 10605] diffusion training Loss: 0.04437182191759348
2024-11-05 05:37:06,057 - INFO - [diffusion][Epoch 10605] diffusion learning rate: 0.001
2024-11-05 05:37:06,059 - INFO - [diffusion][Epoch 10605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:06,060 - INFO - [diffusion][Epoch 10606] Epoch 10607/12000
2024-11-05 05:37:10,201 - INFO - [diffusion][Epoch 10606] diffusion training Loss: 0.05275802593678236
2024-11-05 05:37:10,203 - INFO - [diffusion][Epoch 10606] diffusion learning rate: 0.001
2024-11-05 05:37:10,205 - INFO - [diffusion][Epoch 10606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:10,207 - INFO - [diffusion][Epoch 10607] Epoch 10608/12000
2024-11-05 05:37:14,307 - INFO - [diffusion][Epoch 10607] diffusion training Loss: 0.050360675901174545
2024-11-05 05:37:14,309 - INFO - [diffusion][Epoch 10607] diffusion learning rate: 0.001
2024-11-05 05:37:14,311 - INFO - [diffusion][Epoch 10607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:14,312 - INFO - [diffusion][Epoch 10608] Epoch 10609/12000
2024-11-05 05:37:18,416 - INFO - [diffusion][Epoch 10608] diffusion training Loss: 0.046457032673060894
2024-11-05 05:37:18,419 - INFO - [diffusion][Epoch 10608] diffusion learning rate: 0.001
2024-11-05 05:37:18,421 - INFO - [diffusion][Epoch 10608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:18,422 - INFO - [diffusion][Epoch 10609] Epoch 10610/12000
2024-11-05 05:37:22,557 - INFO - [diffusion][Epoch 10609] diffusion training Loss: 0.050262222066521645
2024-11-05 05:37:22,559 - INFO - [diffusion][Epoch 10609] diffusion learning rate: 0.001
2024-11-05 05:37:22,560 - INFO - [diffusion][Epoch 10609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:22,562 - INFO - [diffusion][Epoch 10610] Epoch 10611/12000
2024-11-05 05:37:26,677 - INFO - [diffusion][Epoch 10610] diffusion training Loss: 0.04963177442550659
2024-11-05 05:37:26,679 - INFO - [diffusion][Epoch 10610] diffusion learning rate: 0.001
2024-11-05 05:37:26,681 - INFO - [diffusion][Epoch 10610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:26,683 - INFO - [diffusion][Epoch 10611] Epoch 10612/12000
2024-11-05 05:37:30,812 - INFO - [diffusion][Epoch 10611] diffusion training Loss: 0.04593534581363201
2024-11-05 05:37:30,814 - INFO - [diffusion][Epoch 10611] diffusion learning rate: 0.001
2024-11-05 05:37:30,816 - INFO - [diffusion][Epoch 10611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:30,817 - INFO - [diffusion][Epoch 10612] Epoch 10613/12000
2024-11-05 05:37:34,890 - INFO - [diffusion][Epoch 10612] diffusion training Loss: 0.045612070709466934
2024-11-05 05:37:34,892 - INFO - [diffusion][Epoch 10612] diffusion learning rate: 0.001
2024-11-05 05:37:34,894 - INFO - [diffusion][Epoch 10612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:34,895 - INFO - [diffusion][Epoch 10613] Epoch 10614/12000
2024-11-05 05:37:38,962 - INFO - [diffusion][Epoch 10613] diffusion training Loss: 0.04665449820458889
2024-11-05 05:37:38,964 - INFO - [diffusion][Epoch 10613] diffusion learning rate: 0.001
2024-11-05 05:37:38,966 - INFO - [diffusion][Epoch 10613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:38,967 - INFO - [diffusion][Epoch 10614] Epoch 10615/12000
2024-11-05 05:37:43,111 - INFO - [diffusion][Epoch 10614] diffusion training Loss: 0.04477937240153551
2024-11-05 05:37:43,113 - INFO - [diffusion][Epoch 10614] diffusion learning rate: 0.001
2024-11-05 05:37:43,114 - INFO - [diffusion][Epoch 10614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:43,115 - INFO - [diffusion][Epoch 10615] Epoch 10616/12000
2024-11-05 05:37:47,271 - INFO - [diffusion][Epoch 10615] diffusion training Loss: 0.048507402651011944
2024-11-05 05:37:47,273 - INFO - [diffusion][Epoch 10615] diffusion learning rate: 0.001
2024-11-05 05:37:47,274 - INFO - [diffusion][Epoch 10615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:47,276 - INFO - [diffusion][Epoch 10616] Epoch 10617/12000
2024-11-05 05:37:51,827 - INFO - [diffusion][Epoch 10616] diffusion training Loss: 0.04983608331531286
2024-11-05 05:37:51,829 - INFO - [diffusion][Epoch 10616] diffusion learning rate: 0.001
2024-11-05 05:37:51,831 - INFO - [diffusion][Epoch 10616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:51,832 - INFO - [diffusion][Epoch 10617] Epoch 10618/12000
2024-11-05 05:37:55,878 - INFO - [diffusion][Epoch 10617] diffusion training Loss: 0.04836476594209671
2024-11-05 05:37:55,879 - INFO - [diffusion][Epoch 10617] diffusion learning rate: 0.001
2024-11-05 05:37:55,881 - INFO - [diffusion][Epoch 10617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:55,882 - INFO - [diffusion][Epoch 10618] Epoch 10619/12000
2024-11-05 05:37:59,792 - INFO - [diffusion][Epoch 10618] diffusion training Loss: 0.0461131539195776
2024-11-05 05:37:59,795 - INFO - [diffusion][Epoch 10618] diffusion learning rate: 0.001
2024-11-05 05:37:59,797 - INFO - [diffusion][Epoch 10618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:37:59,798 - INFO - [diffusion][Epoch 10619] Epoch 10620/12000
2024-11-05 05:38:03,888 - INFO - [diffusion][Epoch 10619] diffusion training Loss: 0.04903757385909557
2024-11-05 05:38:03,890 - INFO - [diffusion][Epoch 10619] diffusion learning rate: 0.001
2024-11-05 05:38:03,891 - INFO - [diffusion][Epoch 10619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:03,892 - INFO - [diffusion][Epoch 10620] Epoch 10621/12000
2024-11-05 05:38:07,995 - INFO - [diffusion][Epoch 10620] diffusion training Loss: 0.04748562444001436
2024-11-05 05:38:07,997 - INFO - [diffusion][Epoch 10620] diffusion learning rate: 0.001
2024-11-05 05:38:07,998 - INFO - [diffusion][Epoch 10620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:08,000 - INFO - [diffusion][Epoch 10621] Epoch 10622/12000
2024-11-05 05:38:12,074 - INFO - [diffusion][Epoch 10621] diffusion training Loss: 0.04621928930282593
2024-11-05 05:38:12,415 - INFO - [diffusion][Epoch 10621] diffusion learning rate: 0.001
2024-11-05 05:38:12,417 - INFO - [diffusion][Epoch 10621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:12,418 - INFO - [diffusion][Epoch 10622] Epoch 10623/12000
2024-11-05 05:38:16,537 - INFO - [diffusion][Epoch 10622] diffusion training Loss: 0.052773027680814266
2024-11-05 05:38:16,539 - INFO - [diffusion][Epoch 10622] diffusion learning rate: 0.001
2024-11-05 05:38:16,541 - INFO - [diffusion][Epoch 10622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:16,542 - INFO - [diffusion][Epoch 10623] Epoch 10624/12000
2024-11-05 05:38:20,496 - INFO - [diffusion][Epoch 10623] diffusion training Loss: 0.04552190471440554
2024-11-05 05:38:20,498 - INFO - [diffusion][Epoch 10623] diffusion learning rate: 0.001
2024-11-05 05:38:20,500 - INFO - [diffusion][Epoch 10623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:20,501 - INFO - [diffusion][Epoch 10624] Epoch 10625/12000
2024-11-05 05:38:24,576 - INFO - [diffusion][Epoch 10624] diffusion training Loss: 0.05007057171314955
2024-11-05 05:38:24,578 - INFO - [diffusion][Epoch 10624] diffusion learning rate: 0.001
2024-11-05 05:38:24,580 - INFO - [diffusion][Epoch 10624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:24,582 - INFO - [diffusion][Epoch 10625] Epoch 10626/12000
2024-11-05 05:38:28,648 - INFO - [diffusion][Epoch 10625] diffusion training Loss: 0.040984632447361946
2024-11-05 05:38:28,650 - INFO - [diffusion][Epoch 10625] diffusion learning rate: 0.001
2024-11-05 05:38:28,651 - INFO - [diffusion][Epoch 10625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:28,653 - INFO - [diffusion][Epoch 10626] Epoch 10627/12000
2024-11-05 05:38:32,743 - INFO - [diffusion][Epoch 10626] diffusion training Loss: 0.051995293237268925
2024-11-05 05:38:32,745 - INFO - [diffusion][Epoch 10626] diffusion learning rate: 0.001
2024-11-05 05:38:32,747 - INFO - [diffusion][Epoch 10626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:32,748 - INFO - [diffusion][Epoch 10627] Epoch 10628/12000
2024-11-05 05:38:36,821 - INFO - [diffusion][Epoch 10627] diffusion training Loss: 0.042145456187427044
2024-11-05 05:38:36,823 - INFO - [diffusion][Epoch 10627] diffusion learning rate: 0.001
2024-11-05 05:38:36,825 - INFO - [diffusion][Epoch 10627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:36,826 - INFO - [diffusion][Epoch 10628] Epoch 10629/12000
2024-11-05 05:38:40,926 - INFO - [diffusion][Epoch 10628] diffusion training Loss: 0.045781683176755905
2024-11-05 05:38:40,928 - INFO - [diffusion][Epoch 10628] diffusion learning rate: 0.001
2024-11-05 05:38:40,929 - INFO - [diffusion][Epoch 10628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:40,931 - INFO - [diffusion][Epoch 10629] Epoch 10630/12000
2024-11-05 05:38:45,015 - INFO - [diffusion][Epoch 10629] diffusion training Loss: 0.046281701885163784
2024-11-05 05:38:45,017 - INFO - [diffusion][Epoch 10629] diffusion learning rate: 0.001
2024-11-05 05:38:45,019 - INFO - [diffusion][Epoch 10629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:45,020 - INFO - [diffusion][Epoch 10630] Epoch 10631/12000
2024-11-05 05:38:49,136 - INFO - [diffusion][Epoch 10630] diffusion training Loss: 0.045053442008793354
2024-11-05 05:38:49,138 - INFO - [diffusion][Epoch 10630] diffusion learning rate: 0.001
2024-11-05 05:38:49,140 - INFO - [diffusion][Epoch 10630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:49,141 - INFO - [diffusion][Epoch 10631] Epoch 10632/12000
2024-11-05 05:38:53,264 - INFO - [diffusion][Epoch 10631] diffusion training Loss: 0.05021852068603039
2024-11-05 05:38:53,266 - INFO - [diffusion][Epoch 10631] diffusion learning rate: 0.001
2024-11-05 05:38:53,268 - INFO - [diffusion][Epoch 10631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:53,270 - INFO - [diffusion][Epoch 10632] Epoch 10633/12000
2024-11-05 05:38:57,287 - INFO - [diffusion][Epoch 10632] diffusion training Loss: 0.04392333794385195
2024-11-05 05:38:57,289 - INFO - [diffusion][Epoch 10632] diffusion learning rate: 0.001
2024-11-05 05:38:57,291 - INFO - [diffusion][Epoch 10632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:38:57,292 - INFO - [diffusion][Epoch 10633] Epoch 10634/12000
2024-11-05 05:39:01,203 - INFO - [diffusion][Epoch 10633] diffusion training Loss: 0.047362858429551125
2024-11-05 05:39:01,206 - INFO - [diffusion][Epoch 10633] diffusion learning rate: 0.001
2024-11-05 05:39:01,208 - INFO - [diffusion][Epoch 10633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:01,209 - INFO - [diffusion][Epoch 10634] Epoch 10635/12000
2024-11-05 05:39:05,348 - INFO - [diffusion][Epoch 10634] diffusion training Loss: 0.048940072767436504
2024-11-05 05:39:05,350 - INFO - [diffusion][Epoch 10634] diffusion learning rate: 0.001
2024-11-05 05:39:05,351 - INFO - [diffusion][Epoch 10634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:05,352 - INFO - [diffusion][Epoch 10635] Epoch 10636/12000
2024-11-05 05:39:09,453 - INFO - [diffusion][Epoch 10635] diffusion training Loss: 0.05077049322426319
2024-11-05 05:39:09,455 - INFO - [diffusion][Epoch 10635] diffusion learning rate: 0.001
2024-11-05 05:39:09,457 - INFO - [diffusion][Epoch 10635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:09,458 - INFO - [diffusion][Epoch 10636] Epoch 10637/12000
2024-11-05 05:39:13,549 - INFO - [diffusion][Epoch 10636] diffusion training Loss: 0.04418251849710941
2024-11-05 05:39:13,550 - INFO - [diffusion][Epoch 10636] diffusion learning rate: 0.001
2024-11-05 05:39:13,552 - INFO - [diffusion][Epoch 10636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:13,553 - INFO - [diffusion][Epoch 10637] Epoch 10638/12000
2024-11-05 05:39:17,853 - INFO - [diffusion][Epoch 10637] diffusion training Loss: 0.05020802654325962
2024-11-05 05:39:17,856 - INFO - [diffusion][Epoch 10637] diffusion learning rate: 0.001
2024-11-05 05:39:17,857 - INFO - [diffusion][Epoch 10637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:17,859 - INFO - [diffusion][Epoch 10638] Epoch 10639/12000
2024-11-05 05:39:21,939 - INFO - [diffusion][Epoch 10638] diffusion training Loss: 0.04517318680882454
2024-11-05 05:39:21,941 - INFO - [diffusion][Epoch 10638] diffusion learning rate: 0.001
2024-11-05 05:39:21,943 - INFO - [diffusion][Epoch 10638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:21,944 - INFO - [diffusion][Epoch 10639] Epoch 10640/12000
2024-11-05 05:39:26,063 - INFO - [diffusion][Epoch 10639] diffusion training Loss: 0.04967551864683628
2024-11-05 05:39:26,065 - INFO - [diffusion][Epoch 10639] diffusion learning rate: 0.001
2024-11-05 05:39:26,067 - INFO - [diffusion][Epoch 10639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:26,068 - INFO - [diffusion][Epoch 10640] Epoch 10641/12000
2024-11-05 05:39:30,192 - INFO - [diffusion][Epoch 10640] diffusion training Loss: 0.05120459012687206
2024-11-05 05:39:30,194 - INFO - [diffusion][Epoch 10640] diffusion learning rate: 0.001
2024-11-05 05:39:30,196 - INFO - [diffusion][Epoch 10640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:30,197 - INFO - [diffusion][Epoch 10641] Epoch 10642/12000
2024-11-05 05:39:34,312 - INFO - [diffusion][Epoch 10641] diffusion training Loss: 0.04384975228458643
2024-11-05 05:39:34,314 - INFO - [diffusion][Epoch 10641] diffusion learning rate: 0.001
2024-11-05 05:39:34,315 - INFO - [diffusion][Epoch 10641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:34,316 - INFO - [diffusion][Epoch 10642] Epoch 10643/12000
2024-11-05 05:39:38,408 - INFO - [diffusion][Epoch 10642] diffusion training Loss: 0.045645713806152344
2024-11-05 05:39:38,410 - INFO - [diffusion][Epoch 10642] diffusion learning rate: 0.001
2024-11-05 05:39:38,412 - INFO - [diffusion][Epoch 10642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:38,413 - INFO - [diffusion][Epoch 10643] Epoch 10644/12000
2024-11-05 05:39:42,561 - INFO - [diffusion][Epoch 10643] diffusion training Loss: 0.04573927167803049
2024-11-05 05:39:42,563 - INFO - [diffusion][Epoch 10643] diffusion learning rate: 0.001
2024-11-05 05:39:42,565 - INFO - [diffusion][Epoch 10643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:42,566 - INFO - [diffusion][Epoch 10644] Epoch 10645/12000
2024-11-05 05:39:46,632 - INFO - [diffusion][Epoch 10644] diffusion training Loss: 0.05159551929682493
2024-11-05 05:39:46,634 - INFO - [diffusion][Epoch 10644] diffusion learning rate: 0.001
2024-11-05 05:39:46,636 - INFO - [diffusion][Epoch 10644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:46,637 - INFO - [diffusion][Epoch 10645] Epoch 10646/12000
2024-11-05 05:39:50,755 - INFO - [diffusion][Epoch 10645] diffusion training Loss: 0.050565033219754696
2024-11-05 05:39:50,758 - INFO - [diffusion][Epoch 10645] diffusion learning rate: 0.001
2024-11-05 05:39:50,760 - INFO - [diffusion][Epoch 10645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:50,761 - INFO - [diffusion][Epoch 10646] Epoch 10647/12000
2024-11-05 05:39:54,782 - INFO - [diffusion][Epoch 10646] diffusion training Loss: 0.04549820441752672
2024-11-05 05:39:54,784 - INFO - [diffusion][Epoch 10646] diffusion learning rate: 0.001
2024-11-05 05:39:54,786 - INFO - [diffusion][Epoch 10646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:54,787 - INFO - [diffusion][Epoch 10647] Epoch 10648/12000
2024-11-05 05:39:58,910 - INFO - [diffusion][Epoch 10647] diffusion training Loss: 0.04750227741897106
2024-11-05 05:39:58,912 - INFO - [diffusion][Epoch 10647] diffusion learning rate: 0.001
2024-11-05 05:39:58,914 - INFO - [diffusion][Epoch 10647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:39:58,915 - INFO - [diffusion][Epoch 10648] Epoch 10649/12000
2024-11-05 05:40:02,985 - INFO - [diffusion][Epoch 10648] diffusion training Loss: 0.04838897846639156
2024-11-05 05:40:02,988 - INFO - [diffusion][Epoch 10648] diffusion learning rate: 0.001
2024-11-05 05:40:02,990 - INFO - [diffusion][Epoch 10648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:02,992 - INFO - [diffusion][Epoch 10649] Epoch 10650/12000
2024-11-05 05:40:07,050 - INFO - [diffusion][Epoch 10649] diffusion training Loss: 0.04830484837293625
2024-11-05 05:40:07,051 - INFO - [diffusion][Epoch 10649] diffusion learning rate: 0.001
2024-11-05 05:40:07,053 - INFO - [diffusion][Epoch 10649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:07,054 - INFO - [diffusion][Epoch 10650] Epoch 10651/12000
2024-11-05 05:40:10,956 - INFO - [diffusion][Epoch 10650] diffusion training Loss: 0.05176982656121254
2024-11-05 05:40:10,958 - INFO - [diffusion][Epoch 10650] diffusion learning rate: 0.001
2024-11-05 05:40:10,960 - INFO - [diffusion][Epoch 10650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:10,961 - INFO - [diffusion][Epoch 10651] Epoch 10652/12000
2024-11-05 05:40:14,913 - INFO - [diffusion][Epoch 10651] diffusion training Loss: 0.04818489123135805
2024-11-05 05:40:14,915 - INFO - [diffusion][Epoch 10651] diffusion learning rate: 0.001
2024-11-05 05:40:14,916 - INFO - [diffusion][Epoch 10651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:14,918 - INFO - [diffusion][Epoch 10652] Epoch 10653/12000
2024-11-05 05:40:19,062 - INFO - [diffusion][Epoch 10652] diffusion training Loss: 0.05124434642493725
2024-11-05 05:40:19,064 - INFO - [diffusion][Epoch 10652] diffusion learning rate: 0.001
2024-11-05 05:40:19,066 - INFO - [diffusion][Epoch 10652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:19,067 - INFO - [diffusion][Epoch 10653] Epoch 10654/12000
2024-11-05 05:40:23,151 - INFO - [diffusion][Epoch 10653] diffusion training Loss: 0.04777282848954201
2024-11-05 05:40:23,153 - INFO - [diffusion][Epoch 10653] diffusion learning rate: 0.001
2024-11-05 05:40:23,155 - INFO - [diffusion][Epoch 10653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:23,156 - INFO - [diffusion][Epoch 10654] Epoch 10655/12000
2024-11-05 05:40:27,291 - INFO - [diffusion][Epoch 10654] diffusion training Loss: 0.05008077993988991
2024-11-05 05:40:27,293 - INFO - [diffusion][Epoch 10654] diffusion learning rate: 0.001
2024-11-05 05:40:27,294 - INFO - [diffusion][Epoch 10654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:27,296 - INFO - [diffusion][Epoch 10655] Epoch 10656/12000
2024-11-05 05:40:31,406 - INFO - [diffusion][Epoch 10655] diffusion training Loss: 0.04706752020865679
2024-11-05 05:40:31,408 - INFO - [diffusion][Epoch 10655] diffusion learning rate: 0.001
2024-11-05 05:40:31,410 - INFO - [diffusion][Epoch 10655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:31,411 - INFO - [diffusion][Epoch 10656] Epoch 10657/12000
2024-11-05 05:40:35,508 - INFO - [diffusion][Epoch 10656] diffusion training Loss: 0.04912588465958834
2024-11-05 05:40:35,510 - INFO - [diffusion][Epoch 10656] diffusion learning rate: 0.001
2024-11-05 05:40:35,512 - INFO - [diffusion][Epoch 10656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:35,514 - INFO - [diffusion][Epoch 10657] Epoch 10658/12000
2024-11-05 05:40:39,828 - INFO - [diffusion][Epoch 10657] diffusion training Loss: 0.04848397243767977
2024-11-05 05:40:39,830 - INFO - [diffusion][Epoch 10657] diffusion learning rate: 0.001
2024-11-05 05:40:39,832 - INFO - [diffusion][Epoch 10657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:39,833 - INFO - [diffusion][Epoch 10658] Epoch 10659/12000
2024-11-05 05:40:43,913 - INFO - [diffusion][Epoch 10658] diffusion training Loss: 0.04879518412053585
2024-11-05 05:40:43,915 - INFO - [diffusion][Epoch 10658] diffusion learning rate: 0.001
2024-11-05 05:40:43,917 - INFO - [diffusion][Epoch 10658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:43,918 - INFO - [diffusion][Epoch 10659] Epoch 10660/12000
2024-11-05 05:40:47,809 - INFO - [diffusion][Epoch 10659] diffusion training Loss: 0.05019343364983797
2024-11-05 05:40:47,811 - INFO - [diffusion][Epoch 10659] diffusion learning rate: 0.001
2024-11-05 05:40:47,813 - INFO - [diffusion][Epoch 10659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:47,814 - INFO - [diffusion][Epoch 10660] Epoch 10661/12000
2024-11-05 05:40:51,820 - INFO - [diffusion][Epoch 10660] diffusion training Loss: 0.041436124593019485
2024-11-05 05:40:51,822 - INFO - [diffusion][Epoch 10660] diffusion learning rate: 0.001
2024-11-05 05:40:51,824 - INFO - [diffusion][Epoch 10660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:51,825 - INFO - [diffusion][Epoch 10661] Epoch 10662/12000
2024-11-05 05:40:55,931 - INFO - [diffusion][Epoch 10661] diffusion training Loss: 0.047545974142849445
2024-11-05 05:40:55,932 - INFO - [diffusion][Epoch 10661] diffusion learning rate: 0.001
2024-11-05 05:40:55,935 - INFO - [diffusion][Epoch 10661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:40:55,936 - INFO - [diffusion][Epoch 10662] Epoch 10663/12000
2024-11-05 05:41:00,029 - INFO - [diffusion][Epoch 10662] diffusion training Loss: 0.04702104348689318
2024-11-05 05:41:00,031 - INFO - [diffusion][Epoch 10662] diffusion learning rate: 0.001
2024-11-05 05:41:00,033 - INFO - [diffusion][Epoch 10662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:00,034 - INFO - [diffusion][Epoch 10663] Epoch 10664/12000
2024-11-05 05:41:04,163 - INFO - [diffusion][Epoch 10663] diffusion training Loss: 0.04675929993391037
2024-11-05 05:41:04,167 - INFO - [diffusion][Epoch 10663] diffusion learning rate: 0.001
2024-11-05 05:41:04,168 - INFO - [diffusion][Epoch 10663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:04,170 - INFO - [diffusion][Epoch 10664] Epoch 10665/12000
2024-11-05 05:41:08,216 - INFO - [diffusion][Epoch 10664] diffusion training Loss: 0.04528815485537052
2024-11-05 05:41:08,218 - INFO - [diffusion][Epoch 10664] diffusion learning rate: 0.001
2024-11-05 05:41:08,220 - INFO - [diffusion][Epoch 10664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:08,221 - INFO - [diffusion][Epoch 10665] Epoch 10666/12000
2024-11-05 05:41:12,314 - INFO - [diffusion][Epoch 10665] diffusion training Loss: 0.04651363845914602
2024-11-05 05:41:12,414 - INFO - [diffusion][Epoch 10665] diffusion learning rate: 0.001
2024-11-05 05:41:12,416 - INFO - [diffusion][Epoch 10665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:12,417 - INFO - [diffusion][Epoch 10666] Epoch 10667/12000
2024-11-05 05:41:16,377 - INFO - [diffusion][Epoch 10666] diffusion training Loss: 0.047324120067059994
2024-11-05 05:41:16,379 - INFO - [diffusion][Epoch 10666] diffusion learning rate: 0.001
2024-11-05 05:41:16,380 - INFO - [diffusion][Epoch 10666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:16,382 - INFO - [diffusion][Epoch 10667] Epoch 10668/12000
2024-11-05 05:41:20,515 - INFO - [diffusion][Epoch 10667] diffusion training Loss: 0.04509760532528162
2024-11-05 05:41:20,517 - INFO - [diffusion][Epoch 10667] diffusion learning rate: 0.001
2024-11-05 05:41:20,519 - INFO - [diffusion][Epoch 10667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:20,520 - INFO - [diffusion][Epoch 10668] Epoch 10669/12000
2024-11-05 05:41:24,566 - INFO - [diffusion][Epoch 10668] diffusion training Loss: 0.04814158007502556
2024-11-05 05:41:24,568 - INFO - [diffusion][Epoch 10668] diffusion learning rate: 0.001
2024-11-05 05:41:24,570 - INFO - [diffusion][Epoch 10668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:24,571 - INFO - [diffusion][Epoch 10669] Epoch 10670/12000
2024-11-05 05:41:28,623 - INFO - [diffusion][Epoch 10669] diffusion training Loss: 0.0509021058678627
2024-11-05 05:41:28,625 - INFO - [diffusion][Epoch 10669] diffusion learning rate: 0.001
2024-11-05 05:41:28,627 - INFO - [diffusion][Epoch 10669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:28,628 - INFO - [diffusion][Epoch 10670] Epoch 10671/12000
2024-11-05 05:41:32,699 - INFO - [diffusion][Epoch 10670] diffusion training Loss: 0.04629559628665447
2024-11-05 05:41:32,701 - INFO - [diffusion][Epoch 10670] diffusion learning rate: 0.001
2024-11-05 05:41:32,703 - INFO - [diffusion][Epoch 10670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:32,704 - INFO - [diffusion][Epoch 10671] Epoch 10672/12000
2024-11-05 05:41:36,782 - INFO - [diffusion][Epoch 10671] diffusion training Loss: 0.04739385098218918
2024-11-05 05:41:36,784 - INFO - [diffusion][Epoch 10671] diffusion learning rate: 0.001
2024-11-05 05:41:36,785 - INFO - [diffusion][Epoch 10671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:36,787 - INFO - [diffusion][Epoch 10672] Epoch 10673/12000
2024-11-05 05:41:40,881 - INFO - [diffusion][Epoch 10672] diffusion training Loss: 0.04510166682302952
2024-11-05 05:41:40,883 - INFO - [diffusion][Epoch 10672] diffusion learning rate: 0.001
2024-11-05 05:41:40,884 - INFO - [diffusion][Epoch 10672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:40,886 - INFO - [diffusion][Epoch 10673] Epoch 10674/12000
2024-11-05 05:41:44,964 - INFO - [diffusion][Epoch 10673] diffusion training Loss: 0.04823056608438492
2024-11-05 05:41:44,967 - INFO - [diffusion][Epoch 10673] diffusion learning rate: 0.001
2024-11-05 05:41:45,008 - INFO - [diffusion][Epoch 10673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:45,009 - INFO - [diffusion][Epoch 10674] Epoch 10675/12000
2024-11-05 05:41:49,065 - INFO - [diffusion][Epoch 10674] diffusion training Loss: 0.04293346405029297
2024-11-05 05:41:49,067 - INFO - [diffusion][Epoch 10674] diffusion learning rate: 0.001
2024-11-05 05:41:49,068 - INFO - [diffusion][Epoch 10674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:49,070 - INFO - [diffusion][Epoch 10675] Epoch 10676/12000
2024-11-05 05:41:53,140 - INFO - [diffusion][Epoch 10675] diffusion training Loss: 0.05192956980317831
2024-11-05 05:41:53,142 - INFO - [diffusion][Epoch 10675] diffusion learning rate: 0.001
2024-11-05 05:41:53,144 - INFO - [diffusion][Epoch 10675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:53,145 - INFO - [diffusion][Epoch 10676] Epoch 10677/12000
2024-11-05 05:41:57,251 - INFO - [diffusion][Epoch 10676] diffusion training Loss: 0.05419026501476765
2024-11-05 05:41:57,253 - INFO - [diffusion][Epoch 10676] diffusion learning rate: 0.001
2024-11-05 05:41:57,255 - INFO - [diffusion][Epoch 10676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:41:57,256 - INFO - [diffusion][Epoch 10677] Epoch 10678/12000
2024-11-05 05:42:01,351 - INFO - [diffusion][Epoch 10677] diffusion training Loss: 0.047179355286061764
2024-11-05 05:42:01,353 - INFO - [diffusion][Epoch 10677] diffusion learning rate: 0.001
2024-11-05 05:42:01,354 - INFO - [diffusion][Epoch 10677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:01,356 - INFO - [diffusion][Epoch 10678] Epoch 10679/12000
2024-11-05 05:42:05,460 - INFO - [diffusion][Epoch 10678] diffusion training Loss: 0.04960796609520912
2024-11-05 05:42:05,464 - INFO - [diffusion][Epoch 10678] diffusion learning rate: 0.001
2024-11-05 05:42:05,465 - INFO - [diffusion][Epoch 10678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:05,467 - INFO - [diffusion][Epoch 10679] Epoch 10680/12000
2024-11-05 05:42:09,553 - INFO - [diffusion][Epoch 10679] diffusion training Loss: 0.047270745038986206
2024-11-05 05:42:09,555 - INFO - [diffusion][Epoch 10679] diffusion learning rate: 0.001
2024-11-05 05:42:09,557 - INFO - [diffusion][Epoch 10679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:09,558 - INFO - [diffusion][Epoch 10680] Epoch 10681/12000
2024-11-05 05:42:13,718 - INFO - [diffusion][Epoch 10680] diffusion training Loss: 0.05166992824524641
2024-11-05 05:42:13,720 - INFO - [diffusion][Epoch 10680] diffusion learning rate: 0.001
2024-11-05 05:42:13,722 - INFO - [diffusion][Epoch 10680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:13,723 - INFO - [diffusion][Epoch 10681] Epoch 10682/12000
2024-11-05 05:42:17,831 - INFO - [diffusion][Epoch 10681] diffusion training Loss: 0.045831129886209965
2024-11-05 05:42:17,833 - INFO - [diffusion][Epoch 10681] diffusion learning rate: 0.001
2024-11-05 05:42:17,835 - INFO - [diffusion][Epoch 10681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:17,836 - INFO - [diffusion][Epoch 10682] Epoch 10683/12000
2024-11-05 05:42:21,930 - INFO - [diffusion][Epoch 10682] diffusion training Loss: 0.044129046611487865
2024-11-05 05:42:21,932 - INFO - [diffusion][Epoch 10682] diffusion learning rate: 0.001
2024-11-05 05:42:21,933 - INFO - [diffusion][Epoch 10682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:21,935 - INFO - [diffusion][Epoch 10683] Epoch 10684/12000
2024-11-05 05:42:26,030 - INFO - [diffusion][Epoch 10683] diffusion training Loss: 0.047697948291897774
2024-11-05 05:42:26,032 - INFO - [diffusion][Epoch 10683] diffusion learning rate: 0.001
2024-11-05 05:42:26,033 - INFO - [diffusion][Epoch 10683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:26,035 - INFO - [diffusion][Epoch 10684] Epoch 10685/12000
2024-11-05 05:42:30,172 - INFO - [diffusion][Epoch 10684] diffusion training Loss: 0.04592381324619055
2024-11-05 05:42:30,174 - INFO - [diffusion][Epoch 10684] diffusion learning rate: 0.001
2024-11-05 05:42:30,176 - INFO - [diffusion][Epoch 10684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:30,178 - INFO - [diffusion][Epoch 10685] Epoch 10686/12000
2024-11-05 05:42:34,268 - INFO - [diffusion][Epoch 10685] diffusion training Loss: 0.04784065950661898
2024-11-05 05:42:34,270 - INFO - [diffusion][Epoch 10685] diffusion learning rate: 0.001
2024-11-05 05:42:34,272 - INFO - [diffusion][Epoch 10685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:34,273 - INFO - [diffusion][Epoch 10686] Epoch 10687/12000
2024-11-05 05:42:38,404 - INFO - [diffusion][Epoch 10686] diffusion training Loss: 0.043992371298372746
2024-11-05 05:42:38,406 - INFO - [diffusion][Epoch 10686] diffusion learning rate: 0.001
2024-11-05 05:42:38,408 - INFO - [diffusion][Epoch 10686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:38,409 - INFO - [diffusion][Epoch 10687] Epoch 10688/12000
2024-11-05 05:42:42,517 - INFO - [diffusion][Epoch 10687] diffusion training Loss: 0.04758861754089594
2024-11-05 05:42:42,519 - INFO - [diffusion][Epoch 10687] diffusion learning rate: 0.001
2024-11-05 05:42:42,521 - INFO - [diffusion][Epoch 10687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:42,522 - INFO - [diffusion][Epoch 10688] Epoch 10689/12000
2024-11-05 05:42:46,619 - INFO - [diffusion][Epoch 10688] diffusion training Loss: 0.044193828478455544
2024-11-05 05:42:46,621 - INFO - [diffusion][Epoch 10688] diffusion learning rate: 0.001
2024-11-05 05:42:46,622 - INFO - [diffusion][Epoch 10688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:46,624 - INFO - [diffusion][Epoch 10689] Epoch 10690/12000
2024-11-05 05:42:50,797 - INFO - [diffusion][Epoch 10689] diffusion training Loss: 0.05002261698246002
2024-11-05 05:42:50,799 - INFO - [diffusion][Epoch 10689] diffusion learning rate: 0.001
2024-11-05 05:42:50,801 - INFO - [diffusion][Epoch 10689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:50,802 - INFO - [diffusion][Epoch 10690] Epoch 10691/12000
2024-11-05 05:42:54,900 - INFO - [diffusion][Epoch 10690] diffusion training Loss: 0.050124868750572205
2024-11-05 05:42:54,902 - INFO - [diffusion][Epoch 10690] diffusion learning rate: 0.001
2024-11-05 05:42:54,903 - INFO - [diffusion][Epoch 10690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:54,905 - INFO - [diffusion][Epoch 10691] Epoch 10692/12000
2024-11-05 05:42:58,789 - INFO - [diffusion][Epoch 10691] diffusion training Loss: 0.04913945309817791
2024-11-05 05:42:58,791 - INFO - [diffusion][Epoch 10691] diffusion learning rate: 0.001
2024-11-05 05:42:58,793 - INFO - [diffusion][Epoch 10691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:42:58,794 - INFO - [diffusion][Epoch 10692] Epoch 10693/12000
2024-11-05 05:43:02,864 - INFO - [diffusion][Epoch 10692] diffusion training Loss: 0.051500347442924976
2024-11-05 05:43:02,866 - INFO - [diffusion][Epoch 10692] diffusion learning rate: 0.001
2024-11-05 05:43:02,893 - INFO - [diffusion][Epoch 10692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:02,895 - INFO - [diffusion][Epoch 10693] Epoch 10694/12000
2024-11-05 05:43:06,971 - INFO - [diffusion][Epoch 10693] diffusion training Loss: 0.051212857477366924
2024-11-05 05:43:06,975 - INFO - [diffusion][Epoch 10693] diffusion learning rate: 0.001
2024-11-05 05:43:06,977 - INFO - [diffusion][Epoch 10693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:06,978 - INFO - [diffusion][Epoch 10694] Epoch 10695/12000
2024-11-05 05:43:11,037 - INFO - [diffusion][Epoch 10694] diffusion training Loss: 0.04831462353467941
2024-11-05 05:43:11,039 - INFO - [diffusion][Epoch 10694] diffusion learning rate: 0.001
2024-11-05 05:43:11,040 - INFO - [diffusion][Epoch 10694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:11,042 - INFO - [diffusion][Epoch 10695] Epoch 10696/12000
2024-11-05 05:43:15,127 - INFO - [diffusion][Epoch 10695] diffusion training Loss: 0.045099145732820034
2024-11-05 05:43:15,129 - INFO - [diffusion][Epoch 10695] diffusion learning rate: 0.001
2024-11-05 05:43:15,158 - INFO - [diffusion][Epoch 10695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:15,159 - INFO - [diffusion][Epoch 10696] Epoch 10697/12000
2024-11-05 05:43:19,284 - INFO - [diffusion][Epoch 10696] diffusion training Loss: 0.04721240885555744
2024-11-05 05:43:19,286 - INFO - [diffusion][Epoch 10696] diffusion learning rate: 0.001
2024-11-05 05:43:19,288 - INFO - [diffusion][Epoch 10696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:19,289 - INFO - [diffusion][Epoch 10697] Epoch 10698/12000
2024-11-05 05:43:23,429 - INFO - [diffusion][Epoch 10697] diffusion training Loss: 0.048366570845246315
2024-11-05 05:43:23,430 - INFO - [diffusion][Epoch 10697] diffusion learning rate: 0.001
2024-11-05 05:43:23,432 - INFO - [diffusion][Epoch 10697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:23,433 - INFO - [diffusion][Epoch 10698] Epoch 10699/12000
2024-11-05 05:43:27,512 - INFO - [diffusion][Epoch 10698] diffusion training Loss: 0.0477958619594574
2024-11-05 05:43:27,514 - INFO - [diffusion][Epoch 10698] diffusion learning rate: 0.001
2024-11-05 05:43:27,516 - INFO - [diffusion][Epoch 10698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:27,517 - INFO - [diffusion][Epoch 10699] Epoch 10700/12000
2024-11-05 05:43:31,661 - INFO - [diffusion][Epoch 10699] diffusion training Loss: 0.043982974253594875
2024-11-05 05:43:31,663 - INFO - [diffusion][Epoch 10699] diffusion learning rate: 0.001
2024-11-05 05:43:31,665 - INFO - [diffusion][Epoch 10699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:31,666 - INFO - [diffusion][Epoch 10700] Epoch 10701/12000
2024-11-05 05:43:35,783 - INFO - [diffusion][Epoch 10700] diffusion training Loss: 0.052191332913935184
2024-11-05 05:43:35,785 - INFO - [diffusion][Epoch 10700] diffusion learning rate: 0.001
2024-11-05 05:43:35,786 - INFO - [diffusion][Epoch 10700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:35,788 - INFO - [diffusion][Epoch 10701] Epoch 10702/12000
2024-11-05 05:43:39,925 - INFO - [diffusion][Epoch 10701] diffusion training Loss: 0.05162403732538223
2024-11-05 05:43:39,927 - INFO - [diffusion][Epoch 10701] diffusion learning rate: 0.001
2024-11-05 05:43:39,928 - INFO - [diffusion][Epoch 10701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:39,929 - INFO - [diffusion][Epoch 10702] Epoch 10703/12000
2024-11-05 05:43:44,053 - INFO - [diffusion][Epoch 10702] diffusion training Loss: 0.04623523820191622
2024-11-05 05:43:44,055 - INFO - [diffusion][Epoch 10702] diffusion learning rate: 0.001
2024-11-05 05:43:44,057 - INFO - [diffusion][Epoch 10702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:44,058 - INFO - [diffusion][Epoch 10703] Epoch 10704/12000
2024-11-05 05:43:48,197 - INFO - [diffusion][Epoch 10703] diffusion training Loss: 0.055089169181883335
2024-11-05 05:43:48,199 - INFO - [diffusion][Epoch 10703] diffusion learning rate: 0.001
2024-11-05 05:43:48,200 - INFO - [diffusion][Epoch 10703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:48,202 - INFO - [diffusion][Epoch 10704] Epoch 10705/12000
2024-11-05 05:43:52,294 - INFO - [diffusion][Epoch 10704] diffusion training Loss: 0.051172840408980846
2024-11-05 05:43:52,296 - INFO - [diffusion][Epoch 10704] diffusion learning rate: 0.001
2024-11-05 05:43:52,298 - INFO - [diffusion][Epoch 10704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:52,299 - INFO - [diffusion][Epoch 10705] Epoch 10706/12000
2024-11-05 05:43:56,416 - INFO - [diffusion][Epoch 10705] diffusion training Loss: 0.04639224894344807
2024-11-05 05:43:56,418 - INFO - [diffusion][Epoch 10705] diffusion learning rate: 0.001
2024-11-05 05:43:56,419 - INFO - [diffusion][Epoch 10705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:43:56,421 - INFO - [diffusion][Epoch 10706] Epoch 10707/12000
2024-11-05 05:44:00,524 - INFO - [diffusion][Epoch 10706] diffusion training Loss: 0.05215869564563036
2024-11-05 05:44:00,526 - INFO - [diffusion][Epoch 10706] diffusion learning rate: 0.001
2024-11-05 05:44:00,528 - INFO - [diffusion][Epoch 10706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:00,529 - INFO - [diffusion][Epoch 10707] Epoch 10708/12000
2024-11-05 05:44:04,638 - INFO - [diffusion][Epoch 10707] diffusion training Loss: 0.04889432527124882
2024-11-05 05:44:04,640 - INFO - [diffusion][Epoch 10707] diffusion learning rate: 0.001
2024-11-05 05:44:04,642 - INFO - [diffusion][Epoch 10707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:04,643 - INFO - [diffusion][Epoch 10708] Epoch 10709/12000
2024-11-05 05:44:08,641 - INFO - [diffusion][Epoch 10708] diffusion training Loss: 0.05211768765002489
2024-11-05 05:44:08,645 - INFO - [diffusion][Epoch 10708] diffusion learning rate: 0.001
2024-11-05 05:44:08,646 - INFO - [diffusion][Epoch 10708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:08,648 - INFO - [diffusion][Epoch 10709] Epoch 10710/12000
2024-11-05 05:44:12,734 - INFO - [diffusion][Epoch 10709] diffusion training Loss: 0.04680642578750849
2024-11-05 05:44:12,736 - INFO - [diffusion][Epoch 10709] diffusion learning rate: 0.001
2024-11-05 05:44:12,738 - INFO - [diffusion][Epoch 10709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:12,739 - INFO - [diffusion][Epoch 10710] Epoch 10711/12000
2024-11-05 05:44:16,715 - INFO - [diffusion][Epoch 10710] diffusion training Loss: 0.046943483874201775
2024-11-05 05:44:16,717 - INFO - [diffusion][Epoch 10710] diffusion learning rate: 0.001
2024-11-05 05:44:16,719 - INFO - [diffusion][Epoch 10710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:16,720 - INFO - [diffusion][Epoch 10711] Epoch 10712/12000
2024-11-05 05:44:20,821 - INFO - [diffusion][Epoch 10711] diffusion training Loss: 0.04670468904078007
2024-11-05 05:44:20,823 - INFO - [diffusion][Epoch 10711] diffusion learning rate: 0.001
2024-11-05 05:44:20,824 - INFO - [diffusion][Epoch 10711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:20,828 - INFO - [diffusion][Epoch 10712] Epoch 10713/12000
2024-11-05 05:44:24,953 - INFO - [diffusion][Epoch 10712] diffusion training Loss: 0.04350551497191191
2024-11-05 05:44:24,955 - INFO - [diffusion][Epoch 10712] diffusion learning rate: 0.001
2024-11-05 05:44:24,957 - INFO - [diffusion][Epoch 10712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:24,958 - INFO - [diffusion][Epoch 10713] Epoch 10714/12000
2024-11-05 05:44:29,023 - INFO - [diffusion][Epoch 10713] diffusion training Loss: 0.046871100552380085
2024-11-05 05:44:29,025 - INFO - [diffusion][Epoch 10713] diffusion learning rate: 0.001
2024-11-05 05:44:29,027 - INFO - [diffusion][Epoch 10713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:29,028 - INFO - [diffusion][Epoch 10714] Epoch 10715/12000
2024-11-05 05:44:33,095 - INFO - [diffusion][Epoch 10714] diffusion training Loss: 0.055476740933954716
2024-11-05 05:44:33,097 - INFO - [diffusion][Epoch 10714] diffusion learning rate: 0.001
2024-11-05 05:44:33,099 - INFO - [diffusion][Epoch 10714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:33,100 - INFO - [diffusion][Epoch 10715] Epoch 10716/12000
2024-11-05 05:44:37,070 - INFO - [diffusion][Epoch 10715] diffusion training Loss: 0.04969331994652748
2024-11-05 05:44:37,072 - INFO - [diffusion][Epoch 10715] diffusion learning rate: 0.001
2024-11-05 05:44:37,074 - INFO - [diffusion][Epoch 10715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:37,075 - INFO - [diffusion][Epoch 10716] Epoch 10717/12000
2024-11-05 05:44:41,126 - INFO - [diffusion][Epoch 10716] diffusion training Loss: 0.04468223359435797
2024-11-05 05:44:41,128 - INFO - [diffusion][Epoch 10716] diffusion learning rate: 0.001
2024-11-05 05:44:41,130 - INFO - [diffusion][Epoch 10716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:41,131 - INFO - [diffusion][Epoch 10717] Epoch 10718/12000
2024-11-05 05:44:45,257 - INFO - [diffusion][Epoch 10717] diffusion training Loss: 0.0522202355787158
2024-11-05 05:44:45,259 - INFO - [diffusion][Epoch 10717] diffusion learning rate: 0.001
2024-11-05 05:44:45,261 - INFO - [diffusion][Epoch 10717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:45,262 - INFO - [diffusion][Epoch 10718] Epoch 10719/12000
2024-11-05 05:44:49,344 - INFO - [diffusion][Epoch 10718] diffusion training Loss: 0.041748651303350925
2024-11-05 05:44:49,346 - INFO - [diffusion][Epoch 10718] diffusion learning rate: 0.001
2024-11-05 05:44:49,347 - INFO - [diffusion][Epoch 10718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:49,349 - INFO - [diffusion][Epoch 10719] Epoch 10720/12000
2024-11-05 05:44:53,446 - INFO - [diffusion][Epoch 10719] diffusion training Loss: 0.04815402999520302
2024-11-05 05:44:53,448 - INFO - [diffusion][Epoch 10719] diffusion learning rate: 0.001
2024-11-05 05:44:53,450 - INFO - [diffusion][Epoch 10719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:53,453 - INFO - [diffusion][Epoch 10720] Epoch 10721/12000
2024-11-05 05:44:57,758 - INFO - [diffusion][Epoch 10720] diffusion training Loss: 0.050651414319872856
2024-11-05 05:44:57,760 - INFO - [diffusion][Epoch 10720] diffusion learning rate: 0.001
2024-11-05 05:44:57,762 - INFO - [diffusion][Epoch 10720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:44:57,763 - INFO - [diffusion][Epoch 10721] Epoch 10722/12000
2024-11-05 05:45:01,861 - INFO - [diffusion][Epoch 10721] diffusion training Loss: 0.04640793055295944
2024-11-05 05:45:01,863 - INFO - [diffusion][Epoch 10721] diffusion learning rate: 0.001
2024-11-05 05:45:01,864 - INFO - [diffusion][Epoch 10721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:01,865 - INFO - [diffusion][Epoch 10722] Epoch 10723/12000
2024-11-05 05:45:05,926 - INFO - [diffusion][Epoch 10722] diffusion training Loss: 0.04860295169055462
2024-11-05 05:45:05,928 - INFO - [diffusion][Epoch 10722] diffusion learning rate: 0.001
2024-11-05 05:45:05,930 - INFO - [diffusion][Epoch 10722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:05,931 - INFO - [diffusion][Epoch 10723] Epoch 10724/12000
2024-11-05 05:45:10,026 - INFO - [diffusion][Epoch 10723] diffusion training Loss: 0.04516721423715353
2024-11-05 05:45:10,030 - INFO - [diffusion][Epoch 10723] diffusion learning rate: 0.001
2024-11-05 05:45:10,032 - INFO - [diffusion][Epoch 10723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:10,033 - INFO - [diffusion][Epoch 10724] Epoch 10725/12000
2024-11-05 05:45:14,114 - INFO - [diffusion][Epoch 10724] diffusion training Loss: 0.0497982120141387
2024-11-05 05:45:14,116 - INFO - [diffusion][Epoch 10724] diffusion learning rate: 0.001
2024-11-05 05:45:14,118 - INFO - [diffusion][Epoch 10724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:14,120 - INFO - [diffusion][Epoch 10725] Epoch 10726/12000
2024-11-05 05:45:18,176 - INFO - [diffusion][Epoch 10725] diffusion training Loss: 0.04654135275632143
2024-11-05 05:45:18,178 - INFO - [diffusion][Epoch 10725] diffusion learning rate: 0.001
2024-11-05 05:45:18,180 - INFO - [diffusion][Epoch 10725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:18,181 - INFO - [diffusion][Epoch 10726] Epoch 10727/12000
2024-11-05 05:45:22,283 - INFO - [diffusion][Epoch 10726] diffusion training Loss: 0.046037121675908566
2024-11-05 05:45:22,285 - INFO - [diffusion][Epoch 10726] diffusion learning rate: 0.001
2024-11-05 05:45:22,287 - INFO - [diffusion][Epoch 10726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:22,288 - INFO - [diffusion][Epoch 10727] Epoch 10728/12000
2024-11-05 05:45:26,377 - INFO - [diffusion][Epoch 10727] diffusion training Loss: 0.05193019937723875
2024-11-05 05:45:26,379 - INFO - [diffusion][Epoch 10727] diffusion learning rate: 0.001
2024-11-05 05:45:26,382 - INFO - [diffusion][Epoch 10727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:26,383 - INFO - [diffusion][Epoch 10728] Epoch 10729/12000
2024-11-05 05:45:30,378 - INFO - [diffusion][Epoch 10728] diffusion training Loss: 0.05211211834102869
2024-11-05 05:45:30,380 - INFO - [diffusion][Epoch 10728] diffusion learning rate: 0.001
2024-11-05 05:45:30,382 - INFO - [diffusion][Epoch 10728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:30,383 - INFO - [diffusion][Epoch 10729] Epoch 10730/12000
2024-11-05 05:45:34,478 - INFO - [diffusion][Epoch 10729] diffusion training Loss: 0.050288110971450806
2024-11-05 05:45:34,480 - INFO - [diffusion][Epoch 10729] diffusion learning rate: 0.001
2024-11-05 05:45:34,482 - INFO - [diffusion][Epoch 10729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:34,484 - INFO - [diffusion][Epoch 10730] Epoch 10731/12000
2024-11-05 05:45:38,582 - INFO - [diffusion][Epoch 10730] diffusion training Loss: 0.04807356093078852
2024-11-05 05:45:38,584 - INFO - [diffusion][Epoch 10730] diffusion learning rate: 0.001
2024-11-05 05:45:38,586 - INFO - [diffusion][Epoch 10730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:38,587 - INFO - [diffusion][Epoch 10731] Epoch 10732/12000
2024-11-05 05:45:42,700 - INFO - [diffusion][Epoch 10731] diffusion training Loss: 0.04742064047604799
2024-11-05 05:45:42,702 - INFO - [diffusion][Epoch 10731] diffusion learning rate: 0.001
2024-11-05 05:45:42,703 - INFO - [diffusion][Epoch 10731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:42,705 - INFO - [diffusion][Epoch 10732] Epoch 10733/12000
2024-11-05 05:45:46,818 - INFO - [diffusion][Epoch 10732] diffusion training Loss: 0.04678107611835003
2024-11-05 05:45:46,820 - INFO - [diffusion][Epoch 10732] diffusion learning rate: 0.001
2024-11-05 05:45:46,822 - INFO - [diffusion][Epoch 10732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:46,823 - INFO - [diffusion][Epoch 10733] Epoch 10734/12000
2024-11-05 05:45:50,917 - INFO - [diffusion][Epoch 10733] diffusion training Loss: 0.04717268142849207
2024-11-05 05:45:50,919 - INFO - [diffusion][Epoch 10733] diffusion learning rate: 0.001
2024-11-05 05:45:50,921 - INFO - [diffusion][Epoch 10733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:50,922 - INFO - [diffusion][Epoch 10734] Epoch 10735/12000
2024-11-05 05:45:55,001 - INFO - [diffusion][Epoch 10734] diffusion training Loss: 0.044666316360235214
2024-11-05 05:45:55,003 - INFO - [diffusion][Epoch 10734] diffusion learning rate: 0.001
2024-11-05 05:45:55,005 - INFO - [diffusion][Epoch 10734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:55,006 - INFO - [diffusion][Epoch 10735] Epoch 10736/12000
2024-11-05 05:45:59,104 - INFO - [diffusion][Epoch 10735] diffusion training Loss: 0.04865104425698519
2024-11-05 05:45:59,105 - INFO - [diffusion][Epoch 10735] diffusion learning rate: 0.001
2024-11-05 05:45:59,107 - INFO - [diffusion][Epoch 10735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:45:59,108 - INFO - [diffusion][Epoch 10736] Epoch 10737/12000
2024-11-05 05:46:03,187 - INFO - [diffusion][Epoch 10736] diffusion training Loss: 0.04960004519671202
2024-11-05 05:46:03,189 - INFO - [diffusion][Epoch 10736] diffusion learning rate: 0.001
2024-11-05 05:46:03,191 - INFO - [diffusion][Epoch 10736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:03,193 - INFO - [diffusion][Epoch 10737] Epoch 10738/12000
2024-11-05 05:46:07,254 - INFO - [diffusion][Epoch 10737] diffusion training Loss: 0.046266304329037666
2024-11-05 05:46:07,256 - INFO - [diffusion][Epoch 10737] diffusion learning rate: 0.001
2024-11-05 05:46:07,258 - INFO - [diffusion][Epoch 10737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:07,260 - INFO - [diffusion][Epoch 10738] Epoch 10739/12000
2024-11-05 05:46:11,446 - INFO - [diffusion][Epoch 10738] diffusion training Loss: 0.04677881859242916
2024-11-05 05:46:11,450 - INFO - [diffusion][Epoch 10738] diffusion learning rate: 0.001
2024-11-05 05:46:11,451 - INFO - [diffusion][Epoch 10738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:11,453 - INFO - [diffusion][Epoch 10739] Epoch 10740/12000
2024-11-05 05:46:15,557 - INFO - [diffusion][Epoch 10739] diffusion training Loss: 0.04938374739140272
2024-11-05 05:46:15,559 - INFO - [diffusion][Epoch 10739] diffusion learning rate: 0.001
2024-11-05 05:46:15,561 - INFO - [diffusion][Epoch 10739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:15,562 - INFO - [diffusion][Epoch 10740] Epoch 10741/12000
2024-11-05 05:46:19,855 - INFO - [diffusion][Epoch 10740] diffusion training Loss: 0.048177694901824
2024-11-05 05:46:19,857 - INFO - [diffusion][Epoch 10740] diffusion learning rate: 0.001
2024-11-05 05:46:19,859 - INFO - [diffusion][Epoch 10740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:19,860 - INFO - [diffusion][Epoch 10741] Epoch 10742/12000
2024-11-05 05:46:23,964 - INFO - [diffusion][Epoch 10741] diffusion training Loss: 0.04731206316500902
2024-11-05 05:46:23,966 - INFO - [diffusion][Epoch 10741] diffusion learning rate: 0.001
2024-11-05 05:46:23,968 - INFO - [diffusion][Epoch 10741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:23,969 - INFO - [diffusion][Epoch 10742] Epoch 10743/12000
2024-11-05 05:46:28,053 - INFO - [diffusion][Epoch 10742] diffusion training Loss: 0.04746405314654112
2024-11-05 05:46:28,055 - INFO - [diffusion][Epoch 10742] diffusion learning rate: 0.001
2024-11-05 05:46:28,057 - INFO - [diffusion][Epoch 10742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:28,059 - INFO - [diffusion][Epoch 10743] Epoch 10744/12000
2024-11-05 05:46:32,164 - INFO - [diffusion][Epoch 10743] diffusion training Loss: 0.05093747656792402
2024-11-05 05:46:32,166 - INFO - [diffusion][Epoch 10743] diffusion learning rate: 0.001
2024-11-05 05:46:32,168 - INFO - [diffusion][Epoch 10743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:32,169 - INFO - [diffusion][Epoch 10744] Epoch 10745/12000
2024-11-05 05:46:36,290 - INFO - [diffusion][Epoch 10744] diffusion training Loss: 0.05071138311177492
2024-11-05 05:46:36,292 - INFO - [diffusion][Epoch 10744] diffusion learning rate: 0.001
2024-11-05 05:46:36,294 - INFO - [diffusion][Epoch 10744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:36,295 - INFO - [diffusion][Epoch 10745] Epoch 10746/12000
2024-11-05 05:46:40,386 - INFO - [diffusion][Epoch 10745] diffusion training Loss: 0.050744700245559216
2024-11-05 05:46:40,388 - INFO - [diffusion][Epoch 10745] diffusion learning rate: 0.001
2024-11-05 05:46:40,390 - INFO - [diffusion][Epoch 10745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:40,391 - INFO - [diffusion][Epoch 10746] Epoch 10747/12000
2024-11-05 05:46:44,442 - INFO - [diffusion][Epoch 10746] diffusion training Loss: 0.045073190703988075
2024-11-05 05:46:44,444 - INFO - [diffusion][Epoch 10746] diffusion learning rate: 0.001
2024-11-05 05:46:44,446 - INFO - [diffusion][Epoch 10746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:44,447 - INFO - [diffusion][Epoch 10747] Epoch 10748/12000
2024-11-05 05:46:48,584 - INFO - [diffusion][Epoch 10747] diffusion training Loss: 0.04802290350198746
2024-11-05 05:46:48,586 - INFO - [diffusion][Epoch 10747] diffusion learning rate: 0.001
2024-11-05 05:46:48,588 - INFO - [diffusion][Epoch 10747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:48,589 - INFO - [diffusion][Epoch 10748] Epoch 10749/12000
2024-11-05 05:46:52,688 - INFO - [diffusion][Epoch 10748] diffusion training Loss: 0.05590066220611334
2024-11-05 05:46:52,690 - INFO - [diffusion][Epoch 10748] diffusion learning rate: 0.001
2024-11-05 05:46:52,691 - INFO - [diffusion][Epoch 10748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:52,693 - INFO - [diffusion][Epoch 10749] Epoch 10750/12000
2024-11-05 05:46:56,821 - INFO - [diffusion][Epoch 10749] diffusion training Loss: 0.04551059938967228
2024-11-05 05:46:56,823 - INFO - [diffusion][Epoch 10749] diffusion learning rate: 0.001
2024-11-05 05:46:56,825 - INFO - [diffusion][Epoch 10749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:46:56,826 - INFO - [diffusion][Epoch 10750] Epoch 10751/12000
2024-11-05 05:47:00,900 - INFO - [diffusion][Epoch 10750] diffusion training Loss: 0.048422764986753464
2024-11-05 05:47:00,902 - INFO - [diffusion][Epoch 10750] diffusion learning rate: 0.001
2024-11-05 05:47:00,904 - INFO - [diffusion][Epoch 10750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:00,905 - INFO - [diffusion][Epoch 10751] Epoch 10752/12000
2024-11-05 05:47:04,984 - INFO - [diffusion][Epoch 10751] diffusion training Loss: 0.05201495159417391
2024-11-05 05:47:04,986 - INFO - [diffusion][Epoch 10751] diffusion learning rate: 0.001
2024-11-05 05:47:04,987 - INFO - [diffusion][Epoch 10751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:04,989 - INFO - [diffusion][Epoch 10752] Epoch 10753/12000
2024-11-05 05:47:09,148 - INFO - [diffusion][Epoch 10752] diffusion training Loss: 0.04673717264086008
2024-11-05 05:47:09,150 - INFO - [diffusion][Epoch 10752] diffusion learning rate: 0.001
2024-11-05 05:47:09,151 - INFO - [diffusion][Epoch 10752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:09,152 - INFO - [diffusion][Epoch 10753] Epoch 10754/12000
2024-11-05 05:47:13,231 - INFO - [diffusion][Epoch 10753] diffusion training Loss: 0.04960703197866678
2024-11-05 05:47:13,234 - INFO - [diffusion][Epoch 10753] diffusion learning rate: 0.001
2024-11-05 05:47:13,236 - INFO - [diffusion][Epoch 10753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:13,237 - INFO - [diffusion][Epoch 10754] Epoch 10755/12000
2024-11-05 05:47:17,326 - INFO - [diffusion][Epoch 10754] diffusion training Loss: 0.048711178824305534
2024-11-05 05:47:17,327 - INFO - [diffusion][Epoch 10754] diffusion learning rate: 0.001
2024-11-05 05:47:17,329 - INFO - [diffusion][Epoch 10754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:17,331 - INFO - [diffusion][Epoch 10755] Epoch 10756/12000
2024-11-05 05:47:21,440 - INFO - [diffusion][Epoch 10755] diffusion training Loss: 0.05273168161511421
2024-11-05 05:47:21,442 - INFO - [diffusion][Epoch 10755] diffusion learning rate: 0.001
2024-11-05 05:47:21,444 - INFO - [diffusion][Epoch 10755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:21,445 - INFO - [diffusion][Epoch 10756] Epoch 10757/12000
2024-11-05 05:47:25,530 - INFO - [diffusion][Epoch 10756] diffusion training Loss: 0.04753263108432293
2024-11-05 05:47:25,532 - INFO - [diffusion][Epoch 10756] diffusion learning rate: 0.001
2024-11-05 05:47:25,533 - INFO - [diffusion][Epoch 10756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:25,535 - INFO - [diffusion][Epoch 10757] Epoch 10758/12000
2024-11-05 05:47:29,628 - INFO - [diffusion][Epoch 10757] diffusion training Loss: 0.05780952516943216
2024-11-05 05:47:29,630 - INFO - [diffusion][Epoch 10757] diffusion learning rate: 0.001
2024-11-05 05:47:29,631 - INFO - [diffusion][Epoch 10757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:29,633 - INFO - [diffusion][Epoch 10758] Epoch 10759/12000
2024-11-05 05:47:33,738 - INFO - [diffusion][Epoch 10758] diffusion training Loss: 0.04485886823385954
2024-11-05 05:47:33,740 - INFO - [diffusion][Epoch 10758] diffusion learning rate: 0.001
2024-11-05 05:47:33,742 - INFO - [diffusion][Epoch 10758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:33,743 - INFO - [diffusion][Epoch 10759] Epoch 10760/12000
2024-11-05 05:47:37,998 - INFO - [diffusion][Epoch 10759] diffusion training Loss: 0.054399073123931885
2024-11-05 05:47:38,000 - INFO - [diffusion][Epoch 10759] diffusion learning rate: 0.001
2024-11-05 05:47:38,002 - INFO - [diffusion][Epoch 10759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:38,003 - INFO - [diffusion][Epoch 10760] Epoch 10761/12000
2024-11-05 05:47:42,090 - INFO - [diffusion][Epoch 10760] diffusion training Loss: 0.0517656160518527
2024-11-05 05:47:42,092 - INFO - [diffusion][Epoch 10760] diffusion learning rate: 0.001
2024-11-05 05:47:42,094 - INFO - [diffusion][Epoch 10760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:42,095 - INFO - [diffusion][Epoch 10761] Epoch 10762/12000
2024-11-05 05:47:46,153 - INFO - [diffusion][Epoch 10761] diffusion training Loss: 0.04985867068171501
2024-11-05 05:47:46,155 - INFO - [diffusion][Epoch 10761] diffusion learning rate: 0.001
2024-11-05 05:47:46,157 - INFO - [diffusion][Epoch 10761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:46,158 - INFO - [diffusion][Epoch 10762] Epoch 10763/12000
2024-11-05 05:47:50,224 - INFO - [diffusion][Epoch 10762] diffusion training Loss: 0.047724139876663685
2024-11-05 05:47:50,226 - INFO - [diffusion][Epoch 10762] diffusion learning rate: 0.001
2024-11-05 05:47:50,228 - INFO - [diffusion][Epoch 10762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:50,229 - INFO - [diffusion][Epoch 10763] Epoch 10764/12000
2024-11-05 05:47:54,170 - INFO - [diffusion][Epoch 10763] diffusion training Loss: 0.04276043362915516
2024-11-05 05:47:54,172 - INFO - [diffusion][Epoch 10763] diffusion learning rate: 0.001
2024-11-05 05:47:54,174 - INFO - [diffusion][Epoch 10763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:54,175 - INFO - [diffusion][Epoch 10764] Epoch 10765/12000
2024-11-05 05:47:58,159 - INFO - [diffusion][Epoch 10764] diffusion training Loss: 0.04784514848142862
2024-11-05 05:47:58,161 - INFO - [diffusion][Epoch 10764] diffusion learning rate: 0.001
2024-11-05 05:47:58,163 - INFO - [diffusion][Epoch 10764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:47:58,164 - INFO - [diffusion][Epoch 10765] Epoch 10766/12000
2024-11-05 05:48:02,229 - INFO - [diffusion][Epoch 10765] diffusion training Loss: 0.04661249276250601
2024-11-05 05:48:02,231 - INFO - [diffusion][Epoch 10765] diffusion learning rate: 0.001
2024-11-05 05:48:02,233 - INFO - [diffusion][Epoch 10765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:02,234 - INFO - [diffusion][Epoch 10766] Epoch 10767/12000
2024-11-05 05:48:06,350 - INFO - [diffusion][Epoch 10766] diffusion training Loss: 0.04653056710958481
2024-11-05 05:48:06,352 - INFO - [diffusion][Epoch 10766] diffusion learning rate: 0.001
2024-11-05 05:48:06,353 - INFO - [diffusion][Epoch 10766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:06,355 - INFO - [diffusion][Epoch 10767] Epoch 10768/12000
2024-11-05 05:48:10,425 - INFO - [diffusion][Epoch 10767] diffusion training Loss: 0.04729862418025732
2024-11-05 05:48:10,427 - INFO - [diffusion][Epoch 10767] diffusion learning rate: 0.001
2024-11-05 05:48:10,429 - INFO - [diffusion][Epoch 10767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:10,430 - INFO - [diffusion][Epoch 10768] Epoch 10769/12000
2024-11-05 05:48:14,509 - INFO - [diffusion][Epoch 10768] diffusion training Loss: 0.04598995950073004
2024-11-05 05:48:14,513 - INFO - [diffusion][Epoch 10768] diffusion learning rate: 0.001
2024-11-05 05:48:14,515 - INFO - [diffusion][Epoch 10768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:14,516 - INFO - [diffusion][Epoch 10769] Epoch 10770/12000
2024-11-05 05:48:18,616 - INFO - [diffusion][Epoch 10769] diffusion training Loss: 0.048230914399027824
2024-11-05 05:48:18,618 - INFO - [diffusion][Epoch 10769] diffusion learning rate: 0.001
2024-11-05 05:48:18,620 - INFO - [diffusion][Epoch 10769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:18,621 - INFO - [diffusion][Epoch 10770] Epoch 10771/12000
2024-11-05 05:48:22,758 - INFO - [diffusion][Epoch 10770] diffusion training Loss: 0.04769429564476013
2024-11-05 05:48:22,760 - INFO - [diffusion][Epoch 10770] diffusion learning rate: 0.001
2024-11-05 05:48:22,761 - INFO - [diffusion][Epoch 10770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:22,763 - INFO - [diffusion][Epoch 10771] Epoch 10772/12000
2024-11-05 05:48:26,931 - INFO - [diffusion][Epoch 10771] diffusion training Loss: 0.052021611481904984
2024-11-05 05:48:26,933 - INFO - [diffusion][Epoch 10771] diffusion learning rate: 0.001
2024-11-05 05:48:26,935 - INFO - [diffusion][Epoch 10771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:26,937 - INFO - [diffusion][Epoch 10772] Epoch 10773/12000
2024-11-05 05:48:31,030 - INFO - [diffusion][Epoch 10772] diffusion training Loss: 0.04470490664243698
2024-11-05 05:48:31,032 - INFO - [diffusion][Epoch 10772] diffusion learning rate: 0.001
2024-11-05 05:48:31,034 - INFO - [diffusion][Epoch 10772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:31,035 - INFO - [diffusion][Epoch 10773] Epoch 10774/12000
2024-11-05 05:48:35,145 - INFO - [diffusion][Epoch 10773] diffusion training Loss: 0.05079841334372759
2024-11-05 05:48:35,147 - INFO - [diffusion][Epoch 10773] diffusion learning rate: 0.001
2024-11-05 05:48:35,149 - INFO - [diffusion][Epoch 10773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:35,150 - INFO - [diffusion][Epoch 10774] Epoch 10775/12000
2024-11-05 05:48:39,242 - INFO - [diffusion][Epoch 10774] diffusion training Loss: 0.04836390260607004
2024-11-05 05:48:39,244 - INFO - [diffusion][Epoch 10774] diffusion learning rate: 0.001
2024-11-05 05:48:39,246 - INFO - [diffusion][Epoch 10774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:39,248 - INFO - [diffusion][Epoch 10775] Epoch 10776/12000
2024-11-05 05:48:43,366 - INFO - [diffusion][Epoch 10775] diffusion training Loss: 0.04631664790213108
2024-11-05 05:48:43,368 - INFO - [diffusion][Epoch 10775] diffusion learning rate: 0.001
2024-11-05 05:48:43,370 - INFO - [diffusion][Epoch 10775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:43,371 - INFO - [diffusion][Epoch 10776] Epoch 10777/12000
2024-11-05 05:48:47,452 - INFO - [diffusion][Epoch 10776] diffusion training Loss: 0.051384768448770046
2024-11-05 05:48:47,454 - INFO - [diffusion][Epoch 10776] diffusion learning rate: 0.001
2024-11-05 05:48:47,455 - INFO - [diffusion][Epoch 10776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:47,458 - INFO - [diffusion][Epoch 10777] Epoch 10778/12000
2024-11-05 05:48:51,587 - INFO - [diffusion][Epoch 10777] diffusion training Loss: 0.0476794084534049
2024-11-05 05:48:51,589 - INFO - [diffusion][Epoch 10777] diffusion learning rate: 0.001
2024-11-05 05:48:51,591 - INFO - [diffusion][Epoch 10777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:51,592 - INFO - [diffusion][Epoch 10778] Epoch 10779/12000
2024-11-05 05:48:55,682 - INFO - [diffusion][Epoch 10778] diffusion training Loss: 0.05012398213148117
2024-11-05 05:48:55,684 - INFO - [diffusion][Epoch 10778] diffusion learning rate: 0.001
2024-11-05 05:48:55,800 - INFO - [diffusion][Epoch 10778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:48:55,801 - INFO - [diffusion][Epoch 10779] Epoch 10780/12000
2024-11-05 05:49:00,441 - INFO - [diffusion][Epoch 10779] diffusion training Loss: 0.05240310449153185
2024-11-05 05:49:00,443 - INFO - [diffusion][Epoch 10779] diffusion learning rate: 0.001
2024-11-05 05:49:00,445 - INFO - [diffusion][Epoch 10779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:00,447 - INFO - [diffusion][Epoch 10780] Epoch 10781/12000
2024-11-05 05:49:04,537 - INFO - [diffusion][Epoch 10780] diffusion training Loss: 0.047850171104073524
2024-11-05 05:49:04,539 - INFO - [diffusion][Epoch 10780] diffusion learning rate: 0.001
2024-11-05 05:49:04,541 - INFO - [diffusion][Epoch 10780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:04,542 - INFO - [diffusion][Epoch 10781] Epoch 10782/12000
2024-11-05 05:49:08,623 - INFO - [diffusion][Epoch 10781] diffusion training Loss: 0.04819036088883877
2024-11-05 05:49:08,625 - INFO - [diffusion][Epoch 10781] diffusion learning rate: 0.001
2024-11-05 05:49:08,627 - INFO - [diffusion][Epoch 10781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:08,628 - INFO - [diffusion][Epoch 10782] Epoch 10783/12000
2024-11-05 05:49:12,629 - INFO - [diffusion][Epoch 10782] diffusion training Loss: 0.04871761240065098
2024-11-05 05:49:12,707 - INFO - [diffusion][Epoch 10782] diffusion learning rate: 0.001
2024-11-05 05:49:12,709 - INFO - [diffusion][Epoch 10782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:12,710 - INFO - [diffusion][Epoch 10783] Epoch 10784/12000
2024-11-05 05:49:16,760 - INFO - [diffusion][Epoch 10783] diffusion training Loss: 0.0493498956784606
2024-11-05 05:49:16,764 - INFO - [diffusion][Epoch 10783] diffusion learning rate: 0.001
2024-11-05 05:49:16,766 - INFO - [diffusion][Epoch 10783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:16,767 - INFO - [diffusion][Epoch 10784] Epoch 10785/12000
2024-11-05 05:49:20,832 - INFO - [diffusion][Epoch 10784] diffusion training Loss: 0.042022780515253544
2024-11-05 05:49:20,834 - INFO - [diffusion][Epoch 10784] diffusion learning rate: 0.001
2024-11-05 05:49:20,836 - INFO - [diffusion][Epoch 10784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:20,837 - INFO - [diffusion][Epoch 10785] Epoch 10786/12000
2024-11-05 05:49:24,935 - INFO - [diffusion][Epoch 10785] diffusion training Loss: 0.05072038806974888
2024-11-05 05:49:24,937 - INFO - [diffusion][Epoch 10785] diffusion learning rate: 0.001
2024-11-05 05:49:24,967 - INFO - [diffusion][Epoch 10785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:24,968 - INFO - [diffusion][Epoch 10786] Epoch 10787/12000
2024-11-05 05:49:28,998 - INFO - [diffusion][Epoch 10786] diffusion training Loss: 0.04513327404856682
2024-11-05 05:49:29,000 - INFO - [diffusion][Epoch 10786] diffusion learning rate: 0.001
2024-11-05 05:49:29,002 - INFO - [diffusion][Epoch 10786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:29,003 - INFO - [diffusion][Epoch 10787] Epoch 10788/12000
2024-11-05 05:49:33,110 - INFO - [diffusion][Epoch 10787] diffusion training Loss: 0.04544937890022993
2024-11-05 05:49:33,112 - INFO - [diffusion][Epoch 10787] diffusion learning rate: 0.001
2024-11-05 05:49:33,114 - INFO - [diffusion][Epoch 10787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:33,115 - INFO - [diffusion][Epoch 10788] Epoch 10789/12000
2024-11-05 05:49:37,188 - INFO - [diffusion][Epoch 10788] diffusion training Loss: 0.04526673909276724
2024-11-05 05:49:37,190 - INFO - [diffusion][Epoch 10788] diffusion learning rate: 0.001
2024-11-05 05:49:37,191 - INFO - [diffusion][Epoch 10788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:37,193 - INFO - [diffusion][Epoch 10789] Epoch 10790/12000
2024-11-05 05:49:41,291 - INFO - [diffusion][Epoch 10789] diffusion training Loss: 0.04878083523362875
2024-11-05 05:49:41,292 - INFO - [diffusion][Epoch 10789] diffusion learning rate: 0.001
2024-11-05 05:49:41,294 - INFO - [diffusion][Epoch 10789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:41,295 - INFO - [diffusion][Epoch 10790] Epoch 10791/12000
2024-11-05 05:49:45,371 - INFO - [diffusion][Epoch 10790] diffusion training Loss: 0.0463804267346859
2024-11-05 05:49:45,373 - INFO - [diffusion][Epoch 10790] diffusion learning rate: 0.001
2024-11-05 05:49:45,412 - INFO - [diffusion][Epoch 10790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:45,413 - INFO - [diffusion][Epoch 10791] Epoch 10792/12000
2024-11-05 05:49:49,504 - INFO - [diffusion][Epoch 10791] diffusion training Loss: 0.04309303965419531
2024-11-05 05:49:49,506 - INFO - [diffusion][Epoch 10791] diffusion learning rate: 0.001
2024-11-05 05:49:49,508 - INFO - [diffusion][Epoch 10791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:49,509 - INFO - [diffusion][Epoch 10792] Epoch 10793/12000
2024-11-05 05:49:53,601 - INFO - [diffusion][Epoch 10792] diffusion training Loss: 0.051052466966211796
2024-11-05 05:49:53,603 - INFO - [diffusion][Epoch 10792] diffusion learning rate: 0.001
2024-11-05 05:49:53,605 - INFO - [diffusion][Epoch 10792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:53,606 - INFO - [diffusion][Epoch 10793] Epoch 10794/12000
2024-11-05 05:49:57,670 - INFO - [diffusion][Epoch 10793] diffusion training Loss: 0.04765211325138807
2024-11-05 05:49:57,672 - INFO - [diffusion][Epoch 10793] diffusion learning rate: 0.001
2024-11-05 05:49:57,674 - INFO - [diffusion][Epoch 10793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:49:57,675 - INFO - [diffusion][Epoch 10794] Epoch 10795/12000
2024-11-05 05:50:01,719 - INFO - [diffusion][Epoch 10794] diffusion training Loss: 0.04991069529205561
2024-11-05 05:50:01,721 - INFO - [diffusion][Epoch 10794] diffusion learning rate: 0.001
2024-11-05 05:50:01,723 - INFO - [diffusion][Epoch 10794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:01,724 - INFO - [diffusion][Epoch 10795] Epoch 10796/12000
2024-11-05 05:50:05,833 - INFO - [diffusion][Epoch 10795] diffusion training Loss: 0.04720731545239687
2024-11-05 05:50:05,839 - INFO - [diffusion][Epoch 10795] diffusion learning rate: 0.001
2024-11-05 05:50:05,841 - INFO - [diffusion][Epoch 10795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:05,842 - INFO - [diffusion][Epoch 10796] Epoch 10797/12000
2024-11-05 05:50:09,921 - INFO - [diffusion][Epoch 10796] diffusion training Loss: 0.05043511372059584
2024-11-05 05:50:09,923 - INFO - [diffusion][Epoch 10796] diffusion learning rate: 0.001
2024-11-05 05:50:09,925 - INFO - [diffusion][Epoch 10796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:09,926 - INFO - [diffusion][Epoch 10797] Epoch 10798/12000
2024-11-05 05:50:14,035 - INFO - [diffusion][Epoch 10797] diffusion training Loss: 0.04818073567003012
2024-11-05 05:50:14,037 - INFO - [diffusion][Epoch 10797] diffusion learning rate: 0.001
2024-11-05 05:50:14,039 - INFO - [diffusion][Epoch 10797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:14,041 - INFO - [diffusion][Epoch 10798] Epoch 10799/12000
2024-11-05 05:50:18,112 - INFO - [diffusion][Epoch 10798] diffusion training Loss: 0.04904659651219845
2024-11-05 05:50:18,116 - INFO - [diffusion][Epoch 10798] diffusion learning rate: 0.001
2024-11-05 05:50:18,118 - INFO - [diffusion][Epoch 10798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:18,119 - INFO - [diffusion][Epoch 10799] Epoch 10800/12000
2024-11-05 05:50:22,210 - INFO - [diffusion][Epoch 10799] diffusion training Loss: 0.04777246993035078
2024-11-05 05:50:22,212 - INFO - [diffusion][Epoch 10799] diffusion learning rate: 0.001
2024-11-05 05:50:22,232 - INFO - [diffusion][Epoch 10799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:22,233 - INFO - [diffusion][Epoch 10800] Epoch 10801/12000
2024-11-05 05:50:26,961 - INFO - [diffusion][Epoch 10800] diffusion training Loss: 0.050935594365000725
2024-11-05 05:50:26,963 - INFO - [diffusion][Epoch 10800] diffusion learning rate: 0.001
2024-11-05 05:50:26,965 - INFO - [diffusion][Epoch 10800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:26,966 - INFO - [diffusion][Epoch 10801] Epoch 10802/12000
2024-11-05 05:50:31,090 - INFO - [diffusion][Epoch 10801] diffusion training Loss: 0.046459365636110306
2024-11-05 05:50:31,092 - INFO - [diffusion][Epoch 10801] diffusion learning rate: 0.001
2024-11-05 05:50:31,094 - INFO - [diffusion][Epoch 10801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:31,095 - INFO - [diffusion][Epoch 10802] Epoch 10803/12000
2024-11-05 05:50:35,245 - INFO - [diffusion][Epoch 10802] diffusion training Loss: 0.052482280880212784
2024-11-05 05:50:35,247 - INFO - [diffusion][Epoch 10802] diffusion learning rate: 0.001
2024-11-05 05:50:35,248 - INFO - [diffusion][Epoch 10802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:35,249 - INFO - [diffusion][Epoch 10803] Epoch 10804/12000
2024-11-05 05:50:39,369 - INFO - [diffusion][Epoch 10803] diffusion training Loss: 0.04902044590562582
2024-11-05 05:50:39,372 - INFO - [diffusion][Epoch 10803] diffusion learning rate: 0.001
2024-11-05 05:50:39,373 - INFO - [diffusion][Epoch 10803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:39,375 - INFO - [diffusion][Epoch 10804] Epoch 10805/12000
2024-11-05 05:50:43,475 - INFO - [diffusion][Epoch 10804] diffusion training Loss: 0.046601674519479275
2024-11-05 05:50:43,477 - INFO - [diffusion][Epoch 10804] diffusion learning rate: 0.001
2024-11-05 05:50:43,479 - INFO - [diffusion][Epoch 10804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:43,480 - INFO - [diffusion][Epoch 10805] Epoch 10806/12000
2024-11-05 05:50:47,389 - INFO - [diffusion][Epoch 10805] diffusion training Loss: 0.04827482998371124
2024-11-05 05:50:47,392 - INFO - [diffusion][Epoch 10805] diffusion learning rate: 0.001
2024-11-05 05:50:47,395 - INFO - [diffusion][Epoch 10805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:47,396 - INFO - [diffusion][Epoch 10806] Epoch 10807/12000
2024-11-05 05:50:51,514 - INFO - [diffusion][Epoch 10806] diffusion training Loss: 0.05237787775695324
2024-11-05 05:50:51,516 - INFO - [diffusion][Epoch 10806] diffusion learning rate: 0.001
2024-11-05 05:50:51,518 - INFO - [diffusion][Epoch 10806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:51,519 - INFO - [diffusion][Epoch 10807] Epoch 10808/12000
2024-11-05 05:50:55,596 - INFO - [diffusion][Epoch 10807] diffusion training Loss: 0.04762168321758509
2024-11-05 05:50:55,598 - INFO - [diffusion][Epoch 10807] diffusion learning rate: 0.001
2024-11-05 05:50:55,600 - INFO - [diffusion][Epoch 10807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:55,601 - INFO - [diffusion][Epoch 10808] Epoch 10809/12000
2024-11-05 05:50:59,681 - INFO - [diffusion][Epoch 10808] diffusion training Loss: 0.05319503787904978
2024-11-05 05:50:59,683 - INFO - [diffusion][Epoch 10808] diffusion learning rate: 0.001
2024-11-05 05:50:59,684 - INFO - [diffusion][Epoch 10808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:50:59,686 - INFO - [diffusion][Epoch 10809] Epoch 10810/12000
2024-11-05 05:51:03,783 - INFO - [diffusion][Epoch 10809] diffusion training Loss: 0.0463019460439682
2024-11-05 05:51:03,785 - INFO - [diffusion][Epoch 10809] diffusion learning rate: 0.001
2024-11-05 05:51:03,787 - INFO - [diffusion][Epoch 10809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:03,788 - INFO - [diffusion][Epoch 10810] Epoch 10811/12000
2024-11-05 05:51:07,920 - INFO - [diffusion][Epoch 10810] diffusion training Loss: 0.047485933639109135
2024-11-05 05:51:07,922 - INFO - [diffusion][Epoch 10810] diffusion learning rate: 0.001
2024-11-05 05:51:07,924 - INFO - [diffusion][Epoch 10810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:07,925 - INFO - [diffusion][Epoch 10811] Epoch 10812/12000
2024-11-05 05:51:12,024 - INFO - [diffusion][Epoch 10811] diffusion training Loss: 0.04965948686003685
2024-11-05 05:51:12,026 - INFO - [diffusion][Epoch 10811] diffusion learning rate: 0.001
2024-11-05 05:51:12,028 - INFO - [diffusion][Epoch 10811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:12,029 - INFO - [diffusion][Epoch 10812] Epoch 10813/12000
2024-11-05 05:51:16,124 - INFO - [diffusion][Epoch 10812] diffusion training Loss: 0.0502537926658988
2024-11-05 05:51:16,126 - INFO - [diffusion][Epoch 10812] diffusion learning rate: 0.001
2024-11-05 05:51:16,127 - INFO - [diffusion][Epoch 10812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:16,129 - INFO - [diffusion][Epoch 10813] Epoch 10814/12000
2024-11-05 05:51:20,229 - INFO - [diffusion][Epoch 10813] diffusion training Loss: 0.04822931718081236
2024-11-05 05:51:20,233 - INFO - [diffusion][Epoch 10813] diffusion learning rate: 0.001
2024-11-05 05:51:20,234 - INFO - [diffusion][Epoch 10813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:20,236 - INFO - [diffusion][Epoch 10814] Epoch 10815/12000
2024-11-05 05:51:24,313 - INFO - [diffusion][Epoch 10814] diffusion training Loss: 0.05142173636704683
2024-11-05 05:51:24,315 - INFO - [diffusion][Epoch 10814] diffusion learning rate: 0.001
2024-11-05 05:51:24,317 - INFO - [diffusion][Epoch 10814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:24,318 - INFO - [diffusion][Epoch 10815] Epoch 10816/12000
2024-11-05 05:51:28,217 - INFO - [diffusion][Epoch 10815] diffusion training Loss: 0.044293287210166454
2024-11-05 05:51:28,219 - INFO - [diffusion][Epoch 10815] diffusion learning rate: 0.001
2024-11-05 05:51:28,220 - INFO - [diffusion][Epoch 10815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:28,222 - INFO - [diffusion][Epoch 10816] Epoch 10817/12000
2024-11-05 05:51:32,096 - INFO - [diffusion][Epoch 10816] diffusion training Loss: 0.052027931436896324
2024-11-05 05:51:32,098 - INFO - [diffusion][Epoch 10816] diffusion learning rate: 0.001
2024-11-05 05:51:32,101 - INFO - [diffusion][Epoch 10816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:32,102 - INFO - [diffusion][Epoch 10817] Epoch 10818/12000
2024-11-05 05:51:36,216 - INFO - [diffusion][Epoch 10817] diffusion training Loss: 0.0494648776948452
2024-11-05 05:51:36,218 - INFO - [diffusion][Epoch 10817] diffusion learning rate: 0.001
2024-11-05 05:51:36,219 - INFO - [diffusion][Epoch 10817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:36,221 - INFO - [diffusion][Epoch 10818] Epoch 10819/12000
2024-11-05 05:51:40,351 - INFO - [diffusion][Epoch 10818] diffusion training Loss: 0.05107604432851076
2024-11-05 05:51:40,353 - INFO - [diffusion][Epoch 10818] diffusion learning rate: 0.001
2024-11-05 05:51:40,354 - INFO - [diffusion][Epoch 10818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:40,356 - INFO - [diffusion][Epoch 10819] Epoch 10820/12000
2024-11-05 05:51:44,474 - INFO - [diffusion][Epoch 10819] diffusion training Loss: 0.04945724178105593
2024-11-05 05:51:44,476 - INFO - [diffusion][Epoch 10819] diffusion learning rate: 0.001
2024-11-05 05:51:44,478 - INFO - [diffusion][Epoch 10819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:44,479 - INFO - [diffusion][Epoch 10820] Epoch 10821/12000
2024-11-05 05:51:48,887 - INFO - [diffusion][Epoch 10820] diffusion training Loss: 0.04922401160001755
2024-11-05 05:51:48,889 - INFO - [diffusion][Epoch 10820] diffusion learning rate: 0.001
2024-11-05 05:51:48,891 - INFO - [diffusion][Epoch 10820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:48,892 - INFO - [diffusion][Epoch 10821] Epoch 10822/12000
2024-11-05 05:51:53,044 - INFO - [diffusion][Epoch 10821] diffusion training Loss: 0.04763788543641567
2024-11-05 05:51:53,046 - INFO - [diffusion][Epoch 10821] diffusion learning rate: 0.001
2024-11-05 05:51:53,048 - INFO - [diffusion][Epoch 10821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:53,050 - INFO - [diffusion][Epoch 10822] Epoch 10823/12000
2024-11-05 05:51:57,064 - INFO - [diffusion][Epoch 10822] diffusion training Loss: 0.04731254279613495
2024-11-05 05:51:57,066 - INFO - [diffusion][Epoch 10822] diffusion learning rate: 0.001
2024-11-05 05:51:57,068 - INFO - [diffusion][Epoch 10822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:51:57,069 - INFO - [diffusion][Epoch 10823] Epoch 10824/12000
2024-11-05 05:52:01,219 - INFO - [diffusion][Epoch 10823] diffusion training Loss: 0.04694991186261177
2024-11-05 05:52:01,221 - INFO - [diffusion][Epoch 10823] diffusion learning rate: 0.001
2024-11-05 05:52:01,223 - INFO - [diffusion][Epoch 10823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:01,224 - INFO - [diffusion][Epoch 10824] Epoch 10825/12000
2024-11-05 05:52:05,359 - INFO - [diffusion][Epoch 10824] diffusion training Loss: 0.04811807814985514
2024-11-05 05:52:05,361 - INFO - [diffusion][Epoch 10824] diffusion learning rate: 0.001
2024-11-05 05:52:05,363 - INFO - [diffusion][Epoch 10824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:05,364 - INFO - [diffusion][Epoch 10825] Epoch 10826/12000
2024-11-05 05:52:09,481 - INFO - [diffusion][Epoch 10825] diffusion training Loss: 0.04566211346536875
2024-11-05 05:52:09,483 - INFO - [diffusion][Epoch 10825] diffusion learning rate: 0.001
2024-11-05 05:52:09,485 - INFO - [diffusion][Epoch 10825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:09,486 - INFO - [diffusion][Epoch 10826] Epoch 10827/12000
2024-11-05 05:52:13,578 - INFO - [diffusion][Epoch 10826] diffusion training Loss: 0.050477306358516216
2024-11-05 05:52:13,580 - INFO - [diffusion][Epoch 10826] diffusion learning rate: 0.001
2024-11-05 05:52:13,581 - INFO - [diffusion][Epoch 10826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:13,582 - INFO - [diffusion][Epoch 10827] Epoch 10828/12000
2024-11-05 05:52:17,667 - INFO - [diffusion][Epoch 10827] diffusion training Loss: 0.04658074863255024
2024-11-05 05:52:17,669 - INFO - [diffusion][Epoch 10827] diffusion learning rate: 0.001
2024-11-05 05:52:17,671 - INFO - [diffusion][Epoch 10827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:17,672 - INFO - [diffusion][Epoch 10828] Epoch 10829/12000
2024-11-05 05:52:21,742 - INFO - [diffusion][Epoch 10828] diffusion training Loss: 0.046227154321968555
2024-11-05 05:52:21,745 - INFO - [diffusion][Epoch 10828] diffusion learning rate: 0.001
2024-11-05 05:52:21,747 - INFO - [diffusion][Epoch 10828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:21,751 - INFO - [diffusion][Epoch 10829] Epoch 10830/12000
2024-11-05 05:52:25,856 - INFO - [diffusion][Epoch 10829] diffusion training Loss: 0.047326184809207916
2024-11-05 05:52:25,858 - INFO - [diffusion][Epoch 10829] diffusion learning rate: 0.001
2024-11-05 05:52:25,887 - INFO - [diffusion][Epoch 10829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:25,888 - INFO - [diffusion][Epoch 10830] Epoch 10831/12000
2024-11-05 05:52:29,996 - INFO - [diffusion][Epoch 10830] diffusion training Loss: 0.04641415551304817
2024-11-05 05:52:29,998 - INFO - [diffusion][Epoch 10830] diffusion learning rate: 0.001
2024-11-05 05:52:30,000 - INFO - [diffusion][Epoch 10830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:30,001 - INFO - [diffusion][Epoch 10831] Epoch 10832/12000
2024-11-05 05:52:33,941 - INFO - [diffusion][Epoch 10831] diffusion training Loss: 0.04502785485237837
2024-11-05 05:52:33,943 - INFO - [diffusion][Epoch 10831] diffusion learning rate: 0.001
2024-11-05 05:52:33,945 - INFO - [diffusion][Epoch 10831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:33,946 - INFO - [diffusion][Epoch 10832] Epoch 10833/12000
2024-11-05 05:52:38,024 - INFO - [diffusion][Epoch 10832] diffusion training Loss: 0.04578008968383074
2024-11-05 05:52:38,026 - INFO - [diffusion][Epoch 10832] diffusion learning rate: 0.001
2024-11-05 05:52:38,053 - INFO - [diffusion][Epoch 10832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:38,054 - INFO - [diffusion][Epoch 10833] Epoch 10834/12000
2024-11-05 05:52:42,153 - INFO - [diffusion][Epoch 10833] diffusion training Loss: 0.048148591071367264
2024-11-05 05:52:42,155 - INFO - [diffusion][Epoch 10833] diffusion learning rate: 0.001
2024-11-05 05:52:42,156 - INFO - [diffusion][Epoch 10833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:42,157 - INFO - [diffusion][Epoch 10834] Epoch 10835/12000
2024-11-05 05:52:46,247 - INFO - [diffusion][Epoch 10834] diffusion training Loss: 0.04837822448462248
2024-11-05 05:52:46,249 - INFO - [diffusion][Epoch 10834] diffusion learning rate: 0.001
2024-11-05 05:52:46,251 - INFO - [diffusion][Epoch 10834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:46,252 - INFO - [diffusion][Epoch 10835] Epoch 10836/12000
2024-11-05 05:52:50,300 - INFO - [diffusion][Epoch 10835] diffusion training Loss: 0.044542728923261166
2024-11-05 05:52:50,302 - INFO - [diffusion][Epoch 10835] diffusion learning rate: 0.001
2024-11-05 05:52:50,304 - INFO - [diffusion][Epoch 10835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:50,305 - INFO - [diffusion][Epoch 10836] Epoch 10837/12000
2024-11-05 05:52:54,403 - INFO - [diffusion][Epoch 10836] diffusion training Loss: 0.05057848244905472
2024-11-05 05:52:54,405 - INFO - [diffusion][Epoch 10836] diffusion learning rate: 0.001
2024-11-05 05:52:54,407 - INFO - [diffusion][Epoch 10836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:54,408 - INFO - [diffusion][Epoch 10837] Epoch 10838/12000
2024-11-05 05:52:58,487 - INFO - [diffusion][Epoch 10837] diffusion training Loss: 0.047260718420147896
2024-11-05 05:52:58,489 - INFO - [diffusion][Epoch 10837] diffusion learning rate: 0.001
2024-11-05 05:52:58,491 - INFO - [diffusion][Epoch 10837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:52:58,492 - INFO - [diffusion][Epoch 10838] Epoch 10839/12000
2024-11-05 05:53:02,454 - INFO - [diffusion][Epoch 10838] diffusion training Loss: 0.04633686877787113
2024-11-05 05:53:02,456 - INFO - [diffusion][Epoch 10838] diffusion learning rate: 0.001
2024-11-05 05:53:02,458 - INFO - [diffusion][Epoch 10838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:02,460 - INFO - [diffusion][Epoch 10839] Epoch 10840/12000
2024-11-05 05:53:06,394 - INFO - [diffusion][Epoch 10839] diffusion training Loss: 0.0441810917109251
2024-11-05 05:53:06,396 - INFO - [diffusion][Epoch 10839] diffusion learning rate: 0.001
2024-11-05 05:53:06,397 - INFO - [diffusion][Epoch 10839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:06,398 - INFO - [diffusion][Epoch 10840] Epoch 10841/12000
2024-11-05 05:53:10,693 - INFO - [diffusion][Epoch 10840] diffusion training Loss: 0.049506641924381256
2024-11-05 05:53:10,695 - INFO - [diffusion][Epoch 10840] diffusion learning rate: 0.001
2024-11-05 05:53:10,697 - INFO - [diffusion][Epoch 10840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:10,698 - INFO - [diffusion][Epoch 10841] Epoch 10842/12000
2024-11-05 05:53:14,789 - INFO - [diffusion][Epoch 10841] diffusion training Loss: 0.0479767182841897
2024-11-05 05:53:14,790 - INFO - [diffusion][Epoch 10841] diffusion learning rate: 0.001
2024-11-05 05:53:14,792 - INFO - [diffusion][Epoch 10841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:14,793 - INFO - [diffusion][Epoch 10842] Epoch 10843/12000
2024-11-05 05:53:18,831 - INFO - [diffusion][Epoch 10842] diffusion training Loss: 0.04530482552945614
2024-11-05 05:53:18,833 - INFO - [diffusion][Epoch 10842] diffusion learning rate: 0.001
2024-11-05 05:53:18,835 - INFO - [diffusion][Epoch 10842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:18,836 - INFO - [diffusion][Epoch 10843] Epoch 10844/12000
2024-11-05 05:53:22,906 - INFO - [diffusion][Epoch 10843] diffusion training Loss: 0.04933827742934227
2024-11-05 05:53:22,910 - INFO - [diffusion][Epoch 10843] diffusion learning rate: 0.001
2024-11-05 05:53:22,912 - INFO - [diffusion][Epoch 10843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:22,913 - INFO - [diffusion][Epoch 10844] Epoch 10845/12000
2024-11-05 05:53:27,038 - INFO - [diffusion][Epoch 10844] diffusion training Loss: 0.048121536150574684
2024-11-05 05:53:27,040 - INFO - [diffusion][Epoch 10844] diffusion learning rate: 0.001
2024-11-05 05:53:27,041 - INFO - [diffusion][Epoch 10844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:27,043 - INFO - [diffusion][Epoch 10845] Epoch 10846/12000
2024-11-05 05:53:31,152 - INFO - [diffusion][Epoch 10845] diffusion training Loss: 0.05002960376441479
2024-11-05 05:53:31,154 - INFO - [diffusion][Epoch 10845] diffusion learning rate: 0.001
2024-11-05 05:53:31,156 - INFO - [diffusion][Epoch 10845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:31,157 - INFO - [diffusion][Epoch 10846] Epoch 10847/12000
2024-11-05 05:53:35,119 - INFO - [diffusion][Epoch 10846] diffusion training Loss: 0.048825125209987164
2024-11-05 05:53:35,121 - INFO - [diffusion][Epoch 10846] diffusion learning rate: 0.001
2024-11-05 05:53:35,122 - INFO - [diffusion][Epoch 10846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:35,124 - INFO - [diffusion][Epoch 10847] Epoch 10848/12000
2024-11-05 05:53:39,199 - INFO - [diffusion][Epoch 10847] diffusion training Loss: 0.04397566244006157
2024-11-05 05:53:39,200 - INFO - [diffusion][Epoch 10847] diffusion learning rate: 0.001
2024-11-05 05:53:39,202 - INFO - [diffusion][Epoch 10847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:39,204 - INFO - [diffusion][Epoch 10848] Epoch 10849/12000
2024-11-05 05:53:43,260 - INFO - [diffusion][Epoch 10848] diffusion training Loss: 0.04824452940374613
2024-11-05 05:53:43,262 - INFO - [diffusion][Epoch 10848] diffusion learning rate: 0.001
2024-11-05 05:53:43,264 - INFO - [diffusion][Epoch 10848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:43,265 - INFO - [diffusion][Epoch 10849] Epoch 10850/12000
2024-11-05 05:53:47,337 - INFO - [diffusion][Epoch 10849] diffusion training Loss: 0.04918221104890108
2024-11-05 05:53:47,339 - INFO - [diffusion][Epoch 10849] diffusion learning rate: 0.001
2024-11-05 05:53:47,341 - INFO - [diffusion][Epoch 10849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:47,342 - INFO - [diffusion][Epoch 10850] Epoch 10851/12000
2024-11-05 05:53:51,421 - INFO - [diffusion][Epoch 10850] diffusion training Loss: 0.050497754476964474
2024-11-05 05:53:51,423 - INFO - [diffusion][Epoch 10850] diffusion learning rate: 0.001
2024-11-05 05:53:51,425 - INFO - [diffusion][Epoch 10850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:51,428 - INFO - [diffusion][Epoch 10851] Epoch 10852/12000
2024-11-05 05:53:55,485 - INFO - [diffusion][Epoch 10851] diffusion training Loss: 0.045742577873170376
2024-11-05 05:53:55,487 - INFO - [diffusion][Epoch 10851] diffusion learning rate: 0.001
2024-11-05 05:53:55,489 - INFO - [diffusion][Epoch 10851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:55,490 - INFO - [diffusion][Epoch 10852] Epoch 10853/12000
2024-11-05 05:53:59,612 - INFO - [diffusion][Epoch 10852] diffusion training Loss: 0.049088519997894764
2024-11-05 05:53:59,614 - INFO - [diffusion][Epoch 10852] diffusion learning rate: 0.001
2024-11-05 05:53:59,616 - INFO - [diffusion][Epoch 10852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:53:59,617 - INFO - [diffusion][Epoch 10853] Epoch 10854/12000
2024-11-05 05:54:03,646 - INFO - [diffusion][Epoch 10853] diffusion training Loss: 0.048699239268898964
2024-11-05 05:54:03,648 - INFO - [diffusion][Epoch 10853] diffusion learning rate: 0.001
2024-11-05 05:54:03,650 - INFO - [diffusion][Epoch 10853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:03,651 - INFO - [diffusion][Epoch 10854] Epoch 10855/12000
2024-11-05 05:54:07,709 - INFO - [diffusion][Epoch 10854] diffusion training Loss: 0.04887634236365557
2024-11-05 05:54:07,711 - INFO - [diffusion][Epoch 10854] diffusion learning rate: 0.001
2024-11-05 05:54:07,713 - INFO - [diffusion][Epoch 10854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:07,714 - INFO - [diffusion][Epoch 10855] Epoch 10856/12000
2024-11-05 05:54:11,810 - INFO - [diffusion][Epoch 10855] diffusion training Loss: 0.049761868081986904
2024-11-05 05:54:11,812 - INFO - [diffusion][Epoch 10855] diffusion learning rate: 0.001
2024-11-05 05:54:11,814 - INFO - [diffusion][Epoch 10855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:11,815 - INFO - [diffusion][Epoch 10856] Epoch 10857/12000
2024-11-05 05:54:15,893 - INFO - [diffusion][Epoch 10856] diffusion training Loss: 0.05062101315706968
2024-11-05 05:54:15,895 - INFO - [diffusion][Epoch 10856] diffusion learning rate: 0.001
2024-11-05 05:54:15,897 - INFO - [diffusion][Epoch 10856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:15,898 - INFO - [diffusion][Epoch 10857] Epoch 10858/12000
2024-11-05 05:54:19,992 - INFO - [diffusion][Epoch 10857] diffusion training Loss: 0.04622438829392195
2024-11-05 05:54:19,994 - INFO - [diffusion][Epoch 10857] diffusion learning rate: 0.001
2024-11-05 05:54:19,996 - INFO - [diffusion][Epoch 10857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:19,997 - INFO - [diffusion][Epoch 10858] Epoch 10859/12000
2024-11-05 05:54:24,055 - INFO - [diffusion][Epoch 10858] diffusion training Loss: 0.04860086552798748
2024-11-05 05:54:24,058 - INFO - [diffusion][Epoch 10858] diffusion learning rate: 0.001
2024-11-05 05:54:24,060 - INFO - [diffusion][Epoch 10858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:24,061 - INFO - [diffusion][Epoch 10859] Epoch 10860/12000
2024-11-05 05:54:28,145 - INFO - [diffusion][Epoch 10859] diffusion training Loss: 0.05126938968896866
2024-11-05 05:54:28,147 - INFO - [diffusion][Epoch 10859] diffusion learning rate: 0.001
2024-11-05 05:54:28,149 - INFO - [diffusion][Epoch 10859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:28,150 - INFO - [diffusion][Epoch 10860] Epoch 10861/12000
2024-11-05 05:54:32,255 - INFO - [diffusion][Epoch 10860] diffusion training Loss: 0.04793417174369097
2024-11-05 05:54:32,257 - INFO - [diffusion][Epoch 10860] diffusion learning rate: 0.001
2024-11-05 05:54:32,259 - INFO - [diffusion][Epoch 10860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:32,260 - INFO - [diffusion][Epoch 10861] Epoch 10862/12000
2024-11-05 05:54:36,524 - INFO - [diffusion][Epoch 10861] diffusion training Loss: 0.046620956622064114
2024-11-05 05:54:36,525 - INFO - [diffusion][Epoch 10861] diffusion learning rate: 0.001
2024-11-05 05:54:36,527 - INFO - [diffusion][Epoch 10861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:36,528 - INFO - [diffusion][Epoch 10862] Epoch 10863/12000
2024-11-05 05:54:40,624 - INFO - [diffusion][Epoch 10862] diffusion training Loss: 0.05098579078912735
2024-11-05 05:54:40,626 - INFO - [diffusion][Epoch 10862] diffusion learning rate: 0.001
2024-11-05 05:54:40,654 - INFO - [diffusion][Epoch 10862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:40,655 - INFO - [diffusion][Epoch 10863] Epoch 10864/12000
2024-11-05 05:54:44,565 - INFO - [diffusion][Epoch 10863] diffusion training Loss: 0.04473663028329611
2024-11-05 05:54:44,567 - INFO - [diffusion][Epoch 10863] diffusion learning rate: 0.001
2024-11-05 05:54:44,568 - INFO - [diffusion][Epoch 10863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:44,570 - INFO - [diffusion][Epoch 10864] Epoch 10865/12000
2024-11-05 05:54:48,549 - INFO - [diffusion][Epoch 10864] diffusion training Loss: 0.04645111225545406
2024-11-05 05:54:48,551 - INFO - [diffusion][Epoch 10864] diffusion learning rate: 0.001
2024-11-05 05:54:48,553 - INFO - [diffusion][Epoch 10864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:48,554 - INFO - [diffusion][Epoch 10865] Epoch 10866/12000
2024-11-05 05:54:52,683 - INFO - [diffusion][Epoch 10865] diffusion training Loss: 0.04394794348627329
2024-11-05 05:54:52,685 - INFO - [diffusion][Epoch 10865] diffusion learning rate: 0.001
2024-11-05 05:54:52,686 - INFO - [diffusion][Epoch 10865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:52,688 - INFO - [diffusion][Epoch 10866] Epoch 10867/12000
2024-11-05 05:54:56,759 - INFO - [diffusion][Epoch 10866] diffusion training Loss: 0.04388690087944269
2024-11-05 05:54:56,761 - INFO - [diffusion][Epoch 10866] diffusion learning rate: 0.001
2024-11-05 05:54:56,763 - INFO - [diffusion][Epoch 10866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:54:56,764 - INFO - [diffusion][Epoch 10867] Epoch 10868/12000
2024-11-05 05:55:00,832 - INFO - [diffusion][Epoch 10867] diffusion training Loss: 0.0471901623532176
2024-11-05 05:55:00,834 - INFO - [diffusion][Epoch 10867] diffusion learning rate: 0.001
2024-11-05 05:55:00,836 - INFO - [diffusion][Epoch 10867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:00,837 - INFO - [diffusion][Epoch 10868] Epoch 10869/12000
2024-11-05 05:55:04,805 - INFO - [diffusion][Epoch 10868] diffusion training Loss: 0.044630298390984535
2024-11-05 05:55:04,807 - INFO - [diffusion][Epoch 10868] diffusion learning rate: 0.001
2024-11-05 05:55:04,809 - INFO - [diffusion][Epoch 10868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:04,810 - INFO - [diffusion][Epoch 10869] Epoch 10870/12000
2024-11-05 05:55:08,906 - INFO - [diffusion][Epoch 10869] diffusion training Loss: 0.04798332415521145
2024-11-05 05:55:08,908 - INFO - [diffusion][Epoch 10869] diffusion learning rate: 0.001
2024-11-05 05:55:08,909 - INFO - [diffusion][Epoch 10869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:08,911 - INFO - [diffusion][Epoch 10870] Epoch 10871/12000
2024-11-05 05:55:13,029 - INFO - [diffusion][Epoch 10870] diffusion training Loss: 0.04843719583004713
2024-11-05 05:55:13,031 - INFO - [diffusion][Epoch 10870] diffusion learning rate: 0.001
2024-11-05 05:55:13,033 - INFO - [diffusion][Epoch 10870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:13,034 - INFO - [diffusion][Epoch 10871] Epoch 10872/12000
2024-11-05 05:55:17,108 - INFO - [diffusion][Epoch 10871] diffusion training Loss: 0.04453088063746691
2024-11-05 05:55:17,110 - INFO - [diffusion][Epoch 10871] diffusion learning rate: 0.001
2024-11-05 05:55:17,112 - INFO - [diffusion][Epoch 10871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:17,113 - INFO - [diffusion][Epoch 10872] Epoch 10873/12000
2024-11-05 05:55:21,203 - INFO - [diffusion][Epoch 10872] diffusion training Loss: 0.046117110177874565
2024-11-05 05:55:21,205 - INFO - [diffusion][Epoch 10872] diffusion learning rate: 0.001
2024-11-05 05:55:21,207 - INFO - [diffusion][Epoch 10872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:21,208 - INFO - [diffusion][Epoch 10873] Epoch 10874/12000
2024-11-05 05:55:25,300 - INFO - [diffusion][Epoch 10873] diffusion training Loss: 0.04878265131264925
2024-11-05 05:55:25,304 - INFO - [diffusion][Epoch 10873] diffusion learning rate: 0.001
2024-11-05 05:55:25,306 - INFO - [diffusion][Epoch 10873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:25,307 - INFO - [diffusion][Epoch 10874] Epoch 10875/12000
2024-11-05 05:55:29,397 - INFO - [diffusion][Epoch 10874] diffusion training Loss: 0.05006771348416805
2024-11-05 05:55:29,399 - INFO - [diffusion][Epoch 10874] diffusion learning rate: 0.001
2024-11-05 05:55:29,401 - INFO - [diffusion][Epoch 10874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:29,402 - INFO - [diffusion][Epoch 10875] Epoch 10876/12000
2024-11-05 05:55:33,470 - INFO - [diffusion][Epoch 10875] diffusion training Loss: 0.046228405088186264
2024-11-05 05:55:33,472 - INFO - [diffusion][Epoch 10875] diffusion learning rate: 0.001
2024-11-05 05:55:33,512 - INFO - [diffusion][Epoch 10875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:33,513 - INFO - [diffusion][Epoch 10876] Epoch 10877/12000
2024-11-05 05:55:37,607 - INFO - [diffusion][Epoch 10876] diffusion training Loss: 0.048589590936899185
2024-11-05 05:55:37,609 - INFO - [diffusion][Epoch 10876] diffusion learning rate: 0.001
2024-11-05 05:55:37,610 - INFO - [diffusion][Epoch 10876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:37,612 - INFO - [diffusion][Epoch 10877] Epoch 10878/12000
2024-11-05 05:55:41,681 - INFO - [diffusion][Epoch 10877] diffusion training Loss: 0.050145139917731285
2024-11-05 05:55:41,683 - INFO - [diffusion][Epoch 10877] diffusion learning rate: 0.001
2024-11-05 05:55:41,685 - INFO - [diffusion][Epoch 10877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:41,686 - INFO - [diffusion][Epoch 10878] Epoch 10879/12000
2024-11-05 05:55:45,761 - INFO - [diffusion][Epoch 10878] diffusion training Loss: 0.044326817616820335
2024-11-05 05:55:45,762 - INFO - [diffusion][Epoch 10878] diffusion learning rate: 0.001
2024-11-05 05:55:45,764 - INFO - [diffusion][Epoch 10878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:45,766 - INFO - [diffusion][Epoch 10879] Epoch 10880/12000
2024-11-05 05:55:49,835 - INFO - [diffusion][Epoch 10879] diffusion training Loss: 0.05128716118633747
2024-11-05 05:55:49,837 - INFO - [diffusion][Epoch 10879] diffusion learning rate: 0.001
2024-11-05 05:55:49,865 - INFO - [diffusion][Epoch 10879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:49,866 - INFO - [diffusion][Epoch 10880] Epoch 10881/12000
2024-11-05 05:55:53,799 - INFO - [diffusion][Epoch 10880] diffusion training Loss: 0.05203623231500387
2024-11-05 05:55:53,803 - INFO - [diffusion][Epoch 10880] diffusion learning rate: 0.001
2024-11-05 05:55:53,805 - INFO - [diffusion][Epoch 10880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:53,807 - INFO - [diffusion][Epoch 10881] Epoch 10882/12000
2024-11-05 05:55:57,726 - INFO - [diffusion][Epoch 10881] diffusion training Loss: 0.04679745156317949
2024-11-05 05:55:57,728 - INFO - [diffusion][Epoch 10881] diffusion learning rate: 0.001
2024-11-05 05:55:57,730 - INFO - [diffusion][Epoch 10881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:55:57,731 - INFO - [diffusion][Epoch 10882] Epoch 10883/12000
2024-11-05 05:56:01,975 - INFO - [diffusion][Epoch 10882] diffusion training Loss: 0.05181210860610008
2024-11-05 05:56:01,977 - INFO - [diffusion][Epoch 10882] diffusion learning rate: 0.001
2024-11-05 05:56:01,979 - INFO - [diffusion][Epoch 10882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:01,980 - INFO - [diffusion][Epoch 10883] Epoch 10884/12000
2024-11-05 05:56:06,078 - INFO - [diffusion][Epoch 10883] diffusion training Loss: 0.052038331516087055
2024-11-05 05:56:06,080 - INFO - [diffusion][Epoch 10883] diffusion learning rate: 0.001
2024-11-05 05:56:06,107 - INFO - [diffusion][Epoch 10883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:06,109 - INFO - [diffusion][Epoch 10884] Epoch 10885/12000
2024-11-05 05:56:10,196 - INFO - [diffusion][Epoch 10884] diffusion training Loss: 0.043960425071418285
2024-11-05 05:56:10,198 - INFO - [diffusion][Epoch 10884] diffusion learning rate: 0.001
2024-11-05 05:56:10,200 - INFO - [diffusion][Epoch 10884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:10,201 - INFO - [diffusion][Epoch 10885] Epoch 10886/12000
2024-11-05 05:56:14,291 - INFO - [diffusion][Epoch 10885] diffusion training Loss: 0.048385621048510075
2024-11-05 05:56:14,293 - INFO - [diffusion][Epoch 10885] diffusion learning rate: 0.001
2024-11-05 05:56:14,295 - INFO - [diffusion][Epoch 10885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:14,296 - INFO - [diffusion][Epoch 10886] Epoch 10887/12000
2024-11-05 05:56:18,363 - INFO - [diffusion][Epoch 10886] diffusion training Loss: 0.045572190545499325
2024-11-05 05:56:18,366 - INFO - [diffusion][Epoch 10886] diffusion learning rate: 0.001
2024-11-05 05:56:18,368 - INFO - [diffusion][Epoch 10886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:18,369 - INFO - [diffusion][Epoch 10887] Epoch 10888/12000
2024-11-05 05:56:22,486 - INFO - [diffusion][Epoch 10887] diffusion training Loss: 0.05173247028142214
2024-11-05 05:56:22,488 - INFO - [diffusion][Epoch 10887] diffusion learning rate: 0.001
2024-11-05 05:56:22,490 - INFO - [diffusion][Epoch 10887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:22,491 - INFO - [diffusion][Epoch 10888] Epoch 10889/12000
2024-11-05 05:56:26,608 - INFO - [diffusion][Epoch 10888] diffusion training Loss: 0.04543074779212475
2024-11-05 05:56:26,611 - INFO - [diffusion][Epoch 10888] diffusion learning rate: 0.001
2024-11-05 05:56:26,613 - INFO - [diffusion][Epoch 10888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:26,614 - INFO - [diffusion][Epoch 10889] Epoch 10890/12000
2024-11-05 05:56:30,712 - INFO - [diffusion][Epoch 10889] diffusion training Loss: 0.04436481650918722
2024-11-05 05:56:30,714 - INFO - [diffusion][Epoch 10889] diffusion learning rate: 0.001
2024-11-05 05:56:30,716 - INFO - [diffusion][Epoch 10889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:30,717 - INFO - [diffusion][Epoch 10890] Epoch 10891/12000
2024-11-05 05:56:34,804 - INFO - [diffusion][Epoch 10890] diffusion training Loss: 0.04561689682304859
2024-11-05 05:56:34,806 - INFO - [diffusion][Epoch 10890] diffusion learning rate: 0.001
2024-11-05 05:56:34,807 - INFO - [diffusion][Epoch 10890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:34,809 - INFO - [diffusion][Epoch 10891] Epoch 10892/12000
2024-11-05 05:56:38,886 - INFO - [diffusion][Epoch 10891] diffusion training Loss: 0.052351306192576885
2024-11-05 05:56:38,888 - INFO - [diffusion][Epoch 10891] diffusion learning rate: 0.001
2024-11-05 05:56:38,889 - INFO - [diffusion][Epoch 10891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:38,891 - INFO - [diffusion][Epoch 10892] Epoch 10893/12000
2024-11-05 05:56:42,961 - INFO - [diffusion][Epoch 10892] diffusion training Loss: 0.04544859938323498
2024-11-05 05:56:42,963 - INFO - [diffusion][Epoch 10892] diffusion learning rate: 0.001
2024-11-05 05:56:42,965 - INFO - [diffusion][Epoch 10892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:42,966 - INFO - [diffusion][Epoch 10893] Epoch 10894/12000
2024-11-05 05:56:47,070 - INFO - [diffusion][Epoch 10893] diffusion training Loss: 0.04859269876033068
2024-11-05 05:56:47,072 - INFO - [diffusion][Epoch 10893] diffusion learning rate: 0.001
2024-11-05 05:56:47,073 - INFO - [diffusion][Epoch 10893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:47,075 - INFO - [diffusion][Epoch 10894] Epoch 10895/12000
2024-11-05 05:56:51,170 - INFO - [diffusion][Epoch 10894] diffusion training Loss: 0.05470420606434345
2024-11-05 05:56:51,172 - INFO - [diffusion][Epoch 10894] diffusion learning rate: 0.001
2024-11-05 05:56:51,173 - INFO - [diffusion][Epoch 10894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:51,175 - INFO - [diffusion][Epoch 10895] Epoch 10896/12000
2024-11-05 05:56:55,155 - INFO - [diffusion][Epoch 10895] diffusion training Loss: 0.03857485577464104
2024-11-05 05:56:55,157 - INFO - [diffusion][Epoch 10895] diffusion learning rate: 0.001
2024-11-05 05:56:55,159 - INFO - [diffusion][Epoch 10895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:55,161 - INFO - [diffusion][Epoch 10896] Epoch 10897/12000
2024-11-05 05:56:59,145 - INFO - [diffusion][Epoch 10896] diffusion training Loss: 0.054284884594380856
2024-11-05 05:56:59,147 - INFO - [diffusion][Epoch 10896] diffusion learning rate: 0.001
2024-11-05 05:56:59,149 - INFO - [diffusion][Epoch 10896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:56:59,150 - INFO - [diffusion][Epoch 10897] Epoch 10898/12000
2024-11-05 05:57:03,242 - INFO - [diffusion][Epoch 10897] diffusion training Loss: 0.05343123525381088
2024-11-05 05:57:03,244 - INFO - [diffusion][Epoch 10897] diffusion learning rate: 0.001
2024-11-05 05:57:03,246 - INFO - [diffusion][Epoch 10897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:03,248 - INFO - [diffusion][Epoch 10898] Epoch 10899/12000
2024-11-05 05:57:07,371 - INFO - [diffusion][Epoch 10898] diffusion training Loss: 0.050109731033444405
2024-11-05 05:57:07,373 - INFO - [diffusion][Epoch 10898] diffusion learning rate: 0.001
2024-11-05 05:57:07,374 - INFO - [diffusion][Epoch 10898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:07,376 - INFO - [diffusion][Epoch 10899] Epoch 10900/12000
2024-11-05 05:57:11,250 - INFO - [diffusion][Epoch 10899] diffusion training Loss: 0.051555163227021694
2024-11-05 05:57:11,252 - INFO - [diffusion][Epoch 10899] diffusion learning rate: 0.001
2024-11-05 05:57:11,254 - INFO - [diffusion][Epoch 10899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:11,255 - INFO - [diffusion][Epoch 10900] Epoch 10901/12000
2024-11-05 05:57:15,315 - INFO - [diffusion][Epoch 10900] diffusion training Loss: 0.04972679913043976
2024-11-05 05:57:15,317 - INFO - [diffusion][Epoch 10900] diffusion learning rate: 0.001
2024-11-05 05:57:15,318 - INFO - [diffusion][Epoch 10900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:15,320 - INFO - [diffusion][Epoch 10901] Epoch 10902/12000
2024-11-05 05:57:19,408 - INFO - [diffusion][Epoch 10901] diffusion training Loss: 0.04896086826920509
2024-11-05 05:57:19,410 - INFO - [diffusion][Epoch 10901] diffusion learning rate: 0.001
2024-11-05 05:57:19,412 - INFO - [diffusion][Epoch 10901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:19,413 - INFO - [diffusion][Epoch 10902] Epoch 10903/12000
2024-11-05 05:57:23,526 - INFO - [diffusion][Epoch 10902] diffusion training Loss: 0.04794380534440279
2024-11-05 05:57:23,527 - INFO - [diffusion][Epoch 10902] diffusion learning rate: 0.001
2024-11-05 05:57:23,529 - INFO - [diffusion][Epoch 10902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:23,530 - INFO - [diffusion][Epoch 10903] Epoch 10904/12000
2024-11-05 05:57:27,635 - INFO - [diffusion][Epoch 10903] diffusion training Loss: 0.0500639658421278
2024-11-05 05:57:27,639 - INFO - [diffusion][Epoch 10903] diffusion learning rate: 0.001
2024-11-05 05:57:27,641 - INFO - [diffusion][Epoch 10903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:27,642 - INFO - [diffusion][Epoch 10904] Epoch 10905/12000
2024-11-05 05:57:31,769 - INFO - [diffusion][Epoch 10904] diffusion training Loss: 0.04697158746421337
2024-11-05 05:57:31,771 - INFO - [diffusion][Epoch 10904] diffusion learning rate: 0.001
2024-11-05 05:57:31,773 - INFO - [diffusion][Epoch 10904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:31,774 - INFO - [diffusion][Epoch 10905] Epoch 10906/12000
2024-11-05 05:57:35,898 - INFO - [diffusion][Epoch 10905] diffusion training Loss: 0.0496404068544507
2024-11-05 05:57:35,900 - INFO - [diffusion][Epoch 10905] diffusion learning rate: 0.001
2024-11-05 05:57:35,902 - INFO - [diffusion][Epoch 10905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:35,903 - INFO - [diffusion][Epoch 10906] Epoch 10907/12000
2024-11-05 05:57:39,865 - INFO - [diffusion][Epoch 10906] diffusion training Loss: 0.04766984470188618
2024-11-05 05:57:39,867 - INFO - [diffusion][Epoch 10906] diffusion learning rate: 0.001
2024-11-05 05:57:39,869 - INFO - [diffusion][Epoch 10906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:39,870 - INFO - [diffusion][Epoch 10907] Epoch 10908/12000
2024-11-05 05:57:43,985 - INFO - [diffusion][Epoch 10907] diffusion training Loss: 0.04377898573875427
2024-11-05 05:57:43,987 - INFO - [diffusion][Epoch 10907] diffusion learning rate: 0.001
2024-11-05 05:57:43,989 - INFO - [diffusion][Epoch 10907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:43,990 - INFO - [diffusion][Epoch 10908] Epoch 10909/12000
2024-11-05 05:57:48,075 - INFO - [diffusion][Epoch 10908] diffusion training Loss: 0.04731729254126549
2024-11-05 05:57:48,077 - INFO - [diffusion][Epoch 10908] diffusion learning rate: 0.001
2024-11-05 05:57:48,078 - INFO - [diffusion][Epoch 10908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:48,080 - INFO - [diffusion][Epoch 10909] Epoch 10910/12000
2024-11-05 05:57:52,155 - INFO - [diffusion][Epoch 10909] diffusion training Loss: 0.04610159620642662
2024-11-05 05:57:52,157 - INFO - [diffusion][Epoch 10909] diffusion learning rate: 0.001
2024-11-05 05:57:52,159 - INFO - [diffusion][Epoch 10909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:52,160 - INFO - [diffusion][Epoch 10910] Epoch 10911/12000
2024-11-05 05:57:56,224 - INFO - [diffusion][Epoch 10910] diffusion training Loss: 0.04596061818301678
2024-11-05 05:57:56,227 - INFO - [diffusion][Epoch 10910] diffusion learning rate: 0.001
2024-11-05 05:57:56,229 - INFO - [diffusion][Epoch 10910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:57:56,230 - INFO - [diffusion][Epoch 10911] Epoch 10912/12000
2024-11-05 05:58:00,360 - INFO - [diffusion][Epoch 10911] diffusion training Loss: 0.05349298659712076
2024-11-05 05:58:00,362 - INFO - [diffusion][Epoch 10911] diffusion learning rate: 0.001
2024-11-05 05:58:00,364 - INFO - [diffusion][Epoch 10911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:00,365 - INFO - [diffusion][Epoch 10912] Epoch 10913/12000
2024-11-05 05:58:04,465 - INFO - [diffusion][Epoch 10912] diffusion training Loss: 0.04525782726705074
2024-11-05 05:58:04,466 - INFO - [diffusion][Epoch 10912] diffusion learning rate: 0.001
2024-11-05 05:58:04,468 - INFO - [diffusion][Epoch 10912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:04,471 - INFO - [diffusion][Epoch 10913] Epoch 10914/12000
2024-11-05 05:58:08,540 - INFO - [diffusion][Epoch 10913] diffusion training Loss: 0.049700960516929626
2024-11-05 05:58:08,541 - INFO - [diffusion][Epoch 10913] diffusion learning rate: 0.001
2024-11-05 05:58:08,543 - INFO - [diffusion][Epoch 10913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:08,544 - INFO - [diffusion][Epoch 10914] Epoch 10915/12000
2024-11-05 05:58:12,682 - INFO - [diffusion][Epoch 10914] diffusion training Loss: 0.04345275741070509
2024-11-05 05:58:12,684 - INFO - [diffusion][Epoch 10914] diffusion learning rate: 0.001
2024-11-05 05:58:12,686 - INFO - [diffusion][Epoch 10914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:12,687 - INFO - [diffusion][Epoch 10915] Epoch 10916/12000
2024-11-05 05:58:16,812 - INFO - [diffusion][Epoch 10915] diffusion training Loss: 0.0507596954703331
2024-11-05 05:58:16,813 - INFO - [diffusion][Epoch 10915] diffusion learning rate: 0.001
2024-11-05 05:58:16,816 - INFO - [diffusion][Epoch 10915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:16,817 - INFO - [diffusion][Epoch 10916] Epoch 10917/12000
2024-11-05 05:58:20,798 - INFO - [diffusion][Epoch 10916] diffusion training Loss: 0.04531368613243103
2024-11-05 05:58:20,800 - INFO - [diffusion][Epoch 10916] diffusion learning rate: 0.001
2024-11-05 05:58:20,801 - INFO - [diffusion][Epoch 10916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:20,802 - INFO - [diffusion][Epoch 10917] Epoch 10918/12000
2024-11-05 05:58:24,596 - INFO - [diffusion][Epoch 10917] diffusion training Loss: 0.05048105586320162
2024-11-05 05:58:24,598 - INFO - [diffusion][Epoch 10917] diffusion learning rate: 0.001
2024-11-05 05:58:24,600 - INFO - [diffusion][Epoch 10917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:24,602 - INFO - [diffusion][Epoch 10918] Epoch 10919/12000
2024-11-05 05:58:28,658 - INFO - [diffusion][Epoch 10918] diffusion training Loss: 0.053927638567984104
2024-11-05 05:58:28,662 - INFO - [diffusion][Epoch 10918] diffusion learning rate: 0.001
2024-11-05 05:58:28,664 - INFO - [diffusion][Epoch 10918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:28,665 - INFO - [diffusion][Epoch 10919] Epoch 10920/12000
2024-11-05 05:58:32,754 - INFO - [diffusion][Epoch 10919] diffusion training Loss: 0.047276705503463745
2024-11-05 05:58:32,756 - INFO - [diffusion][Epoch 10919] diffusion learning rate: 0.001
2024-11-05 05:58:32,757 - INFO - [diffusion][Epoch 10919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:32,759 - INFO - [diffusion][Epoch 10920] Epoch 10921/12000
2024-11-05 05:58:36,837 - INFO - [diffusion][Epoch 10920] diffusion training Loss: 0.04966465290635824
2024-11-05 05:58:36,840 - INFO - [diffusion][Epoch 10920] diffusion learning rate: 0.001
2024-11-05 05:58:36,841 - INFO - [diffusion][Epoch 10920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:36,843 - INFO - [diffusion][Epoch 10921] Epoch 10922/12000
2024-11-05 05:58:40,941 - INFO - [diffusion][Epoch 10921] diffusion training Loss: 0.04777491092681885
2024-11-05 05:58:40,943 - INFO - [diffusion][Epoch 10921] diffusion learning rate: 0.001
2024-11-05 05:58:40,945 - INFO - [diffusion][Epoch 10921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:40,946 - INFO - [diffusion][Epoch 10922] Epoch 10923/12000
2024-11-05 05:58:45,015 - INFO - [diffusion][Epoch 10922] diffusion training Loss: 0.04537082277238369
2024-11-05 05:58:45,016 - INFO - [diffusion][Epoch 10922] diffusion learning rate: 0.001
2024-11-05 05:58:45,018 - INFO - [diffusion][Epoch 10922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:45,019 - INFO - [diffusion][Epoch 10923] Epoch 10924/12000
2024-11-05 05:58:49,077 - INFO - [diffusion][Epoch 10923] diffusion training Loss: 0.05186565313488245
2024-11-05 05:58:49,079 - INFO - [diffusion][Epoch 10923] diffusion learning rate: 0.001
2024-11-05 05:58:49,081 - INFO - [diffusion][Epoch 10923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:49,082 - INFO - [diffusion][Epoch 10924] Epoch 10925/12000
2024-11-05 05:58:53,187 - INFO - [diffusion][Epoch 10924] diffusion training Loss: 0.04606486391276121
2024-11-05 05:58:53,189 - INFO - [diffusion][Epoch 10924] diffusion learning rate: 0.001
2024-11-05 05:58:53,191 - INFO - [diffusion][Epoch 10924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:53,193 - INFO - [diffusion][Epoch 10925] Epoch 10926/12000
2024-11-05 05:58:57,570 - INFO - [diffusion][Epoch 10925] diffusion training Loss: 0.04767035041004419
2024-11-05 05:58:57,572 - INFO - [diffusion][Epoch 10925] diffusion learning rate: 0.001
2024-11-05 05:58:57,574 - INFO - [diffusion][Epoch 10925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:58:57,575 - INFO - [diffusion][Epoch 10926] Epoch 10927/12000
2024-11-05 05:59:01,650 - INFO - [diffusion][Epoch 10926] diffusion training Loss: 0.04533982556313276
2024-11-05 05:59:01,652 - INFO - [diffusion][Epoch 10926] diffusion learning rate: 0.001
2024-11-05 05:59:01,654 - INFO - [diffusion][Epoch 10926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:01,655 - INFO - [diffusion][Epoch 10927] Epoch 10928/12000
2024-11-05 05:59:05,727 - INFO - [diffusion][Epoch 10927] diffusion training Loss: 0.046553269028663635
2024-11-05 05:59:05,729 - INFO - [diffusion][Epoch 10927] diffusion learning rate: 0.001
2024-11-05 05:59:05,730 - INFO - [diffusion][Epoch 10927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:05,732 - INFO - [diffusion][Epoch 10928] Epoch 10929/12000
2024-11-05 05:59:09,820 - INFO - [diffusion][Epoch 10928] diffusion training Loss: 0.04725455492734909
2024-11-05 05:59:09,822 - INFO - [diffusion][Epoch 10928] diffusion learning rate: 0.001
2024-11-05 05:59:09,824 - INFO - [diffusion][Epoch 10928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:09,825 - INFO - [diffusion][Epoch 10929] Epoch 10930/12000
2024-11-05 05:59:13,877 - INFO - [diffusion][Epoch 10929] diffusion training Loss: 0.04408860020339489
2024-11-05 05:59:13,879 - INFO - [diffusion][Epoch 10929] diffusion learning rate: 0.001
2024-11-05 05:59:13,881 - INFO - [diffusion][Epoch 10929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:13,882 - INFO - [diffusion][Epoch 10930] Epoch 10931/12000
2024-11-05 05:59:18,007 - INFO - [diffusion][Epoch 10930] diffusion training Loss: 0.04642005078494549
2024-11-05 05:59:18,009 - INFO - [diffusion][Epoch 10930] diffusion learning rate: 0.001
2024-11-05 05:59:18,011 - INFO - [diffusion][Epoch 10930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:18,012 - INFO - [diffusion][Epoch 10931] Epoch 10932/12000
2024-11-05 05:59:22,107 - INFO - [diffusion][Epoch 10931] diffusion training Loss: 0.04684612900018692
2024-11-05 05:59:22,152 - INFO - [diffusion][Epoch 10931] diffusion learning rate: 0.001
2024-11-05 05:59:22,157 - INFO - [diffusion][Epoch 10931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:22,159 - INFO - [diffusion][Epoch 10932] Epoch 10933/12000
2024-11-05 05:59:26,264 - INFO - [diffusion][Epoch 10932] diffusion training Loss: 0.0422032019123435
2024-11-05 05:59:26,266 - INFO - [diffusion][Epoch 10932] diffusion learning rate: 0.001
2024-11-05 05:59:26,268 - INFO - [diffusion][Epoch 10932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:26,269 - INFO - [diffusion][Epoch 10933] Epoch 10934/12000
2024-11-05 05:59:30,331 - INFO - [diffusion][Epoch 10933] diffusion training Loss: 0.04286746867001057
2024-11-05 05:59:30,334 - INFO - [diffusion][Epoch 10933] diffusion learning rate: 0.001
2024-11-05 05:59:30,336 - INFO - [diffusion][Epoch 10933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:30,337 - INFO - [diffusion][Epoch 10934] Epoch 10935/12000
2024-11-05 05:59:34,419 - INFO - [diffusion][Epoch 10934] diffusion training Loss: 0.04480903781950474
2024-11-05 05:59:34,421 - INFO - [diffusion][Epoch 10934] diffusion learning rate: 0.001
2024-11-05 05:59:34,423 - INFO - [diffusion][Epoch 10934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:34,424 - INFO - [diffusion][Epoch 10935] Epoch 10936/12000
2024-11-05 05:59:38,492 - INFO - [diffusion][Epoch 10935] diffusion training Loss: 0.0465965922921896
2024-11-05 05:59:38,494 - INFO - [diffusion][Epoch 10935] diffusion learning rate: 0.001
2024-11-05 05:59:38,496 - INFO - [diffusion][Epoch 10935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:38,497 - INFO - [diffusion][Epoch 10936] Epoch 10937/12000
2024-11-05 05:59:42,542 - INFO - [diffusion][Epoch 10936] diffusion training Loss: 0.04451317060738802
2024-11-05 05:59:42,544 - INFO - [diffusion][Epoch 10936] diffusion learning rate: 0.001
2024-11-05 05:59:42,545 - INFO - [diffusion][Epoch 10936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:42,547 - INFO - [diffusion][Epoch 10937] Epoch 10938/12000
2024-11-05 05:59:46,647 - INFO - [diffusion][Epoch 10937] diffusion training Loss: 0.04898381698876619
2024-11-05 05:59:46,649 - INFO - [diffusion][Epoch 10937] diffusion learning rate: 0.001
2024-11-05 05:59:46,651 - INFO - [diffusion][Epoch 10937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:46,652 - INFO - [diffusion][Epoch 10938] Epoch 10939/12000
2024-11-05 05:59:50,742 - INFO - [diffusion][Epoch 10938] diffusion training Loss: 0.044929386116564274
2024-11-05 05:59:50,744 - INFO - [diffusion][Epoch 10938] diffusion learning rate: 0.001
2024-11-05 05:59:50,746 - INFO - [diffusion][Epoch 10938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:50,747 - INFO - [diffusion][Epoch 10939] Epoch 10940/12000
2024-11-05 05:59:54,872 - INFO - [diffusion][Epoch 10939] diffusion training Loss: 0.04526303242892027
2024-11-05 05:59:54,874 - INFO - [diffusion][Epoch 10939] diffusion learning rate: 0.001
2024-11-05 05:59:54,875 - INFO - [diffusion][Epoch 10939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:54,876 - INFO - [diffusion][Epoch 10940] Epoch 10941/12000
2024-11-05 05:59:58,966 - INFO - [diffusion][Epoch 10940] diffusion training Loss: 0.04520749859511852
2024-11-05 05:59:58,968 - INFO - [diffusion][Epoch 10940] diffusion learning rate: 0.001
2024-11-05 05:59:58,970 - INFO - [diffusion][Epoch 10940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 05:59:58,971 - INFO - [diffusion][Epoch 10941] Epoch 10942/12000
2024-11-05 06:00:03,084 - INFO - [diffusion][Epoch 10941] diffusion training Loss: 0.045542145147919655
2024-11-05 06:00:03,086 - INFO - [diffusion][Epoch 10941] diffusion learning rate: 0.001
2024-11-05 06:00:03,088 - INFO - [diffusion][Epoch 10941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:03,089 - INFO - [diffusion][Epoch 10942] Epoch 10943/12000
2024-11-05 06:00:07,165 - INFO - [diffusion][Epoch 10942] diffusion training Loss: 0.044647764414548874
2024-11-05 06:00:07,167 - INFO - [diffusion][Epoch 10942] diffusion learning rate: 0.001
2024-11-05 06:00:07,169 - INFO - [diffusion][Epoch 10942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:07,170 - INFO - [diffusion][Epoch 10943] Epoch 10944/12000
2024-11-05 06:00:11,307 - INFO - [diffusion][Epoch 10943] diffusion training Loss: 0.046597098000347614
2024-11-05 06:00:11,309 - INFO - [diffusion][Epoch 10943] diffusion learning rate: 0.001
2024-11-05 06:00:11,311 - INFO - [diffusion][Epoch 10943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:11,312 - INFO - [diffusion][Epoch 10944] Epoch 10945/12000
2024-11-05 06:00:15,276 - INFO - [diffusion][Epoch 10944] diffusion training Loss: 0.04830258060246706
2024-11-05 06:00:15,278 - INFO - [diffusion][Epoch 10944] diffusion learning rate: 0.001
2024-11-05 06:00:15,280 - INFO - [diffusion][Epoch 10944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:15,281 - INFO - [diffusion][Epoch 10945] Epoch 10946/12000
2024-11-05 06:00:20,018 - INFO - [diffusion][Epoch 10945] diffusion training Loss: 0.046758164651691914
2024-11-05 06:00:20,020 - INFO - [diffusion][Epoch 10945] diffusion learning rate: 0.001
2024-11-05 06:00:20,022 - INFO - [diffusion][Epoch 10945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:20,023 - INFO - [diffusion][Epoch 10946] Epoch 10947/12000
2024-11-05 06:00:24,098 - INFO - [diffusion][Epoch 10946] diffusion training Loss: 0.04942707158625126
2024-11-05 06:00:24,100 - INFO - [diffusion][Epoch 10946] diffusion learning rate: 0.001
2024-11-05 06:00:24,102 - INFO - [diffusion][Epoch 10946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:24,103 - INFO - [diffusion][Epoch 10947] Epoch 10948/12000
2024-11-05 06:00:27,999 - INFO - [diffusion][Epoch 10947] diffusion training Loss: 0.046734184958040714
2024-11-05 06:00:28,001 - INFO - [diffusion][Epoch 10947] diffusion learning rate: 0.001
2024-11-05 06:00:28,002 - INFO - [diffusion][Epoch 10947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:28,003 - INFO - [diffusion][Epoch 10948] Epoch 10949/12000
2024-11-05 06:00:31,807 - INFO - [diffusion][Epoch 10948] diffusion training Loss: 0.05105155147612095
2024-11-05 06:00:31,811 - INFO - [diffusion][Epoch 10948] diffusion learning rate: 0.001
2024-11-05 06:00:31,813 - INFO - [diffusion][Epoch 10948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:31,814 - INFO - [diffusion][Epoch 10949] Epoch 10950/12000
2024-11-05 06:00:35,709 - INFO - [diffusion][Epoch 10949] diffusion training Loss: 0.045532677322626114
2024-11-05 06:00:35,711 - INFO - [diffusion][Epoch 10949] diffusion learning rate: 0.001
2024-11-05 06:00:35,713 - INFO - [diffusion][Epoch 10949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:35,714 - INFO - [diffusion][Epoch 10950] Epoch 10951/12000
2024-11-05 06:00:39,802 - INFO - [diffusion][Epoch 10950] diffusion training Loss: 0.049196070060133934
2024-11-05 06:00:39,804 - INFO - [diffusion][Epoch 10950] diffusion learning rate: 0.001
2024-11-05 06:00:39,806 - INFO - [diffusion][Epoch 10950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:39,807 - INFO - [diffusion][Epoch 10951] Epoch 10952/12000
2024-11-05 06:00:43,873 - INFO - [diffusion][Epoch 10951] diffusion training Loss: 0.04775571823120117
2024-11-05 06:00:43,874 - INFO - [diffusion][Epoch 10951] diffusion learning rate: 0.001
2024-11-05 06:00:43,876 - INFO - [diffusion][Epoch 10951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:43,877 - INFO - [diffusion][Epoch 10952] Epoch 10953/12000
2024-11-05 06:00:47,961 - INFO - [diffusion][Epoch 10952] diffusion training Loss: 0.0516183078289032
2024-11-05 06:00:47,963 - INFO - [diffusion][Epoch 10952] diffusion learning rate: 0.001
2024-11-05 06:00:47,965 - INFO - [diffusion][Epoch 10952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:47,966 - INFO - [diffusion][Epoch 10953] Epoch 10954/12000
2024-11-05 06:00:51,897 - INFO - [diffusion][Epoch 10953] diffusion training Loss: 0.04980915226042271
2024-11-05 06:00:51,899 - INFO - [diffusion][Epoch 10953] diffusion learning rate: 0.001
2024-11-05 06:00:51,900 - INFO - [diffusion][Epoch 10953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:51,902 - INFO - [diffusion][Epoch 10954] Epoch 10955/12000
2024-11-05 06:00:55,967 - INFO - [diffusion][Epoch 10954] diffusion training Loss: 0.050950526259839535
2024-11-05 06:00:55,969 - INFO - [diffusion][Epoch 10954] diffusion learning rate: 0.001
2024-11-05 06:00:55,971 - INFO - [diffusion][Epoch 10954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:00:55,972 - INFO - [diffusion][Epoch 10955] Epoch 10956/12000
2024-11-05 06:01:00,051 - INFO - [diffusion][Epoch 10955] diffusion training Loss: 0.04515925981104374
2024-11-05 06:01:00,053 - INFO - [diffusion][Epoch 10955] diffusion learning rate: 0.001
2024-11-05 06:01:00,054 - INFO - [diffusion][Epoch 10955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:00,056 - INFO - [diffusion][Epoch 10956] Epoch 10957/12000
2024-11-05 06:01:04,190 - INFO - [diffusion][Epoch 10956] diffusion training Loss: 0.04500872828066349
2024-11-05 06:01:04,192 - INFO - [diffusion][Epoch 10956] diffusion learning rate: 0.001
2024-11-05 06:01:04,193 - INFO - [diffusion][Epoch 10956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:04,195 - INFO - [diffusion][Epoch 10957] Epoch 10958/12000
2024-11-05 06:01:08,275 - INFO - [diffusion][Epoch 10957] diffusion training Loss: 0.04696052800863981
2024-11-05 06:01:08,278 - INFO - [diffusion][Epoch 10957] diffusion learning rate: 0.001
2024-11-05 06:01:08,280 - INFO - [diffusion][Epoch 10957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:08,281 - INFO - [diffusion][Epoch 10958] Epoch 10959/12000
2024-11-05 06:01:12,384 - INFO - [diffusion][Epoch 10958] diffusion training Loss: 0.04772649332880974
2024-11-05 06:01:12,386 - INFO - [diffusion][Epoch 10958] diffusion learning rate: 0.001
2024-11-05 06:01:12,388 - INFO - [diffusion][Epoch 10958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:12,389 - INFO - [diffusion][Epoch 10959] Epoch 10960/12000
2024-11-05 06:01:16,487 - INFO - [diffusion][Epoch 10959] diffusion training Loss: 0.048317257314920425
2024-11-05 06:01:16,488 - INFO - [diffusion][Epoch 10959] diffusion learning rate: 0.001
2024-11-05 06:01:16,490 - INFO - [diffusion][Epoch 10959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:16,492 - INFO - [diffusion][Epoch 10960] Epoch 10961/12000
2024-11-05 06:01:20,539 - INFO - [diffusion][Epoch 10960] diffusion training Loss: 0.048901948146522045
2024-11-05 06:01:20,541 - INFO - [diffusion][Epoch 10960] diffusion learning rate: 0.001
2024-11-05 06:01:20,542 - INFO - [diffusion][Epoch 10960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:20,544 - INFO - [diffusion][Epoch 10961] Epoch 10962/12000
2024-11-05 06:01:24,627 - INFO - [diffusion][Epoch 10961] diffusion training Loss: 0.045512187294662
2024-11-05 06:01:24,629 - INFO - [diffusion][Epoch 10961] diffusion learning rate: 0.001
2024-11-05 06:01:24,631 - INFO - [diffusion][Epoch 10961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:24,632 - INFO - [diffusion][Epoch 10962] Epoch 10963/12000
2024-11-05 06:01:28,725 - INFO - [diffusion][Epoch 10962] diffusion training Loss: 0.04840410687029362
2024-11-05 06:01:28,727 - INFO - [diffusion][Epoch 10962] diffusion learning rate: 0.001
2024-11-05 06:01:28,728 - INFO - [diffusion][Epoch 10962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:28,730 - INFO - [diffusion][Epoch 10963] Epoch 10964/12000
2024-11-05 06:01:32,854 - INFO - [diffusion][Epoch 10963] diffusion training Loss: 0.053894586861133575
2024-11-05 06:01:32,858 - INFO - [diffusion][Epoch 10963] diffusion learning rate: 0.001
2024-11-05 06:01:32,860 - INFO - [diffusion][Epoch 10963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:32,861 - INFO - [diffusion][Epoch 10964] Epoch 10965/12000
2024-11-05 06:01:36,988 - INFO - [diffusion][Epoch 10964] diffusion training Loss: 0.0503914225846529
2024-11-05 06:01:37,005 - INFO - [diffusion][Epoch 10964] diffusion learning rate: 0.001
2024-11-05 06:01:37,007 - INFO - [diffusion][Epoch 10964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:37,008 - INFO - [diffusion][Epoch 10965] Epoch 10966/12000
2024-11-05 06:01:41,390 - INFO - [diffusion][Epoch 10965] diffusion training Loss: 0.04446298070251942
2024-11-05 06:01:41,392 - INFO - [diffusion][Epoch 10965] diffusion learning rate: 0.001
2024-11-05 06:01:41,394 - INFO - [diffusion][Epoch 10965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:41,395 - INFO - [diffusion][Epoch 10966] Epoch 10967/12000
2024-11-05 06:01:45,487 - INFO - [diffusion][Epoch 10966] diffusion training Loss: 0.049462828785181046
2024-11-05 06:01:45,489 - INFO - [diffusion][Epoch 10966] diffusion learning rate: 0.001
2024-11-05 06:01:45,491 - INFO - [diffusion][Epoch 10966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:45,492 - INFO - [diffusion][Epoch 10967] Epoch 10968/12000
2024-11-05 06:01:49,578 - INFO - [diffusion][Epoch 10967] diffusion training Loss: 0.046789318323135376
2024-11-05 06:01:49,580 - INFO - [diffusion][Epoch 10967] diffusion learning rate: 0.001
2024-11-05 06:01:49,581 - INFO - [diffusion][Epoch 10967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:49,583 - INFO - [diffusion][Epoch 10968] Epoch 10969/12000
2024-11-05 06:01:53,608 - INFO - [diffusion][Epoch 10968] diffusion training Loss: 0.043567806482315063
2024-11-05 06:01:53,610 - INFO - [diffusion][Epoch 10968] diffusion learning rate: 0.001
2024-11-05 06:01:53,612 - INFO - [diffusion][Epoch 10968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:53,613 - INFO - [diffusion][Epoch 10969] Epoch 10970/12000
2024-11-05 06:01:57,696 - INFO - [diffusion][Epoch 10969] diffusion training Loss: 0.04305051639676094
2024-11-05 06:01:57,698 - INFO - [diffusion][Epoch 10969] diffusion learning rate: 0.001
2024-11-05 06:01:57,700 - INFO - [diffusion][Epoch 10969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:01:57,701 - INFO - [diffusion][Epoch 10970] Epoch 10971/12000
2024-11-05 06:02:01,759 - INFO - [diffusion][Epoch 10970] diffusion training Loss: 0.0498646330088377
2024-11-05 06:02:01,761 - INFO - [diffusion][Epoch 10970] diffusion learning rate: 0.001
2024-11-05 06:02:01,763 - INFO - [diffusion][Epoch 10970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:01,764 - INFO - [diffusion][Epoch 10971] Epoch 10972/12000
2024-11-05 06:02:05,898 - INFO - [diffusion][Epoch 10971] diffusion training Loss: 0.05087824072688818
2024-11-05 06:02:05,899 - INFO - [diffusion][Epoch 10971] diffusion learning rate: 0.001
2024-11-05 06:02:05,901 - INFO - [diffusion][Epoch 10971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:05,903 - INFO - [diffusion][Epoch 10972] Epoch 10973/12000
2024-11-05 06:02:10,003 - INFO - [diffusion][Epoch 10972] diffusion training Loss: 0.04507491271942854
2024-11-05 06:02:10,005 - INFO - [diffusion][Epoch 10972] diffusion learning rate: 0.001
2024-11-05 06:02:10,007 - INFO - [diffusion][Epoch 10972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:10,008 - INFO - [diffusion][Epoch 10973] Epoch 10974/12000
2024-11-05 06:02:14,126 - INFO - [diffusion][Epoch 10973] diffusion training Loss: 0.0491732656955719
2024-11-05 06:02:14,128 - INFO - [diffusion][Epoch 10973] diffusion learning rate: 0.001
2024-11-05 06:02:14,130 - INFO - [diffusion][Epoch 10973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:14,131 - INFO - [diffusion][Epoch 10974] Epoch 10975/12000
2024-11-05 06:02:18,245 - INFO - [diffusion][Epoch 10974] diffusion training Loss: 0.04801507480442524
2024-11-05 06:02:18,247 - INFO - [diffusion][Epoch 10974] diffusion learning rate: 0.001
2024-11-05 06:02:18,249 - INFO - [diffusion][Epoch 10974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:18,250 - INFO - [diffusion][Epoch 10975] Epoch 10976/12000
2024-11-05 06:02:22,337 - INFO - [diffusion][Epoch 10975] diffusion training Loss: 0.04812897741794586
2024-11-05 06:02:22,339 - INFO - [diffusion][Epoch 10975] diffusion learning rate: 0.001
2024-11-05 06:02:22,341 - INFO - [diffusion][Epoch 10975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:22,342 - INFO - [diffusion][Epoch 10976] Epoch 10977/12000
2024-11-05 06:02:26,404 - INFO - [diffusion][Epoch 10976] diffusion training Loss: 0.050552680157124996
2024-11-05 06:02:26,406 - INFO - [diffusion][Epoch 10976] diffusion learning rate: 0.001
2024-11-05 06:02:26,408 - INFO - [diffusion][Epoch 10976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:26,409 - INFO - [diffusion][Epoch 10977] Epoch 10978/12000
2024-11-05 06:02:30,519 - INFO - [diffusion][Epoch 10977] diffusion training Loss: 0.04843968618661165
2024-11-05 06:02:30,521 - INFO - [diffusion][Epoch 10977] diffusion learning rate: 0.001
2024-11-05 06:02:30,523 - INFO - [diffusion][Epoch 10977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:30,524 - INFO - [diffusion][Epoch 10978] Epoch 10979/12000
2024-11-05 06:02:34,615 - INFO - [diffusion][Epoch 10978] diffusion training Loss: 0.04962532501667738
2024-11-05 06:02:34,619 - INFO - [diffusion][Epoch 10978] diffusion learning rate: 0.001
2024-11-05 06:02:34,621 - INFO - [diffusion][Epoch 10978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:34,622 - INFO - [diffusion][Epoch 10979] Epoch 10980/12000
2024-11-05 06:02:38,771 - INFO - [diffusion][Epoch 10979] diffusion training Loss: 0.04604748450219631
2024-11-05 06:02:38,773 - INFO - [diffusion][Epoch 10979] diffusion learning rate: 0.001
2024-11-05 06:02:38,774 - INFO - [diffusion][Epoch 10979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:38,776 - INFO - [diffusion][Epoch 10980] Epoch 10981/12000
2024-11-05 06:02:42,824 - INFO - [diffusion][Epoch 10980] diffusion training Loss: 0.045816557481884956
2024-11-05 06:02:42,827 - INFO - [diffusion][Epoch 10980] diffusion learning rate: 0.001
2024-11-05 06:02:42,828 - INFO - [diffusion][Epoch 10980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:42,830 - INFO - [diffusion][Epoch 10981] Epoch 10982/12000
2024-11-05 06:02:46,939 - INFO - [diffusion][Epoch 10981] diffusion training Loss: 0.04873499181121588
2024-11-05 06:02:46,941 - INFO - [diffusion][Epoch 10981] diffusion learning rate: 0.001
2024-11-05 06:02:46,943 - INFO - [diffusion][Epoch 10981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:46,944 - INFO - [diffusion][Epoch 10982] Epoch 10983/12000
2024-11-05 06:02:51,037 - INFO - [diffusion][Epoch 10982] diffusion training Loss: 0.04131474159657955
2024-11-05 06:02:51,039 - INFO - [diffusion][Epoch 10982] diffusion learning rate: 0.001
2024-11-05 06:02:51,041 - INFO - [diffusion][Epoch 10982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:51,042 - INFO - [diffusion][Epoch 10983] Epoch 10984/12000
2024-11-05 06:02:55,118 - INFO - [diffusion][Epoch 10983] diffusion training Loss: 0.04815038200467825
2024-11-05 06:02:55,120 - INFO - [diffusion][Epoch 10983] diffusion learning rate: 0.001
2024-11-05 06:02:55,122 - INFO - [diffusion][Epoch 10983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:55,123 - INFO - [diffusion][Epoch 10984] Epoch 10985/12000
2024-11-05 06:02:59,218 - INFO - [diffusion][Epoch 10984] diffusion training Loss: 0.04723948985338211
2024-11-05 06:02:59,220 - INFO - [diffusion][Epoch 10984] diffusion learning rate: 0.001
2024-11-05 06:02:59,222 - INFO - [diffusion][Epoch 10984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:02:59,223 - INFO - [diffusion][Epoch 10985] Epoch 10986/12000
2024-11-05 06:03:03,335 - INFO - [diffusion][Epoch 10985] diffusion training Loss: 0.04825384449213743
2024-11-05 06:03:03,337 - INFO - [diffusion][Epoch 10985] diffusion learning rate: 0.001
2024-11-05 06:03:03,339 - INFO - [diffusion][Epoch 10985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:03,340 - INFO - [diffusion][Epoch 10986] Epoch 10987/12000
2024-11-05 06:03:07,431 - INFO - [diffusion][Epoch 10986] diffusion training Loss: 0.04442603327333927
2024-11-05 06:03:07,433 - INFO - [diffusion][Epoch 10986] diffusion learning rate: 0.001
2024-11-05 06:03:07,435 - INFO - [diffusion][Epoch 10986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:07,436 - INFO - [diffusion][Epoch 10987] Epoch 10988/12000
2024-11-05 06:03:11,569 - INFO - [diffusion][Epoch 10987] diffusion training Loss: 0.04947518929839134
2024-11-05 06:03:11,571 - INFO - [diffusion][Epoch 10987] diffusion learning rate: 0.001
2024-11-05 06:03:11,573 - INFO - [diffusion][Epoch 10987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:11,574 - INFO - [diffusion][Epoch 10988] Epoch 10989/12000
2024-11-05 06:03:15,673 - INFO - [diffusion][Epoch 10988] diffusion training Loss: 0.04762824159115553
2024-11-05 06:03:15,675 - INFO - [diffusion][Epoch 10988] diffusion learning rate: 0.001
2024-11-05 06:03:15,677 - INFO - [diffusion][Epoch 10988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:15,678 - INFO - [diffusion][Epoch 10989] Epoch 10990/12000
2024-11-05 06:03:19,763 - INFO - [diffusion][Epoch 10989] diffusion training Loss: 0.04535574745386839
2024-11-05 06:03:19,765 - INFO - [diffusion][Epoch 10989] diffusion learning rate: 0.001
2024-11-05 06:03:19,767 - INFO - [diffusion][Epoch 10989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:19,768 - INFO - [diffusion][Epoch 10990] Epoch 10991/12000
2024-11-05 06:03:23,838 - INFO - [diffusion][Epoch 10990] diffusion training Loss: 0.0505008352920413
2024-11-05 06:03:23,840 - INFO - [diffusion][Epoch 10990] diffusion learning rate: 0.001
2024-11-05 06:03:23,842 - INFO - [diffusion][Epoch 10990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:23,844 - INFO - [diffusion][Epoch 10991] Epoch 10992/12000
2024-11-05 06:03:28,007 - INFO - [diffusion][Epoch 10991] diffusion training Loss: 0.04691370390355587
2024-11-05 06:03:28,009 - INFO - [diffusion][Epoch 10991] diffusion learning rate: 0.001
2024-11-05 06:03:28,011 - INFO - [diffusion][Epoch 10991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:28,012 - INFO - [diffusion][Epoch 10992] Epoch 10993/12000
2024-11-05 06:03:32,110 - INFO - [diffusion][Epoch 10992] diffusion training Loss: 0.04949908424168825
2024-11-05 06:03:32,112 - INFO - [diffusion][Epoch 10992] diffusion learning rate: 0.001
2024-11-05 06:03:32,114 - INFO - [diffusion][Epoch 10992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:32,115 - INFO - [diffusion][Epoch 10993] Epoch 10994/12000
2024-11-05 06:03:36,218 - INFO - [diffusion][Epoch 10993] diffusion training Loss: 0.041832576505839825
2024-11-05 06:03:36,221 - INFO - [diffusion][Epoch 10993] diffusion learning rate: 0.001
2024-11-05 06:03:36,223 - INFO - [diffusion][Epoch 10993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:36,224 - INFO - [diffusion][Epoch 10994] Epoch 10995/12000
2024-11-05 06:03:40,137 - INFO - [diffusion][Epoch 10994] diffusion training Loss: 0.04737907648086548
2024-11-05 06:03:40,139 - INFO - [diffusion][Epoch 10994] diffusion learning rate: 0.001
2024-11-05 06:03:40,140 - INFO - [diffusion][Epoch 10994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:40,142 - INFO - [diffusion][Epoch 10995] Epoch 10996/12000
2024-11-05 06:03:44,223 - INFO - [diffusion][Epoch 10995] diffusion training Loss: 0.045565889216959476
2024-11-05 06:03:44,225 - INFO - [diffusion][Epoch 10995] diffusion learning rate: 0.001
2024-11-05 06:03:44,227 - INFO - [diffusion][Epoch 10995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:44,228 - INFO - [diffusion][Epoch 10996] Epoch 10997/12000
2024-11-05 06:03:48,320 - INFO - [diffusion][Epoch 10996] diffusion training Loss: 0.049767352640628815
2024-11-05 06:03:48,322 - INFO - [diffusion][Epoch 10996] diffusion learning rate: 0.001
2024-11-05 06:03:48,324 - INFO - [diffusion][Epoch 10996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:48,325 - INFO - [diffusion][Epoch 10997] Epoch 10998/12000
2024-11-05 06:03:52,424 - INFO - [diffusion][Epoch 10997] diffusion training Loss: 0.0452351626008749
2024-11-05 06:03:52,426 - INFO - [diffusion][Epoch 10997] diffusion learning rate: 0.001
2024-11-05 06:03:52,427 - INFO - [diffusion][Epoch 10997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:52,429 - INFO - [diffusion][Epoch 10998] Epoch 10999/12000
2024-11-05 06:03:56,560 - INFO - [diffusion][Epoch 10998] diffusion training Loss: 0.05091980192810297
2024-11-05 06:03:56,562 - INFO - [diffusion][Epoch 10998] diffusion learning rate: 0.001
2024-11-05 06:03:56,564 - INFO - [diffusion][Epoch 10998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:03:56,565 - INFO - [diffusion][Epoch 10999] Epoch 11000/12000
2024-11-05 06:04:00,707 - INFO - [diffusion][Epoch 10999] diffusion training Loss: 0.047973932698369026
2024-11-05 06:04:00,709 - INFO - [diffusion][Epoch 10999] diffusion learning rate: 0.001
2024-11-05 06:04:00,900 - INFO - [diffusion][Epoch 10999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:00,901 - INFO - [diffusion][Epoch 11000] Epoch 11001/12000
2024-11-05 06:04:04,972 - INFO - [diffusion][Epoch 11000] diffusion training Loss: 0.042180873453617096
2024-11-05 06:04:04,974 - INFO - [diffusion][Epoch 11000] diffusion learning rate: 0.001
2024-11-05 06:04:04,976 - INFO - [diffusion][Epoch 11000] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:04,977 - INFO - [diffusion][Epoch 11001] Epoch 11002/12000
2024-11-05 06:04:09,099 - INFO - [diffusion][Epoch 11001] diffusion training Loss: 0.04821301158517599
2024-11-05 06:04:09,101 - INFO - [diffusion][Epoch 11001] diffusion learning rate: 0.001
2024-11-05 06:04:09,103 - INFO - [diffusion][Epoch 11001] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:09,104 - INFO - [diffusion][Epoch 11002] Epoch 11003/12000
2024-11-05 06:04:13,215 - INFO - [diffusion][Epoch 11002] diffusion training Loss: 0.04616905469447374
2024-11-05 06:04:13,217 - INFO - [diffusion][Epoch 11002] diffusion learning rate: 0.001
2024-11-05 06:04:13,219 - INFO - [diffusion][Epoch 11002] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:13,220 - INFO - [diffusion][Epoch 11003] Epoch 11004/12000
2024-11-05 06:04:17,343 - INFO - [diffusion][Epoch 11003] diffusion training Loss: 0.055921146646142006
2024-11-05 06:04:17,345 - INFO - [diffusion][Epoch 11003] diffusion learning rate: 0.001
2024-11-05 06:04:17,347 - INFO - [diffusion][Epoch 11003] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:17,348 - INFO - [diffusion][Epoch 11004] Epoch 11005/12000
2024-11-05 06:04:21,457 - INFO - [diffusion][Epoch 11004] diffusion training Loss: 0.04759620130062103
2024-11-05 06:04:21,459 - INFO - [diffusion][Epoch 11004] diffusion learning rate: 0.001
2024-11-05 06:04:21,461 - INFO - [diffusion][Epoch 11004] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:21,462 - INFO - [diffusion][Epoch 11005] Epoch 11006/12000
2024-11-05 06:04:25,388 - INFO - [diffusion][Epoch 11005] diffusion training Loss: 0.0457023186609149
2024-11-05 06:04:25,390 - INFO - [diffusion][Epoch 11005] diffusion learning rate: 0.001
2024-11-05 06:04:25,392 - INFO - [diffusion][Epoch 11005] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:25,393 - INFO - [diffusion][Epoch 11006] Epoch 11007/12000
2024-11-05 06:04:29,492 - INFO - [diffusion][Epoch 11006] diffusion training Loss: 0.05016419477760792
2024-11-05 06:04:29,494 - INFO - [diffusion][Epoch 11006] diffusion learning rate: 0.001
2024-11-05 06:04:29,496 - INFO - [diffusion][Epoch 11006] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:29,497 - INFO - [diffusion][Epoch 11007] Epoch 11008/12000
2024-11-05 06:04:33,782 - INFO - [diffusion][Epoch 11007] diffusion training Loss: 0.04787679202854633
2024-11-05 06:04:33,784 - INFO - [diffusion][Epoch 11007] diffusion learning rate: 0.001
2024-11-05 06:04:33,811 - INFO - [diffusion][Epoch 11007] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:33,812 - INFO - [diffusion][Epoch 11008] Epoch 11009/12000
2024-11-05 06:04:37,915 - INFO - [diffusion][Epoch 11008] diffusion training Loss: 0.04596870671957731
2024-11-05 06:04:37,919 - INFO - [diffusion][Epoch 11008] diffusion learning rate: 0.001
2024-11-05 06:04:37,920 - INFO - [diffusion][Epoch 11008] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:37,922 - INFO - [diffusion][Epoch 11009] Epoch 11010/12000
2024-11-05 06:04:42,028 - INFO - [diffusion][Epoch 11009] diffusion training Loss: 0.04355616122484207
2024-11-05 06:04:42,030 - INFO - [diffusion][Epoch 11009] diffusion learning rate: 0.001
2024-11-05 06:04:42,032 - INFO - [diffusion][Epoch 11009] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:42,033 - INFO - [diffusion][Epoch 11010] Epoch 11011/12000
2024-11-05 06:04:46,170 - INFO - [diffusion][Epoch 11010] diffusion training Loss: 0.04422199260443449
2024-11-05 06:04:46,172 - INFO - [diffusion][Epoch 11010] diffusion learning rate: 0.001
2024-11-05 06:04:46,173 - INFO - [diffusion][Epoch 11010] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:46,175 - INFO - [diffusion][Epoch 11011] Epoch 11012/12000
2024-11-05 06:04:50,286 - INFO - [diffusion][Epoch 11011] diffusion training Loss: 0.04877232387661934
2024-11-05 06:04:50,288 - INFO - [diffusion][Epoch 11011] diffusion learning rate: 0.001
2024-11-05 06:04:50,290 - INFO - [diffusion][Epoch 11011] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:50,291 - INFO - [diffusion][Epoch 11012] Epoch 11013/12000
2024-11-05 06:04:54,413 - INFO - [diffusion][Epoch 11012] diffusion training Loss: 0.04665784724056721
2024-11-05 06:04:54,415 - INFO - [diffusion][Epoch 11012] diffusion learning rate: 0.001
2024-11-05 06:04:54,417 - INFO - [diffusion][Epoch 11012] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:54,418 - INFO - [diffusion][Epoch 11013] Epoch 11014/12000
2024-11-05 06:04:58,526 - INFO - [diffusion][Epoch 11013] diffusion training Loss: 0.046201984398067
2024-11-05 06:04:58,528 - INFO - [diffusion][Epoch 11013] diffusion learning rate: 0.001
2024-11-05 06:04:58,530 - INFO - [diffusion][Epoch 11013] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:04:58,531 - INFO - [diffusion][Epoch 11014] Epoch 11015/12000
2024-11-05 06:05:02,619 - INFO - [diffusion][Epoch 11014] diffusion training Loss: 0.05070796329528093
2024-11-05 06:05:02,621 - INFO - [diffusion][Epoch 11014] diffusion learning rate: 0.001
2024-11-05 06:05:02,622 - INFO - [diffusion][Epoch 11014] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:02,624 - INFO - [diffusion][Epoch 11015] Epoch 11016/12000
2024-11-05 06:05:06,745 - INFO - [diffusion][Epoch 11015] diffusion training Loss: 0.04454257991164923
2024-11-05 06:05:06,747 - INFO - [diffusion][Epoch 11015] diffusion learning rate: 0.001
2024-11-05 06:05:06,749 - INFO - [diffusion][Epoch 11015] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:06,750 - INFO - [diffusion][Epoch 11016] Epoch 11017/12000
2024-11-05 06:05:10,859 - INFO - [diffusion][Epoch 11016] diffusion training Loss: 0.04793549608439207
2024-11-05 06:05:10,861 - INFO - [diffusion][Epoch 11016] diffusion learning rate: 0.001
2024-11-05 06:05:10,862 - INFO - [diffusion][Epoch 11016] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:10,864 - INFO - [diffusion][Epoch 11017] Epoch 11018/12000
2024-11-05 06:05:14,952 - INFO - [diffusion][Epoch 11017] diffusion training Loss: 0.0480361245572567
2024-11-05 06:05:14,954 - INFO - [diffusion][Epoch 11017] diffusion learning rate: 0.001
2024-11-05 06:05:14,956 - INFO - [diffusion][Epoch 11017] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:14,957 - INFO - [diffusion][Epoch 11018] Epoch 11019/12000
2024-11-05 06:05:19,018 - INFO - [diffusion][Epoch 11018] diffusion training Loss: 0.049336119554936886
2024-11-05 06:05:19,019 - INFO - [diffusion][Epoch 11018] diffusion learning rate: 0.001
2024-11-05 06:05:19,021 - INFO - [diffusion][Epoch 11018] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:19,022 - INFO - [diffusion][Epoch 11019] Epoch 11020/12000
2024-11-05 06:05:23,118 - INFO - [diffusion][Epoch 11019] diffusion training Loss: 0.048153054900467396
2024-11-05 06:05:23,120 - INFO - [diffusion][Epoch 11019] diffusion learning rate: 0.001
2024-11-05 06:05:23,121 - INFO - [diffusion][Epoch 11019] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:23,123 - INFO - [diffusion][Epoch 11020] Epoch 11021/12000
2024-11-05 06:05:27,203 - INFO - [diffusion][Epoch 11020] diffusion training Loss: 0.04857037216424942
2024-11-05 06:05:27,205 - INFO - [diffusion][Epoch 11020] diffusion learning rate: 0.001
2024-11-05 06:05:27,206 - INFO - [diffusion][Epoch 11020] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:27,208 - INFO - [diffusion][Epoch 11021] Epoch 11022/12000
2024-11-05 06:05:31,234 - INFO - [diffusion][Epoch 11021] diffusion training Loss: 0.046243361197412014
2024-11-05 06:05:31,236 - INFO - [diffusion][Epoch 11021] diffusion learning rate: 0.001
2024-11-05 06:05:31,237 - INFO - [diffusion][Epoch 11021] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:31,239 - INFO - [diffusion][Epoch 11022] Epoch 11023/12000
2024-11-05 06:05:35,320 - INFO - [diffusion][Epoch 11022] diffusion training Loss: 0.04655367508530617
2024-11-05 06:05:35,322 - INFO - [diffusion][Epoch 11022] diffusion learning rate: 0.001
2024-11-05 06:05:35,324 - INFO - [diffusion][Epoch 11022] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:35,325 - INFO - [diffusion][Epoch 11023] Epoch 11024/12000
2024-11-05 06:05:39,419 - INFO - [diffusion][Epoch 11023] diffusion training Loss: 0.05187645833939314
2024-11-05 06:05:39,423 - INFO - [diffusion][Epoch 11023] diffusion learning rate: 0.001
2024-11-05 06:05:39,425 - INFO - [diffusion][Epoch 11023] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:39,426 - INFO - [diffusion][Epoch 11024] Epoch 11025/12000
2024-11-05 06:05:43,533 - INFO - [diffusion][Epoch 11024] diffusion training Loss: 0.04615671746432781
2024-11-05 06:05:43,535 - INFO - [diffusion][Epoch 11024] diffusion learning rate: 0.001
2024-11-05 06:05:43,536 - INFO - [diffusion][Epoch 11024] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:43,538 - INFO - [diffusion][Epoch 11025] Epoch 11026/12000
2024-11-05 06:05:47,621 - INFO - [diffusion][Epoch 11025] diffusion training Loss: 0.04988781549036503
2024-11-05 06:05:47,623 - INFO - [diffusion][Epoch 11025] diffusion learning rate: 0.001
2024-11-05 06:05:47,625 - INFO - [diffusion][Epoch 11025] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:47,626 - INFO - [diffusion][Epoch 11026] Epoch 11027/12000
2024-11-05 06:05:51,725 - INFO - [diffusion][Epoch 11026] diffusion training Loss: 0.04334045201539993
2024-11-05 06:05:51,727 - INFO - [diffusion][Epoch 11026] diffusion learning rate: 0.001
2024-11-05 06:05:51,729 - INFO - [diffusion][Epoch 11026] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:51,730 - INFO - [diffusion][Epoch 11027] Epoch 11028/12000
2024-11-05 06:05:56,575 - INFO - [diffusion][Epoch 11027] diffusion training Loss: 0.04747983906418085
2024-11-05 06:05:56,576 - INFO - [diffusion][Epoch 11027] diffusion learning rate: 0.001
2024-11-05 06:05:56,578 - INFO - [diffusion][Epoch 11027] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:05:56,579 - INFO - [diffusion][Epoch 11028] Epoch 11029/12000
2024-11-05 06:06:00,698 - INFO - [diffusion][Epoch 11028] diffusion training Loss: 0.04553792718797922
2024-11-05 06:06:00,700 - INFO - [diffusion][Epoch 11028] diffusion learning rate: 0.001
2024-11-05 06:06:00,727 - INFO - [diffusion][Epoch 11028] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:00,728 - INFO - [diffusion][Epoch 11029] Epoch 11030/12000
2024-11-05 06:06:04,825 - INFO - [diffusion][Epoch 11029] diffusion training Loss: 0.04371428769081831
2024-11-05 06:06:04,827 - INFO - [diffusion][Epoch 11029] diffusion learning rate: 0.001
2024-11-05 06:06:04,828 - INFO - [diffusion][Epoch 11029] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:04,830 - INFO - [diffusion][Epoch 11030] Epoch 11031/12000
2024-11-05 06:06:08,930 - INFO - [diffusion][Epoch 11030] diffusion training Loss: 0.04993217345327139
2024-11-05 06:06:08,932 - INFO - [diffusion][Epoch 11030] diffusion learning rate: 0.001
2024-11-05 06:06:08,934 - INFO - [diffusion][Epoch 11030] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:08,935 - INFO - [diffusion][Epoch 11031] Epoch 11032/12000
2024-11-05 06:06:12,869 - INFO - [diffusion][Epoch 11031] diffusion training Loss: 0.049914151430130005
2024-11-05 06:06:12,871 - INFO - [diffusion][Epoch 11031] diffusion learning rate: 0.001
2024-11-05 06:06:12,890 - INFO - [diffusion][Epoch 11031] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:12,891 - INFO - [diffusion][Epoch 11032] Epoch 11033/12000
2024-11-05 06:06:16,993 - INFO - [diffusion][Epoch 11032] diffusion training Loss: 0.042853337712585926
2024-11-05 06:06:16,995 - INFO - [diffusion][Epoch 11032] diffusion learning rate: 0.001
2024-11-05 06:06:16,996 - INFO - [diffusion][Epoch 11032] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:16,998 - INFO - [diffusion][Epoch 11033] Epoch 11034/12000
2024-11-05 06:06:21,123 - INFO - [diffusion][Epoch 11033] diffusion training Loss: 0.04896212927997112
2024-11-05 06:06:21,125 - INFO - [diffusion][Epoch 11033] diffusion learning rate: 0.001
2024-11-05 06:06:21,127 - INFO - [diffusion][Epoch 11033] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:21,128 - INFO - [diffusion][Epoch 11034] Epoch 11035/12000
2024-11-05 06:06:25,221 - INFO - [diffusion][Epoch 11034] diffusion training Loss: 0.050137720070779324
2024-11-05 06:06:25,223 - INFO - [diffusion][Epoch 11034] diffusion learning rate: 0.001
2024-11-05 06:06:25,250 - INFO - [diffusion][Epoch 11034] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:25,251 - INFO - [diffusion][Epoch 11035] Epoch 11036/12000
2024-11-05 06:06:29,345 - INFO - [diffusion][Epoch 11035] diffusion training Loss: 0.046034300699830055
2024-11-05 06:06:29,347 - INFO - [diffusion][Epoch 11035] diffusion learning rate: 0.001
2024-11-05 06:06:29,348 - INFO - [diffusion][Epoch 11035] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:29,349 - INFO - [diffusion][Epoch 11036] Epoch 11037/12000
2024-11-05 06:06:33,434 - INFO - [diffusion][Epoch 11036] diffusion training Loss: 0.04856870882213116
2024-11-05 06:06:33,436 - INFO - [diffusion][Epoch 11036] diffusion learning rate: 0.001
2024-11-05 06:06:33,437 - INFO - [diffusion][Epoch 11036] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:33,438 - INFO - [diffusion][Epoch 11037] Epoch 11038/12000
2024-11-05 06:06:37,539 - INFO - [diffusion][Epoch 11037] diffusion training Loss: 0.048028948716819286
2024-11-05 06:06:37,541 - INFO - [diffusion][Epoch 11037] diffusion learning rate: 0.001
2024-11-05 06:06:37,580 - INFO - [diffusion][Epoch 11037] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:37,582 - INFO - [diffusion][Epoch 11038] Epoch 11039/12000
2024-11-05 06:06:41,618 - INFO - [diffusion][Epoch 11038] diffusion training Loss: 0.04232699517160654
2024-11-05 06:06:41,622 - INFO - [diffusion][Epoch 11038] diffusion learning rate: 0.001
2024-11-05 06:06:41,623 - INFO - [diffusion][Epoch 11038] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:41,625 - INFO - [diffusion][Epoch 11039] Epoch 11040/12000
2024-11-05 06:06:45,694 - INFO - [diffusion][Epoch 11039] diffusion training Loss: 0.045549594797194004
2024-11-05 06:06:45,696 - INFO - [diffusion][Epoch 11039] diffusion learning rate: 0.001
2024-11-05 06:06:45,698 - INFO - [diffusion][Epoch 11039] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:45,700 - INFO - [diffusion][Epoch 11040] Epoch 11041/12000
2024-11-05 06:06:49,829 - INFO - [diffusion][Epoch 11040] diffusion training Loss: 0.047649085521698
2024-11-05 06:06:49,831 - INFO - [diffusion][Epoch 11040] diffusion learning rate: 0.001
2024-11-05 06:06:49,858 - INFO - [diffusion][Epoch 11040] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:49,859 - INFO - [diffusion][Epoch 11041] Epoch 11042/12000
2024-11-05 06:06:53,981 - INFO - [diffusion][Epoch 11041] diffusion training Loss: 0.04475077893584967
2024-11-05 06:06:53,983 - INFO - [diffusion][Epoch 11041] diffusion learning rate: 0.001
2024-11-05 06:06:53,985 - INFO - [diffusion][Epoch 11041] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:53,986 - INFO - [diffusion][Epoch 11042] Epoch 11043/12000
2024-11-05 06:06:58,123 - INFO - [diffusion][Epoch 11042] diffusion training Loss: 0.04927764739841223
2024-11-05 06:06:58,125 - INFO - [diffusion][Epoch 11042] diffusion learning rate: 0.001
2024-11-05 06:06:58,127 - INFO - [diffusion][Epoch 11042] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:06:58,129 - INFO - [diffusion][Epoch 11043] Epoch 11044/12000
2024-11-05 06:07:02,279 - INFO - [diffusion][Epoch 11043] diffusion training Loss: 0.04902460239827633
2024-11-05 06:07:02,281 - INFO - [diffusion][Epoch 11043] diffusion learning rate: 0.001
2024-11-05 06:07:02,283 - INFO - [diffusion][Epoch 11043] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:02,284 - INFO - [diffusion][Epoch 11044] Epoch 11045/12000
2024-11-05 06:07:06,383 - INFO - [diffusion][Epoch 11044] diffusion training Loss: 0.053629481233656406
2024-11-05 06:07:06,385 - INFO - [diffusion][Epoch 11044] diffusion learning rate: 0.001
2024-11-05 06:07:06,425 - INFO - [diffusion][Epoch 11044] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:06,427 - INFO - [diffusion][Epoch 11045] Epoch 11046/12000
2024-11-05 06:07:10,552 - INFO - [diffusion][Epoch 11045] diffusion training Loss: 0.04555255454033613
2024-11-05 06:07:10,554 - INFO - [diffusion][Epoch 11045] diffusion learning rate: 0.001
2024-11-05 06:07:10,556 - INFO - [diffusion][Epoch 11045] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:10,557 - INFO - [diffusion][Epoch 11046] Epoch 11047/12000
2024-11-05 06:07:14,691 - INFO - [diffusion][Epoch 11046] diffusion training Loss: 0.04468944389373064
2024-11-05 06:07:14,693 - INFO - [diffusion][Epoch 11046] diffusion learning rate: 0.001
2024-11-05 06:07:14,695 - INFO - [diffusion][Epoch 11046] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:14,696 - INFO - [diffusion][Epoch 11047] Epoch 11048/12000
2024-11-05 06:07:18,815 - INFO - [diffusion][Epoch 11047] diffusion training Loss: 0.04363288916647434
2024-11-05 06:07:18,817 - INFO - [diffusion][Epoch 11047] diffusion learning rate: 0.001
2024-11-05 06:07:18,818 - INFO - [diffusion][Epoch 11047] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:18,820 - INFO - [diffusion][Epoch 11048] Epoch 11049/12000
2024-11-05 06:07:23,033 - INFO - [diffusion][Epoch 11048] diffusion training Loss: 0.0514720818027854
2024-11-05 06:07:23,035 - INFO - [diffusion][Epoch 11048] diffusion learning rate: 0.001
2024-11-05 06:07:23,036 - INFO - [diffusion][Epoch 11048] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:23,037 - INFO - [diffusion][Epoch 11049] Epoch 11050/12000
2024-11-05 06:07:27,072 - INFO - [diffusion][Epoch 11049] diffusion training Loss: 0.0537283793091774
2024-11-05 06:07:27,074 - INFO - [diffusion][Epoch 11049] diffusion learning rate: 0.001
2024-11-05 06:07:27,075 - INFO - [diffusion][Epoch 11049] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:27,077 - INFO - [diffusion][Epoch 11050] Epoch 11051/12000
2024-11-05 06:07:31,181 - INFO - [diffusion][Epoch 11050] diffusion training Loss: 0.044539812952280045
2024-11-05 06:07:31,183 - INFO - [diffusion][Epoch 11050] diffusion learning rate: 0.001
2024-11-05 06:07:31,185 - INFO - [diffusion][Epoch 11050] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:31,186 - INFO - [diffusion][Epoch 11051] Epoch 11052/12000
2024-11-05 06:07:35,287 - INFO - [diffusion][Epoch 11051] diffusion training Loss: 0.04848971497267485
2024-11-05 06:07:35,289 - INFO - [diffusion][Epoch 11051] diffusion learning rate: 0.001
2024-11-05 06:07:35,291 - INFO - [diffusion][Epoch 11051] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:35,292 - INFO - [diffusion][Epoch 11052] Epoch 11053/12000
2024-11-05 06:07:39,430 - INFO - [diffusion][Epoch 11052] diffusion training Loss: 0.05445679184049368
2024-11-05 06:07:39,431 - INFO - [diffusion][Epoch 11052] diffusion learning rate: 0.001
2024-11-05 06:07:39,433 - INFO - [diffusion][Epoch 11052] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:39,434 - INFO - [diffusion][Epoch 11053] Epoch 11054/12000
2024-11-05 06:07:43,513 - INFO - [diffusion][Epoch 11053] diffusion training Loss: 0.05102983210235834
2024-11-05 06:07:43,516 - INFO - [diffusion][Epoch 11053] diffusion learning rate: 0.001
2024-11-05 06:07:43,518 - INFO - [diffusion][Epoch 11053] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:43,519 - INFO - [diffusion][Epoch 11054] Epoch 11055/12000
2024-11-05 06:07:47,631 - INFO - [diffusion][Epoch 11054] diffusion training Loss: 0.044421699829399586
2024-11-05 06:07:47,634 - INFO - [diffusion][Epoch 11054] diffusion learning rate: 0.001
2024-11-05 06:07:47,635 - INFO - [diffusion][Epoch 11054] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:47,637 - INFO - [diffusion][Epoch 11055] Epoch 11056/12000
2024-11-05 06:07:51,744 - INFO - [diffusion][Epoch 11055] diffusion training Loss: 0.044032045640051365
2024-11-05 06:07:51,746 - INFO - [diffusion][Epoch 11055] diffusion learning rate: 0.001
2024-11-05 06:07:51,748 - INFO - [diffusion][Epoch 11055] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:51,750 - INFO - [diffusion][Epoch 11056] Epoch 11057/12000
2024-11-05 06:07:55,843 - INFO - [diffusion][Epoch 11056] diffusion training Loss: 0.04812518786638975
2024-11-05 06:07:55,845 - INFO - [diffusion][Epoch 11056] diffusion learning rate: 0.001
2024-11-05 06:07:55,846 - INFO - [diffusion][Epoch 11056] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:55,848 - INFO - [diffusion][Epoch 11057] Epoch 11058/12000
2024-11-05 06:07:59,904 - INFO - [diffusion][Epoch 11057] diffusion training Loss: 0.04627285152673721
2024-11-05 06:07:59,906 - INFO - [diffusion][Epoch 11057] diffusion learning rate: 0.001
2024-11-05 06:07:59,933 - INFO - [diffusion][Epoch 11057] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:07:59,934 - INFO - [diffusion][Epoch 11058] Epoch 11059/12000
2024-11-05 06:08:04,064 - INFO - [diffusion][Epoch 11058] diffusion training Loss: 0.04998209048062563
2024-11-05 06:08:04,066 - INFO - [diffusion][Epoch 11058] diffusion learning rate: 0.001
2024-11-05 06:08:04,068 - INFO - [diffusion][Epoch 11058] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:04,069 - INFO - [diffusion][Epoch 11059] Epoch 11060/12000
2024-11-05 06:08:08,201 - INFO - [diffusion][Epoch 11059] diffusion training Loss: 0.04902460239827633
2024-11-05 06:08:08,203 - INFO - [diffusion][Epoch 11059] diffusion learning rate: 0.001
2024-11-05 06:08:08,205 - INFO - [diffusion][Epoch 11059] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:08,206 - INFO - [diffusion][Epoch 11060] Epoch 11061/12000
2024-11-05 06:08:12,295 - INFO - [diffusion][Epoch 11060] diffusion training Loss: 0.050970916636288166
2024-11-05 06:08:12,298 - INFO - [diffusion][Epoch 11060] diffusion learning rate: 0.001
2024-11-05 06:08:12,299 - INFO - [diffusion][Epoch 11060] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:12,301 - INFO - [diffusion][Epoch 11061] Epoch 11062/12000
2024-11-05 06:08:16,400 - INFO - [diffusion][Epoch 11061] diffusion training Loss: 0.04943385533988476
2024-11-05 06:08:16,402 - INFO - [diffusion][Epoch 11061] diffusion learning rate: 0.001
2024-11-05 06:08:16,404 - INFO - [diffusion][Epoch 11061] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:16,405 - INFO - [diffusion][Epoch 11062] Epoch 11063/12000
2024-11-05 06:08:20,506 - INFO - [diffusion][Epoch 11062] diffusion training Loss: 0.04633101262152195
2024-11-05 06:08:20,508 - INFO - [diffusion][Epoch 11062] diffusion learning rate: 0.001
2024-11-05 06:08:20,510 - INFO - [diffusion][Epoch 11062] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:20,511 - INFO - [diffusion][Epoch 11063] Epoch 11064/12000
2024-11-05 06:08:24,606 - INFO - [diffusion][Epoch 11063] diffusion training Loss: 0.04996883310377598
2024-11-05 06:08:24,608 - INFO - [diffusion][Epoch 11063] diffusion learning rate: 0.001
2024-11-05 06:08:24,609 - INFO - [diffusion][Epoch 11063] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:24,611 - INFO - [diffusion][Epoch 11064] Epoch 11065/12000
2024-11-05 06:08:28,682 - INFO - [diffusion][Epoch 11064] diffusion training Loss: 0.04337377846240997
2024-11-05 06:08:28,684 - INFO - [diffusion][Epoch 11064] diffusion learning rate: 0.001
2024-11-05 06:08:28,686 - INFO - [diffusion][Epoch 11064] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:28,687 - INFO - [diffusion][Epoch 11065] Epoch 11066/12000
2024-11-05 06:08:32,801 - INFO - [diffusion][Epoch 11065] diffusion training Loss: 0.0471952548250556
2024-11-05 06:08:32,803 - INFO - [diffusion][Epoch 11065] diffusion learning rate: 0.001
2024-11-05 06:08:32,805 - INFO - [diffusion][Epoch 11065] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:32,806 - INFO - [diffusion][Epoch 11066] Epoch 11067/12000
2024-11-05 06:08:36,931 - INFO - [diffusion][Epoch 11066] diffusion training Loss: 0.04726886469870806
2024-11-05 06:08:36,933 - INFO - [diffusion][Epoch 11066] diffusion learning rate: 0.001
2024-11-05 06:08:36,935 - INFO - [diffusion][Epoch 11066] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:36,936 - INFO - [diffusion][Epoch 11067] Epoch 11068/12000
2024-11-05 06:08:40,881 - INFO - [diffusion][Epoch 11067] diffusion training Loss: 0.04449947737157345
2024-11-05 06:08:40,883 - INFO - [diffusion][Epoch 11067] diffusion learning rate: 0.001
2024-11-05 06:08:40,909 - INFO - [diffusion][Epoch 11067] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:40,911 - INFO - [diffusion][Epoch 11068] Epoch 11069/12000
2024-11-05 06:08:45,056 - INFO - [diffusion][Epoch 11068] diffusion training Loss: 0.04498388059437275
2024-11-05 06:08:45,060 - INFO - [diffusion][Epoch 11068] diffusion learning rate: 0.001
2024-11-05 06:08:45,061 - INFO - [diffusion][Epoch 11068] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:45,063 - INFO - [diffusion][Epoch 11069] Epoch 11070/12000
2024-11-05 06:08:49,488 - INFO - [diffusion][Epoch 11069] diffusion training Loss: 0.047611325047910213
2024-11-05 06:08:49,489 - INFO - [diffusion][Epoch 11069] diffusion learning rate: 0.001
2024-11-05 06:08:49,491 - INFO - [diffusion][Epoch 11069] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:49,492 - INFO - [diffusion][Epoch 11070] Epoch 11071/12000
2024-11-05 06:08:53,580 - INFO - [diffusion][Epoch 11070] diffusion training Loss: 0.04898377228528261
2024-11-05 06:08:53,582 - INFO - [diffusion][Epoch 11070] diffusion learning rate: 0.001
2024-11-05 06:08:53,584 - INFO - [diffusion][Epoch 11070] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:53,585 - INFO - [diffusion][Epoch 11071] Epoch 11072/12000
2024-11-05 06:08:57,501 - INFO - [diffusion][Epoch 11071] diffusion training Loss: 0.04633825644850731
2024-11-05 06:08:57,503 - INFO - [diffusion][Epoch 11071] diffusion learning rate: 0.001
2024-11-05 06:08:57,505 - INFO - [diffusion][Epoch 11071] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:08:57,506 - INFO - [diffusion][Epoch 11072] Epoch 11073/12000
2024-11-05 06:09:01,333 - INFO - [diffusion][Epoch 11072] diffusion training Loss: 0.04630635492503643
2024-11-05 06:09:01,335 - INFO - [diffusion][Epoch 11072] diffusion learning rate: 0.001
2024-11-05 06:09:01,336 - INFO - [diffusion][Epoch 11072] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:01,338 - INFO - [diffusion][Epoch 11073] Epoch 11074/12000
2024-11-05 06:09:05,423 - INFO - [diffusion][Epoch 11073] diffusion training Loss: 0.05101518239825964
2024-11-05 06:09:05,425 - INFO - [diffusion][Epoch 11073] diffusion learning rate: 0.001
2024-11-05 06:09:05,427 - INFO - [diffusion][Epoch 11073] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:05,428 - INFO - [diffusion][Epoch 11074] Epoch 11075/12000
2024-11-05 06:09:09,537 - INFO - [diffusion][Epoch 11074] diffusion training Loss: 0.051503803580999374
2024-11-05 06:09:09,539 - INFO - [diffusion][Epoch 11074] diffusion learning rate: 0.001
2024-11-05 06:09:09,541 - INFO - [diffusion][Epoch 11074] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:09,542 - INFO - [diffusion][Epoch 11075] Epoch 11076/12000
2024-11-05 06:09:13,622 - INFO - [diffusion][Epoch 11075] diffusion training Loss: 0.046214742586016655
2024-11-05 06:09:13,624 - INFO - [diffusion][Epoch 11075] diffusion learning rate: 0.001
2024-11-05 06:09:13,626 - INFO - [diffusion][Epoch 11075] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:13,627 - INFO - [diffusion][Epoch 11076] Epoch 11077/12000
2024-11-05 06:09:17,723 - INFO - [diffusion][Epoch 11076] diffusion training Loss: 0.050069184973835945
2024-11-05 06:09:17,725 - INFO - [diffusion][Epoch 11076] diffusion learning rate: 0.001
2024-11-05 06:09:17,727 - INFO - [diffusion][Epoch 11076] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:17,728 - INFO - [diffusion][Epoch 11077] Epoch 11078/12000
2024-11-05 06:09:21,798 - INFO - [diffusion][Epoch 11077] diffusion training Loss: 0.04571603890508413
2024-11-05 06:09:21,800 - INFO - [diffusion][Epoch 11077] diffusion learning rate: 0.001
2024-11-05 06:09:21,802 - INFO - [diffusion][Epoch 11077] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:21,803 - INFO - [diffusion][Epoch 11078] Epoch 11079/12000
2024-11-05 06:09:25,863 - INFO - [diffusion][Epoch 11078] diffusion training Loss: 0.04597441479563713
2024-11-05 06:09:25,865 - INFO - [diffusion][Epoch 11078] diffusion learning rate: 0.001
2024-11-05 06:09:25,867 - INFO - [diffusion][Epoch 11078] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:25,868 - INFO - [diffusion][Epoch 11079] Epoch 11080/12000
2024-11-05 06:09:29,939 - INFO - [diffusion][Epoch 11079] diffusion training Loss: 0.04518259968608618
2024-11-05 06:09:29,941 - INFO - [diffusion][Epoch 11079] diffusion learning rate: 0.001
2024-11-05 06:09:29,968 - INFO - [diffusion][Epoch 11079] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:29,969 - INFO - [diffusion][Epoch 11080] Epoch 11081/12000
2024-11-05 06:09:34,054 - INFO - [diffusion][Epoch 11080] diffusion training Loss: 0.048462117090821266
2024-11-05 06:09:34,056 - INFO - [diffusion][Epoch 11080] diffusion learning rate: 0.001
2024-11-05 06:09:34,057 - INFO - [diffusion][Epoch 11080] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:34,059 - INFO - [diffusion][Epoch 11081] Epoch 11082/12000
2024-11-05 06:09:38,183 - INFO - [diffusion][Epoch 11081] diffusion training Loss: 0.04601311031728983
2024-11-05 06:09:38,185 - INFO - [diffusion][Epoch 11081] diffusion learning rate: 0.001
2024-11-05 06:09:38,187 - INFO - [diffusion][Epoch 11081] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:38,188 - INFO - [diffusion][Epoch 11082] Epoch 11083/12000
2024-11-05 06:09:42,274 - INFO - [diffusion][Epoch 11082] diffusion training Loss: 0.04979384131729603
2024-11-05 06:09:42,276 - INFO - [diffusion][Epoch 11082] diffusion learning rate: 0.001
2024-11-05 06:09:42,278 - INFO - [diffusion][Epoch 11082] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:42,280 - INFO - [diffusion][Epoch 11083] Epoch 11084/12000
2024-11-05 06:09:46,376 - INFO - [diffusion][Epoch 11083] diffusion training Loss: 0.05667297542095184
2024-11-05 06:09:46,380 - INFO - [diffusion][Epoch 11083] diffusion learning rate: 0.001
2024-11-05 06:09:46,386 - INFO - [diffusion][Epoch 11083] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:46,387 - INFO - [diffusion][Epoch 11084] Epoch 11085/12000
2024-11-05 06:09:50,485 - INFO - [diffusion][Epoch 11084] diffusion training Loss: 0.04560615960508585
2024-11-05 06:09:50,487 - INFO - [diffusion][Epoch 11084] diffusion learning rate: 0.001
2024-11-05 06:09:50,489 - INFO - [diffusion][Epoch 11084] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:50,490 - INFO - [diffusion][Epoch 11085] Epoch 11086/12000
2024-11-05 06:09:54,633 - INFO - [diffusion][Epoch 11085] diffusion training Loss: 0.0500997556373477
2024-11-05 06:09:54,635 - INFO - [diffusion][Epoch 11085] diffusion learning rate: 0.001
2024-11-05 06:09:54,637 - INFO - [diffusion][Epoch 11085] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:54,638 - INFO - [diffusion][Epoch 11086] Epoch 11087/12000
2024-11-05 06:09:58,726 - INFO - [diffusion][Epoch 11086] diffusion training Loss: 0.04734210483729839
2024-11-05 06:09:58,728 - INFO - [diffusion][Epoch 11086] diffusion learning rate: 0.001
2024-11-05 06:09:58,729 - INFO - [diffusion][Epoch 11086] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:09:58,731 - INFO - [diffusion][Epoch 11087] Epoch 11088/12000
2024-11-05 06:10:02,812 - INFO - [diffusion][Epoch 11087] diffusion training Loss: 0.045634567737579346
2024-11-05 06:10:02,814 - INFO - [diffusion][Epoch 11087] diffusion learning rate: 0.001
2024-11-05 06:10:02,815 - INFO - [diffusion][Epoch 11087] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:02,817 - INFO - [diffusion][Epoch 11088] Epoch 11089/12000
2024-11-05 06:10:06,993 - INFO - [diffusion][Epoch 11088] diffusion training Loss: 0.04516104515641928
2024-11-05 06:10:06,995 - INFO - [diffusion][Epoch 11088] diffusion learning rate: 0.001
2024-11-05 06:10:06,997 - INFO - [diffusion][Epoch 11088] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:06,998 - INFO - [diffusion][Epoch 11089] Epoch 11090/12000
2024-11-05 06:10:11,107 - INFO - [diffusion][Epoch 11089] diffusion training Loss: 0.04806745983660221
2024-11-05 06:10:11,109 - INFO - [diffusion][Epoch 11089] diffusion learning rate: 0.001
2024-11-05 06:10:11,110 - INFO - [diffusion][Epoch 11089] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:11,112 - INFO - [diffusion][Epoch 11090] Epoch 11091/12000
2024-11-05 06:10:15,199 - INFO - [diffusion][Epoch 11090] diffusion training Loss: 0.04696195758879185
2024-11-05 06:10:15,200 - INFO - [diffusion][Epoch 11090] diffusion learning rate: 0.001
2024-11-05 06:10:15,202 - INFO - [diffusion][Epoch 11090] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:15,204 - INFO - [diffusion][Epoch 11091] Epoch 11092/12000
2024-11-05 06:10:19,165 - INFO - [diffusion][Epoch 11091] diffusion training Loss: 0.05873490963131189
2024-11-05 06:10:19,167 - INFO - [diffusion][Epoch 11091] diffusion learning rate: 0.001
2024-11-05 06:10:19,169 - INFO - [diffusion][Epoch 11091] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:19,170 - INFO - [diffusion][Epoch 11092] Epoch 11093/12000
2024-11-05 06:10:23,238 - INFO - [diffusion][Epoch 11092] diffusion training Loss: 0.05267401318997145
2024-11-05 06:10:23,240 - INFO - [diffusion][Epoch 11092] diffusion learning rate: 0.001
2024-11-05 06:10:23,241 - INFO - [diffusion][Epoch 11092] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:23,243 - INFO - [diffusion][Epoch 11093] Epoch 11094/12000
2024-11-05 06:10:27,337 - INFO - [diffusion][Epoch 11093] diffusion training Loss: 0.043480122461915016
2024-11-05 06:10:27,339 - INFO - [diffusion][Epoch 11093] diffusion learning rate: 0.001
2024-11-05 06:10:27,341 - INFO - [diffusion][Epoch 11093] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:27,342 - INFO - [diffusion][Epoch 11094] Epoch 11095/12000
2024-11-05 06:10:31,460 - INFO - [diffusion][Epoch 11094] diffusion training Loss: 0.048691303469240665
2024-11-05 06:10:31,462 - INFO - [diffusion][Epoch 11094] diffusion learning rate: 0.001
2024-11-05 06:10:31,464 - INFO - [diffusion][Epoch 11094] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:31,465 - INFO - [diffusion][Epoch 11095] Epoch 11096/12000
2024-11-05 06:10:35,595 - INFO - [diffusion][Epoch 11095] diffusion training Loss: 0.04711022134870291
2024-11-05 06:10:35,597 - INFO - [diffusion][Epoch 11095] diffusion learning rate: 0.001
2024-11-05 06:10:35,599 - INFO - [diffusion][Epoch 11095] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:35,600 - INFO - [diffusion][Epoch 11096] Epoch 11097/12000
2024-11-05 06:10:39,653 - INFO - [diffusion][Epoch 11096] diffusion training Loss: 0.05180083680897951
2024-11-05 06:10:39,655 - INFO - [diffusion][Epoch 11096] diffusion learning rate: 0.001
2024-11-05 06:10:39,657 - INFO - [diffusion][Epoch 11096] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:39,658 - INFO - [diffusion][Epoch 11097] Epoch 11098/12000
2024-11-05 06:10:43,730 - INFO - [diffusion][Epoch 11097] diffusion training Loss: 0.05156882852315903
2024-11-05 06:10:43,732 - INFO - [diffusion][Epoch 11097] diffusion learning rate: 0.001
2024-11-05 06:10:43,733 - INFO - [diffusion][Epoch 11097] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:43,735 - INFO - [diffusion][Epoch 11098] Epoch 11099/12000
2024-11-05 06:10:47,621 - INFO - [diffusion][Epoch 11098] diffusion training Loss: 0.04641106352210045
2024-11-05 06:10:47,625 - INFO - [diffusion][Epoch 11098] diffusion learning rate: 0.001
2024-11-05 06:10:47,627 - INFO - [diffusion][Epoch 11098] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:47,628 - INFO - [diffusion][Epoch 11099] Epoch 11100/12000
2024-11-05 06:10:51,549 - INFO - [diffusion][Epoch 11099] diffusion training Loss: 0.04752259887754917
2024-11-05 06:10:51,551 - INFO - [diffusion][Epoch 11099] diffusion learning rate: 0.001
2024-11-05 06:10:51,579 - INFO - [diffusion][Epoch 11099] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:51,581 - INFO - [diffusion][Epoch 11100] Epoch 11101/12000
2024-11-05 06:10:55,716 - INFO - [diffusion][Epoch 11100] diffusion training Loss: 0.04893406480550766
2024-11-05 06:10:55,718 - INFO - [diffusion][Epoch 11100] diffusion learning rate: 0.001
2024-11-05 06:10:55,719 - INFO - [diffusion][Epoch 11100] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:55,721 - INFO - [diffusion][Epoch 11101] Epoch 11102/12000
2024-11-05 06:10:59,828 - INFO - [diffusion][Epoch 11101] diffusion training Loss: 0.04886594135314226
2024-11-05 06:10:59,830 - INFO - [diffusion][Epoch 11101] diffusion learning rate: 0.001
2024-11-05 06:10:59,832 - INFO - [diffusion][Epoch 11101] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:10:59,833 - INFO - [diffusion][Epoch 11102] Epoch 11103/12000
2024-11-05 06:11:03,705 - INFO - [diffusion][Epoch 11102] diffusion training Loss: 0.04582643322646618
2024-11-05 06:11:03,707 - INFO - [diffusion][Epoch 11102] diffusion learning rate: 0.001
2024-11-05 06:11:03,709 - INFO - [diffusion][Epoch 11102] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:03,710 - INFO - [diffusion][Epoch 11103] Epoch 11104/12000
2024-11-05 06:11:07,608 - INFO - [diffusion][Epoch 11103] diffusion training Loss: 0.044783965684473515
2024-11-05 06:11:07,611 - INFO - [diffusion][Epoch 11103] diffusion learning rate: 0.001
2024-11-05 06:11:07,613 - INFO - [diffusion][Epoch 11103] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:07,615 - INFO - [diffusion][Epoch 11104] Epoch 11105/12000
2024-11-05 06:11:11,680 - INFO - [diffusion][Epoch 11104] diffusion training Loss: 0.04454268701374531
2024-11-05 06:11:11,682 - INFO - [diffusion][Epoch 11104] diffusion learning rate: 0.001
2024-11-05 06:11:11,683 - INFO - [diffusion][Epoch 11104] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:11,685 - INFO - [diffusion][Epoch 11105] Epoch 11106/12000
2024-11-05 06:11:15,792 - INFO - [diffusion][Epoch 11105] diffusion training Loss: 0.053140224888920784
2024-11-05 06:11:15,794 - INFO - [diffusion][Epoch 11105] diffusion learning rate: 0.001
2024-11-05 06:11:15,796 - INFO - [diffusion][Epoch 11105] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:15,797 - INFO - [diffusion][Epoch 11106] Epoch 11107/12000
2024-11-05 06:11:19,876 - INFO - [diffusion][Epoch 11106] diffusion training Loss: 0.04817848186939955
2024-11-05 06:11:19,878 - INFO - [diffusion][Epoch 11106] diffusion learning rate: 0.001
2024-11-05 06:11:19,880 - INFO - [diffusion][Epoch 11106] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:19,881 - INFO - [diffusion][Epoch 11107] Epoch 11108/12000
2024-11-05 06:11:24,180 - INFO - [diffusion][Epoch 11107] diffusion training Loss: 0.048381137661635876
2024-11-05 06:11:24,182 - INFO - [diffusion][Epoch 11107] diffusion learning rate: 0.001
2024-11-05 06:11:24,184 - INFO - [diffusion][Epoch 11107] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:24,185 - INFO - [diffusion][Epoch 11108] Epoch 11109/12000
2024-11-05 06:11:28,274 - INFO - [diffusion][Epoch 11108] diffusion training Loss: 0.04596187360584736
2024-11-05 06:11:28,276 - INFO - [diffusion][Epoch 11108] diffusion learning rate: 0.001
2024-11-05 06:11:28,277 - INFO - [diffusion][Epoch 11108] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:28,279 - INFO - [diffusion][Epoch 11109] Epoch 11110/12000
2024-11-05 06:11:32,383 - INFO - [diffusion][Epoch 11109] diffusion training Loss: 0.04979519825428724
2024-11-05 06:11:32,385 - INFO - [diffusion][Epoch 11109] diffusion learning rate: 0.001
2024-11-05 06:11:32,387 - INFO - [diffusion][Epoch 11109] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:32,389 - INFO - [diffusion][Epoch 11110] Epoch 11111/12000
2024-11-05 06:11:36,453 - INFO - [diffusion][Epoch 11110] diffusion training Loss: 0.05233187414705753
2024-11-05 06:11:36,455 - INFO - [diffusion][Epoch 11110] diffusion learning rate: 0.001
2024-11-05 06:11:36,457 - INFO - [diffusion][Epoch 11110] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:36,458 - INFO - [diffusion][Epoch 11111] Epoch 11112/12000
2024-11-05 06:11:40,400 - INFO - [diffusion][Epoch 11111] diffusion training Loss: 0.049530938267707825
2024-11-05 06:11:40,402 - INFO - [diffusion][Epoch 11111] diffusion learning rate: 0.001
2024-11-05 06:11:40,404 - INFO - [diffusion][Epoch 11111] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:40,405 - INFO - [diffusion][Epoch 11112] Epoch 11113/12000
2024-11-05 06:11:44,512 - INFO - [diffusion][Epoch 11112] diffusion training Loss: 0.046461280435323715
2024-11-05 06:11:44,514 - INFO - [diffusion][Epoch 11112] diffusion learning rate: 0.001
2024-11-05 06:11:44,515 - INFO - [diffusion][Epoch 11112] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:44,517 - INFO - [diffusion][Epoch 11113] Epoch 11114/12000
2024-11-05 06:11:48,633 - INFO - [diffusion][Epoch 11113] diffusion training Loss: 0.0472572585567832
2024-11-05 06:11:48,637 - INFO - [diffusion][Epoch 11113] diffusion learning rate: 0.001
2024-11-05 06:11:48,638 - INFO - [diffusion][Epoch 11113] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:48,640 - INFO - [diffusion][Epoch 11114] Epoch 11115/12000
2024-11-05 06:11:52,725 - INFO - [diffusion][Epoch 11114] diffusion training Loss: 0.04523573350161314
2024-11-05 06:11:52,727 - INFO - [diffusion][Epoch 11114] diffusion learning rate: 0.001
2024-11-05 06:11:52,729 - INFO - [diffusion][Epoch 11114] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:52,730 - INFO - [diffusion][Epoch 11115] Epoch 11116/12000
2024-11-05 06:11:56,806 - INFO - [diffusion][Epoch 11115] diffusion training Loss: 0.04886308126151562
2024-11-05 06:11:56,808 - INFO - [diffusion][Epoch 11115] diffusion learning rate: 0.001
2024-11-05 06:11:56,848 - INFO - [diffusion][Epoch 11115] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:11:56,849 - INFO - [diffusion][Epoch 11116] Epoch 11117/12000
2024-11-05 06:12:00,939 - INFO - [diffusion][Epoch 11116] diffusion training Loss: 0.05885144881904125
2024-11-05 06:12:00,941 - INFO - [diffusion][Epoch 11116] diffusion learning rate: 0.001
2024-11-05 06:12:00,942 - INFO - [diffusion][Epoch 11116] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:00,944 - INFO - [diffusion][Epoch 11117] Epoch 11118/12000
2024-11-05 06:12:05,056 - INFO - [diffusion][Epoch 11117] diffusion training Loss: 0.04304277617484331
2024-11-05 06:12:05,058 - INFO - [diffusion][Epoch 11117] diffusion learning rate: 0.001
2024-11-05 06:12:05,060 - INFO - [diffusion][Epoch 11117] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:05,061 - INFO - [diffusion][Epoch 11118] Epoch 11119/12000
2024-11-05 06:12:09,154 - INFO - [diffusion][Epoch 11118] diffusion training Loss: 0.04969100560992956
2024-11-05 06:12:09,156 - INFO - [diffusion][Epoch 11118] diffusion learning rate: 0.001
2024-11-05 06:12:09,157 - INFO - [diffusion][Epoch 11118] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:09,158 - INFO - [diffusion][Epoch 11119] Epoch 11120/12000
2024-11-05 06:12:13,251 - INFO - [diffusion][Epoch 11119] diffusion training Loss: 0.04706406034529209
2024-11-05 06:12:13,256 - INFO - [diffusion][Epoch 11119] diffusion learning rate: 0.001
2024-11-05 06:12:13,275 - INFO - [diffusion][Epoch 11119] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:13,277 - INFO - [diffusion][Epoch 11120] Epoch 11121/12000
2024-11-05 06:12:17,383 - INFO - [diffusion][Epoch 11120] diffusion training Loss: 0.04749242402613163
2024-11-05 06:12:17,385 - INFO - [diffusion][Epoch 11120] diffusion learning rate: 0.001
2024-11-05 06:12:17,387 - INFO - [diffusion][Epoch 11120] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:17,388 - INFO - [diffusion][Epoch 11121] Epoch 11122/12000
2024-11-05 06:12:21,480 - INFO - [diffusion][Epoch 11121] diffusion training Loss: 0.04493769817054272
2024-11-05 06:12:21,482 - INFO - [diffusion][Epoch 11121] diffusion learning rate: 0.001
2024-11-05 06:12:21,483 - INFO - [diffusion][Epoch 11121] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:21,485 - INFO - [diffusion][Epoch 11122] Epoch 11123/12000
2024-11-05 06:12:25,608 - INFO - [diffusion][Epoch 11122] diffusion training Loss: 0.042643828317523
2024-11-05 06:12:25,610 - INFO - [diffusion][Epoch 11122] diffusion learning rate: 0.001
2024-11-05 06:12:25,611 - INFO - [diffusion][Epoch 11122] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:25,613 - INFO - [diffusion][Epoch 11123] Epoch 11124/12000
2024-11-05 06:12:29,509 - INFO - [diffusion][Epoch 11123] diffusion training Loss: 0.04658696334809065
2024-11-05 06:12:29,512 - INFO - [diffusion][Epoch 11123] diffusion learning rate: 0.001
2024-11-05 06:12:29,532 - INFO - [diffusion][Epoch 11123] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:29,533 - INFO - [diffusion][Epoch 11124] Epoch 11125/12000
2024-11-05 06:12:33,598 - INFO - [diffusion][Epoch 11124] diffusion training Loss: 0.0456421785056591
2024-11-05 06:12:33,600 - INFO - [diffusion][Epoch 11124] diffusion learning rate: 0.001
2024-11-05 06:12:33,602 - INFO - [diffusion][Epoch 11124] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:33,603 - INFO - [diffusion][Epoch 11125] Epoch 11126/12000
2024-11-05 06:12:37,657 - INFO - [diffusion][Epoch 11125] diffusion training Loss: 0.04912550840526819
2024-11-05 06:12:37,659 - INFO - [diffusion][Epoch 11125] diffusion learning rate: 0.001
2024-11-05 06:12:37,661 - INFO - [diffusion][Epoch 11125] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:37,662 - INFO - [diffusion][Epoch 11126] Epoch 11127/12000
2024-11-05 06:12:41,722 - INFO - [diffusion][Epoch 11126] diffusion training Loss: 0.04806708823889494
2024-11-05 06:12:41,724 - INFO - [diffusion][Epoch 11126] diffusion learning rate: 0.001
2024-11-05 06:12:41,725 - INFO - [diffusion][Epoch 11126] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:41,727 - INFO - [diffusion][Epoch 11127] Epoch 11128/12000
2024-11-05 06:12:45,794 - INFO - [diffusion][Epoch 11127] diffusion training Loss: 0.044588908553123474
2024-11-05 06:12:45,796 - INFO - [diffusion][Epoch 11127] diffusion learning rate: 0.001
2024-11-05 06:12:45,797 - INFO - [diffusion][Epoch 11127] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:45,799 - INFO - [diffusion][Epoch 11128] Epoch 11129/12000
2024-11-05 06:12:50,131 - INFO - [diffusion][Epoch 11128] diffusion training Loss: 0.044651443138718605
2024-11-05 06:12:50,134 - INFO - [diffusion][Epoch 11128] diffusion learning rate: 0.001
2024-11-05 06:12:50,136 - INFO - [diffusion][Epoch 11128] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:50,138 - INFO - [diffusion][Epoch 11129] Epoch 11130/12000
2024-11-05 06:12:54,244 - INFO - [diffusion][Epoch 11129] diffusion training Loss: 0.04873768053948879
2024-11-05 06:12:54,246 - INFO - [diffusion][Epoch 11129] diffusion learning rate: 0.001
2024-11-05 06:12:54,247 - INFO - [diffusion][Epoch 11129] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:54,249 - INFO - [diffusion][Epoch 11130] Epoch 11131/12000
2024-11-05 06:12:58,298 - INFO - [diffusion][Epoch 11130] diffusion training Loss: 0.0488473754376173
2024-11-05 06:12:58,300 - INFO - [diffusion][Epoch 11130] diffusion learning rate: 0.001
2024-11-05 06:12:58,302 - INFO - [diffusion][Epoch 11130] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:12:58,303 - INFO - [diffusion][Epoch 11131] Epoch 11132/12000
2024-11-05 06:13:02,364 - INFO - [diffusion][Epoch 11131] diffusion training Loss: 0.04775907285511494
2024-11-05 06:13:02,366 - INFO - [diffusion][Epoch 11131] diffusion learning rate: 0.001
2024-11-05 06:13:02,387 - INFO - [diffusion][Epoch 11131] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:02,388 - INFO - [diffusion][Epoch 11132] Epoch 11133/12000
2024-11-05 06:13:06,314 - INFO - [diffusion][Epoch 11132] diffusion training Loss: 0.04317859746515751
2024-11-05 06:13:06,316 - INFO - [diffusion][Epoch 11132] diffusion learning rate: 0.001
2024-11-05 06:13:06,318 - INFO - [diffusion][Epoch 11132] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:06,319 - INFO - [diffusion][Epoch 11133] Epoch 11134/12000
2024-11-05 06:13:10,398 - INFO - [diffusion][Epoch 11133] diffusion training Loss: 0.047478243708610535
2024-11-05 06:13:10,400 - INFO - [diffusion][Epoch 11133] diffusion learning rate: 0.001
2024-11-05 06:13:10,402 - INFO - [diffusion][Epoch 11133] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:10,403 - INFO - [diffusion][Epoch 11134] Epoch 11135/12000
2024-11-05 06:13:14,460 - INFO - [diffusion][Epoch 11134] diffusion training Loss: 0.04905176814645529
2024-11-05 06:13:14,462 - INFO - [diffusion][Epoch 11134] diffusion learning rate: 0.001
2024-11-05 06:13:14,464 - INFO - [diffusion][Epoch 11134] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:14,465 - INFO - [diffusion][Epoch 11135] Epoch 11136/12000
2024-11-05 06:13:18,513 - INFO - [diffusion][Epoch 11135] diffusion training Loss: 0.050325886346399784
2024-11-05 06:13:18,515 - INFO - [diffusion][Epoch 11135] diffusion learning rate: 0.001
2024-11-05 06:13:18,516 - INFO - [diffusion][Epoch 11135] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:18,517 - INFO - [diffusion][Epoch 11136] Epoch 11137/12000
2024-11-05 06:13:22,600 - INFO - [diffusion][Epoch 11136] diffusion training Loss: 0.04392777755856514
2024-11-05 06:13:22,602 - INFO - [diffusion][Epoch 11136] diffusion learning rate: 0.001
2024-11-05 06:13:22,603 - INFO - [diffusion][Epoch 11136] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:22,606 - INFO - [diffusion][Epoch 11137] Epoch 11138/12000
2024-11-05 06:13:26,496 - INFO - [diffusion][Epoch 11137] diffusion training Loss: 0.050368170253932476
2024-11-05 06:13:26,497 - INFO - [diffusion][Epoch 11137] diffusion learning rate: 0.001
2024-11-05 06:13:26,499 - INFO - [diffusion][Epoch 11137] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:26,500 - INFO - [diffusion][Epoch 11138] Epoch 11139/12000
2024-11-05 06:13:30,558 - INFO - [diffusion][Epoch 11138] diffusion training Loss: 0.048989725299179554
2024-11-05 06:13:30,560 - INFO - [diffusion][Epoch 11138] diffusion learning rate: 0.001
2024-11-05 06:13:30,561 - INFO - [diffusion][Epoch 11138] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:30,562 - INFO - [diffusion][Epoch 11139] Epoch 11140/12000
2024-11-05 06:13:34,645 - INFO - [diffusion][Epoch 11139] diffusion training Loss: 0.04484419245272875
2024-11-05 06:13:34,647 - INFO - [diffusion][Epoch 11139] diffusion learning rate: 0.001
2024-11-05 06:13:34,649 - INFO - [diffusion][Epoch 11139] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:34,650 - INFO - [diffusion][Epoch 11140] Epoch 11141/12000
2024-11-05 06:13:38,691 - INFO - [diffusion][Epoch 11140] diffusion training Loss: 0.044457186944782734
2024-11-05 06:13:38,693 - INFO - [diffusion][Epoch 11140] diffusion learning rate: 0.001
2024-11-05 06:13:38,695 - INFO - [diffusion][Epoch 11140] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:38,696 - INFO - [diffusion][Epoch 11141] Epoch 11142/12000
2024-11-05 06:13:42,761 - INFO - [diffusion][Epoch 11141] diffusion training Loss: 0.051457774825394154
2024-11-05 06:13:42,763 - INFO - [diffusion][Epoch 11141] diffusion learning rate: 0.001
2024-11-05 06:13:42,765 - INFO - [diffusion][Epoch 11141] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:42,766 - INFO - [diffusion][Epoch 11142] Epoch 11143/12000
2024-11-05 06:13:46,815 - INFO - [diffusion][Epoch 11142] diffusion training Loss: 0.04212591890245676
2024-11-05 06:13:46,817 - INFO - [diffusion][Epoch 11142] diffusion learning rate: 0.001
2024-11-05 06:13:46,819 - INFO - [diffusion][Epoch 11142] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:46,820 - INFO - [diffusion][Epoch 11143] Epoch 11144/12000
2024-11-05 06:13:50,861 - INFO - [diffusion][Epoch 11143] diffusion training Loss: 0.04894778784364462
2024-11-05 06:13:50,865 - INFO - [diffusion][Epoch 11143] diffusion learning rate: 0.001
2024-11-05 06:13:50,867 - INFO - [diffusion][Epoch 11143] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:50,868 - INFO - [diffusion][Epoch 11144] Epoch 11145/12000
2024-11-05 06:13:54,924 - INFO - [diffusion][Epoch 11144] diffusion training Loss: 0.04177085589617491
2024-11-05 06:13:54,926 - INFO - [diffusion][Epoch 11144] diffusion learning rate: 0.001
2024-11-05 06:13:54,928 - INFO - [diffusion][Epoch 11144] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:54,929 - INFO - [diffusion][Epoch 11145] Epoch 11146/12000
2024-11-05 06:13:58,986 - INFO - [diffusion][Epoch 11145] diffusion training Loss: 0.04960295930504799
2024-11-05 06:13:58,988 - INFO - [diffusion][Epoch 11145] diffusion learning rate: 0.001
2024-11-05 06:13:58,990 - INFO - [diffusion][Epoch 11145] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:13:58,992 - INFO - [diffusion][Epoch 11146] Epoch 11147/12000
2024-11-05 06:14:03,064 - INFO - [diffusion][Epoch 11146] diffusion training Loss: 0.04864331241697073
2024-11-05 06:14:03,066 - INFO - [diffusion][Epoch 11146] diffusion learning rate: 0.001
2024-11-05 06:14:03,068 - INFO - [diffusion][Epoch 11146] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:03,069 - INFO - [diffusion][Epoch 11147] Epoch 11148/12000
2024-11-05 06:14:07,187 - INFO - [diffusion][Epoch 11147] diffusion training Loss: 0.049327862448990345
2024-11-05 06:14:07,189 - INFO - [diffusion][Epoch 11147] diffusion learning rate: 0.001
2024-11-05 06:14:07,191 - INFO - [diffusion][Epoch 11147] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:07,192 - INFO - [diffusion][Epoch 11148] Epoch 11149/12000
2024-11-05 06:14:11,280 - INFO - [diffusion][Epoch 11148] diffusion training Loss: 0.049846009351313114
2024-11-05 06:14:11,282 - INFO - [diffusion][Epoch 11148] diffusion learning rate: 0.001
2024-11-05 06:14:11,284 - INFO - [diffusion][Epoch 11148] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:11,286 - INFO - [diffusion][Epoch 11149] Epoch 11150/12000
2024-11-05 06:14:15,397 - INFO - [diffusion][Epoch 11149] diffusion training Loss: 0.05085872020572424
2024-11-05 06:14:15,399 - INFO - [diffusion][Epoch 11149] diffusion learning rate: 0.001
2024-11-05 06:14:15,400 - INFO - [diffusion][Epoch 11149] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:15,402 - INFO - [diffusion][Epoch 11150] Epoch 11151/12000
2024-11-05 06:14:19,440 - INFO - [diffusion][Epoch 11150] diffusion training Loss: 0.053058006800711155
2024-11-05 06:14:19,442 - INFO - [diffusion][Epoch 11150] diffusion learning rate: 0.001
2024-11-05 06:14:19,444 - INFO - [diffusion][Epoch 11150] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:19,445 - INFO - [diffusion][Epoch 11151] Epoch 11152/12000
2024-11-05 06:14:23,562 - INFO - [diffusion][Epoch 11151] diffusion training Loss: 0.04923982359468937
2024-11-05 06:14:23,564 - INFO - [diffusion][Epoch 11151] diffusion learning rate: 0.001
2024-11-05 06:14:23,565 - INFO - [diffusion][Epoch 11151] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:23,567 - INFO - [diffusion][Epoch 11152] Epoch 11153/12000
2024-11-05 06:14:27,625 - INFO - [diffusion][Epoch 11152] diffusion training Loss: 0.04636971466243267
2024-11-05 06:14:27,628 - INFO - [diffusion][Epoch 11152] diffusion learning rate: 0.001
2024-11-05 06:14:27,629 - INFO - [diffusion][Epoch 11152] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:27,631 - INFO - [diffusion][Epoch 11153] Epoch 11154/12000
2024-11-05 06:14:31,663 - INFO - [diffusion][Epoch 11153] diffusion training Loss: 0.047909485176205635
2024-11-05 06:14:31,665 - INFO - [diffusion][Epoch 11153] diffusion learning rate: 0.001
2024-11-05 06:14:31,667 - INFO - [diffusion][Epoch 11153] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:31,668 - INFO - [diffusion][Epoch 11154] Epoch 11155/12000
2024-11-05 06:14:35,776 - INFO - [diffusion][Epoch 11154] diffusion training Loss: 0.053804670460522175
2024-11-05 06:14:35,778 - INFO - [diffusion][Epoch 11154] diffusion learning rate: 0.001
2024-11-05 06:14:35,780 - INFO - [diffusion][Epoch 11154] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:35,781 - INFO - [diffusion][Epoch 11155] Epoch 11156/12000
2024-11-05 06:14:39,897 - INFO - [diffusion][Epoch 11155] diffusion training Loss: 0.05249460134655237
2024-11-05 06:14:39,899 - INFO - [diffusion][Epoch 11155] diffusion learning rate: 0.001
2024-11-05 06:14:39,900 - INFO - [diffusion][Epoch 11155] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:39,902 - INFO - [diffusion][Epoch 11156] Epoch 11157/12000
2024-11-05 06:14:44,036 - INFO - [diffusion][Epoch 11156] diffusion training Loss: 0.051432982087135315
2024-11-05 06:14:44,038 - INFO - [diffusion][Epoch 11156] diffusion learning rate: 0.001
2024-11-05 06:14:44,040 - INFO - [diffusion][Epoch 11156] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:44,041 - INFO - [diffusion][Epoch 11157] Epoch 11158/12000
2024-11-05 06:14:48,157 - INFO - [diffusion][Epoch 11157] diffusion training Loss: 0.04693914018571377
2024-11-05 06:14:48,159 - INFO - [diffusion][Epoch 11157] diffusion learning rate: 0.001
2024-11-05 06:14:48,161 - INFO - [diffusion][Epoch 11157] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:48,162 - INFO - [diffusion][Epoch 11158] Epoch 11159/12000
2024-11-05 06:14:52,303 - INFO - [diffusion][Epoch 11158] diffusion training Loss: 0.0490935780107975
2024-11-05 06:14:52,307 - INFO - [diffusion][Epoch 11158] diffusion learning rate: 0.001
2024-11-05 06:14:52,308 - INFO - [diffusion][Epoch 11158] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:52,310 - INFO - [diffusion][Epoch 11159] Epoch 11160/12000
2024-11-05 06:14:56,355 - INFO - [diffusion][Epoch 11159] diffusion training Loss: 0.049456256441771984
2024-11-05 06:14:56,357 - INFO - [diffusion][Epoch 11159] diffusion learning rate: 0.001
2024-11-05 06:14:56,359 - INFO - [diffusion][Epoch 11159] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:14:56,360 - INFO - [diffusion][Epoch 11160] Epoch 11161/12000
2024-11-05 06:15:00,429 - INFO - [diffusion][Epoch 11160] diffusion training Loss: 0.046866039745509624
2024-11-05 06:15:00,431 - INFO - [diffusion][Epoch 11160] diffusion learning rate: 0.001
2024-11-05 06:15:00,433 - INFO - [diffusion][Epoch 11160] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:00,434 - INFO - [diffusion][Epoch 11161] Epoch 11162/12000
2024-11-05 06:15:04,499 - INFO - [diffusion][Epoch 11161] diffusion training Loss: 0.046965690329670906
2024-11-05 06:15:04,501 - INFO - [diffusion][Epoch 11161] diffusion learning rate: 0.001
2024-11-05 06:15:04,502 - INFO - [diffusion][Epoch 11161] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:04,504 - INFO - [diffusion][Epoch 11162] Epoch 11163/12000
2024-11-05 06:15:08,610 - INFO - [diffusion][Epoch 11162] diffusion training Loss: 0.04723009280860424
2024-11-05 06:15:08,612 - INFO - [diffusion][Epoch 11162] diffusion learning rate: 0.001
2024-11-05 06:15:08,614 - INFO - [diffusion][Epoch 11162] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:08,615 - INFO - [diffusion][Epoch 11163] Epoch 11164/12000
2024-11-05 06:15:12,698 - INFO - [diffusion][Epoch 11163] diffusion training Loss: 0.04566193465143442
2024-11-05 06:15:12,700 - INFO - [diffusion][Epoch 11163] diffusion learning rate: 0.001
2024-11-05 06:15:12,702 - INFO - [diffusion][Epoch 11163] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:12,703 - INFO - [diffusion][Epoch 11164] Epoch 11165/12000
2024-11-05 06:15:16,828 - INFO - [diffusion][Epoch 11164] diffusion training Loss: 0.04881741385906935
2024-11-05 06:15:16,831 - INFO - [diffusion][Epoch 11164] diffusion learning rate: 0.001
2024-11-05 06:15:16,832 - INFO - [diffusion][Epoch 11164] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:16,834 - INFO - [diffusion][Epoch 11165] Epoch 11166/12000
2024-11-05 06:15:20,941 - INFO - [diffusion][Epoch 11165] diffusion training Loss: 0.04854304902255535
2024-11-05 06:15:20,943 - INFO - [diffusion][Epoch 11165] diffusion learning rate: 0.001
2024-11-05 06:15:20,944 - INFO - [diffusion][Epoch 11165] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:20,946 - INFO - [diffusion][Epoch 11166] Epoch 11167/12000
2024-11-05 06:15:25,048 - INFO - [diffusion][Epoch 11166] diffusion training Loss: 0.046869516372680664
2024-11-05 06:15:25,050 - INFO - [diffusion][Epoch 11166] diffusion learning rate: 0.001
2024-11-05 06:15:25,051 - INFO - [diffusion][Epoch 11166] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:25,053 - INFO - [diffusion][Epoch 11167] Epoch 11168/12000
2024-11-05 06:15:29,162 - INFO - [diffusion][Epoch 11167] diffusion training Loss: 0.04834262654185295
2024-11-05 06:15:29,164 - INFO - [diffusion][Epoch 11167] diffusion learning rate: 0.001
2024-11-05 06:15:29,165 - INFO - [diffusion][Epoch 11167] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:29,167 - INFO - [diffusion][Epoch 11168] Epoch 11169/12000
2024-11-05 06:15:33,292 - INFO - [diffusion][Epoch 11168] diffusion training Loss: 0.04798406641930342
2024-11-05 06:15:33,294 - INFO - [diffusion][Epoch 11168] diffusion learning rate: 0.001
2024-11-05 06:15:33,295 - INFO - [diffusion][Epoch 11168] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:33,297 - INFO - [diffusion][Epoch 11169] Epoch 11170/12000
2024-11-05 06:15:37,395 - INFO - [diffusion][Epoch 11169] diffusion training Loss: 0.04709946643561125
2024-11-05 06:15:37,397 - INFO - [diffusion][Epoch 11169] diffusion learning rate: 0.001
2024-11-05 06:15:37,399 - INFO - [diffusion][Epoch 11169] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:37,400 - INFO - [diffusion][Epoch 11170] Epoch 11171/12000
2024-11-05 06:15:41,482 - INFO - [diffusion][Epoch 11170] diffusion training Loss: 0.047158729285001755
2024-11-05 06:15:41,484 - INFO - [diffusion][Epoch 11170] diffusion learning rate: 0.001
2024-11-05 06:15:41,486 - INFO - [diffusion][Epoch 11170] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:41,487 - INFO - [diffusion][Epoch 11171] Epoch 11172/12000
2024-11-05 06:15:46,048 - INFO - [diffusion][Epoch 11171] diffusion training Loss: 0.05071741621941328
2024-11-05 06:15:46,050 - INFO - [diffusion][Epoch 11171] diffusion learning rate: 0.001
2024-11-05 06:15:46,051 - INFO - [diffusion][Epoch 11171] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:46,053 - INFO - [diffusion][Epoch 11172] Epoch 11173/12000
2024-11-05 06:15:50,119 - INFO - [diffusion][Epoch 11172] diffusion training Loss: 0.04566388111561537
2024-11-05 06:15:50,121 - INFO - [diffusion][Epoch 11172] diffusion learning rate: 0.001
2024-11-05 06:15:50,122 - INFO - [diffusion][Epoch 11172] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:50,124 - INFO - [diffusion][Epoch 11173] Epoch 11174/12000
2024-11-05 06:15:54,252 - INFO - [diffusion][Epoch 11173] diffusion training Loss: 0.05132393725216389
2024-11-05 06:15:54,255 - INFO - [diffusion][Epoch 11173] diffusion learning rate: 0.001
2024-11-05 06:15:54,257 - INFO - [diffusion][Epoch 11173] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:54,258 - INFO - [diffusion][Epoch 11174] Epoch 11175/12000
2024-11-05 06:15:58,406 - INFO - [diffusion][Epoch 11174] diffusion training Loss: 0.042123922146856785
2024-11-05 06:15:58,408 - INFO - [diffusion][Epoch 11174] diffusion learning rate: 0.001
2024-11-05 06:15:58,430 - INFO - [diffusion][Epoch 11174] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:15:58,432 - INFO - [diffusion][Epoch 11175] Epoch 11176/12000
2024-11-05 06:16:02,530 - INFO - [diffusion][Epoch 11175] diffusion training Loss: 0.04220175743103027
2024-11-05 06:16:02,532 - INFO - [diffusion][Epoch 11175] diffusion learning rate: 0.001
2024-11-05 06:16:02,534 - INFO - [diffusion][Epoch 11175] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:02,535 - INFO - [diffusion][Epoch 11176] Epoch 11177/12000
2024-11-05 06:16:06,650 - INFO - [diffusion][Epoch 11176] diffusion training Loss: 0.04091400466859341
2024-11-05 06:16:06,652 - INFO - [diffusion][Epoch 11176] diffusion learning rate: 0.001
2024-11-05 06:16:06,654 - INFO - [diffusion][Epoch 11176] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:06,655 - INFO - [diffusion][Epoch 11177] Epoch 11178/12000
2024-11-05 06:16:10,773 - INFO - [diffusion][Epoch 11177] diffusion training Loss: 0.04341321252286434
2024-11-05 06:16:10,775 - INFO - [diffusion][Epoch 11177] diffusion learning rate: 0.001
2024-11-05 06:16:10,777 - INFO - [diffusion][Epoch 11177] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:10,779 - INFO - [diffusion][Epoch 11178] Epoch 11179/12000
2024-11-05 06:16:14,854 - INFO - [diffusion][Epoch 11178] diffusion training Loss: 0.05103254783898592
2024-11-05 06:16:14,856 - INFO - [diffusion][Epoch 11178] diffusion learning rate: 0.001
2024-11-05 06:16:14,885 - INFO - [diffusion][Epoch 11178] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:14,887 - INFO - [diffusion][Epoch 11179] Epoch 11180/12000
2024-11-05 06:16:18,979 - INFO - [diffusion][Epoch 11179] diffusion training Loss: 0.046065812930464745
2024-11-05 06:16:18,982 - INFO - [diffusion][Epoch 11179] diffusion learning rate: 0.001
2024-11-05 06:16:18,984 - INFO - [diffusion][Epoch 11179] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:18,985 - INFO - [diffusion][Epoch 11180] Epoch 11181/12000
2024-11-05 06:16:23,072 - INFO - [diffusion][Epoch 11180] diffusion training Loss: 0.04222218506038189
2024-11-05 06:16:23,074 - INFO - [diffusion][Epoch 11180] diffusion learning rate: 0.001
2024-11-05 06:16:23,075 - INFO - [diffusion][Epoch 11180] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:23,076 - INFO - [diffusion][Epoch 11181] Epoch 11182/12000
2024-11-05 06:16:27,198 - INFO - [diffusion][Epoch 11181] diffusion training Loss: 0.050663710571825504
2024-11-05 06:16:27,200 - INFO - [diffusion][Epoch 11181] diffusion learning rate: 0.001
2024-11-05 06:16:27,202 - INFO - [diffusion][Epoch 11181] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:27,203 - INFO - [diffusion][Epoch 11182] Epoch 11183/12000
2024-11-05 06:16:31,318 - INFO - [diffusion][Epoch 11182] diffusion training Loss: 0.04585248790681362
2024-11-05 06:16:31,320 - INFO - [diffusion][Epoch 11182] diffusion learning rate: 0.001
2024-11-05 06:16:31,322 - INFO - [diffusion][Epoch 11182] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:31,323 - INFO - [diffusion][Epoch 11183] Epoch 11184/12000
2024-11-05 06:16:35,423 - INFO - [diffusion][Epoch 11183] diffusion training Loss: 0.04925973527133465
2024-11-05 06:16:35,425 - INFO - [diffusion][Epoch 11183] diffusion learning rate: 0.001
2024-11-05 06:16:35,427 - INFO - [diffusion][Epoch 11183] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:35,428 - INFO - [diffusion][Epoch 11184] Epoch 11185/12000
2024-11-05 06:16:39,539 - INFO - [diffusion][Epoch 11184] diffusion training Loss: 0.046289531514048576
2024-11-05 06:16:39,541 - INFO - [diffusion][Epoch 11184] diffusion learning rate: 0.001
2024-11-05 06:16:39,543 - INFO - [diffusion][Epoch 11184] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:39,544 - INFO - [diffusion][Epoch 11185] Epoch 11186/12000
2024-11-05 06:16:43,671 - INFO - [diffusion][Epoch 11185] diffusion training Loss: 0.04216038342565298
2024-11-05 06:16:43,673 - INFO - [diffusion][Epoch 11185] diffusion learning rate: 0.001
2024-11-05 06:16:43,674 - INFO - [diffusion][Epoch 11185] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:43,676 - INFO - [diffusion][Epoch 11186] Epoch 11187/12000
2024-11-05 06:16:47,790 - INFO - [diffusion][Epoch 11186] diffusion training Loss: 0.04302540794014931
2024-11-05 06:16:47,792 - INFO - [diffusion][Epoch 11186] diffusion learning rate: 0.001
2024-11-05 06:16:47,820 - INFO - [diffusion][Epoch 11186] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:47,821 - INFO - [diffusion][Epoch 11187] Epoch 11188/12000
2024-11-05 06:16:51,906 - INFO - [diffusion][Epoch 11187] diffusion training Loss: 0.04900521878153086
2024-11-05 06:16:51,907 - INFO - [diffusion][Epoch 11187] diffusion learning rate: 0.001
2024-11-05 06:16:51,909 - INFO - [diffusion][Epoch 11187] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:51,910 - INFO - [diffusion][Epoch 11188] Epoch 11189/12000
2024-11-05 06:16:55,990 - INFO - [diffusion][Epoch 11188] diffusion training Loss: 0.05201678164303303
2024-11-05 06:16:55,993 - INFO - [diffusion][Epoch 11188] diffusion learning rate: 0.001
2024-11-05 06:16:55,995 - INFO - [diffusion][Epoch 11188] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:16:55,996 - INFO - [diffusion][Epoch 11189] Epoch 11190/12000
2024-11-05 06:17:00,117 - INFO - [diffusion][Epoch 11189] diffusion training Loss: 0.048075414262712
2024-11-05 06:17:00,119 - INFO - [diffusion][Epoch 11189] diffusion learning rate: 0.001
2024-11-05 06:17:00,139 - INFO - [diffusion][Epoch 11189] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:00,140 - INFO - [diffusion][Epoch 11190] Epoch 11191/12000
2024-11-05 06:17:04,232 - INFO - [diffusion][Epoch 11190] diffusion training Loss: 0.04710867442190647
2024-11-05 06:17:04,234 - INFO - [diffusion][Epoch 11190] diffusion learning rate: 0.001
2024-11-05 06:17:04,236 - INFO - [diffusion][Epoch 11190] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:04,237 - INFO - [diffusion][Epoch 11191] Epoch 11192/12000
2024-11-05 06:17:08,572 - INFO - [diffusion][Epoch 11191] diffusion training Loss: 0.046148194931447506
2024-11-05 06:17:08,574 - INFO - [diffusion][Epoch 11191] diffusion learning rate: 0.001
2024-11-05 06:17:08,576 - INFO - [diffusion][Epoch 11191] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:08,578 - INFO - [diffusion][Epoch 11192] Epoch 11193/12000
2024-11-05 06:17:12,642 - INFO - [diffusion][Epoch 11192] diffusion training Loss: 0.04843102488666773
2024-11-05 06:17:12,719 - INFO - [diffusion][Epoch 11192] diffusion learning rate: 0.001
2024-11-05 06:17:12,721 - INFO - [diffusion][Epoch 11192] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:12,722 - INFO - [diffusion][Epoch 11193] Epoch 11194/12000
2024-11-05 06:17:16,809 - INFO - [diffusion][Epoch 11193] diffusion training Loss: 0.04740989673882723
2024-11-05 06:17:16,811 - INFO - [diffusion][Epoch 11193] diffusion learning rate: 0.001
2024-11-05 06:17:16,813 - INFO - [diffusion][Epoch 11193] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:16,814 - INFO - [diffusion][Epoch 11194] Epoch 11195/12000
2024-11-05 06:17:20,912 - INFO - [diffusion][Epoch 11194] diffusion training Loss: 0.04800337925553322
2024-11-05 06:17:20,914 - INFO - [diffusion][Epoch 11194] diffusion learning rate: 0.001
2024-11-05 06:17:20,916 - INFO - [diffusion][Epoch 11194] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:20,917 - INFO - [diffusion][Epoch 11195] Epoch 11196/12000
2024-11-05 06:17:25,005 - INFO - [diffusion][Epoch 11195] diffusion training Loss: 0.04930168483406305
2024-11-05 06:17:25,007 - INFO - [diffusion][Epoch 11195] diffusion learning rate: 0.001
2024-11-05 06:17:25,009 - INFO - [diffusion][Epoch 11195] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:25,010 - INFO - [diffusion][Epoch 11196] Epoch 11197/12000
2024-11-05 06:17:29,081 - INFO - [diffusion][Epoch 11196] diffusion training Loss: 0.04453538730740547
2024-11-05 06:17:29,084 - INFO - [diffusion][Epoch 11196] diffusion learning rate: 0.001
2024-11-05 06:17:29,086 - INFO - [diffusion][Epoch 11196] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:29,087 - INFO - [diffusion][Epoch 11197] Epoch 11198/12000
2024-11-05 06:17:33,175 - INFO - [diffusion][Epoch 11197] diffusion training Loss: 0.0470132902264595
2024-11-05 06:17:33,177 - INFO - [diffusion][Epoch 11197] diffusion learning rate: 0.001
2024-11-05 06:17:33,179 - INFO - [diffusion][Epoch 11197] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:33,180 - INFO - [diffusion][Epoch 11198] Epoch 11199/12000
2024-11-05 06:17:37,260 - INFO - [diffusion][Epoch 11198] diffusion training Loss: 0.044705224223434925
2024-11-05 06:17:37,262 - INFO - [diffusion][Epoch 11198] diffusion learning rate: 0.001
2024-11-05 06:17:37,264 - INFO - [diffusion][Epoch 11198] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:37,265 - INFO - [diffusion][Epoch 11199] Epoch 11200/12000
2024-11-05 06:17:41,377 - INFO - [diffusion][Epoch 11199] diffusion training Loss: 0.049982489086687565
2024-11-05 06:17:41,379 - INFO - [diffusion][Epoch 11199] diffusion learning rate: 0.001
2024-11-05 06:17:41,381 - INFO - [diffusion][Epoch 11199] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:41,382 - INFO - [diffusion][Epoch 11200] Epoch 11201/12000
2024-11-05 06:17:45,482 - INFO - [diffusion][Epoch 11200] diffusion training Loss: 0.04147715773433447
2024-11-05 06:17:45,484 - INFO - [diffusion][Epoch 11200] diffusion learning rate: 0.001
2024-11-05 06:17:45,486 - INFO - [diffusion][Epoch 11200] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:45,487 - INFO - [diffusion][Epoch 11201] Epoch 11202/12000
2024-11-05 06:17:49,558 - INFO - [diffusion][Epoch 11201] diffusion training Loss: 0.04924192652106285
2024-11-05 06:17:49,560 - INFO - [diffusion][Epoch 11201] diffusion learning rate: 0.001
2024-11-05 06:17:49,566 - INFO - [diffusion][Epoch 11201] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:49,567 - INFO - [diffusion][Epoch 11202] Epoch 11203/12000
2024-11-05 06:17:53,645 - INFO - [diffusion][Epoch 11202] diffusion training Loss: 0.048865554854273796
2024-11-05 06:17:53,647 - INFO - [diffusion][Epoch 11202] diffusion learning rate: 0.001
2024-11-05 06:17:53,649 - INFO - [diffusion][Epoch 11202] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:53,650 - INFO - [diffusion][Epoch 11203] Epoch 11204/12000
2024-11-05 06:17:57,758 - INFO - [diffusion][Epoch 11203] diffusion training Loss: 0.04653398133814335
2024-11-05 06:17:57,762 - INFO - [diffusion][Epoch 11203] diffusion learning rate: 0.001
2024-11-05 06:17:57,763 - INFO - [diffusion][Epoch 11203] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:17:57,764 - INFO - [diffusion][Epoch 11204] Epoch 11205/12000
2024-11-05 06:18:01,872 - INFO - [diffusion][Epoch 11204] diffusion training Loss: 0.048308467492461205
2024-11-05 06:18:01,874 - INFO - [diffusion][Epoch 11204] diffusion learning rate: 0.001
2024-11-05 06:18:01,876 - INFO - [diffusion][Epoch 11204] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:01,877 - INFO - [diffusion][Epoch 11205] Epoch 11206/12000
2024-11-05 06:18:05,939 - INFO - [diffusion][Epoch 11205] diffusion training Loss: 0.04496190510690212
2024-11-05 06:18:05,941 - INFO - [diffusion][Epoch 11205] diffusion learning rate: 0.001
2024-11-05 06:18:05,943 - INFO - [diffusion][Epoch 11205] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:05,944 - INFO - [diffusion][Epoch 11206] Epoch 11207/12000
2024-11-05 06:18:09,987 - INFO - [diffusion][Epoch 11206] diffusion training Loss: 0.04274837672710419
2024-11-05 06:18:09,989 - INFO - [diffusion][Epoch 11206] diffusion learning rate: 0.001
2024-11-05 06:18:09,990 - INFO - [diffusion][Epoch 11206] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:09,992 - INFO - [diffusion][Epoch 11207] Epoch 11208/12000
2024-11-05 06:18:14,044 - INFO - [diffusion][Epoch 11207] diffusion training Loss: 0.0533045195043087
2024-11-05 06:18:14,046 - INFO - [diffusion][Epoch 11207] diffusion learning rate: 0.001
2024-11-05 06:18:14,048 - INFO - [diffusion][Epoch 11207] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:14,049 - INFO - [diffusion][Epoch 11208] Epoch 11209/12000
2024-11-05 06:18:18,123 - INFO - [diffusion][Epoch 11208] diffusion training Loss: 0.04482619930058718
2024-11-05 06:18:18,125 - INFO - [diffusion][Epoch 11208] diffusion learning rate: 0.001
2024-11-05 06:18:18,127 - INFO - [diffusion][Epoch 11208] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:18,128 - INFO - [diffusion][Epoch 11209] Epoch 11210/12000
2024-11-05 06:18:22,183 - INFO - [diffusion][Epoch 11209] diffusion training Loss: 0.044660584069788456
2024-11-05 06:18:22,185 - INFO - [diffusion][Epoch 11209] diffusion learning rate: 0.001
2024-11-05 06:18:22,213 - INFO - [diffusion][Epoch 11209] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:22,215 - INFO - [diffusion][Epoch 11210] Epoch 11211/12000
2024-11-05 06:18:26,323 - INFO - [diffusion][Epoch 11210] diffusion training Loss: 0.046392181888222694
2024-11-05 06:18:26,325 - INFO - [diffusion][Epoch 11210] diffusion learning rate: 0.001
2024-11-05 06:18:26,326 - INFO - [diffusion][Epoch 11210] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:26,328 - INFO - [diffusion][Epoch 11211] Epoch 11212/12000
2024-11-05 06:18:30,354 - INFO - [diffusion][Epoch 11211] diffusion training Loss: 0.05193200893700123
2024-11-05 06:18:30,356 - INFO - [diffusion][Epoch 11211] diffusion learning rate: 0.001
2024-11-05 06:18:30,358 - INFO - [diffusion][Epoch 11211] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:30,359 - INFO - [diffusion][Epoch 11212] Epoch 11213/12000
2024-11-05 06:18:34,398 - INFO - [diffusion][Epoch 11212] diffusion training Loss: 0.04326779302209616
2024-11-05 06:18:34,400 - INFO - [diffusion][Epoch 11212] diffusion learning rate: 0.001
2024-11-05 06:18:34,402 - INFO - [diffusion][Epoch 11212] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:34,403 - INFO - [diffusion][Epoch 11213] Epoch 11214/12000
2024-11-05 06:18:38,517 - INFO - [diffusion][Epoch 11213] diffusion training Loss: 0.044914813712239265
2024-11-05 06:18:38,519 - INFO - [diffusion][Epoch 11213] diffusion learning rate: 0.001
2024-11-05 06:18:38,520 - INFO - [diffusion][Epoch 11213] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:38,521 - INFO - [diffusion][Epoch 11214] Epoch 11215/12000
2024-11-05 06:18:42,578 - INFO - [diffusion][Epoch 11214] diffusion training Loss: 0.04340529441833496
2024-11-05 06:18:42,580 - INFO - [diffusion][Epoch 11214] diffusion learning rate: 0.001
2024-11-05 06:18:42,581 - INFO - [diffusion][Epoch 11214] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:42,583 - INFO - [diffusion][Epoch 11215] Epoch 11216/12000
2024-11-05 06:18:46,644 - INFO - [diffusion][Epoch 11215] diffusion training Loss: 0.0453523863106966
2024-11-05 06:18:46,646 - INFO - [diffusion][Epoch 11215] diffusion learning rate: 0.001
2024-11-05 06:18:46,648 - INFO - [diffusion][Epoch 11215] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:46,649 - INFO - [diffusion][Epoch 11216] Epoch 11217/12000
2024-11-05 06:18:50,773 - INFO - [diffusion][Epoch 11216] diffusion training Loss: 0.04950389452278614
2024-11-05 06:18:50,775 - INFO - [diffusion][Epoch 11216] diffusion learning rate: 0.001
2024-11-05 06:18:50,777 - INFO - [diffusion][Epoch 11216] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:50,778 - INFO - [diffusion][Epoch 11217] Epoch 11218/12000
2024-11-05 06:18:54,833 - INFO - [diffusion][Epoch 11217] diffusion training Loss: 0.04367684479802847
2024-11-05 06:18:54,835 - INFO - [diffusion][Epoch 11217] diffusion learning rate: 0.001
2024-11-05 06:18:54,836 - INFO - [diffusion][Epoch 11217] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:54,838 - INFO - [diffusion][Epoch 11218] Epoch 11219/12000
2024-11-05 06:18:58,896 - INFO - [diffusion][Epoch 11218] diffusion training Loss: 0.04707405623048544
2024-11-05 06:18:58,900 - INFO - [diffusion][Epoch 11218] diffusion learning rate: 0.001
2024-11-05 06:18:58,901 - INFO - [diffusion][Epoch 11218] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:18:58,903 - INFO - [diffusion][Epoch 11219] Epoch 11220/12000
2024-11-05 06:19:02,961 - INFO - [diffusion][Epoch 11219] diffusion training Loss: 0.04530880134552717
2024-11-05 06:19:02,964 - INFO - [diffusion][Epoch 11219] diffusion learning rate: 0.001
2024-11-05 06:19:02,965 - INFO - [diffusion][Epoch 11219] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:02,967 - INFO - [diffusion][Epoch 11220] Epoch 11221/12000
2024-11-05 06:19:06,887 - INFO - [diffusion][Epoch 11220] diffusion training Loss: 0.05129174515604973
2024-11-05 06:19:06,889 - INFO - [diffusion][Epoch 11220] diffusion learning rate: 0.001
2024-11-05 06:19:06,890 - INFO - [diffusion][Epoch 11220] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:06,892 - INFO - [diffusion][Epoch 11221] Epoch 11222/12000
2024-11-05 06:19:10,989 - INFO - [diffusion][Epoch 11221] diffusion training Loss: 0.05159333813935518
2024-11-05 06:19:10,991 - INFO - [diffusion][Epoch 11221] diffusion learning rate: 0.001
2024-11-05 06:19:10,992 - INFO - [diffusion][Epoch 11221] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:10,994 - INFO - [diffusion][Epoch 11222] Epoch 11223/12000
2024-11-05 06:19:15,070 - INFO - [diffusion][Epoch 11222] diffusion training Loss: 0.05300839338451624
2024-11-05 06:19:15,072 - INFO - [diffusion][Epoch 11222] diffusion learning rate: 0.001
2024-11-05 06:19:15,074 - INFO - [diffusion][Epoch 11222] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:15,075 - INFO - [diffusion][Epoch 11223] Epoch 11224/12000
2024-11-05 06:19:19,129 - INFO - [diffusion][Epoch 11223] diffusion training Loss: 0.044668085873126984
2024-11-05 06:19:19,131 - INFO - [diffusion][Epoch 11223] diffusion learning rate: 0.001
2024-11-05 06:19:19,133 - INFO - [diffusion][Epoch 11223] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:19,134 - INFO - [diffusion][Epoch 11224] Epoch 11225/12000
2024-11-05 06:19:23,284 - INFO - [diffusion][Epoch 11224] diffusion training Loss: 0.04622643534094095
2024-11-05 06:19:23,286 - INFO - [diffusion][Epoch 11224] diffusion learning rate: 0.001
2024-11-05 06:19:23,288 - INFO - [diffusion][Epoch 11224] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:23,289 - INFO - [diffusion][Epoch 11225] Epoch 11226/12000
2024-11-05 06:19:27,392 - INFO - [diffusion][Epoch 11225] diffusion training Loss: 0.05093700252473354
2024-11-05 06:19:27,394 - INFO - [diffusion][Epoch 11225] diffusion learning rate: 0.001
2024-11-05 06:19:27,396 - INFO - [diffusion][Epoch 11225] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:27,397 - INFO - [diffusion][Epoch 11226] Epoch 11227/12000
2024-11-05 06:19:31,372 - INFO - [diffusion][Epoch 11226] diffusion training Loss: 0.04505511559545994
2024-11-05 06:19:31,374 - INFO - [diffusion][Epoch 11226] diffusion learning rate: 0.001
2024-11-05 06:19:31,404 - INFO - [diffusion][Epoch 11226] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:31,406 - INFO - [diffusion][Epoch 11227] Epoch 11228/12000
2024-11-05 06:19:35,483 - INFO - [diffusion][Epoch 11227] diffusion training Loss: 0.04557718802243471
2024-11-05 06:19:35,485 - INFO - [diffusion][Epoch 11227] diffusion learning rate: 0.001
2024-11-05 06:19:35,487 - INFO - [diffusion][Epoch 11227] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:35,488 - INFO - [diffusion][Epoch 11228] Epoch 11229/12000
2024-11-05 06:19:39,594 - INFO - [diffusion][Epoch 11228] diffusion training Loss: 0.049338776618242264
2024-11-05 06:19:39,596 - INFO - [diffusion][Epoch 11228] diffusion learning rate: 0.001
2024-11-05 06:19:39,597 - INFO - [diffusion][Epoch 11228] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:39,598 - INFO - [diffusion][Epoch 11229] Epoch 11230/12000
2024-11-05 06:19:43,705 - INFO - [diffusion][Epoch 11229] diffusion training Loss: 0.0485391840338707
2024-11-05 06:19:43,707 - INFO - [diffusion][Epoch 11229] diffusion learning rate: 0.001
2024-11-05 06:19:43,709 - INFO - [diffusion][Epoch 11229] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:43,710 - INFO - [diffusion][Epoch 11230] Epoch 11231/12000
2024-11-05 06:19:47,810 - INFO - [diffusion][Epoch 11230] diffusion training Loss: 0.045829229056835175
2024-11-05 06:19:47,812 - INFO - [diffusion][Epoch 11230] diffusion learning rate: 0.001
2024-11-05 06:19:47,813 - INFO - [diffusion][Epoch 11230] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:47,815 - INFO - [diffusion][Epoch 11231] Epoch 11232/12000
2024-11-05 06:19:51,891 - INFO - [diffusion][Epoch 11231] diffusion training Loss: 0.04448920488357544
2024-11-05 06:19:51,893 - INFO - [diffusion][Epoch 11231] diffusion learning rate: 0.001
2024-11-05 06:19:51,895 - INFO - [diffusion][Epoch 11231] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:51,896 - INFO - [diffusion][Epoch 11232] Epoch 11233/12000
2024-11-05 06:19:55,902 - INFO - [diffusion][Epoch 11232] diffusion training Loss: 0.04948724713176489
2024-11-05 06:19:55,904 - INFO - [diffusion][Epoch 11232] diffusion learning rate: 0.001
2024-11-05 06:19:55,906 - INFO - [diffusion][Epoch 11232] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:55,907 - INFO - [diffusion][Epoch 11233] Epoch 11234/12000
2024-11-05 06:19:59,874 - INFO - [diffusion][Epoch 11233] diffusion training Loss: 0.04204961098730564
2024-11-05 06:19:59,877 - INFO - [diffusion][Epoch 11233] diffusion learning rate: 0.001
2024-11-05 06:19:59,879 - INFO - [diffusion][Epoch 11233] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:19:59,880 - INFO - [diffusion][Epoch 11234] Epoch 11235/12000
2024-11-05 06:20:03,956 - INFO - [diffusion][Epoch 11234] diffusion training Loss: 0.049404993653297424
2024-11-05 06:20:03,958 - INFO - [diffusion][Epoch 11234] diffusion learning rate: 0.001
2024-11-05 06:20:03,960 - INFO - [diffusion][Epoch 11234] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:03,961 - INFO - [diffusion][Epoch 11235] Epoch 11236/12000
2024-11-05 06:20:08,046 - INFO - [diffusion][Epoch 11235] diffusion training Loss: 0.04759923368692398
2024-11-05 06:20:08,048 - INFO - [diffusion][Epoch 11235] diffusion learning rate: 0.001
2024-11-05 06:20:08,050 - INFO - [diffusion][Epoch 11235] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:08,051 - INFO - [diffusion][Epoch 11236] Epoch 11237/12000
2024-11-05 06:20:12,198 - INFO - [diffusion][Epoch 11236] diffusion training Loss: 0.044589826837182045
2024-11-05 06:20:12,200 - INFO - [diffusion][Epoch 11236] diffusion learning rate: 0.001
2024-11-05 06:20:12,202 - INFO - [diffusion][Epoch 11236] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:12,203 - INFO - [diffusion][Epoch 11237] Epoch 11238/12000
2024-11-05 06:20:16,103 - INFO - [diffusion][Epoch 11237] diffusion training Loss: 0.0488040903583169
2024-11-05 06:20:16,106 - INFO - [diffusion][Epoch 11237] diffusion learning rate: 0.001
2024-11-05 06:20:16,108 - INFO - [diffusion][Epoch 11237] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:16,110 - INFO - [diffusion][Epoch 11238] Epoch 11239/12000
2024-11-05 06:20:20,168 - INFO - [diffusion][Epoch 11238] diffusion training Loss: 0.05413350462913513
2024-11-05 06:20:20,170 - INFO - [diffusion][Epoch 11238] diffusion learning rate: 0.001
2024-11-05 06:20:20,171 - INFO - [diffusion][Epoch 11238] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:20,173 - INFO - [diffusion][Epoch 11239] Epoch 11240/12000
2024-11-05 06:20:24,288 - INFO - [diffusion][Epoch 11239] diffusion training Loss: 0.05121296923607588
2024-11-05 06:20:24,289 - INFO - [diffusion][Epoch 11239] diffusion learning rate: 0.001
2024-11-05 06:20:24,291 - INFO - [diffusion][Epoch 11239] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:24,292 - INFO - [diffusion][Epoch 11240] Epoch 11241/12000
2024-11-05 06:20:28,419 - INFO - [diffusion][Epoch 11240] diffusion training Loss: 0.04478255566209555
2024-11-05 06:20:28,421 - INFO - [diffusion][Epoch 11240] diffusion learning rate: 0.001
2024-11-05 06:20:28,422 - INFO - [diffusion][Epoch 11240] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:28,424 - INFO - [diffusion][Epoch 11241] Epoch 11242/12000
2024-11-05 06:20:32,480 - INFO - [diffusion][Epoch 11241] diffusion training Loss: 0.04984008055180311
2024-11-05 06:20:32,482 - INFO - [diffusion][Epoch 11241] diffusion learning rate: 0.001
2024-11-05 06:20:32,484 - INFO - [diffusion][Epoch 11241] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:32,485 - INFO - [diffusion][Epoch 11242] Epoch 11243/12000
2024-11-05 06:20:36,546 - INFO - [diffusion][Epoch 11242] diffusion training Loss: 0.043965933844447136
2024-11-05 06:20:36,548 - INFO - [diffusion][Epoch 11242] diffusion learning rate: 0.001
2024-11-05 06:20:36,550 - INFO - [diffusion][Epoch 11242] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:36,551 - INFO - [diffusion][Epoch 11243] Epoch 11244/12000
2024-11-05 06:20:40,642 - INFO - [diffusion][Epoch 11243] diffusion training Loss: 0.04861751198768616
2024-11-05 06:20:40,644 - INFO - [diffusion][Epoch 11243] diffusion learning rate: 0.001
2024-11-05 06:20:40,646 - INFO - [diffusion][Epoch 11243] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:40,647 - INFO - [diffusion][Epoch 11244] Epoch 11245/12000
2024-11-05 06:20:44,758 - INFO - [diffusion][Epoch 11244] diffusion training Loss: 0.05074779596179724
2024-11-05 06:20:44,760 - INFO - [diffusion][Epoch 11244] diffusion learning rate: 0.001
2024-11-05 06:20:44,762 - INFO - [diffusion][Epoch 11244] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:44,763 - INFO - [diffusion][Epoch 11245] Epoch 11246/12000
2024-11-05 06:20:48,887 - INFO - [diffusion][Epoch 11245] diffusion training Loss: 0.04448543209582567
2024-11-05 06:20:48,889 - INFO - [diffusion][Epoch 11245] diffusion learning rate: 0.001
2024-11-05 06:20:48,891 - INFO - [diffusion][Epoch 11245] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:48,892 - INFO - [diffusion][Epoch 11246] Epoch 11247/12000
2024-11-05 06:20:53,011 - INFO - [diffusion][Epoch 11246] diffusion training Loss: 0.04702966660261154
2024-11-05 06:20:53,013 - INFO - [diffusion][Epoch 11246] diffusion learning rate: 0.001
2024-11-05 06:20:53,015 - INFO - [diffusion][Epoch 11246] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:53,016 - INFO - [diffusion][Epoch 11247] Epoch 11248/12000
2024-11-05 06:20:57,118 - INFO - [diffusion][Epoch 11247] diffusion training Loss: 0.04783589951694012
2024-11-05 06:20:57,120 - INFO - [diffusion][Epoch 11247] diffusion learning rate: 0.001
2024-11-05 06:20:57,122 - INFO - [diffusion][Epoch 11247] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:20:57,124 - INFO - [diffusion][Epoch 11248] Epoch 11249/12000
2024-11-05 06:21:01,211 - INFO - [diffusion][Epoch 11248] diffusion training Loss: 0.05313359014689922
2024-11-05 06:21:01,214 - INFO - [diffusion][Epoch 11248] diffusion learning rate: 0.001
2024-11-05 06:21:01,216 - INFO - [diffusion][Epoch 11248] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:01,218 - INFO - [diffusion][Epoch 11249] Epoch 11250/12000
2024-11-05 06:21:05,324 - INFO - [diffusion][Epoch 11249] diffusion training Loss: 0.0415284251794219
2024-11-05 06:21:05,326 - INFO - [diffusion][Epoch 11249] diffusion learning rate: 0.001
2024-11-05 06:21:05,328 - INFO - [diffusion][Epoch 11249] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:05,329 - INFO - [diffusion][Epoch 11250] Epoch 11251/12000
2024-11-05 06:21:09,377 - INFO - [diffusion][Epoch 11250] diffusion training Loss: 0.050382958725094795
2024-11-05 06:21:09,379 - INFO - [diffusion][Epoch 11250] diffusion learning rate: 0.001
2024-11-05 06:21:09,381 - INFO - [diffusion][Epoch 11250] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:09,382 - INFO - [diffusion][Epoch 11251] Epoch 11252/12000
2024-11-05 06:21:13,425 - INFO - [diffusion][Epoch 11251] diffusion training Loss: 0.04862857423722744
2024-11-05 06:21:13,427 - INFO - [diffusion][Epoch 11251] diffusion learning rate: 0.001
2024-11-05 06:21:13,428 - INFO - [diffusion][Epoch 11251] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:13,430 - INFO - [diffusion][Epoch 11252] Epoch 11253/12000
2024-11-05 06:21:18,143 - INFO - [diffusion][Epoch 11252] diffusion training Loss: 0.04492334183305502
2024-11-05 06:21:18,144 - INFO - [diffusion][Epoch 11252] diffusion learning rate: 0.001
2024-11-05 06:21:18,146 - INFO - [diffusion][Epoch 11252] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:18,147 - INFO - [diffusion][Epoch 11253] Epoch 11254/12000
2024-11-05 06:21:22,227 - INFO - [diffusion][Epoch 11253] diffusion training Loss: 0.04839929938316345
2024-11-05 06:21:22,229 - INFO - [diffusion][Epoch 11253] diffusion learning rate: 0.001
2024-11-05 06:21:22,256 - INFO - [diffusion][Epoch 11253] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:22,257 - INFO - [diffusion][Epoch 11254] Epoch 11255/12000
2024-11-05 06:21:26,350 - INFO - [diffusion][Epoch 11254] diffusion training Loss: 0.051374693401157856
2024-11-05 06:21:26,352 - INFO - [diffusion][Epoch 11254] diffusion learning rate: 0.001
2024-11-05 06:21:26,353 - INFO - [diffusion][Epoch 11254] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:26,355 - INFO - [diffusion][Epoch 11255] Epoch 11256/12000
2024-11-05 06:21:30,456 - INFO - [diffusion][Epoch 11255] diffusion training Loss: 0.04263748601078987
2024-11-05 06:21:30,496 - INFO - [diffusion][Epoch 11255] diffusion learning rate: 0.001
2024-11-05 06:21:30,497 - INFO - [diffusion][Epoch 11255] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:30,499 - INFO - [diffusion][Epoch 11256] Epoch 11257/12000
2024-11-05 06:21:34,595 - INFO - [diffusion][Epoch 11256] diffusion training Loss: 0.04288502037525177
2024-11-05 06:21:34,597 - INFO - [diffusion][Epoch 11256] diffusion learning rate: 0.001
2024-11-05 06:21:34,599 - INFO - [diffusion][Epoch 11256] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:34,600 - INFO - [diffusion][Epoch 11257] Epoch 11258/12000
2024-11-05 06:21:38,646 - INFO - [diffusion][Epoch 11257] diffusion training Loss: 0.04785313177853823
2024-11-05 06:21:38,647 - INFO - [diffusion][Epoch 11257] diffusion learning rate: 0.001
2024-11-05 06:21:38,649 - INFO - [diffusion][Epoch 11257] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:38,650 - INFO - [diffusion][Epoch 11258] Epoch 11259/12000
2024-11-05 06:21:42,776 - INFO - [diffusion][Epoch 11258] diffusion training Loss: 0.04609122686088085
2024-11-05 06:21:42,778 - INFO - [diffusion][Epoch 11258] diffusion learning rate: 0.001
2024-11-05 06:21:42,779 - INFO - [diffusion][Epoch 11258] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:42,781 - INFO - [diffusion][Epoch 11259] Epoch 11260/12000
2024-11-05 06:21:46,876 - INFO - [diffusion][Epoch 11259] diffusion training Loss: 0.048744143918156624
2024-11-05 06:21:46,878 - INFO - [diffusion][Epoch 11259] diffusion learning rate: 0.001
2024-11-05 06:21:46,880 - INFO - [diffusion][Epoch 11259] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:46,881 - INFO - [diffusion][Epoch 11260] Epoch 11261/12000
2024-11-05 06:21:50,976 - INFO - [diffusion][Epoch 11260] diffusion training Loss: 0.044113109819591045
2024-11-05 06:21:50,977 - INFO - [diffusion][Epoch 11260] diffusion learning rate: 0.001
2024-11-05 06:21:50,979 - INFO - [diffusion][Epoch 11260] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:50,980 - INFO - [diffusion][Epoch 11261] Epoch 11262/12000
2024-11-05 06:21:55,077 - INFO - [diffusion][Epoch 11261] diffusion training Loss: 0.04590950906276703
2024-11-05 06:21:55,079 - INFO - [diffusion][Epoch 11261] diffusion learning rate: 0.001
2024-11-05 06:21:55,081 - INFO - [diffusion][Epoch 11261] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:55,082 - INFO - [diffusion][Epoch 11262] Epoch 11263/12000
2024-11-05 06:21:59,149 - INFO - [diffusion][Epoch 11262] diffusion training Loss: 0.043156412430107594
2024-11-05 06:21:59,151 - INFO - [diffusion][Epoch 11262] diffusion learning rate: 0.001
2024-11-05 06:21:59,184 - INFO - [diffusion][Epoch 11262] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:21:59,186 - INFO - [diffusion][Epoch 11263] Epoch 11264/12000
2024-11-05 06:22:03,254 - INFO - [diffusion][Epoch 11263] diffusion training Loss: 0.04471457190811634
2024-11-05 06:22:03,257 - INFO - [diffusion][Epoch 11263] diffusion learning rate: 0.001
2024-11-05 06:22:03,259 - INFO - [diffusion][Epoch 11263] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:03,260 - INFO - [diffusion][Epoch 11264] Epoch 11265/12000
2024-11-05 06:22:07,363 - INFO - [diffusion][Epoch 11264] diffusion training Loss: 0.04392827581614256
2024-11-05 06:22:07,365 - INFO - [diffusion][Epoch 11264] diffusion learning rate: 0.001
2024-11-05 06:22:07,367 - INFO - [diffusion][Epoch 11264] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:07,368 - INFO - [diffusion][Epoch 11265] Epoch 11266/12000
2024-11-05 06:22:11,472 - INFO - [diffusion][Epoch 11265] diffusion training Loss: 0.04927742946892977
2024-11-05 06:22:11,474 - INFO - [diffusion][Epoch 11265] diffusion learning rate: 0.001
2024-11-05 06:22:11,476 - INFO - [diffusion][Epoch 11265] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:11,477 - INFO - [diffusion][Epoch 11266] Epoch 11267/12000
2024-11-05 06:22:15,586 - INFO - [diffusion][Epoch 11266] diffusion training Loss: 0.05082213785499334
2024-11-05 06:22:15,588 - INFO - [diffusion][Epoch 11266] diffusion learning rate: 0.001
2024-11-05 06:22:15,617 - INFO - [diffusion][Epoch 11266] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:15,618 - INFO - [diffusion][Epoch 11267] Epoch 11268/12000
2024-11-05 06:22:19,717 - INFO - [diffusion][Epoch 11267] diffusion training Loss: 0.05141014605760574
2024-11-05 06:22:19,719 - INFO - [diffusion][Epoch 11267] diffusion learning rate: 0.001
2024-11-05 06:22:19,721 - INFO - [diffusion][Epoch 11267] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:19,722 - INFO - [diffusion][Epoch 11268] Epoch 11269/12000
2024-11-05 06:22:23,818 - INFO - [diffusion][Epoch 11268] diffusion training Loss: 0.05149254109710455
2024-11-05 06:22:23,820 - INFO - [diffusion][Epoch 11268] diffusion learning rate: 0.001
2024-11-05 06:22:23,822 - INFO - [diffusion][Epoch 11268] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:23,823 - INFO - [diffusion][Epoch 11269] Epoch 11270/12000
2024-11-05 06:22:27,937 - INFO - [diffusion][Epoch 11269] diffusion training Loss: 0.04646886605769396
2024-11-05 06:22:27,939 - INFO - [diffusion][Epoch 11269] diffusion learning rate: 0.001
2024-11-05 06:22:27,941 - INFO - [diffusion][Epoch 11269] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:27,942 - INFO - [diffusion][Epoch 11270] Epoch 11271/12000
2024-11-05 06:22:32,020 - INFO - [diffusion][Epoch 11270] diffusion training Loss: 0.05488209240138531
2024-11-05 06:22:32,022 - INFO - [diffusion][Epoch 11270] diffusion learning rate: 0.001
2024-11-05 06:22:32,050 - INFO - [diffusion][Epoch 11270] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:32,051 - INFO - [diffusion][Epoch 11271] Epoch 11272/12000
2024-11-05 06:22:36,171 - INFO - [diffusion][Epoch 11271] diffusion training Loss: 0.04510998260229826
2024-11-05 06:22:36,173 - INFO - [diffusion][Epoch 11271] diffusion learning rate: 0.001
2024-11-05 06:22:36,175 - INFO - [diffusion][Epoch 11271] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:36,176 - INFO - [diffusion][Epoch 11272] Epoch 11273/12000
2024-11-05 06:22:40,268 - INFO - [diffusion][Epoch 11272] diffusion training Loss: 0.04820571467280388
2024-11-05 06:22:40,270 - INFO - [diffusion][Epoch 11272] diffusion learning rate: 0.001
2024-11-05 06:22:40,271 - INFO - [diffusion][Epoch 11272] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:40,273 - INFO - [diffusion][Epoch 11273] Epoch 11274/12000
2024-11-05 06:22:44,646 - INFO - [diffusion][Epoch 11273] diffusion training Loss: 0.046896910294890404
2024-11-05 06:22:44,648 - INFO - [diffusion][Epoch 11273] diffusion learning rate: 0.001
2024-11-05 06:22:44,682 - INFO - [diffusion][Epoch 11273] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:44,683 - INFO - [diffusion][Epoch 11274] Epoch 11275/12000
2024-11-05 06:22:48,792 - INFO - [diffusion][Epoch 11274] diffusion training Loss: 0.05393561813980341
2024-11-05 06:22:48,794 - INFO - [diffusion][Epoch 11274] diffusion learning rate: 0.001
2024-11-05 06:22:48,796 - INFO - [diffusion][Epoch 11274] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:48,797 - INFO - [diffusion][Epoch 11275] Epoch 11276/12000
2024-11-05 06:22:52,890 - INFO - [diffusion][Epoch 11275] diffusion training Loss: 0.0480349026620388
2024-11-05 06:22:52,892 - INFO - [diffusion][Epoch 11275] diffusion learning rate: 0.001
2024-11-05 06:22:52,894 - INFO - [diffusion][Epoch 11275] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:52,895 - INFO - [diffusion][Epoch 11276] Epoch 11277/12000
2024-11-05 06:22:56,958 - INFO - [diffusion][Epoch 11276] diffusion training Loss: 0.04696954786777496
2024-11-05 06:22:56,960 - INFO - [diffusion][Epoch 11276] diffusion learning rate: 0.001
2024-11-05 06:22:56,962 - INFO - [diffusion][Epoch 11276] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:22:56,963 - INFO - [diffusion][Epoch 11277] Epoch 11278/12000
2024-11-05 06:23:01,051 - INFO - [diffusion][Epoch 11277] diffusion training Loss: 0.049127777107059956
2024-11-05 06:23:01,053 - INFO - [diffusion][Epoch 11277] diffusion learning rate: 0.001
2024-11-05 06:23:01,103 - INFO - [diffusion][Epoch 11277] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:01,105 - INFO - [diffusion][Epoch 11278] Epoch 11279/12000
2024-11-05 06:23:05,260 - INFO - [diffusion][Epoch 11278] diffusion training Loss: 0.04619508609175682
2024-11-05 06:23:05,264 - INFO - [diffusion][Epoch 11278] diffusion learning rate: 0.001
2024-11-05 06:23:05,266 - INFO - [diffusion][Epoch 11278] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:05,267 - INFO - [diffusion][Epoch 11279] Epoch 11280/12000
2024-11-05 06:23:09,412 - INFO - [diffusion][Epoch 11279] diffusion training Loss: 0.04602209758013487
2024-11-05 06:23:09,415 - INFO - [diffusion][Epoch 11279] diffusion learning rate: 0.001
2024-11-05 06:23:09,416 - INFO - [diffusion][Epoch 11279] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:09,418 - INFO - [diffusion][Epoch 11280] Epoch 11281/12000
2024-11-05 06:23:13,512 - INFO - [diffusion][Epoch 11280] diffusion training Loss: 0.046620676293969154
2024-11-05 06:23:13,514 - INFO - [diffusion][Epoch 11280] diffusion learning rate: 0.001
2024-11-05 06:23:13,516 - INFO - [diffusion][Epoch 11280] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:13,517 - INFO - [diffusion][Epoch 11281] Epoch 11282/12000
2024-11-05 06:23:17,648 - INFO - [diffusion][Epoch 11281] diffusion training Loss: 0.047079441137611866
2024-11-05 06:23:17,650 - INFO - [diffusion][Epoch 11281] diffusion learning rate: 0.001
2024-11-05 06:23:17,652 - INFO - [diffusion][Epoch 11281] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:17,653 - INFO - [diffusion][Epoch 11282] Epoch 11283/12000
2024-11-05 06:23:21,744 - INFO - [diffusion][Epoch 11282] diffusion training Loss: 0.05059878248721361
2024-11-05 06:23:21,746 - INFO - [diffusion][Epoch 11282] diffusion learning rate: 0.001
2024-11-05 06:23:21,748 - INFO - [diffusion][Epoch 11282] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:21,749 - INFO - [diffusion][Epoch 11283] Epoch 11284/12000
2024-11-05 06:23:25,873 - INFO - [diffusion][Epoch 11283] diffusion training Loss: 0.0517391050234437
2024-11-05 06:23:25,875 - INFO - [diffusion][Epoch 11283] diffusion learning rate: 0.001
2024-11-05 06:23:25,877 - INFO - [diffusion][Epoch 11283] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:25,878 - INFO - [diffusion][Epoch 11284] Epoch 11285/12000
2024-11-05 06:23:29,993 - INFO - [diffusion][Epoch 11284] diffusion training Loss: 0.04040366783738136
2024-11-05 06:23:29,994 - INFO - [diffusion][Epoch 11284] diffusion learning rate: 0.001
2024-11-05 06:23:29,996 - INFO - [diffusion][Epoch 11284] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:29,997 - INFO - [diffusion][Epoch 11285] Epoch 11286/12000
2024-11-05 06:23:34,120 - INFO - [diffusion][Epoch 11285] diffusion training Loss: 0.049686518497765064
2024-11-05 06:23:34,122 - INFO - [diffusion][Epoch 11285] diffusion learning rate: 0.001
2024-11-05 06:23:34,124 - INFO - [diffusion][Epoch 11285] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:34,125 - INFO - [diffusion][Epoch 11286] Epoch 11287/12000
2024-11-05 06:23:38,246 - INFO - [diffusion][Epoch 11286] diffusion training Loss: 0.05188530497252941
2024-11-05 06:23:38,248 - INFO - [diffusion][Epoch 11286] diffusion learning rate: 0.001
2024-11-05 06:23:38,249 - INFO - [diffusion][Epoch 11286] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:38,251 - INFO - [diffusion][Epoch 11287] Epoch 11288/12000
2024-11-05 06:23:42,345 - INFO - [diffusion][Epoch 11287] diffusion training Loss: 0.04117732401937246
2024-11-05 06:23:42,347 - INFO - [diffusion][Epoch 11287] diffusion learning rate: 0.001
2024-11-05 06:23:42,349 - INFO - [diffusion][Epoch 11287] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:42,350 - INFO - [diffusion][Epoch 11288] Epoch 11289/12000
2024-11-05 06:23:46,457 - INFO - [diffusion][Epoch 11288] diffusion training Loss: 0.049510519951581955
2024-11-05 06:23:46,459 - INFO - [diffusion][Epoch 11288] diffusion learning rate: 0.001
2024-11-05 06:23:46,461 - INFO - [diffusion][Epoch 11288] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:46,462 - INFO - [diffusion][Epoch 11289] Epoch 11290/12000
2024-11-05 06:23:50,544 - INFO - [diffusion][Epoch 11289] diffusion training Loss: 0.04699922353029251
2024-11-05 06:23:50,546 - INFO - [diffusion][Epoch 11289] diffusion learning rate: 0.001
2024-11-05 06:23:50,548 - INFO - [diffusion][Epoch 11289] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:50,549 - INFO - [diffusion][Epoch 11290] Epoch 11291/12000
2024-11-05 06:23:54,665 - INFO - [diffusion][Epoch 11290] diffusion training Loss: 0.04444084968417883
2024-11-05 06:23:54,667 - INFO - [diffusion][Epoch 11290] diffusion learning rate: 0.001
2024-11-05 06:23:54,669 - INFO - [diffusion][Epoch 11290] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:54,670 - INFO - [diffusion][Epoch 11291] Epoch 11292/12000
2024-11-05 06:23:58,715 - INFO - [diffusion][Epoch 11291] diffusion training Loss: 0.05136997625231743
2024-11-05 06:23:58,717 - INFO - [diffusion][Epoch 11291] diffusion learning rate: 0.001
2024-11-05 06:23:58,718 - INFO - [diffusion][Epoch 11291] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:23:58,720 - INFO - [diffusion][Epoch 11292] Epoch 11293/12000
2024-11-05 06:24:02,866 - INFO - [diffusion][Epoch 11292] diffusion training Loss: 0.05182797648012638
2024-11-05 06:24:02,868 - INFO - [diffusion][Epoch 11292] diffusion learning rate: 0.001
2024-11-05 06:24:02,870 - INFO - [diffusion][Epoch 11292] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:02,871 - INFO - [diffusion][Epoch 11293] Epoch 11294/12000
2024-11-05 06:24:06,948 - INFO - [diffusion][Epoch 11293] diffusion training Loss: 0.05061189737170935
2024-11-05 06:24:06,951 - INFO - [diffusion][Epoch 11293] diffusion learning rate: 0.001
2024-11-05 06:24:06,953 - INFO - [diffusion][Epoch 11293] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:06,954 - INFO - [diffusion][Epoch 11294] Epoch 11295/12000
2024-11-05 06:24:11,037 - INFO - [diffusion][Epoch 11294] diffusion training Loss: 0.053324238397181034
2024-11-05 06:24:11,039 - INFO - [diffusion][Epoch 11294] diffusion learning rate: 0.001
2024-11-05 06:24:11,041 - INFO - [diffusion][Epoch 11294] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:11,042 - INFO - [diffusion][Epoch 11295] Epoch 11296/12000
2024-11-05 06:24:15,393 - INFO - [diffusion][Epoch 11295] diffusion training Loss: 0.04905036184936762
2024-11-05 06:24:15,395 - INFO - [diffusion][Epoch 11295] diffusion learning rate: 0.001
2024-11-05 06:24:15,397 - INFO - [diffusion][Epoch 11295] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:15,398 - INFO - [diffusion][Epoch 11296] Epoch 11297/12000
2024-11-05 06:24:19,694 - INFO - [diffusion][Epoch 11296] diffusion training Loss: 0.05058781150728464
2024-11-05 06:24:19,696 - INFO - [diffusion][Epoch 11296] diffusion learning rate: 0.001
2024-11-05 06:24:19,698 - INFO - [diffusion][Epoch 11296] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:19,699 - INFO - [diffusion][Epoch 11297] Epoch 11298/12000
2024-11-05 06:24:23,794 - INFO - [diffusion][Epoch 11297] diffusion training Loss: 0.04791274666786194
2024-11-05 06:24:23,796 - INFO - [diffusion][Epoch 11297] diffusion learning rate: 0.001
2024-11-05 06:24:23,798 - INFO - [diffusion][Epoch 11297] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:23,799 - INFO - [diffusion][Epoch 11298] Epoch 11299/12000
2024-11-05 06:24:27,875 - INFO - [diffusion][Epoch 11298] diffusion training Loss: 0.04793171864002943
2024-11-05 06:24:27,877 - INFO - [diffusion][Epoch 11298] diffusion learning rate: 0.001
2024-11-05 06:24:27,879 - INFO - [diffusion][Epoch 11298] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:27,880 - INFO - [diffusion][Epoch 11299] Epoch 11300/12000
2024-11-05 06:24:31,960 - INFO - [diffusion][Epoch 11299] diffusion training Loss: 0.04903358966112137
2024-11-05 06:24:31,962 - INFO - [diffusion][Epoch 11299] diffusion learning rate: 0.001
2024-11-05 06:24:31,963 - INFO - [diffusion][Epoch 11299] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:31,967 - INFO - [diffusion][Epoch 11300] Epoch 11301/12000
2024-11-05 06:24:36,051 - INFO - [diffusion][Epoch 11300] diffusion training Loss: 0.04872766137123108
2024-11-05 06:24:36,053 - INFO - [diffusion][Epoch 11300] diffusion learning rate: 0.001
2024-11-05 06:24:36,054 - INFO - [diffusion][Epoch 11300] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:36,056 - INFO - [diffusion][Epoch 11301] Epoch 11302/12000
2024-11-05 06:24:40,135 - INFO - [diffusion][Epoch 11301] diffusion training Loss: 0.05054989084601402
2024-11-05 06:24:40,137 - INFO - [diffusion][Epoch 11301] diffusion learning rate: 0.001
2024-11-05 06:24:40,138 - INFO - [diffusion][Epoch 11301] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:40,139 - INFO - [diffusion][Epoch 11302] Epoch 11303/12000
2024-11-05 06:24:44,188 - INFO - [diffusion][Epoch 11302] diffusion training Loss: 0.04521602112799883
2024-11-05 06:24:44,189 - INFO - [diffusion][Epoch 11302] diffusion learning rate: 0.001
2024-11-05 06:24:44,191 - INFO - [diffusion][Epoch 11302] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:44,192 - INFO - [diffusion][Epoch 11303] Epoch 11304/12000
2024-11-05 06:24:48,273 - INFO - [diffusion][Epoch 11303] diffusion training Loss: 0.0476738540455699
2024-11-05 06:24:48,275 - INFO - [diffusion][Epoch 11303] diffusion learning rate: 0.001
2024-11-05 06:24:48,276 - INFO - [diffusion][Epoch 11303] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:48,277 - INFO - [diffusion][Epoch 11304] Epoch 11305/12000
2024-11-05 06:24:52,426 - INFO - [diffusion][Epoch 11304] diffusion training Loss: 0.04438929446041584
2024-11-05 06:24:52,428 - INFO - [diffusion][Epoch 11304] diffusion learning rate: 0.001
2024-11-05 06:24:52,429 - INFO - [diffusion][Epoch 11304] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:52,431 - INFO - [diffusion][Epoch 11305] Epoch 11306/12000
2024-11-05 06:24:56,563 - INFO - [diffusion][Epoch 11305] diffusion training Loss: 0.04526480566710234
2024-11-05 06:24:56,564 - INFO - [diffusion][Epoch 11305] diffusion learning rate: 0.001
2024-11-05 06:24:56,566 - INFO - [diffusion][Epoch 11305] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:24:56,567 - INFO - [diffusion][Epoch 11306] Epoch 11307/12000
2024-11-05 06:25:00,682 - INFO - [diffusion][Epoch 11306] diffusion training Loss: 0.046699908562004566
2024-11-05 06:25:00,684 - INFO - [diffusion][Epoch 11306] diffusion learning rate: 0.001
2024-11-05 06:25:00,686 - INFO - [diffusion][Epoch 11306] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:00,687 - INFO - [diffusion][Epoch 11307] Epoch 11308/12000
2024-11-05 06:25:04,772 - INFO - [diffusion][Epoch 11307] diffusion training Loss: 0.05312942620366812
2024-11-05 06:25:04,774 - INFO - [diffusion][Epoch 11307] diffusion learning rate: 0.001
2024-11-05 06:25:04,776 - INFO - [diffusion][Epoch 11307] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:04,777 - INFO - [diffusion][Epoch 11308] Epoch 11309/12000
2024-11-05 06:25:08,898 - INFO - [diffusion][Epoch 11308] diffusion training Loss: 0.048737465403974056
2024-11-05 06:25:08,902 - INFO - [diffusion][Epoch 11308] diffusion learning rate: 0.001
2024-11-05 06:25:08,903 - INFO - [diffusion][Epoch 11308] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:08,905 - INFO - [diffusion][Epoch 11309] Epoch 11310/12000
2024-11-05 06:25:12,976 - INFO - [diffusion][Epoch 11309] diffusion training Loss: 0.0515126995742321
2024-11-05 06:25:12,978 - INFO - [diffusion][Epoch 11309] diffusion learning rate: 0.001
2024-11-05 06:25:12,980 - INFO - [diffusion][Epoch 11309] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:12,981 - INFO - [diffusion][Epoch 11310] Epoch 11311/12000
2024-11-05 06:25:17,118 - INFO - [diffusion][Epoch 11310] diffusion training Loss: 0.04738042503595352
2024-11-05 06:25:17,120 - INFO - [diffusion][Epoch 11310] diffusion learning rate: 0.001
2024-11-05 06:25:17,122 - INFO - [diffusion][Epoch 11310] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:17,123 - INFO - [diffusion][Epoch 11311] Epoch 11312/12000
2024-11-05 06:25:21,211 - INFO - [diffusion][Epoch 11311] diffusion training Loss: 0.04590647201985121
2024-11-05 06:25:21,213 - INFO - [diffusion][Epoch 11311] diffusion learning rate: 0.001
2024-11-05 06:25:21,216 - INFO - [diffusion][Epoch 11311] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:21,217 - INFO - [diffusion][Epoch 11312] Epoch 11313/12000
2024-11-05 06:25:25,316 - INFO - [diffusion][Epoch 11312] diffusion training Loss: 0.04910140484571457
2024-11-05 06:25:25,318 - INFO - [diffusion][Epoch 11312] diffusion learning rate: 0.001
2024-11-05 06:25:25,320 - INFO - [diffusion][Epoch 11312] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:25,322 - INFO - [diffusion][Epoch 11313] Epoch 11314/12000
2024-11-05 06:25:29,401 - INFO - [diffusion][Epoch 11313] diffusion training Loss: 0.04665654059499502
2024-11-05 06:25:29,403 - INFO - [diffusion][Epoch 11313] diffusion learning rate: 0.001
2024-11-05 06:25:29,405 - INFO - [diffusion][Epoch 11313] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:29,406 - INFO - [diffusion][Epoch 11314] Epoch 11315/12000
2024-11-05 06:25:33,495 - INFO - [diffusion][Epoch 11314] diffusion training Loss: 0.04989912174642086
2024-11-05 06:25:33,497 - INFO - [diffusion][Epoch 11314] diffusion learning rate: 0.001
2024-11-05 06:25:33,498 - INFO - [diffusion][Epoch 11314] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:33,500 - INFO - [diffusion][Epoch 11315] Epoch 11316/12000
2024-11-05 06:25:37,854 - INFO - [diffusion][Epoch 11315] diffusion training Loss: 0.045253392308950424
2024-11-05 06:25:37,856 - INFO - [diffusion][Epoch 11315] diffusion learning rate: 0.001
2024-11-05 06:25:37,880 - INFO - [diffusion][Epoch 11315] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:37,882 - INFO - [diffusion][Epoch 11316] Epoch 11317/12000
2024-11-05 06:25:42,022 - INFO - [diffusion][Epoch 11316] diffusion training Loss: 0.04598967172205448
2024-11-05 06:25:42,024 - INFO - [diffusion][Epoch 11316] diffusion learning rate: 0.001
2024-11-05 06:25:42,026 - INFO - [diffusion][Epoch 11316] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:42,027 - INFO - [diffusion][Epoch 11317] Epoch 11318/12000
2024-11-05 06:25:46,118 - INFO - [diffusion][Epoch 11317] diffusion training Loss: 0.04944597836583853
2024-11-05 06:25:46,120 - INFO - [diffusion][Epoch 11317] diffusion learning rate: 0.001
2024-11-05 06:25:46,122 - INFO - [diffusion][Epoch 11317] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:46,123 - INFO - [diffusion][Epoch 11318] Epoch 11319/12000
2024-11-05 06:25:50,222 - INFO - [diffusion][Epoch 11318] diffusion training Loss: 0.04699159041047096
2024-11-05 06:25:50,224 - INFO - [diffusion][Epoch 11318] diffusion learning rate: 0.001
2024-11-05 06:25:50,225 - INFO - [diffusion][Epoch 11318] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:50,227 - INFO - [diffusion][Epoch 11319] Epoch 11320/12000
2024-11-05 06:25:54,315 - INFO - [diffusion][Epoch 11319] diffusion training Loss: 0.04583305399864912
2024-11-05 06:25:54,317 - INFO - [diffusion][Epoch 11319] diffusion learning rate: 0.001
2024-11-05 06:25:54,343 - INFO - [diffusion][Epoch 11319] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:54,345 - INFO - [diffusion][Epoch 11320] Epoch 11321/12000
2024-11-05 06:25:58,443 - INFO - [diffusion][Epoch 11320] diffusion training Loss: 0.04590100795030594
2024-11-05 06:25:58,445 - INFO - [diffusion][Epoch 11320] diffusion learning rate: 0.001
2024-11-05 06:25:58,447 - INFO - [diffusion][Epoch 11320] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:25:58,448 - INFO - [diffusion][Epoch 11321] Epoch 11322/12000
2024-11-05 06:26:02,576 - INFO - [diffusion][Epoch 11321] diffusion training Loss: 0.04284477233886719
2024-11-05 06:26:02,578 - INFO - [diffusion][Epoch 11321] diffusion learning rate: 0.001
2024-11-05 06:26:02,580 - INFO - [diffusion][Epoch 11321] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:02,581 - INFO - [diffusion][Epoch 11322] Epoch 11323/12000
2024-11-05 06:26:06,690 - INFO - [diffusion][Epoch 11322] diffusion training Loss: 0.046710198279470205
2024-11-05 06:26:06,692 - INFO - [diffusion][Epoch 11322] diffusion learning rate: 0.001
2024-11-05 06:26:06,694 - INFO - [diffusion][Epoch 11322] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:06,695 - INFO - [diffusion][Epoch 11323] Epoch 11324/12000
2024-11-05 06:26:10,732 - INFO - [diffusion][Epoch 11323] diffusion training Loss: 0.04508365038782358
2024-11-05 06:26:10,735 - INFO - [diffusion][Epoch 11323] diffusion learning rate: 0.001
2024-11-05 06:26:10,754 - INFO - [diffusion][Epoch 11323] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:10,756 - INFO - [diffusion][Epoch 11324] Epoch 11325/12000
2024-11-05 06:26:14,860 - INFO - [diffusion][Epoch 11324] diffusion training Loss: 0.05263776704668999
2024-11-05 06:26:14,862 - INFO - [diffusion][Epoch 11324] diffusion learning rate: 0.001
2024-11-05 06:26:14,864 - INFO - [diffusion][Epoch 11324] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:14,866 - INFO - [diffusion][Epoch 11325] Epoch 11326/12000
2024-11-05 06:26:18,967 - INFO - [diffusion][Epoch 11325] diffusion training Loss: 0.0500855278223753
2024-11-05 06:26:18,968 - INFO - [diffusion][Epoch 11325] diffusion learning rate: 0.001
2024-11-05 06:26:18,970 - INFO - [diffusion][Epoch 11325] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:18,972 - INFO - [diffusion][Epoch 11326] Epoch 11327/12000
2024-11-05 06:26:23,036 - INFO - [diffusion][Epoch 11326] diffusion training Loss: 0.04718548059463501
2024-11-05 06:26:23,038 - INFO - [diffusion][Epoch 11326] diffusion learning rate: 0.001
2024-11-05 06:26:23,040 - INFO - [diffusion][Epoch 11326] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:23,041 - INFO - [diffusion][Epoch 11327] Epoch 11328/12000
2024-11-05 06:26:27,164 - INFO - [diffusion][Epoch 11327] diffusion training Loss: 0.04586437251418829
2024-11-05 06:26:27,166 - INFO - [diffusion][Epoch 11327] diffusion learning rate: 0.001
2024-11-05 06:26:27,168 - INFO - [diffusion][Epoch 11327] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:27,193 - INFO - [diffusion][Epoch 11328] Epoch 11329/12000
2024-11-05 06:26:31,310 - INFO - [diffusion][Epoch 11328] diffusion training Loss: 0.045137062668800354
2024-11-05 06:26:31,312 - INFO - [diffusion][Epoch 11328] diffusion learning rate: 0.001
2024-11-05 06:26:31,313 - INFO - [diffusion][Epoch 11328] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:31,315 - INFO - [diffusion][Epoch 11329] Epoch 11330/12000
2024-11-05 06:26:35,404 - INFO - [diffusion][Epoch 11329] diffusion training Loss: 0.046828397549688816
2024-11-05 06:26:35,406 - INFO - [diffusion][Epoch 11329] diffusion learning rate: 0.001
2024-11-05 06:26:35,408 - INFO - [diffusion][Epoch 11329] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:35,409 - INFO - [diffusion][Epoch 11330] Epoch 11331/12000
2024-11-05 06:26:39,528 - INFO - [diffusion][Epoch 11330] diffusion training Loss: 0.047762319445610046
2024-11-05 06:26:39,531 - INFO - [diffusion][Epoch 11330] diffusion learning rate: 0.001
2024-11-05 06:26:39,532 - INFO - [diffusion][Epoch 11330] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:39,534 - INFO - [diffusion][Epoch 11331] Epoch 11332/12000
2024-11-05 06:26:43,612 - INFO - [diffusion][Epoch 11331] diffusion training Loss: 0.04423830006271601
2024-11-05 06:26:43,614 - INFO - [diffusion][Epoch 11331] diffusion learning rate: 0.001
2024-11-05 06:26:43,634 - INFO - [diffusion][Epoch 11331] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:43,635 - INFO - [diffusion][Epoch 11332] Epoch 11333/12000
2024-11-05 06:26:47,723 - INFO - [diffusion][Epoch 11332] diffusion training Loss: 0.04635703656822443
2024-11-05 06:26:47,725 - INFO - [diffusion][Epoch 11332] diffusion learning rate: 0.001
2024-11-05 06:26:47,726 - INFO - [diffusion][Epoch 11332] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:47,728 - INFO - [diffusion][Epoch 11333] Epoch 11334/12000
2024-11-05 06:26:51,836 - INFO - [diffusion][Epoch 11333] diffusion training Loss: 0.05429742205888033
2024-11-05 06:26:51,838 - INFO - [diffusion][Epoch 11333] diffusion learning rate: 0.001
2024-11-05 06:26:51,840 - INFO - [diffusion][Epoch 11333] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:51,841 - INFO - [diffusion][Epoch 11334] Epoch 11335/12000
2024-11-05 06:26:55,945 - INFO - [diffusion][Epoch 11334] diffusion training Loss: 0.047489505261182785
2024-11-05 06:26:55,947 - INFO - [diffusion][Epoch 11334] diffusion learning rate: 0.001
2024-11-05 06:26:55,949 - INFO - [diffusion][Epoch 11334] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:26:55,950 - INFO - [diffusion][Epoch 11335] Epoch 11336/12000
2024-11-05 06:27:00,025 - INFO - [diffusion][Epoch 11335] diffusion training Loss: 0.0495865261182189
2024-11-05 06:27:00,027 - INFO - [diffusion][Epoch 11335] diffusion learning rate: 0.001
2024-11-05 06:27:00,029 - INFO - [diffusion][Epoch 11335] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:00,030 - INFO - [diffusion][Epoch 11336] Epoch 11337/12000
2024-11-05 06:27:04,482 - INFO - [diffusion][Epoch 11336] diffusion training Loss: 0.04521179012954235
2024-11-05 06:27:04,483 - INFO - [diffusion][Epoch 11336] diffusion learning rate: 0.001
2024-11-05 06:27:04,485 - INFO - [diffusion][Epoch 11336] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:04,486 - INFO - [diffusion][Epoch 11337] Epoch 11338/12000
2024-11-05 06:27:08,577 - INFO - [diffusion][Epoch 11337] diffusion training Loss: 0.0447844797745347
2024-11-05 06:27:08,579 - INFO - [diffusion][Epoch 11337] diffusion learning rate: 0.001
2024-11-05 06:27:08,580 - INFO - [diffusion][Epoch 11337] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:08,582 - INFO - [diffusion][Epoch 11338] Epoch 11339/12000
2024-11-05 06:27:12,724 - INFO - [diffusion][Epoch 11338] diffusion training Loss: 0.047600358724594116
2024-11-05 06:27:12,727 - INFO - [diffusion][Epoch 11338] diffusion learning rate: 0.001
2024-11-05 06:27:12,729 - INFO - [diffusion][Epoch 11338] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:12,730 - INFO - [diffusion][Epoch 11339] Epoch 11340/12000
2024-11-05 06:27:16,852 - INFO - [diffusion][Epoch 11339] diffusion training Loss: 0.05133639369159937
2024-11-05 06:27:16,854 - INFO - [diffusion][Epoch 11339] diffusion learning rate: 0.001
2024-11-05 06:27:16,897 - INFO - [diffusion][Epoch 11339] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:16,899 - INFO - [diffusion][Epoch 11340] Epoch 11341/12000
2024-11-05 06:27:21,006 - INFO - [diffusion][Epoch 11340] diffusion training Loss: 0.04726963955909014
2024-11-05 06:27:21,008 - INFO - [diffusion][Epoch 11340] diffusion learning rate: 0.001
2024-11-05 06:27:21,010 - INFO - [diffusion][Epoch 11340] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:21,011 - INFO - [diffusion][Epoch 11341] Epoch 11342/12000
2024-11-05 06:27:25,127 - INFO - [diffusion][Epoch 11341] diffusion training Loss: 0.04852160718291998
2024-11-05 06:27:25,129 - INFO - [diffusion][Epoch 11341] diffusion learning rate: 0.001
2024-11-05 06:27:25,131 - INFO - [diffusion][Epoch 11341] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:25,132 - INFO - [diffusion][Epoch 11342] Epoch 11343/12000
2024-11-05 06:27:29,246 - INFO - [diffusion][Epoch 11342] diffusion training Loss: 0.04700805153697729
2024-11-05 06:27:29,248 - INFO - [diffusion][Epoch 11342] diffusion learning rate: 0.001
2024-11-05 06:27:29,250 - INFO - [diffusion][Epoch 11342] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:29,251 - INFO - [diffusion][Epoch 11343] Epoch 11344/12000
2024-11-05 06:27:33,370 - INFO - [diffusion][Epoch 11343] diffusion training Loss: 0.04786953795701265
2024-11-05 06:27:33,372 - INFO - [diffusion][Epoch 11343] diffusion learning rate: 0.001
2024-11-05 06:27:33,374 - INFO - [diffusion][Epoch 11343] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:33,375 - INFO - [diffusion][Epoch 11344] Epoch 11345/12000
2024-11-05 06:27:37,321 - INFO - [diffusion][Epoch 11344] diffusion training Loss: 0.04620184097439051
2024-11-05 06:27:37,323 - INFO - [diffusion][Epoch 11344] diffusion learning rate: 0.001
2024-11-05 06:27:37,325 - INFO - [diffusion][Epoch 11344] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:37,326 - INFO - [diffusion][Epoch 11345] Epoch 11346/12000
2024-11-05 06:27:41,438 - INFO - [diffusion][Epoch 11345] diffusion training Loss: 0.04682041984051466
2024-11-05 06:27:41,440 - INFO - [diffusion][Epoch 11345] diffusion learning rate: 0.001
2024-11-05 06:27:41,442 - INFO - [diffusion][Epoch 11345] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:41,443 - INFO - [diffusion][Epoch 11346] Epoch 11347/12000
2024-11-05 06:27:45,541 - INFO - [diffusion][Epoch 11346] diffusion training Loss: 0.050773234106600285
2024-11-05 06:27:45,543 - INFO - [diffusion][Epoch 11346] diffusion learning rate: 0.001
2024-11-05 06:27:45,545 - INFO - [diffusion][Epoch 11346] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:45,546 - INFO - [diffusion][Epoch 11347] Epoch 11348/12000
2024-11-05 06:27:49,652 - INFO - [diffusion][Epoch 11347] diffusion training Loss: 0.04731582012027502
2024-11-05 06:27:49,653 - INFO - [diffusion][Epoch 11347] diffusion learning rate: 0.001
2024-11-05 06:27:49,655 - INFO - [diffusion][Epoch 11347] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:49,658 - INFO - [diffusion][Epoch 11348] Epoch 11349/12000
2024-11-05 06:27:53,771 - INFO - [diffusion][Epoch 11348] diffusion training Loss: 0.04548714682459831
2024-11-05 06:27:53,773 - INFO - [diffusion][Epoch 11348] diffusion learning rate: 0.001
2024-11-05 06:27:53,774 - INFO - [diffusion][Epoch 11348] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:53,776 - INFO - [diffusion][Epoch 11349] Epoch 11350/12000
2024-11-05 06:27:57,883 - INFO - [diffusion][Epoch 11349] diffusion training Loss: 0.05024402774870396
2024-11-05 06:27:57,885 - INFO - [diffusion][Epoch 11349] diffusion learning rate: 0.001
2024-11-05 06:27:57,887 - INFO - [diffusion][Epoch 11349] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:27:57,888 - INFO - [diffusion][Epoch 11350] Epoch 11351/12000
2024-11-05 06:28:01,994 - INFO - [diffusion][Epoch 11350] diffusion training Loss: 0.04821314476430416
2024-11-05 06:28:01,996 - INFO - [diffusion][Epoch 11350] diffusion learning rate: 0.001
2024-11-05 06:28:01,998 - INFO - [diffusion][Epoch 11350] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:01,999 - INFO - [diffusion][Epoch 11351] Epoch 11352/12000
2024-11-05 06:28:06,074 - INFO - [diffusion][Epoch 11351] diffusion training Loss: 0.04641526658087969
2024-11-05 06:28:06,076 - INFO - [diffusion][Epoch 11351] diffusion learning rate: 0.001
2024-11-05 06:28:06,078 - INFO - [diffusion][Epoch 11351] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:06,079 - INFO - [diffusion][Epoch 11352] Epoch 11353/12000
2024-11-05 06:28:10,193 - INFO - [diffusion][Epoch 11352] diffusion training Loss: 0.0499075623229146
2024-11-05 06:28:10,195 - INFO - [diffusion][Epoch 11352] diffusion learning rate: 0.001
2024-11-05 06:28:10,197 - INFO - [diffusion][Epoch 11352] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:10,198 - INFO - [diffusion][Epoch 11353] Epoch 11354/12000
2024-11-05 06:28:14,295 - INFO - [diffusion][Epoch 11353] diffusion training Loss: 0.04544251039624214
2024-11-05 06:28:14,299 - INFO - [diffusion][Epoch 11353] diffusion learning rate: 0.001
2024-11-05 06:28:14,300 - INFO - [diffusion][Epoch 11353] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:14,302 - INFO - [diffusion][Epoch 11354] Epoch 11355/12000
2024-11-05 06:28:18,436 - INFO - [diffusion][Epoch 11354] diffusion training Loss: 0.048569800332188606
2024-11-05 06:28:18,437 - INFO - [diffusion][Epoch 11354] diffusion learning rate: 0.001
2024-11-05 06:28:18,439 - INFO - [diffusion][Epoch 11354] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:18,440 - INFO - [diffusion][Epoch 11355] Epoch 11356/12000
2024-11-05 06:28:22,584 - INFO - [diffusion][Epoch 11355] diffusion training Loss: 0.04717564955353737
2024-11-05 06:28:22,587 - INFO - [diffusion][Epoch 11355] diffusion learning rate: 0.001
2024-11-05 06:28:22,588 - INFO - [diffusion][Epoch 11355] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:22,590 - INFO - [diffusion][Epoch 11356] Epoch 11357/12000
2024-11-05 06:28:26,834 - INFO - [diffusion][Epoch 11356] diffusion training Loss: 0.04478509910404682
2024-11-05 06:28:26,836 - INFO - [diffusion][Epoch 11356] diffusion learning rate: 0.001
2024-11-05 06:28:26,838 - INFO - [diffusion][Epoch 11356] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:26,839 - INFO - [diffusion][Epoch 11357] Epoch 11358/12000
2024-11-05 06:28:30,946 - INFO - [diffusion][Epoch 11357] diffusion training Loss: 0.047087705694139004
2024-11-05 06:28:30,948 - INFO - [diffusion][Epoch 11357] diffusion learning rate: 0.001
2024-11-05 06:28:30,949 - INFO - [diffusion][Epoch 11357] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:30,951 - INFO - [diffusion][Epoch 11358] Epoch 11359/12000
2024-11-05 06:28:35,034 - INFO - [diffusion][Epoch 11358] diffusion training Loss: 0.04773320164531469
2024-11-05 06:28:35,036 - INFO - [diffusion][Epoch 11358] diffusion learning rate: 0.001
2024-11-05 06:28:35,038 - INFO - [diffusion][Epoch 11358] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:35,039 - INFO - [diffusion][Epoch 11359] Epoch 11360/12000
2024-11-05 06:28:39,166 - INFO - [diffusion][Epoch 11359] diffusion training Loss: 0.045264930464327335
2024-11-05 06:28:39,168 - INFO - [diffusion][Epoch 11359] diffusion learning rate: 0.001
2024-11-05 06:28:39,187 - INFO - [diffusion][Epoch 11359] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:39,188 - INFO - [diffusion][Epoch 11360] Epoch 11361/12000
2024-11-05 06:28:43,281 - INFO - [diffusion][Epoch 11360] diffusion training Loss: 0.04481283389031887
2024-11-05 06:28:43,283 - INFO - [diffusion][Epoch 11360] diffusion learning rate: 0.001
2024-11-05 06:28:43,284 - INFO - [diffusion][Epoch 11360] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:43,286 - INFO - [diffusion][Epoch 11361] Epoch 11362/12000
2024-11-05 06:28:47,371 - INFO - [diffusion][Epoch 11361] diffusion training Loss: 0.04528470616787672
2024-11-05 06:28:47,373 - INFO - [diffusion][Epoch 11361] diffusion learning rate: 0.001
2024-11-05 06:28:47,374 - INFO - [diffusion][Epoch 11361] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:47,376 - INFO - [diffusion][Epoch 11362] Epoch 11363/12000
2024-11-05 06:28:51,521 - INFO - [diffusion][Epoch 11362] diffusion training Loss: 0.045752931386232376
2024-11-05 06:28:51,523 - INFO - [diffusion][Epoch 11362] diffusion learning rate: 0.001
2024-11-05 06:28:51,525 - INFO - [diffusion][Epoch 11362] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:51,527 - INFO - [diffusion][Epoch 11363] Epoch 11364/12000
2024-11-05 06:28:55,640 - INFO - [diffusion][Epoch 11363] diffusion training Loss: 0.045334525406360626
2024-11-05 06:28:55,642 - INFO - [diffusion][Epoch 11363] diffusion learning rate: 0.001
2024-11-05 06:28:55,643 - INFO - [diffusion][Epoch 11363] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:55,645 - INFO - [diffusion][Epoch 11364] Epoch 11365/12000
2024-11-05 06:28:59,792 - INFO - [diffusion][Epoch 11364] diffusion training Loss: 0.05076588690280914
2024-11-05 06:28:59,794 - INFO - [diffusion][Epoch 11364] diffusion learning rate: 0.001
2024-11-05 06:28:59,796 - INFO - [diffusion][Epoch 11364] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:28:59,797 - INFO - [diffusion][Epoch 11365] Epoch 11366/12000
2024-11-05 06:29:03,862 - INFO - [diffusion][Epoch 11365] diffusion training Loss: 0.050199019722640514
2024-11-05 06:29:03,864 - INFO - [diffusion][Epoch 11365] diffusion learning rate: 0.001
2024-11-05 06:29:03,866 - INFO - [diffusion][Epoch 11365] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:03,867 - INFO - [diffusion][Epoch 11366] Epoch 11367/12000
2024-11-05 06:29:07,949 - INFO - [diffusion][Epoch 11366] diffusion training Loss: 0.04819054715335369
2024-11-05 06:29:07,952 - INFO - [diffusion][Epoch 11366] diffusion learning rate: 0.001
2024-11-05 06:29:07,953 - INFO - [diffusion][Epoch 11366] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:07,955 - INFO - [diffusion][Epoch 11367] Epoch 11368/12000
2024-11-05 06:29:11,880 - INFO - [diffusion][Epoch 11367] diffusion training Loss: 0.04570735339075327
2024-11-05 06:29:11,882 - INFO - [diffusion][Epoch 11367] diffusion learning rate: 0.001
2024-11-05 06:29:11,884 - INFO - [diffusion][Epoch 11367] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:11,886 - INFO - [diffusion][Epoch 11368] Epoch 11369/12000
2024-11-05 06:29:15,672 - INFO - [diffusion][Epoch 11368] diffusion training Loss: 0.05174668878316879
2024-11-05 06:29:15,676 - INFO - [diffusion][Epoch 11368] diffusion learning rate: 0.001
2024-11-05 06:29:15,677 - INFO - [diffusion][Epoch 11368] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:15,679 - INFO - [diffusion][Epoch 11369] Epoch 11370/12000
2024-11-05 06:29:19,550 - INFO - [diffusion][Epoch 11369] diffusion training Loss: 0.04580917675048113
2024-11-05 06:29:19,552 - INFO - [diffusion][Epoch 11369] diffusion learning rate: 0.001
2024-11-05 06:29:19,553 - INFO - [diffusion][Epoch 11369] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:19,555 - INFO - [diffusion][Epoch 11370] Epoch 11371/12000
2024-11-05 06:29:23,657 - INFO - [diffusion][Epoch 11370] diffusion training Loss: 0.04514186643064022
2024-11-05 06:29:23,658 - INFO - [diffusion][Epoch 11370] diffusion learning rate: 0.001
2024-11-05 06:29:23,660 - INFO - [diffusion][Epoch 11370] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:23,662 - INFO - [diffusion][Epoch 11371] Epoch 11372/12000
2024-11-05 06:29:27,762 - INFO - [diffusion][Epoch 11371] diffusion training Loss: 0.04841209016740322
2024-11-05 06:29:27,764 - INFO - [diffusion][Epoch 11371] diffusion learning rate: 0.001
2024-11-05 06:29:27,765 - INFO - [diffusion][Epoch 11371] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:27,767 - INFO - [diffusion][Epoch 11372] Epoch 11373/12000
2024-11-05 06:29:31,881 - INFO - [diffusion][Epoch 11372] diffusion training Loss: 0.04794980771839619
2024-11-05 06:29:31,883 - INFO - [diffusion][Epoch 11372] diffusion learning rate: 0.001
2024-11-05 06:29:31,885 - INFO - [diffusion][Epoch 11372] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:31,886 - INFO - [diffusion][Epoch 11373] Epoch 11374/12000
2024-11-05 06:29:35,933 - INFO - [diffusion][Epoch 11373] diffusion training Loss: 0.04399302788078785
2024-11-05 06:29:35,935 - INFO - [diffusion][Epoch 11373] diffusion learning rate: 0.001
2024-11-05 06:29:35,963 - INFO - [diffusion][Epoch 11373] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:35,964 - INFO - [diffusion][Epoch 11374] Epoch 11375/12000
2024-11-05 06:29:39,879 - INFO - [diffusion][Epoch 11374] diffusion training Loss: 0.04917184356600046
2024-11-05 06:29:39,881 - INFO - [diffusion][Epoch 11374] diffusion learning rate: 0.001
2024-11-05 06:29:39,882 - INFO - [diffusion][Epoch 11374] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:39,884 - INFO - [diffusion][Epoch 11375] Epoch 11376/12000
2024-11-05 06:29:43,709 - INFO - [diffusion][Epoch 11375] diffusion training Loss: 0.045780750922858715
2024-11-05 06:29:43,711 - INFO - [diffusion][Epoch 11375] diffusion learning rate: 0.001
2024-11-05 06:29:43,713 - INFO - [diffusion][Epoch 11375] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:43,714 - INFO - [diffusion][Epoch 11376] Epoch 11377/12000
2024-11-05 06:29:47,812 - INFO - [diffusion][Epoch 11376] diffusion training Loss: 0.05096055008471012
2024-11-05 06:29:47,814 - INFO - [diffusion][Epoch 11376] diffusion learning rate: 0.001
2024-11-05 06:29:47,840 - INFO - [diffusion][Epoch 11376] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:47,842 - INFO - [diffusion][Epoch 11377] Epoch 11378/12000
2024-11-05 06:29:52,153 - INFO - [diffusion][Epoch 11377] diffusion training Loss: 0.04705251846462488
2024-11-05 06:29:52,155 - INFO - [diffusion][Epoch 11377] diffusion learning rate: 0.001
2024-11-05 06:29:52,157 - INFO - [diffusion][Epoch 11377] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:52,158 - INFO - [diffusion][Epoch 11378] Epoch 11379/12000
2024-11-05 06:29:56,262 - INFO - [diffusion][Epoch 11378] diffusion training Loss: 0.04557542968541384
2024-11-05 06:29:56,263 - INFO - [diffusion][Epoch 11378] diffusion learning rate: 0.001
2024-11-05 06:29:56,265 - INFO - [diffusion][Epoch 11378] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:29:56,266 - INFO - [diffusion][Epoch 11379] Epoch 11380/12000
2024-11-05 06:30:00,176 - INFO - [diffusion][Epoch 11379] diffusion training Loss: 0.044304066337645054
2024-11-05 06:30:00,178 - INFO - [diffusion][Epoch 11379] diffusion learning rate: 0.001
2024-11-05 06:30:00,180 - INFO - [diffusion][Epoch 11379] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:00,181 - INFO - [diffusion][Epoch 11380] Epoch 11381/12000
2024-11-05 06:30:04,125 - INFO - [diffusion][Epoch 11380] diffusion training Loss: 0.04296497628092766
2024-11-05 06:30:04,127 - INFO - [diffusion][Epoch 11380] diffusion learning rate: 0.001
2024-11-05 06:30:04,154 - INFO - [diffusion][Epoch 11380] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:04,155 - INFO - [diffusion][Epoch 11381] Epoch 11382/12000
2024-11-05 06:30:08,235 - INFO - [diffusion][Epoch 11381] diffusion training Loss: 0.04772804956883192
2024-11-05 06:30:08,237 - INFO - [diffusion][Epoch 11381] diffusion learning rate: 0.001
2024-11-05 06:30:08,239 - INFO - [diffusion][Epoch 11381] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:08,240 - INFO - [diffusion][Epoch 11382] Epoch 11383/12000
2024-11-05 06:30:12,345 - INFO - [diffusion][Epoch 11382] diffusion training Loss: 0.04153304174542427
2024-11-05 06:30:12,604 - INFO - [diffusion][Epoch 11382] diffusion learning rate: 0.001
2024-11-05 06:30:12,607 - INFO - [diffusion][Epoch 11382] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:12,608 - INFO - [diffusion][Epoch 11383] Epoch 11384/12000
2024-11-05 06:30:16,674 - INFO - [diffusion][Epoch 11383] diffusion training Loss: 0.046893409453332424
2024-11-05 06:30:16,677 - INFO - [diffusion][Epoch 11383] diffusion learning rate: 0.001
2024-11-05 06:30:16,679 - INFO - [diffusion][Epoch 11383] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:16,680 - INFO - [diffusion][Epoch 11384] Epoch 11385/12000
2024-11-05 06:30:20,594 - INFO - [diffusion][Epoch 11384] diffusion training Loss: 0.04833077359944582
2024-11-05 06:30:20,596 - INFO - [diffusion][Epoch 11384] diffusion learning rate: 0.001
2024-11-05 06:30:20,622 - INFO - [diffusion][Epoch 11384] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:20,624 - INFO - [diffusion][Epoch 11385] Epoch 11386/12000
2024-11-05 06:30:24,675 - INFO - [diffusion][Epoch 11385] diffusion training Loss: 0.048545598052442074
2024-11-05 06:30:24,677 - INFO - [diffusion][Epoch 11385] diffusion learning rate: 0.001
2024-11-05 06:30:24,679 - INFO - [diffusion][Epoch 11385] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:24,680 - INFO - [diffusion][Epoch 11386] Epoch 11387/12000
2024-11-05 06:30:28,796 - INFO - [diffusion][Epoch 11386] diffusion training Loss: 0.047290476970374584
2024-11-05 06:30:28,798 - INFO - [diffusion][Epoch 11386] diffusion learning rate: 0.001
2024-11-05 06:30:28,800 - INFO - [diffusion][Epoch 11386] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:28,801 - INFO - [diffusion][Epoch 11387] Epoch 11388/12000
2024-11-05 06:30:32,877 - INFO - [diffusion][Epoch 11387] diffusion training Loss: 0.05001645255833864
2024-11-05 06:30:32,878 - INFO - [diffusion][Epoch 11387] diffusion learning rate: 0.001
2024-11-05 06:30:32,880 - INFO - [diffusion][Epoch 11387] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:32,881 - INFO - [diffusion][Epoch 11388] Epoch 11389/12000
2024-11-05 06:30:36,935 - INFO - [diffusion][Epoch 11388] diffusion training Loss: 0.04465471766889095
2024-11-05 06:30:36,937 - INFO - [diffusion][Epoch 11388] diffusion learning rate: 0.001
2024-11-05 06:30:36,939 - INFO - [diffusion][Epoch 11388] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:36,940 - INFO - [diffusion][Epoch 11389] Epoch 11390/12000
2024-11-05 06:30:41,023 - INFO - [diffusion][Epoch 11389] diffusion training Loss: 0.04315714631229639
2024-11-05 06:30:41,025 - INFO - [diffusion][Epoch 11389] diffusion learning rate: 0.001
2024-11-05 06:30:41,064 - INFO - [diffusion][Epoch 11389] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:41,065 - INFO - [diffusion][Epoch 11390] Epoch 11391/12000
2024-11-05 06:30:45,102 - INFO - [diffusion][Epoch 11390] diffusion training Loss: 0.04275376349687576
2024-11-05 06:30:45,104 - INFO - [diffusion][Epoch 11390] diffusion learning rate: 0.001
2024-11-05 06:30:45,106 - INFO - [diffusion][Epoch 11390] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:45,107 - INFO - [diffusion][Epoch 11391] Epoch 11392/12000
2024-11-05 06:30:49,177 - INFO - [diffusion][Epoch 11391] diffusion training Loss: 0.04518581461161375
2024-11-05 06:30:49,179 - INFO - [diffusion][Epoch 11391] diffusion learning rate: 0.001
2024-11-05 06:30:49,181 - INFO - [diffusion][Epoch 11391] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:49,182 - INFO - [diffusion][Epoch 11392] Epoch 11393/12000
2024-11-05 06:30:53,271 - INFO - [diffusion][Epoch 11392] diffusion training Loss: 0.043476165272295475
2024-11-05 06:30:53,273 - INFO - [diffusion][Epoch 11392] diffusion learning rate: 0.001
2024-11-05 06:30:53,275 - INFO - [diffusion][Epoch 11392] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:53,276 - INFO - [diffusion][Epoch 11393] Epoch 11394/12000
2024-11-05 06:30:57,370 - INFO - [diffusion][Epoch 11393] diffusion training Loss: 0.04362577013671398
2024-11-05 06:30:57,372 - INFO - [diffusion][Epoch 11393] diffusion learning rate: 0.001
2024-11-05 06:30:57,374 - INFO - [diffusion][Epoch 11393] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:30:57,375 - INFO - [diffusion][Epoch 11394] Epoch 11395/12000
2024-11-05 06:31:01,515 - INFO - [diffusion][Epoch 11394] diffusion training Loss: 0.04881758149713278
2024-11-05 06:31:01,517 - INFO - [diffusion][Epoch 11394] diffusion learning rate: 0.001
2024-11-05 06:31:01,519 - INFO - [diffusion][Epoch 11394] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:01,520 - INFO - [diffusion][Epoch 11395] Epoch 11396/12000
2024-11-05 06:31:05,606 - INFO - [diffusion][Epoch 11395] diffusion training Loss: 0.047425681725144386
2024-11-05 06:31:05,608 - INFO - [diffusion][Epoch 11395] diffusion learning rate: 0.001
2024-11-05 06:31:05,610 - INFO - [diffusion][Epoch 11395] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:05,612 - INFO - [diffusion][Epoch 11396] Epoch 11397/12000
2024-11-05 06:31:09,711 - INFO - [diffusion][Epoch 11396] diffusion training Loss: 0.04555809870362282
2024-11-05 06:31:09,713 - INFO - [diffusion][Epoch 11396] diffusion learning rate: 0.001
2024-11-05 06:31:09,741 - INFO - [diffusion][Epoch 11396] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:09,743 - INFO - [diffusion][Epoch 11397] Epoch 11398/12000
2024-11-05 06:31:14,063 - INFO - [diffusion][Epoch 11397] diffusion training Loss: 0.04551482945680618
2024-11-05 06:31:14,065 - INFO - [diffusion][Epoch 11397] diffusion learning rate: 0.001
2024-11-05 06:31:14,067 - INFO - [diffusion][Epoch 11397] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:14,068 - INFO - [diffusion][Epoch 11398] Epoch 11399/12000
2024-11-05 06:31:18,101 - INFO - [diffusion][Epoch 11398] diffusion training Loss: 0.04652647767215967
2024-11-05 06:31:18,105 - INFO - [diffusion][Epoch 11398] diffusion learning rate: 0.001
2024-11-05 06:31:18,107 - INFO - [diffusion][Epoch 11398] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:18,108 - INFO - [diffusion][Epoch 11399] Epoch 11400/12000
2024-11-05 06:31:22,199 - INFO - [diffusion][Epoch 11399] diffusion training Loss: 0.04850019887089729
2024-11-05 06:31:22,201 - INFO - [diffusion][Epoch 11399] diffusion learning rate: 0.001
2024-11-05 06:31:22,202 - INFO - [diffusion][Epoch 11399] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:22,204 - INFO - [diffusion][Epoch 11400] Epoch 11401/12000
2024-11-05 06:31:26,116 - INFO - [diffusion][Epoch 11400] diffusion training Loss: 0.048671419732272625
2024-11-05 06:31:26,118 - INFO - [diffusion][Epoch 11400] diffusion learning rate: 0.001
2024-11-05 06:31:26,120 - INFO - [diffusion][Epoch 11400] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:26,121 - INFO - [diffusion][Epoch 11401] Epoch 11402/12000
2024-11-05 06:31:30,236 - INFO - [diffusion][Epoch 11401] diffusion training Loss: 0.04338005091995001
2024-11-05 06:31:30,238 - INFO - [diffusion][Epoch 11401] diffusion learning rate: 0.001
2024-11-05 06:31:30,239 - INFO - [diffusion][Epoch 11401] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:30,241 - INFO - [diffusion][Epoch 11402] Epoch 11403/12000
2024-11-05 06:31:34,315 - INFO - [diffusion][Epoch 11402] diffusion training Loss: 0.048063427209854126
2024-11-05 06:31:34,317 - INFO - [diffusion][Epoch 11402] diffusion learning rate: 0.001
2024-11-05 06:31:34,318 - INFO - [diffusion][Epoch 11402] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:34,320 - INFO - [diffusion][Epoch 11403] Epoch 11404/12000
2024-11-05 06:31:38,486 - INFO - [diffusion][Epoch 11403] diffusion training Loss: 0.04733559302985668
2024-11-05 06:31:38,488 - INFO - [diffusion][Epoch 11403] diffusion learning rate: 0.001
2024-11-05 06:31:38,490 - INFO - [diffusion][Epoch 11403] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:38,491 - INFO - [diffusion][Epoch 11404] Epoch 11405/12000
2024-11-05 06:31:42,566 - INFO - [diffusion][Epoch 11404] diffusion training Loss: 0.048157332465052605
2024-11-05 06:31:42,586 - INFO - [diffusion][Epoch 11404] diffusion learning rate: 0.001
2024-11-05 06:31:42,588 - INFO - [diffusion][Epoch 11404] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:42,589 - INFO - [diffusion][Epoch 11405] Epoch 11406/12000
2024-11-05 06:31:46,644 - INFO - [diffusion][Epoch 11405] diffusion training Loss: 0.049800354056060314
2024-11-05 06:31:46,646 - INFO - [diffusion][Epoch 11405] diffusion learning rate: 0.001
2024-11-05 06:31:46,648 - INFO - [diffusion][Epoch 11405] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:46,649 - INFO - [diffusion][Epoch 11406] Epoch 11407/12000
2024-11-05 06:31:50,733 - INFO - [diffusion][Epoch 11406] diffusion training Loss: 0.04795771557837725
2024-11-05 06:31:50,735 - INFO - [diffusion][Epoch 11406] diffusion learning rate: 0.001
2024-11-05 06:31:50,737 - INFO - [diffusion][Epoch 11406] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:50,738 - INFO - [diffusion][Epoch 11407] Epoch 11408/12000
2024-11-05 06:31:54,788 - INFO - [diffusion][Epoch 11407] diffusion training Loss: 0.04841871000826359
2024-11-05 06:31:54,790 - INFO - [diffusion][Epoch 11407] diffusion learning rate: 0.001
2024-11-05 06:31:54,791 - INFO - [diffusion][Epoch 11407] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:54,792 - INFO - [diffusion][Epoch 11408] Epoch 11409/12000
2024-11-05 06:31:58,844 - INFO - [diffusion][Epoch 11408] diffusion training Loss: 0.04980709031224251
2024-11-05 06:31:58,846 - INFO - [diffusion][Epoch 11408] diffusion learning rate: 0.001
2024-11-05 06:31:58,848 - INFO - [diffusion][Epoch 11408] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:31:58,849 - INFO - [diffusion][Epoch 11409] Epoch 11410/12000
2024-11-05 06:32:02,885 - INFO - [diffusion][Epoch 11409] diffusion training Loss: 0.049372561275959015
2024-11-05 06:32:02,887 - INFO - [diffusion][Epoch 11409] diffusion learning rate: 0.001
2024-11-05 06:32:02,888 - INFO - [diffusion][Epoch 11409] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:02,890 - INFO - [diffusion][Epoch 11410] Epoch 11411/12000
2024-11-05 06:32:06,937 - INFO - [diffusion][Epoch 11410] diffusion training Loss: 0.041929674334824085
2024-11-05 06:32:06,939 - INFO - [diffusion][Epoch 11410] diffusion learning rate: 0.001
2024-11-05 06:32:06,941 - INFO - [diffusion][Epoch 11410] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:06,942 - INFO - [diffusion][Epoch 11411] Epoch 11412/12000
2024-11-05 06:32:10,979 - INFO - [diffusion][Epoch 11411] diffusion training Loss: 0.04777442663908005
2024-11-05 06:32:10,981 - INFO - [diffusion][Epoch 11411] diffusion learning rate: 0.001
2024-11-05 06:32:10,983 - INFO - [diffusion][Epoch 11411] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:10,984 - INFO - [diffusion][Epoch 11412] Epoch 11413/12000
2024-11-05 06:32:15,033 - INFO - [diffusion][Epoch 11412] diffusion training Loss: 0.046902175061404705
2024-11-05 06:32:15,035 - INFO - [diffusion][Epoch 11412] diffusion learning rate: 0.001
2024-11-05 06:32:15,037 - INFO - [diffusion][Epoch 11412] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:15,038 - INFO - [diffusion][Epoch 11413] Epoch 11414/12000
2024-11-05 06:32:19,092 - INFO - [diffusion][Epoch 11413] diffusion training Loss: 0.054672934114933014
2024-11-05 06:32:19,096 - INFO - [diffusion][Epoch 11413] diffusion learning rate: 0.001
2024-11-05 06:32:19,097 - INFO - [diffusion][Epoch 11413] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:19,098 - INFO - [diffusion][Epoch 11414] Epoch 11415/12000
2024-11-05 06:32:23,136 - INFO - [diffusion][Epoch 11414] diffusion training Loss: 0.04481774289160967
2024-11-05 06:32:23,138 - INFO - [diffusion][Epoch 11414] diffusion learning rate: 0.001
2024-11-05 06:32:23,139 - INFO - [diffusion][Epoch 11414] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:23,140 - INFO - [diffusion][Epoch 11415] Epoch 11416/12000
2024-11-05 06:32:27,169 - INFO - [diffusion][Epoch 11415] diffusion training Loss: 0.049276722595095634
2024-11-05 06:32:27,171 - INFO - [diffusion][Epoch 11415] diffusion learning rate: 0.001
2024-11-05 06:32:27,172 - INFO - [diffusion][Epoch 11415] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:27,174 - INFO - [diffusion][Epoch 11416] Epoch 11417/12000
2024-11-05 06:32:31,062 - INFO - [diffusion][Epoch 11416] diffusion training Loss: 0.046807692386209965
2024-11-05 06:32:31,064 - INFO - [diffusion][Epoch 11416] diffusion learning rate: 0.001
2024-11-05 06:32:31,066 - INFO - [diffusion][Epoch 11416] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:31,067 - INFO - [diffusion][Epoch 11417] Epoch 11418/12000
2024-11-05 06:32:35,068 - INFO - [diffusion][Epoch 11417] diffusion training Loss: 0.0460598012432456
2024-11-05 06:32:35,070 - INFO - [diffusion][Epoch 11417] diffusion learning rate: 0.001
2024-11-05 06:32:35,072 - INFO - [diffusion][Epoch 11417] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:35,073 - INFO - [diffusion][Epoch 11418] Epoch 11419/12000
2024-11-05 06:32:39,120 - INFO - [diffusion][Epoch 11418] diffusion training Loss: 0.0457548126578331
2024-11-05 06:32:39,122 - INFO - [diffusion][Epoch 11418] diffusion learning rate: 0.001
2024-11-05 06:32:39,123 - INFO - [diffusion][Epoch 11418] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:39,125 - INFO - [diffusion][Epoch 11419] Epoch 11420/12000
2024-11-05 06:32:43,139 - INFO - [diffusion][Epoch 11419] diffusion training Loss: 0.04367190506309271
2024-11-05 06:32:43,141 - INFO - [diffusion][Epoch 11419] diffusion learning rate: 0.001
2024-11-05 06:32:43,143 - INFO - [diffusion][Epoch 11419] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:43,144 - INFO - [diffusion][Epoch 11420] Epoch 11421/12000
2024-11-05 06:32:47,225 - INFO - [diffusion][Epoch 11420] diffusion training Loss: 0.044970409013330936
2024-11-05 06:32:47,227 - INFO - [diffusion][Epoch 11420] diffusion learning rate: 0.001
2024-11-05 06:32:47,228 - INFO - [diffusion][Epoch 11420] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:47,229 - INFO - [diffusion][Epoch 11421] Epoch 11422/12000
2024-11-05 06:32:51,294 - INFO - [diffusion][Epoch 11421] diffusion training Loss: 0.045067137107253075
2024-11-05 06:32:51,296 - INFO - [diffusion][Epoch 11421] diffusion learning rate: 0.001
2024-11-05 06:32:51,298 - INFO - [diffusion][Epoch 11421] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:51,299 - INFO - [diffusion][Epoch 11422] Epoch 11423/12000
2024-11-05 06:32:55,467 - INFO - [diffusion][Epoch 11422] diffusion training Loss: 0.04772462975233793
2024-11-05 06:32:55,469 - INFO - [diffusion][Epoch 11422] diffusion learning rate: 0.001
2024-11-05 06:32:55,471 - INFO - [diffusion][Epoch 11422] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:55,473 - INFO - [diffusion][Epoch 11423] Epoch 11424/12000
2024-11-05 06:32:59,572 - INFO - [diffusion][Epoch 11423] diffusion training Loss: 0.043486591428518295
2024-11-05 06:32:59,574 - INFO - [diffusion][Epoch 11423] diffusion learning rate: 0.001
2024-11-05 06:32:59,575 - INFO - [diffusion][Epoch 11423] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:32:59,577 - INFO - [diffusion][Epoch 11424] Epoch 11425/12000
2024-11-05 06:33:03,691 - INFO - [diffusion][Epoch 11424] diffusion training Loss: 0.053040074184536934
2024-11-05 06:33:03,693 - INFO - [diffusion][Epoch 11424] diffusion learning rate: 0.001
2024-11-05 06:33:03,695 - INFO - [diffusion][Epoch 11424] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:03,696 - INFO - [diffusion][Epoch 11425] Epoch 11426/12000
2024-11-05 06:33:07,628 - INFO - [diffusion][Epoch 11425] diffusion training Loss: 0.047417459078133106
2024-11-05 06:33:07,630 - INFO - [diffusion][Epoch 11425] diffusion learning rate: 0.001
2024-11-05 06:33:07,632 - INFO - [diffusion][Epoch 11425] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:07,633 - INFO - [diffusion][Epoch 11426] Epoch 11427/12000
2024-11-05 06:33:11,618 - INFO - [diffusion][Epoch 11426] diffusion training Loss: 0.04898528289049864
2024-11-05 06:33:11,620 - INFO - [diffusion][Epoch 11426] diffusion learning rate: 0.001
2024-11-05 06:33:11,621 - INFO - [diffusion][Epoch 11426] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:11,622 - INFO - [diffusion][Epoch 11427] Epoch 11428/12000
2024-11-05 06:33:15,664 - INFO - [diffusion][Epoch 11427] diffusion training Loss: 0.052809009328484535
2024-11-05 06:33:15,666 - INFO - [diffusion][Epoch 11427] diffusion learning rate: 0.001
2024-11-05 06:33:15,667 - INFO - [diffusion][Epoch 11427] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:15,669 - INFO - [diffusion][Epoch 11428] Epoch 11429/12000
2024-11-05 06:33:19,740 - INFO - [diffusion][Epoch 11428] diffusion training Loss: 0.0483873737975955
2024-11-05 06:33:19,744 - INFO - [diffusion][Epoch 11428] diffusion learning rate: 0.001
2024-11-05 06:33:19,746 - INFO - [diffusion][Epoch 11428] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:19,747 - INFO - [diffusion][Epoch 11429] Epoch 11430/12000
2024-11-05 06:33:23,827 - INFO - [diffusion][Epoch 11429] diffusion training Loss: 0.05009318143129349
2024-11-05 06:33:23,829 - INFO - [diffusion][Epoch 11429] diffusion learning rate: 0.001
2024-11-05 06:33:23,831 - INFO - [diffusion][Epoch 11429] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:23,832 - INFO - [diffusion][Epoch 11430] Epoch 11431/12000
2024-11-05 06:33:27,910 - INFO - [diffusion][Epoch 11430] diffusion training Loss: 0.045501479879021645
2024-11-05 06:33:27,912 - INFO - [diffusion][Epoch 11430] diffusion learning rate: 0.001
2024-11-05 06:33:27,914 - INFO - [diffusion][Epoch 11430] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:27,915 - INFO - [diffusion][Epoch 11431] Epoch 11432/12000
2024-11-05 06:33:31,977 - INFO - [diffusion][Epoch 11431] diffusion training Loss: 0.049188852310180664
2024-11-05 06:33:31,979 - INFO - [diffusion][Epoch 11431] diffusion learning rate: 0.001
2024-11-05 06:33:31,981 - INFO - [diffusion][Epoch 11431] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:31,983 - INFO - [diffusion][Epoch 11432] Epoch 11433/12000
2024-11-05 06:33:36,073 - INFO - [diffusion][Epoch 11432] diffusion training Loss: 0.04627733211964369
2024-11-05 06:33:36,075 - INFO - [diffusion][Epoch 11432] diffusion learning rate: 0.001
2024-11-05 06:33:36,077 - INFO - [diffusion][Epoch 11432] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:36,078 - INFO - [diffusion][Epoch 11433] Epoch 11434/12000
2024-11-05 06:33:40,185 - INFO - [diffusion][Epoch 11433] diffusion training Loss: 0.04726158455014229
2024-11-05 06:33:40,187 - INFO - [diffusion][Epoch 11433] diffusion learning rate: 0.001
2024-11-05 06:33:40,189 - INFO - [diffusion][Epoch 11433] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:40,191 - INFO - [diffusion][Epoch 11434] Epoch 11435/12000
2024-11-05 06:33:44,258 - INFO - [diffusion][Epoch 11434] diffusion training Loss: 0.04693350289016962
2024-11-05 06:33:44,260 - INFO - [diffusion][Epoch 11434] diffusion learning rate: 0.001
2024-11-05 06:33:44,261 - INFO - [diffusion][Epoch 11434] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:44,263 - INFO - [diffusion][Epoch 11435] Epoch 11436/12000
2024-11-05 06:33:48,359 - INFO - [diffusion][Epoch 11435] diffusion training Loss: 0.05086325854063034
2024-11-05 06:33:48,361 - INFO - [diffusion][Epoch 11435] diffusion learning rate: 0.001
2024-11-05 06:33:48,363 - INFO - [diffusion][Epoch 11435] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:48,364 - INFO - [diffusion][Epoch 11436] Epoch 11437/12000
2024-11-05 06:33:52,438 - INFO - [diffusion][Epoch 11436] diffusion training Loss: 0.04819789994508028
2024-11-05 06:33:52,440 - INFO - [diffusion][Epoch 11436] diffusion learning rate: 0.001
2024-11-05 06:33:52,442 - INFO - [diffusion][Epoch 11436] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:52,444 - INFO - [diffusion][Epoch 11437] Epoch 11438/12000
2024-11-05 06:33:56,505 - INFO - [diffusion][Epoch 11437] diffusion training Loss: 0.04845009930431843
2024-11-05 06:33:56,507 - INFO - [diffusion][Epoch 11437] diffusion learning rate: 0.001
2024-11-05 06:33:56,509 - INFO - [diffusion][Epoch 11437] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:33:56,510 - INFO - [diffusion][Epoch 11438] Epoch 11439/12000
2024-11-05 06:34:00,562 - INFO - [diffusion][Epoch 11438] diffusion training Loss: 0.04993722774088383
2024-11-05 06:34:00,564 - INFO - [diffusion][Epoch 11438] diffusion learning rate: 0.001
2024-11-05 06:34:00,566 - INFO - [diffusion][Epoch 11438] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:00,567 - INFO - [diffusion][Epoch 11439] Epoch 11440/12000
2024-11-05 06:34:04,659 - INFO - [diffusion][Epoch 11439] diffusion training Loss: 0.05222245492041111
2024-11-05 06:34:04,661 - INFO - [diffusion][Epoch 11439] diffusion learning rate: 0.001
2024-11-05 06:34:04,663 - INFO - [diffusion][Epoch 11439] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:04,664 - INFO - [diffusion][Epoch 11440] Epoch 11441/12000
2024-11-05 06:34:08,677 - INFO - [diffusion][Epoch 11440] diffusion training Loss: 0.04718697723001242
2024-11-05 06:34:08,679 - INFO - [diffusion][Epoch 11440] diffusion learning rate: 0.001
2024-11-05 06:34:08,681 - INFO - [diffusion][Epoch 11440] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:08,682 - INFO - [diffusion][Epoch 11441] Epoch 11442/12000
2024-11-05 06:34:12,667 - INFO - [diffusion][Epoch 11441] diffusion training Loss: 0.041688050143420696
2024-11-05 06:34:12,669 - INFO - [diffusion][Epoch 11441] diffusion learning rate: 0.001
2024-11-05 06:34:12,671 - INFO - [diffusion][Epoch 11441] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:12,673 - INFO - [diffusion][Epoch 11442] Epoch 11443/12000
2024-11-05 06:34:16,747 - INFO - [diffusion][Epoch 11442] diffusion training Loss: 0.05253060907125473
2024-11-05 06:34:16,749 - INFO - [diffusion][Epoch 11442] diffusion learning rate: 0.001
2024-11-05 06:34:16,750 - INFO - [diffusion][Epoch 11442] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:16,752 - INFO - [diffusion][Epoch 11443] Epoch 11444/12000
2024-11-05 06:34:20,799 - INFO - [diffusion][Epoch 11443] diffusion training Loss: 0.05085004772990942
2024-11-05 06:34:20,803 - INFO - [diffusion][Epoch 11443] diffusion learning rate: 0.001
2024-11-05 06:34:20,804 - INFO - [diffusion][Epoch 11443] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:20,806 - INFO - [diffusion][Epoch 11444] Epoch 11445/12000
2024-11-05 06:34:24,873 - INFO - [diffusion][Epoch 11444] diffusion training Loss: 0.041484893299639225
2024-11-05 06:34:24,875 - INFO - [diffusion][Epoch 11444] diffusion learning rate: 0.001
2024-11-05 06:34:24,877 - INFO - [diffusion][Epoch 11444] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:24,878 - INFO - [diffusion][Epoch 11445] Epoch 11446/12000
2024-11-05 06:34:28,939 - INFO - [diffusion][Epoch 11445] diffusion training Loss: 0.04516866896301508
2024-11-05 06:34:28,941 - INFO - [diffusion][Epoch 11445] diffusion learning rate: 0.001
2024-11-05 06:34:28,943 - INFO - [diffusion][Epoch 11445] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:28,944 - INFO - [diffusion][Epoch 11446] Epoch 11447/12000
2024-11-05 06:34:32,915 - INFO - [diffusion][Epoch 11446] diffusion training Loss: 0.04655861854553223
2024-11-05 06:34:32,916 - INFO - [diffusion][Epoch 11446] diffusion learning rate: 0.001
2024-11-05 06:34:32,918 - INFO - [diffusion][Epoch 11446] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:32,920 - INFO - [diffusion][Epoch 11447] Epoch 11448/12000
2024-11-05 06:34:36,957 - INFO - [diffusion][Epoch 11447] diffusion training Loss: 0.04902227595448494
2024-11-05 06:34:36,959 - INFO - [diffusion][Epoch 11447] diffusion learning rate: 0.001
2024-11-05 06:34:36,961 - INFO - [diffusion][Epoch 11447] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:36,962 - INFO - [diffusion][Epoch 11448] Epoch 11449/12000
2024-11-05 06:34:40,993 - INFO - [diffusion][Epoch 11448] diffusion training Loss: 0.04104652814567089
2024-11-05 06:34:40,995 - INFO - [diffusion][Epoch 11448] diffusion learning rate: 0.001
2024-11-05 06:34:40,997 - INFO - [diffusion][Epoch 11448] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:40,998 - INFO - [diffusion][Epoch 11449] Epoch 11450/12000
2024-11-05 06:34:45,022 - INFO - [diffusion][Epoch 11449] diffusion training Loss: 0.046190766617655754
2024-11-05 06:34:45,024 - INFO - [diffusion][Epoch 11449] diffusion learning rate: 0.001
2024-11-05 06:34:45,026 - INFO - [diffusion][Epoch 11449] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:45,027 - INFO - [diffusion][Epoch 11450] Epoch 11451/12000
2024-11-05 06:34:49,070 - INFO - [diffusion][Epoch 11450] diffusion training Loss: 0.04859172739088535
2024-11-05 06:34:49,072 - INFO - [diffusion][Epoch 11450] diffusion learning rate: 0.001
2024-11-05 06:34:49,073 - INFO - [diffusion][Epoch 11450] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:49,074 - INFO - [diffusion][Epoch 11451] Epoch 11452/12000
2024-11-05 06:34:53,152 - INFO - [diffusion][Epoch 11451] diffusion training Loss: 0.04905109107494354
2024-11-05 06:34:53,154 - INFO - [diffusion][Epoch 11451] diffusion learning rate: 0.001
2024-11-05 06:34:53,156 - INFO - [diffusion][Epoch 11451] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:53,157 - INFO - [diffusion][Epoch 11452] Epoch 11453/12000
2024-11-05 06:34:57,191 - INFO - [diffusion][Epoch 11452] diffusion training Loss: 0.048622485250234604
2024-11-05 06:34:57,193 - INFO - [diffusion][Epoch 11452] diffusion learning rate: 0.001
2024-11-05 06:34:57,194 - INFO - [diffusion][Epoch 11452] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:34:57,196 - INFO - [diffusion][Epoch 11453] Epoch 11454/12000
2024-11-05 06:35:01,266 - INFO - [diffusion][Epoch 11453] diffusion training Loss: 0.046945055946707726
2024-11-05 06:35:01,268 - INFO - [diffusion][Epoch 11453] diffusion learning rate: 0.001
2024-11-05 06:35:01,319 - INFO - [diffusion][Epoch 11453] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:01,320 - INFO - [diffusion][Epoch 11454] Epoch 11455/12000
2024-11-05 06:35:05,390 - INFO - [diffusion][Epoch 11454] diffusion training Loss: 0.04562780261039734
2024-11-05 06:35:05,392 - INFO - [diffusion][Epoch 11454] diffusion learning rate: 0.001
2024-11-05 06:35:05,394 - INFO - [diffusion][Epoch 11454] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:05,395 - INFO - [diffusion][Epoch 11455] Epoch 11456/12000
2024-11-05 06:35:09,486 - INFO - [diffusion][Epoch 11455] diffusion training Loss: 0.0433407174423337
2024-11-05 06:35:09,488 - INFO - [diffusion][Epoch 11455] diffusion learning rate: 0.001
2024-11-05 06:35:09,490 - INFO - [diffusion][Epoch 11455] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:09,491 - INFO - [diffusion][Epoch 11456] Epoch 11457/12000
2024-11-05 06:35:13,493 - INFO - [diffusion][Epoch 11456] diffusion training Loss: 0.04543715808540583
2024-11-05 06:35:13,495 - INFO - [diffusion][Epoch 11456] diffusion learning rate: 0.001
2024-11-05 06:35:13,497 - INFO - [diffusion][Epoch 11456] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:13,498 - INFO - [diffusion][Epoch 11457] Epoch 11458/12000
2024-11-05 06:35:17,577 - INFO - [diffusion][Epoch 11457] diffusion training Loss: 0.046928633004426956
2024-11-05 06:35:17,578 - INFO - [diffusion][Epoch 11457] diffusion learning rate: 0.001
2024-11-05 06:35:17,607 - INFO - [diffusion][Epoch 11457] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:17,608 - INFO - [diffusion][Epoch 11458] Epoch 11459/12000
2024-11-05 06:35:21,687 - INFO - [diffusion][Epoch 11458] diffusion training Loss: 0.04962933901697397
2024-11-05 06:35:21,691 - INFO - [diffusion][Epoch 11458] diffusion learning rate: 0.001
2024-11-05 06:35:21,693 - INFO - [diffusion][Epoch 11458] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:21,694 - INFO - [diffusion][Epoch 11459] Epoch 11460/12000
2024-11-05 06:35:25,777 - INFO - [diffusion][Epoch 11459] diffusion training Loss: 0.0442394120618701
2024-11-05 06:35:25,779 - INFO - [diffusion][Epoch 11459] diffusion learning rate: 0.001
2024-11-05 06:35:25,781 - INFO - [diffusion][Epoch 11459] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:25,782 - INFO - [diffusion][Epoch 11460] Epoch 11461/12000
2024-11-05 06:35:29,914 - INFO - [diffusion][Epoch 11460] diffusion training Loss: 0.04376139212399721
2024-11-05 06:35:29,916 - INFO - [diffusion][Epoch 11460] diffusion learning rate: 0.001
2024-11-05 06:35:29,918 - INFO - [diffusion][Epoch 11460] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:29,919 - INFO - [diffusion][Epoch 11461] Epoch 11462/12000
2024-11-05 06:35:34,054 - INFO - [diffusion][Epoch 11461] diffusion training Loss: 0.043136391788721085
2024-11-05 06:35:34,056 - INFO - [diffusion][Epoch 11461] diffusion learning rate: 0.001
2024-11-05 06:35:34,085 - INFO - [diffusion][Epoch 11461] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:34,086 - INFO - [diffusion][Epoch 11462] Epoch 11463/12000
2024-11-05 06:35:38,197 - INFO - [diffusion][Epoch 11462] diffusion training Loss: 0.04750689025968313
2024-11-05 06:35:38,199 - INFO - [diffusion][Epoch 11462] diffusion learning rate: 0.001
2024-11-05 06:35:38,201 - INFO - [diffusion][Epoch 11462] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:38,202 - INFO - [diffusion][Epoch 11463] Epoch 11464/12000
2024-11-05 06:35:42,369 - INFO - [diffusion][Epoch 11463] diffusion training Loss: 0.0453659575432539
2024-11-05 06:35:42,371 - INFO - [diffusion][Epoch 11463] diffusion learning rate: 0.001
2024-11-05 06:35:42,372 - INFO - [diffusion][Epoch 11463] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:42,374 - INFO - [diffusion][Epoch 11464] Epoch 11465/12000
2024-11-05 06:35:46,484 - INFO - [diffusion][Epoch 11464] diffusion training Loss: 0.04669533483684063
2024-11-05 06:35:46,486 - INFO - [diffusion][Epoch 11464] diffusion learning rate: 0.001
2024-11-05 06:35:46,487 - INFO - [diffusion][Epoch 11464] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:46,489 - INFO - [diffusion][Epoch 11465] Epoch 11466/12000
2024-11-05 06:35:50,535 - INFO - [diffusion][Epoch 11465] diffusion training Loss: 0.047393559478223324
2024-11-05 06:35:50,538 - INFO - [diffusion][Epoch 11465] diffusion learning rate: 0.001
2024-11-05 06:35:50,540 - INFO - [diffusion][Epoch 11465] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:50,541 - INFO - [diffusion][Epoch 11466] Epoch 11467/12000
2024-11-05 06:35:54,657 - INFO - [diffusion][Epoch 11466] diffusion training Loss: 0.05340524483472109
2024-11-05 06:35:54,659 - INFO - [diffusion][Epoch 11466] diffusion learning rate: 0.001
2024-11-05 06:35:54,661 - INFO - [diffusion][Epoch 11466] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:54,662 - INFO - [diffusion][Epoch 11467] Epoch 11468/12000
2024-11-05 06:35:58,773 - INFO - [diffusion][Epoch 11467] diffusion training Loss: 0.04882987402379513
2024-11-05 06:35:58,775 - INFO - [diffusion][Epoch 11467] diffusion learning rate: 0.001
2024-11-05 06:35:58,776 - INFO - [diffusion][Epoch 11467] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:35:58,778 - INFO - [diffusion][Epoch 11468] Epoch 11469/12000
2024-11-05 06:36:02,851 - INFO - [diffusion][Epoch 11468] diffusion training Loss: 0.047916353680193424
2024-11-05 06:36:02,853 - INFO - [diffusion][Epoch 11468] diffusion learning rate: 0.001
2024-11-05 06:36:02,855 - INFO - [diffusion][Epoch 11468] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:02,856 - INFO - [diffusion][Epoch 11469] Epoch 11470/12000
2024-11-05 06:36:06,801 - INFO - [diffusion][Epoch 11469] diffusion training Loss: 0.045097893103957176
2024-11-05 06:36:06,803 - INFO - [diffusion][Epoch 11469] diffusion learning rate: 0.001
2024-11-05 06:36:06,853 - INFO - [diffusion][Epoch 11469] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:06,855 - INFO - [diffusion][Epoch 11470] Epoch 11471/12000
2024-11-05 06:36:10,927 - INFO - [diffusion][Epoch 11470] diffusion training Loss: 0.04506234638392925
2024-11-05 06:36:10,929 - INFO - [diffusion][Epoch 11470] diffusion learning rate: 0.001
2024-11-05 06:36:10,931 - INFO - [diffusion][Epoch 11470] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:10,932 - INFO - [diffusion][Epoch 11471] Epoch 11472/12000
2024-11-05 06:36:15,042 - INFO - [diffusion][Epoch 11471] diffusion training Loss: 0.044914402067661285
2024-11-05 06:36:15,044 - INFO - [diffusion][Epoch 11471] diffusion learning rate: 0.001
2024-11-05 06:36:15,045 - INFO - [diffusion][Epoch 11471] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:15,047 - INFO - [diffusion][Epoch 11472] Epoch 11473/12000
2024-11-05 06:36:19,102 - INFO - [diffusion][Epoch 11472] diffusion training Loss: 0.04536719433963299
2024-11-05 06:36:19,105 - INFO - [diffusion][Epoch 11472] diffusion learning rate: 0.001
2024-11-05 06:36:19,107 - INFO - [diffusion][Epoch 11472] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:19,108 - INFO - [diffusion][Epoch 11473] Epoch 11474/12000
2024-11-05 06:36:23,195 - INFO - [diffusion][Epoch 11473] diffusion training Loss: 0.04399101808667183
2024-11-05 06:36:23,199 - INFO - [diffusion][Epoch 11473] diffusion learning rate: 0.001
2024-11-05 06:36:23,238 - INFO - [diffusion][Epoch 11473] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:23,240 - INFO - [diffusion][Epoch 11474] Epoch 11475/12000
2024-11-05 06:36:27,322 - INFO - [diffusion][Epoch 11474] diffusion training Loss: 0.048474148847162724
2024-11-05 06:36:27,324 - INFO - [diffusion][Epoch 11474] diffusion learning rate: 0.001
2024-11-05 06:36:27,326 - INFO - [diffusion][Epoch 11474] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:27,327 - INFO - [diffusion][Epoch 11475] Epoch 11476/12000
2024-11-05 06:36:31,405 - INFO - [diffusion][Epoch 11475] diffusion training Loss: 0.04862350504845381
2024-11-05 06:36:31,407 - INFO - [diffusion][Epoch 11475] diffusion learning rate: 0.001
2024-11-05 06:36:31,409 - INFO - [diffusion][Epoch 11475] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:31,410 - INFO - [diffusion][Epoch 11476] Epoch 11477/12000
2024-11-05 06:36:35,463 - INFO - [diffusion][Epoch 11476] diffusion training Loss: 0.045010777190327644
2024-11-05 06:36:35,465 - INFO - [diffusion][Epoch 11476] diffusion learning rate: 0.001
2024-11-05 06:36:35,467 - INFO - [diffusion][Epoch 11476] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:35,468 - INFO - [diffusion][Epoch 11477] Epoch 11478/12000
2024-11-05 06:36:39,594 - INFO - [diffusion][Epoch 11477] diffusion training Loss: 0.043367596343159676
2024-11-05 06:36:39,596 - INFO - [diffusion][Epoch 11477] diffusion learning rate: 0.001
2024-11-05 06:36:39,647 - INFO - [diffusion][Epoch 11477] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:39,649 - INFO - [diffusion][Epoch 11478] Epoch 11479/12000
2024-11-05 06:36:43,705 - INFO - [diffusion][Epoch 11478] diffusion training Loss: 0.04661482386291027
2024-11-05 06:36:43,707 - INFO - [diffusion][Epoch 11478] diffusion learning rate: 0.001
2024-11-05 06:36:43,709 - INFO - [diffusion][Epoch 11478] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:43,710 - INFO - [diffusion][Epoch 11479] Epoch 11480/12000
2024-11-05 06:36:47,800 - INFO - [diffusion][Epoch 11479] diffusion training Loss: 0.04790131188929081
2024-11-05 06:36:47,802 - INFO - [diffusion][Epoch 11479] diffusion learning rate: 0.001
2024-11-05 06:36:47,804 - INFO - [diffusion][Epoch 11479] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:47,805 - INFO - [diffusion][Epoch 11480] Epoch 11481/12000
2024-11-05 06:36:51,877 - INFO - [diffusion][Epoch 11480] diffusion training Loss: 0.045493791811168194
2024-11-05 06:36:51,879 - INFO - [diffusion][Epoch 11480] diffusion learning rate: 0.001
2024-11-05 06:36:51,881 - INFO - [diffusion][Epoch 11480] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:51,882 - INFO - [diffusion][Epoch 11481] Epoch 11482/12000
2024-11-05 06:36:56,420 - INFO - [diffusion][Epoch 11481] diffusion training Loss: 0.04707352630794048
2024-11-05 06:36:56,422 - INFO - [diffusion][Epoch 11481] diffusion learning rate: 0.001
2024-11-05 06:36:56,424 - INFO - [diffusion][Epoch 11481] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:36:56,425 - INFO - [diffusion][Epoch 11482] Epoch 11483/12000
2024-11-05 06:37:00,449 - INFO - [diffusion][Epoch 11482] diffusion training Loss: 0.052859436720609665
2024-11-05 06:37:00,451 - INFO - [diffusion][Epoch 11482] diffusion learning rate: 0.001
2024-11-05 06:37:00,453 - INFO - [diffusion][Epoch 11482] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:00,455 - INFO - [diffusion][Epoch 11483] Epoch 11484/12000
2024-11-05 06:37:04,521 - INFO - [diffusion][Epoch 11483] diffusion training Loss: 0.04536318313330412
2024-11-05 06:37:04,659 - INFO - [diffusion][Epoch 11483] diffusion learning rate: 0.001
2024-11-05 06:37:04,743 - INFO - [diffusion][Epoch 11483] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:04,767 - INFO - [diffusion][Epoch 11484] Epoch 11485/12000
2024-11-05 06:37:08,830 - INFO - [diffusion][Epoch 11484] diffusion training Loss: 0.04879242740571499
2024-11-05 06:37:08,832 - INFO - [diffusion][Epoch 11484] diffusion learning rate: 0.001
2024-11-05 06:37:08,834 - INFO - [diffusion][Epoch 11484] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:08,835 - INFO - [diffusion][Epoch 11485] Epoch 11486/12000
2024-11-05 06:37:12,902 - INFO - [diffusion][Epoch 11485] diffusion training Loss: 0.04509161226451397
2024-11-05 06:37:12,904 - INFO - [diffusion][Epoch 11485] diffusion learning rate: 0.001
2024-11-05 06:37:12,906 - INFO - [diffusion][Epoch 11485] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:12,907 - INFO - [diffusion][Epoch 11486] Epoch 11487/12000
2024-11-05 06:37:16,965 - INFO - [diffusion][Epoch 11486] diffusion training Loss: 0.04584467876702547
2024-11-05 06:37:16,967 - INFO - [diffusion][Epoch 11486] diffusion learning rate: 0.001
2024-11-05 06:37:16,968 - INFO - [diffusion][Epoch 11486] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:16,970 - INFO - [diffusion][Epoch 11487] Epoch 11488/12000
2024-11-05 06:37:21,044 - INFO - [diffusion][Epoch 11487] diffusion training Loss: 0.047252241522073746
2024-11-05 06:37:21,046 - INFO - [diffusion][Epoch 11487] diffusion learning rate: 0.001
2024-11-05 06:37:21,048 - INFO - [diffusion][Epoch 11487] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:21,049 - INFO - [diffusion][Epoch 11488] Epoch 11489/12000
2024-11-05 06:37:25,133 - INFO - [diffusion][Epoch 11488] diffusion training Loss: 0.04720002878457308
2024-11-05 06:37:25,137 - INFO - [diffusion][Epoch 11488] diffusion learning rate: 0.001
2024-11-05 06:37:25,139 - INFO - [diffusion][Epoch 11488] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:25,140 - INFO - [diffusion][Epoch 11489] Epoch 11490/12000
2024-11-05 06:37:29,196 - INFO - [diffusion][Epoch 11489] diffusion training Loss: 0.046833968721330166
2024-11-05 06:37:29,198 - INFO - [diffusion][Epoch 11489] diffusion learning rate: 0.001
2024-11-05 06:37:29,200 - INFO - [diffusion][Epoch 11489] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:29,201 - INFO - [diffusion][Epoch 11490] Epoch 11491/12000
2024-11-05 06:37:33,275 - INFO - [diffusion][Epoch 11490] diffusion training Loss: 0.04602912813425064
2024-11-05 06:37:33,277 - INFO - [diffusion][Epoch 11490] diffusion learning rate: 0.001
2024-11-05 06:37:33,278 - INFO - [diffusion][Epoch 11490] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:33,280 - INFO - [diffusion][Epoch 11491] Epoch 11492/12000
2024-11-05 06:37:37,176 - INFO - [diffusion][Epoch 11491] diffusion training Loss: 0.0419097850099206
2024-11-05 06:37:37,177 - INFO - [diffusion][Epoch 11491] diffusion learning rate: 0.001
2024-11-05 06:37:37,204 - INFO - [diffusion][Epoch 11491] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:37,205 - INFO - [diffusion][Epoch 11492] Epoch 11493/12000
2024-11-05 06:37:41,110 - INFO - [diffusion][Epoch 11492] diffusion training Loss: 0.04613621439784765
2024-11-05 06:37:41,112 - INFO - [diffusion][Epoch 11492] diffusion learning rate: 0.001
2024-11-05 06:37:41,114 - INFO - [diffusion][Epoch 11492] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:41,115 - INFO - [diffusion][Epoch 11493] Epoch 11494/12000
2024-11-05 06:37:45,171 - INFO - [diffusion][Epoch 11493] diffusion training Loss: 0.047819568775594234
2024-11-05 06:37:45,173 - INFO - [diffusion][Epoch 11493] diffusion learning rate: 0.001
2024-11-05 06:37:45,175 - INFO - [diffusion][Epoch 11493] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:45,176 - INFO - [diffusion][Epoch 11494] Epoch 11495/12000
2024-11-05 06:37:49,242 - INFO - [diffusion][Epoch 11494] diffusion training Loss: 0.044692615047097206
2024-11-05 06:37:49,244 - INFO - [diffusion][Epoch 11494] diffusion learning rate: 0.001
2024-11-05 06:37:49,245 - INFO - [diffusion][Epoch 11494] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:49,247 - INFO - [diffusion][Epoch 11495] Epoch 11496/12000
2024-11-05 06:37:53,268 - INFO - [diffusion][Epoch 11495] diffusion training Loss: 0.04789113625884056
2024-11-05 06:37:53,270 - INFO - [diffusion][Epoch 11495] diffusion learning rate: 0.001
2024-11-05 06:37:53,272 - INFO - [diffusion][Epoch 11495] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:53,273 - INFO - [diffusion][Epoch 11496] Epoch 11497/12000
2024-11-05 06:37:57,323 - INFO - [diffusion][Epoch 11496] diffusion training Loss: 0.04578486829996109
2024-11-05 06:37:57,325 - INFO - [diffusion][Epoch 11496] diffusion learning rate: 0.001
2024-11-05 06:37:57,327 - INFO - [diffusion][Epoch 11496] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:37:57,328 - INFO - [diffusion][Epoch 11497] Epoch 11498/12000
2024-11-05 06:38:01,448 - INFO - [diffusion][Epoch 11497] diffusion training Loss: 0.04326857626438141
2024-11-05 06:38:01,450 - INFO - [diffusion][Epoch 11497] diffusion learning rate: 0.001
2024-11-05 06:38:01,452 - INFO - [diffusion][Epoch 11497] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:01,453 - INFO - [diffusion][Epoch 11498] Epoch 11499/12000
2024-11-05 06:38:05,539 - INFO - [diffusion][Epoch 11498] diffusion training Loss: 0.047655665315687656
2024-11-05 06:38:05,541 - INFO - [diffusion][Epoch 11498] diffusion learning rate: 0.001
2024-11-05 06:38:05,602 - INFO - [diffusion][Epoch 11498] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:05,603 - INFO - [diffusion][Epoch 11499] Epoch 11500/12000
2024-11-05 06:38:09,614 - INFO - [diffusion][Epoch 11499] diffusion training Loss: 0.045958779752254486
2024-11-05 06:38:09,616 - INFO - [diffusion][Epoch 11499] diffusion learning rate: 0.001
2024-11-05 06:38:09,618 - INFO - [diffusion][Epoch 11499] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:09,619 - INFO - [diffusion][Epoch 11500] Epoch 11501/12000
2024-11-05 06:38:13,616 - INFO - [diffusion][Epoch 11500] diffusion training Loss: 0.04300097934901714
2024-11-05 06:38:13,618 - INFO - [diffusion][Epoch 11500] diffusion learning rate: 0.001
2024-11-05 06:38:13,619 - INFO - [diffusion][Epoch 11500] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:13,621 - INFO - [diffusion][Epoch 11501] Epoch 11502/12000
2024-11-05 06:38:17,691 - INFO - [diffusion][Epoch 11501] diffusion training Loss: 0.052901978604495525
2024-11-05 06:38:17,693 - INFO - [diffusion][Epoch 11501] diffusion learning rate: 0.001
2024-11-05 06:38:17,695 - INFO - [diffusion][Epoch 11501] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:17,696 - INFO - [diffusion][Epoch 11502] Epoch 11503/12000
2024-11-05 06:38:22,039 - INFO - [diffusion][Epoch 11502] diffusion training Loss: 0.04774363059550524
2024-11-05 06:38:22,041 - INFO - [diffusion][Epoch 11502] diffusion learning rate: 0.001
2024-11-05 06:38:22,043 - INFO - [diffusion][Epoch 11502] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:22,044 - INFO - [diffusion][Epoch 11503] Epoch 11504/12000
2024-11-05 06:38:26,130 - INFO - [diffusion][Epoch 11503] diffusion training Loss: 0.04583592340350151
2024-11-05 06:38:26,133 - INFO - [diffusion][Epoch 11503] diffusion learning rate: 0.001
2024-11-05 06:38:26,135 - INFO - [diffusion][Epoch 11503] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:26,136 - INFO - [diffusion][Epoch 11504] Epoch 11505/12000
2024-11-05 06:38:30,195 - INFO - [diffusion][Epoch 11504] diffusion training Loss: 0.04810510762035847
2024-11-05 06:38:30,197 - INFO - [diffusion][Epoch 11504] diffusion learning rate: 0.001
2024-11-05 06:38:30,199 - INFO - [diffusion][Epoch 11504] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:30,200 - INFO - [diffusion][Epoch 11505] Epoch 11506/12000
2024-11-05 06:38:34,281 - INFO - [diffusion][Epoch 11505] diffusion training Loss: 0.04522323980927467
2024-11-05 06:38:34,283 - INFO - [diffusion][Epoch 11505] diffusion learning rate: 0.001
2024-11-05 06:38:34,284 - INFO - [diffusion][Epoch 11505] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:34,285 - INFO - [diffusion][Epoch 11506] Epoch 11507/12000
2024-11-05 06:38:38,342 - INFO - [diffusion][Epoch 11506] diffusion training Loss: 0.049075547605752945
2024-11-05 06:38:38,344 - INFO - [diffusion][Epoch 11506] diffusion learning rate: 0.001
2024-11-05 06:38:38,346 - INFO - [diffusion][Epoch 11506] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:38,347 - INFO - [diffusion][Epoch 11507] Epoch 11508/12000
2024-11-05 06:38:42,399 - INFO - [diffusion][Epoch 11507] diffusion training Loss: 0.04750893637537956
2024-11-05 06:38:42,401 - INFO - [diffusion][Epoch 11507] diffusion learning rate: 0.001
2024-11-05 06:38:42,403 - INFO - [diffusion][Epoch 11507] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:42,404 - INFO - [diffusion][Epoch 11508] Epoch 11509/12000
2024-11-05 06:38:46,487 - INFO - [diffusion][Epoch 11508] diffusion training Loss: 0.04462906811386347
2024-11-05 06:38:46,489 - INFO - [diffusion][Epoch 11508] diffusion learning rate: 0.001
2024-11-05 06:38:46,491 - INFO - [diffusion][Epoch 11508] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:46,492 - INFO - [diffusion][Epoch 11509] Epoch 11510/12000
2024-11-05 06:38:50,539 - INFO - [diffusion][Epoch 11509] diffusion training Loss: 0.04602138325572014
2024-11-05 06:38:50,541 - INFO - [diffusion][Epoch 11509] diffusion learning rate: 0.001
2024-11-05 06:38:50,542 - INFO - [diffusion][Epoch 11509] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:50,544 - INFO - [diffusion][Epoch 11510] Epoch 11511/12000
2024-11-05 06:38:54,622 - INFO - [diffusion][Epoch 11510] diffusion training Loss: 0.05006587132811546
2024-11-05 06:38:54,624 - INFO - [diffusion][Epoch 11510] diffusion learning rate: 0.001
2024-11-05 06:38:54,626 - INFO - [diffusion][Epoch 11510] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:54,627 - INFO - [diffusion][Epoch 11511] Epoch 11512/12000
2024-11-05 06:38:58,738 - INFO - [diffusion][Epoch 11511] diffusion training Loss: 0.041812291368842125
2024-11-05 06:38:58,740 - INFO - [diffusion][Epoch 11511] diffusion learning rate: 0.001
2024-11-05 06:38:58,741 - INFO - [diffusion][Epoch 11511] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:38:58,743 - INFO - [diffusion][Epoch 11512] Epoch 11513/12000
2024-11-05 06:39:02,846 - INFO - [diffusion][Epoch 11512] diffusion training Loss: 0.04644118249416351
2024-11-05 06:39:02,848 - INFO - [diffusion][Epoch 11512] diffusion learning rate: 0.001
2024-11-05 06:39:02,849 - INFO - [diffusion][Epoch 11512] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:02,851 - INFO - [diffusion][Epoch 11513] Epoch 11514/12000
2024-11-05 06:39:06,937 - INFO - [diffusion][Epoch 11513] diffusion training Loss: 0.04944742005318403
2024-11-05 06:39:06,939 - INFO - [diffusion][Epoch 11513] diffusion learning rate: 0.001
2024-11-05 06:39:06,958 - INFO - [diffusion][Epoch 11513] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:06,959 - INFO - [diffusion][Epoch 11514] Epoch 11515/12000
2024-11-05 06:39:10,993 - INFO - [diffusion][Epoch 11514] diffusion training Loss: 0.04873779881745577
2024-11-05 06:39:10,995 - INFO - [diffusion][Epoch 11514] diffusion learning rate: 0.001
2024-11-05 06:39:10,996 - INFO - [diffusion][Epoch 11514] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:10,998 - INFO - [diffusion][Epoch 11515] Epoch 11516/12000
2024-11-05 06:39:14,951 - INFO - [diffusion][Epoch 11515] diffusion training Loss: 0.047507951967418194
2024-11-05 06:39:14,953 - INFO - [diffusion][Epoch 11515] diffusion learning rate: 0.001
2024-11-05 06:39:14,955 - INFO - [diffusion][Epoch 11515] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:14,956 - INFO - [diffusion][Epoch 11516] Epoch 11517/12000
2024-11-05 06:39:18,997 - INFO - [diffusion][Epoch 11516] diffusion training Loss: 0.04712778702378273
2024-11-05 06:39:19,032 - INFO - [diffusion][Epoch 11516] diffusion learning rate: 0.001
2024-11-05 06:39:19,033 - INFO - [diffusion][Epoch 11516] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:19,035 - INFO - [diffusion][Epoch 11517] Epoch 11518/12000
2024-11-05 06:39:23,062 - INFO - [diffusion][Epoch 11517] diffusion training Loss: 0.04707578103989363
2024-11-05 06:39:23,064 - INFO - [diffusion][Epoch 11517] diffusion learning rate: 0.001
2024-11-05 06:39:23,066 - INFO - [diffusion][Epoch 11517] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:23,067 - INFO - [diffusion][Epoch 11518] Epoch 11519/12000
2024-11-05 06:39:27,133 - INFO - [diffusion][Epoch 11518] diffusion training Loss: 0.04657723102718592
2024-11-05 06:39:27,136 - INFO - [diffusion][Epoch 11518] diffusion learning rate: 0.001
2024-11-05 06:39:27,138 - INFO - [diffusion][Epoch 11518] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:27,139 - INFO - [diffusion][Epoch 11519] Epoch 11520/12000
2024-11-05 06:39:31,228 - INFO - [diffusion][Epoch 11519] diffusion training Loss: 0.044059847481548786
2024-11-05 06:39:31,230 - INFO - [diffusion][Epoch 11519] diffusion learning rate: 0.001
2024-11-05 06:39:31,231 - INFO - [diffusion][Epoch 11519] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:31,233 - INFO - [diffusion][Epoch 11520] Epoch 11521/12000
2024-11-05 06:39:35,302 - INFO - [diffusion][Epoch 11520] diffusion training Loss: 0.046908278949558735
2024-11-05 06:39:35,304 - INFO - [diffusion][Epoch 11520] diffusion learning rate: 0.001
2024-11-05 06:39:35,306 - INFO - [diffusion][Epoch 11520] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:35,307 - INFO - [diffusion][Epoch 11521] Epoch 11522/12000
2024-11-05 06:39:39,385 - INFO - [diffusion][Epoch 11521] diffusion training Loss: 0.04284241981804371
2024-11-05 06:39:39,387 - INFO - [diffusion][Epoch 11521] diffusion learning rate: 0.001
2024-11-05 06:39:39,436 - INFO - [diffusion][Epoch 11521] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:39,437 - INFO - [diffusion][Epoch 11522] Epoch 11523/12000
2024-11-05 06:39:43,512 - INFO - [diffusion][Epoch 11522] diffusion training Loss: 0.04042589198797941
2024-11-05 06:39:43,513 - INFO - [diffusion][Epoch 11522] diffusion learning rate: 0.001
2024-11-05 06:39:43,515 - INFO - [diffusion][Epoch 11522] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:43,516 - INFO - [diffusion][Epoch 11523] Epoch 11524/12000
2024-11-05 06:39:47,862 - INFO - [diffusion][Epoch 11523] diffusion training Loss: 0.043768974021077156
2024-11-05 06:39:47,864 - INFO - [diffusion][Epoch 11523] diffusion learning rate: 0.001
2024-11-05 06:39:47,866 - INFO - [diffusion][Epoch 11523] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:47,867 - INFO - [diffusion][Epoch 11524] Epoch 11525/12000
2024-11-05 06:39:51,956 - INFO - [diffusion][Epoch 11524] diffusion training Loss: 0.046613394282758236
2024-11-05 06:39:51,958 - INFO - [diffusion][Epoch 11524] diffusion learning rate: 0.001
2024-11-05 06:39:51,960 - INFO - [diffusion][Epoch 11524] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:51,961 - INFO - [diffusion][Epoch 11525] Epoch 11526/12000
2024-11-05 06:39:56,014 - INFO - [diffusion][Epoch 11525] diffusion training Loss: 0.04519200976938009
2024-11-05 06:39:56,016 - INFO - [diffusion][Epoch 11525] diffusion learning rate: 0.001
2024-11-05 06:39:56,018 - INFO - [diffusion][Epoch 11525] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:39:56,019 - INFO - [diffusion][Epoch 11526] Epoch 11527/12000
2024-11-05 06:40:00,109 - INFO - [diffusion][Epoch 11526] diffusion training Loss: 0.0499531514942646
2024-11-05 06:40:00,111 - INFO - [diffusion][Epoch 11526] diffusion learning rate: 0.001
2024-11-05 06:40:00,113 - INFO - [diffusion][Epoch 11526] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:00,114 - INFO - [diffusion][Epoch 11527] Epoch 11528/12000
2024-11-05 06:40:04,234 - INFO - [diffusion][Epoch 11527] diffusion training Loss: 0.04648279678076506
2024-11-05 06:40:04,236 - INFO - [diffusion][Epoch 11527] diffusion learning rate: 0.001
2024-11-05 06:40:04,238 - INFO - [diffusion][Epoch 11527] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:04,239 - INFO - [diffusion][Epoch 11528] Epoch 11529/12000
2024-11-05 06:40:08,327 - INFO - [diffusion][Epoch 11528] diffusion training Loss: 0.043570962734520435
2024-11-05 06:40:08,329 - INFO - [diffusion][Epoch 11528] diffusion learning rate: 0.001
2024-11-05 06:40:08,331 - INFO - [diffusion][Epoch 11528] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:08,332 - INFO - [diffusion][Epoch 11529] Epoch 11530/12000
2024-11-05 06:40:12,434 - INFO - [diffusion][Epoch 11529] diffusion training Loss: 0.049018530175089836
2024-11-05 06:40:12,702 - INFO - [diffusion][Epoch 11529] diffusion learning rate: 0.001
2024-11-05 06:40:12,704 - INFO - [diffusion][Epoch 11529] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:12,705 - INFO - [diffusion][Epoch 11530] Epoch 11531/12000
2024-11-05 06:40:16,845 - INFO - [diffusion][Epoch 11530] diffusion training Loss: 0.043718245811760426
2024-11-05 06:40:16,847 - INFO - [diffusion][Epoch 11530] diffusion learning rate: 0.001
2024-11-05 06:40:16,848 - INFO - [diffusion][Epoch 11530] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:16,850 - INFO - [diffusion][Epoch 11531] Epoch 11532/12000
2024-11-05 06:40:20,969 - INFO - [diffusion][Epoch 11531] diffusion training Loss: 0.049679236486554146
2024-11-05 06:40:20,971 - INFO - [diffusion][Epoch 11531] diffusion learning rate: 0.001
2024-11-05 06:40:20,973 - INFO - [diffusion][Epoch 11531] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:20,974 - INFO - [diffusion][Epoch 11532] Epoch 11533/12000
2024-11-05 06:40:24,935 - INFO - [diffusion][Epoch 11532] diffusion training Loss: 0.04061275999993086
2024-11-05 06:40:24,937 - INFO - [diffusion][Epoch 11532] diffusion learning rate: 0.001
2024-11-05 06:40:24,939 - INFO - [diffusion][Epoch 11532] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:24,940 - INFO - [diffusion][Epoch 11533] Epoch 11534/12000
2024-11-05 06:40:28,985 - INFO - [diffusion][Epoch 11533] diffusion training Loss: 0.04578360263258219
2024-11-05 06:40:28,989 - INFO - [diffusion][Epoch 11533] diffusion learning rate: 0.001
2024-11-05 06:40:28,991 - INFO - [diffusion][Epoch 11533] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:28,992 - INFO - [diffusion][Epoch 11534] Epoch 11535/12000
2024-11-05 06:40:33,072 - INFO - [diffusion][Epoch 11534] diffusion training Loss: 0.04617641773074865
2024-11-05 06:40:33,073 - INFO - [diffusion][Epoch 11534] diffusion learning rate: 0.001
2024-11-05 06:40:33,075 - INFO - [diffusion][Epoch 11534] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:33,077 - INFO - [diffusion][Epoch 11535] Epoch 11536/12000
2024-11-05 06:40:37,128 - INFO - [diffusion][Epoch 11535] diffusion training Loss: 0.04616626352071762
2024-11-05 06:40:37,131 - INFO - [diffusion][Epoch 11535] diffusion learning rate: 0.001
2024-11-05 06:40:37,132 - INFO - [diffusion][Epoch 11535] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:37,134 - INFO - [diffusion][Epoch 11536] Epoch 11537/12000
2024-11-05 06:40:41,172 - INFO - [diffusion][Epoch 11536] diffusion training Loss: 0.05448778998106718
2024-11-05 06:40:41,220 - INFO - [diffusion][Epoch 11536] diffusion learning rate: 0.001
2024-11-05 06:40:41,222 - INFO - [diffusion][Epoch 11536] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:41,223 - INFO - [diffusion][Epoch 11537] Epoch 11538/12000
2024-11-05 06:40:45,298 - INFO - [diffusion][Epoch 11537] diffusion training Loss: 0.050623429007828236
2024-11-05 06:40:45,299 - INFO - [diffusion][Epoch 11537] diffusion learning rate: 0.001
2024-11-05 06:40:45,301 - INFO - [diffusion][Epoch 11537] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:45,302 - INFO - [diffusion][Epoch 11538] Epoch 11539/12000
2024-11-05 06:40:49,360 - INFO - [diffusion][Epoch 11538] diffusion training Loss: 0.050498430617153645
2024-11-05 06:40:49,362 - INFO - [diffusion][Epoch 11538] diffusion learning rate: 0.001
2024-11-05 06:40:49,363 - INFO - [diffusion][Epoch 11538] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:49,365 - INFO - [diffusion][Epoch 11539] Epoch 11540/12000
2024-11-05 06:40:53,462 - INFO - [diffusion][Epoch 11539] diffusion training Loss: 0.048296560533344746
2024-11-05 06:40:53,464 - INFO - [diffusion][Epoch 11539] diffusion learning rate: 0.001
2024-11-05 06:40:53,492 - INFO - [diffusion][Epoch 11539] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:53,493 - INFO - [diffusion][Epoch 11540] Epoch 11541/12000
2024-11-05 06:40:57,537 - INFO - [diffusion][Epoch 11540] diffusion training Loss: 0.05075496342033148
2024-11-05 06:40:57,539 - INFO - [diffusion][Epoch 11540] diffusion learning rate: 0.001
2024-11-05 06:40:57,540 - INFO - [diffusion][Epoch 11540] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:40:57,542 - INFO - [diffusion][Epoch 11541] Epoch 11542/12000
2024-11-05 06:41:01,654 - INFO - [diffusion][Epoch 11541] diffusion training Loss: 0.047296980395913124
2024-11-05 06:41:01,656 - INFO - [diffusion][Epoch 11541] diffusion learning rate: 0.001
2024-11-05 06:41:01,658 - INFO - [diffusion][Epoch 11541] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:01,659 - INFO - [diffusion][Epoch 11542] Epoch 11543/12000
2024-11-05 06:41:05,805 - INFO - [diffusion][Epoch 11542] diffusion training Loss: 0.047110569663345814
2024-11-05 06:41:05,807 - INFO - [diffusion][Epoch 11542] diffusion learning rate: 0.001
2024-11-05 06:41:05,809 - INFO - [diffusion][Epoch 11542] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:05,810 - INFO - [diffusion][Epoch 11543] Epoch 11544/12000
2024-11-05 06:41:09,877 - INFO - [diffusion][Epoch 11543] diffusion training Loss: 0.0446770703420043
2024-11-05 06:41:09,879 - INFO - [diffusion][Epoch 11543] diffusion learning rate: 0.001
2024-11-05 06:41:09,881 - INFO - [diffusion][Epoch 11543] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:09,882 - INFO - [diffusion][Epoch 11544] Epoch 11545/12000
2024-11-05 06:41:13,976 - INFO - [diffusion][Epoch 11544] diffusion training Loss: 0.049917442724108696
2024-11-05 06:41:13,978 - INFO - [diffusion][Epoch 11544] diffusion learning rate: 0.001
2024-11-05 06:41:13,980 - INFO - [diffusion][Epoch 11544] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:13,981 - INFO - [diffusion][Epoch 11545] Epoch 11546/12000
2024-11-05 06:41:18,103 - INFO - [diffusion][Epoch 11545] diffusion training Loss: 0.04494485259056091
2024-11-05 06:41:18,105 - INFO - [diffusion][Epoch 11545] diffusion learning rate: 0.001
2024-11-05 06:41:18,132 - INFO - [diffusion][Epoch 11545] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:18,133 - INFO - [diffusion][Epoch 11546] Epoch 11547/12000
2024-11-05 06:41:22,255 - INFO - [diffusion][Epoch 11546] diffusion training Loss: 0.045076360926032066
2024-11-05 06:41:22,257 - INFO - [diffusion][Epoch 11546] diffusion learning rate: 0.001
2024-11-05 06:41:22,259 - INFO - [diffusion][Epoch 11546] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:22,261 - INFO - [diffusion][Epoch 11547] Epoch 11548/12000
2024-11-05 06:41:26,358 - INFO - [diffusion][Epoch 11547] diffusion training Loss: 0.049369050189852715
2024-11-05 06:41:26,360 - INFO - [diffusion][Epoch 11547] diffusion learning rate: 0.001
2024-11-05 06:41:26,362 - INFO - [diffusion][Epoch 11547] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:26,363 - INFO - [diffusion][Epoch 11548] Epoch 11549/12000
2024-11-05 06:41:30,415 - INFO - [diffusion][Epoch 11548] diffusion training Loss: 0.04536144435405731
2024-11-05 06:41:30,620 - INFO - [diffusion][Epoch 11548] diffusion learning rate: 0.001
2024-11-05 06:41:30,622 - INFO - [diffusion][Epoch 11548] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:30,623 - INFO - [diffusion][Epoch 11549] Epoch 11550/12000
2024-11-05 06:41:34,489 - INFO - [diffusion][Epoch 11549] diffusion training Loss: 0.046825326047837734
2024-11-05 06:41:34,491 - INFO - [diffusion][Epoch 11549] diffusion learning rate: 0.001
2024-11-05 06:41:34,493 - INFO - [diffusion][Epoch 11549] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:34,494 - INFO - [diffusion][Epoch 11550] Epoch 11551/12000
2024-11-05 06:41:38,410 - INFO - [diffusion][Epoch 11550] diffusion training Loss: 0.04443160258233547
2024-11-05 06:41:38,412 - INFO - [diffusion][Epoch 11550] diffusion learning rate: 0.001
2024-11-05 06:41:38,413 - INFO - [diffusion][Epoch 11550] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:38,415 - INFO - [diffusion][Epoch 11551] Epoch 11552/12000
2024-11-05 06:41:42,499 - INFO - [diffusion][Epoch 11551] diffusion training Loss: 0.049969217739999294
2024-11-05 06:41:42,501 - INFO - [diffusion][Epoch 11551] diffusion learning rate: 0.001
2024-11-05 06:41:42,554 - INFO - [diffusion][Epoch 11551] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:42,556 - INFO - [diffusion][Epoch 11552] Epoch 11553/12000
2024-11-05 06:41:46,477 - INFO - [diffusion][Epoch 11552] diffusion training Loss: 0.045929694548249245
2024-11-05 06:41:46,479 - INFO - [diffusion][Epoch 11552] diffusion learning rate: 0.001
2024-11-05 06:41:46,481 - INFO - [diffusion][Epoch 11552] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:46,482 - INFO - [diffusion][Epoch 11553] Epoch 11554/12000
2024-11-05 06:41:50,532 - INFO - [diffusion][Epoch 11553] diffusion training Loss: 0.05072024930268526
2024-11-05 06:41:50,534 - INFO - [diffusion][Epoch 11553] diffusion learning rate: 0.001
2024-11-05 06:41:50,536 - INFO - [diffusion][Epoch 11553] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:50,537 - INFO - [diffusion][Epoch 11554] Epoch 11555/12000
2024-11-05 06:41:54,601 - INFO - [diffusion][Epoch 11554] diffusion training Loss: 0.044206210412085056
2024-11-05 06:41:54,603 - INFO - [diffusion][Epoch 11554] diffusion learning rate: 0.001
2024-11-05 06:41:54,605 - INFO - [diffusion][Epoch 11554] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:54,606 - INFO - [diffusion][Epoch 11555] Epoch 11556/12000
2024-11-05 06:41:58,664 - INFO - [diffusion][Epoch 11555] diffusion training Loss: 0.048887213692069054
2024-11-05 06:41:58,666 - INFO - [diffusion][Epoch 11555] diffusion learning rate: 0.001
2024-11-05 06:41:58,667 - INFO - [diffusion][Epoch 11555] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:41:58,669 - INFO - [diffusion][Epoch 11556] Epoch 11557/12000
2024-11-05 06:42:02,678 - INFO - [diffusion][Epoch 11556] diffusion training Loss: 0.04262090194970369
2024-11-05 06:42:02,680 - INFO - [diffusion][Epoch 11556] diffusion learning rate: 0.001
2024-11-05 06:42:02,682 - INFO - [diffusion][Epoch 11556] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:02,683 - INFO - [diffusion][Epoch 11557] Epoch 11558/12000
2024-11-05 06:42:06,845 - INFO - [diffusion][Epoch 11557] diffusion training Loss: 0.047822944819927216
2024-11-05 06:42:06,847 - INFO - [diffusion][Epoch 11557] diffusion learning rate: 0.001
2024-11-05 06:42:06,849 - INFO - [diffusion][Epoch 11557] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:06,850 - INFO - [diffusion][Epoch 11558] Epoch 11559/12000
2024-11-05 06:42:10,910 - INFO - [diffusion][Epoch 11558] diffusion training Loss: 0.044191946275532246
2024-11-05 06:42:10,912 - INFO - [diffusion][Epoch 11558] diffusion learning rate: 0.001
2024-11-05 06:42:10,932 - INFO - [diffusion][Epoch 11558] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:10,934 - INFO - [diffusion][Epoch 11559] Epoch 11560/12000
2024-11-05 06:42:14,999 - INFO - [diffusion][Epoch 11559] diffusion training Loss: 0.04454471729695797
2024-11-05 06:42:15,001 - INFO - [diffusion][Epoch 11559] diffusion learning rate: 0.001
2024-11-05 06:42:15,002 - INFO - [diffusion][Epoch 11559] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:15,004 - INFO - [diffusion][Epoch 11560] Epoch 11561/12000
2024-11-05 06:42:19,115 - INFO - [diffusion][Epoch 11560] diffusion training Loss: 0.04493624530732632
2024-11-05 06:42:19,116 - INFO - [diffusion][Epoch 11560] diffusion learning rate: 0.001
2024-11-05 06:42:19,118 - INFO - [diffusion][Epoch 11560] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:19,119 - INFO - [diffusion][Epoch 11561] Epoch 11562/12000
2024-11-05 06:42:23,231 - INFO - [diffusion][Epoch 11561] diffusion training Loss: 0.043040388263762
2024-11-05 06:42:23,233 - INFO - [diffusion][Epoch 11561] diffusion learning rate: 0.001
2024-11-05 06:42:23,235 - INFO - [diffusion][Epoch 11561] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:23,236 - INFO - [diffusion][Epoch 11562] Epoch 11563/12000
2024-11-05 06:42:27,341 - INFO - [diffusion][Epoch 11562] diffusion training Loss: 0.04495654348284006
2024-11-05 06:42:27,343 - INFO - [diffusion][Epoch 11562] diffusion learning rate: 0.001
2024-11-05 06:42:27,345 - INFO - [diffusion][Epoch 11562] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:27,346 - INFO - [diffusion][Epoch 11563] Epoch 11564/12000
2024-11-05 06:42:31,450 - INFO - [diffusion][Epoch 11563] diffusion training Loss: 0.04518100339919329
2024-11-05 06:42:31,454 - INFO - [diffusion][Epoch 11563] diffusion learning rate: 0.001
2024-11-05 06:42:31,455 - INFO - [diffusion][Epoch 11563] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:31,457 - INFO - [diffusion][Epoch 11564] Epoch 11565/12000
2024-11-05 06:42:35,548 - INFO - [diffusion][Epoch 11564] diffusion training Loss: 0.04250919818878174
2024-11-05 06:42:35,550 - INFO - [diffusion][Epoch 11564] diffusion learning rate: 0.001
2024-11-05 06:42:35,551 - INFO - [diffusion][Epoch 11564] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:35,553 - INFO - [diffusion][Epoch 11565] Epoch 11566/12000
2024-11-05 06:42:40,161 - INFO - [diffusion][Epoch 11565] diffusion training Loss: 0.04625769145786762
2024-11-05 06:42:40,163 - INFO - [diffusion][Epoch 11565] diffusion learning rate: 0.001
2024-11-05 06:42:40,164 - INFO - [diffusion][Epoch 11565] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:40,166 - INFO - [diffusion][Epoch 11566] Epoch 11567/12000
2024-11-05 06:42:44,217 - INFO - [diffusion][Epoch 11566] diffusion training Loss: 0.05255436338484287
2024-11-05 06:42:44,219 - INFO - [diffusion][Epoch 11566] diffusion learning rate: 0.001
2024-11-05 06:42:44,221 - INFO - [diffusion][Epoch 11566] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:44,222 - INFO - [diffusion][Epoch 11567] Epoch 11568/12000
2024-11-05 06:42:48,353 - INFO - [diffusion][Epoch 11567] diffusion training Loss: 0.048903439193964005
2024-11-05 06:42:48,355 - INFO - [diffusion][Epoch 11567] diffusion learning rate: 0.001
2024-11-05 06:42:48,357 - INFO - [diffusion][Epoch 11567] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:48,358 - INFO - [diffusion][Epoch 11568] Epoch 11569/12000
2024-11-05 06:42:52,476 - INFO - [diffusion][Epoch 11568] diffusion training Loss: 0.048011414706707
2024-11-05 06:42:52,478 - INFO - [diffusion][Epoch 11568] diffusion learning rate: 0.001
2024-11-05 06:42:52,480 - INFO - [diffusion][Epoch 11568] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:52,481 - INFO - [diffusion][Epoch 11569] Epoch 11570/12000
2024-11-05 06:42:56,520 - INFO - [diffusion][Epoch 11569] diffusion training Loss: 0.047040210105478764
2024-11-05 06:42:56,522 - INFO - [diffusion][Epoch 11569] diffusion learning rate: 0.001
2024-11-05 06:42:56,524 - INFO - [diffusion][Epoch 11569] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:42:56,525 - INFO - [diffusion][Epoch 11570] Epoch 11571/12000
2024-11-05 06:43:00,600 - INFO - [diffusion][Epoch 11570] diffusion training Loss: 0.045249505899846554
2024-11-05 06:43:00,602 - INFO - [diffusion][Epoch 11570] diffusion learning rate: 0.001
2024-11-05 06:43:00,604 - INFO - [diffusion][Epoch 11570] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:00,605 - INFO - [diffusion][Epoch 11571] Epoch 11572/12000
2024-11-05 06:43:04,671 - INFO - [diffusion][Epoch 11571] diffusion training Loss: 0.04568693973124027
2024-11-05 06:43:04,673 - INFO - [diffusion][Epoch 11571] diffusion learning rate: 0.001
2024-11-05 06:43:04,711 - INFO - [diffusion][Epoch 11571] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:04,712 - INFO - [diffusion][Epoch 11572] Epoch 11573/12000
2024-11-05 06:43:08,778 - INFO - [diffusion][Epoch 11572] diffusion training Loss: 0.0484432727098465
2024-11-05 06:43:08,780 - INFO - [diffusion][Epoch 11572] diffusion learning rate: 0.001
2024-11-05 06:43:08,782 - INFO - [diffusion][Epoch 11572] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:08,783 - INFO - [diffusion][Epoch 11573] Epoch 11574/12000
2024-11-05 06:43:12,877 - INFO - [diffusion][Epoch 11573] diffusion training Loss: 0.042633079923689365
2024-11-05 06:43:12,879 - INFO - [diffusion][Epoch 11573] diffusion learning rate: 0.001
2024-11-05 06:43:12,881 - INFO - [diffusion][Epoch 11573] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:12,882 - INFO - [diffusion][Epoch 11574] Epoch 11575/12000
2024-11-05 06:43:16,911 - INFO - [diffusion][Epoch 11574] diffusion training Loss: 0.04434420354664326
2024-11-05 06:43:16,913 - INFO - [diffusion][Epoch 11574] diffusion learning rate: 0.001
2024-11-05 06:43:16,914 - INFO - [diffusion][Epoch 11574] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:16,916 - INFO - [diffusion][Epoch 11575] Epoch 11576/12000
2024-11-05 06:43:20,967 - INFO - [diffusion][Epoch 11575] diffusion training Loss: 0.04733030218631029
2024-11-05 06:43:20,969 - INFO - [diffusion][Epoch 11575] diffusion learning rate: 0.001
2024-11-05 06:43:20,997 - INFO - [diffusion][Epoch 11575] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:20,998 - INFO - [diffusion][Epoch 11576] Epoch 11577/12000
2024-11-05 06:43:25,081 - INFO - [diffusion][Epoch 11576] diffusion training Loss: 0.04509494546800852
2024-11-05 06:43:25,083 - INFO - [diffusion][Epoch 11576] diffusion learning rate: 0.001
2024-11-05 06:43:25,085 - INFO - [diffusion][Epoch 11576] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:25,086 - INFO - [diffusion][Epoch 11577] Epoch 11578/12000
2024-11-05 06:43:29,184 - INFO - [diffusion][Epoch 11577] diffusion training Loss: 0.04228905215859413
2024-11-05 06:43:29,186 - INFO - [diffusion][Epoch 11577] diffusion learning rate: 0.001
2024-11-05 06:43:29,188 - INFO - [diffusion][Epoch 11577] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:29,189 - INFO - [diffusion][Epoch 11578] Epoch 11579/12000
2024-11-05 06:43:33,274 - INFO - [diffusion][Epoch 11578] diffusion training Loss: 0.048227122984826565
2024-11-05 06:43:33,278 - INFO - [diffusion][Epoch 11578] diffusion learning rate: 0.001
2024-11-05 06:43:33,280 - INFO - [diffusion][Epoch 11578] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:33,281 - INFO - [diffusion][Epoch 11579] Epoch 11580/12000
2024-11-05 06:43:37,343 - INFO - [diffusion][Epoch 11579] diffusion training Loss: 0.04101633932441473
2024-11-05 06:43:37,345 - INFO - [diffusion][Epoch 11579] diffusion learning rate: 0.001
2024-11-05 06:43:37,383 - INFO - [diffusion][Epoch 11579] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:37,385 - INFO - [diffusion][Epoch 11580] Epoch 11581/12000
2024-11-05 06:43:41,430 - INFO - [diffusion][Epoch 11580] diffusion training Loss: 0.04724177345633507
2024-11-05 06:43:41,431 - INFO - [diffusion][Epoch 11580] diffusion learning rate: 0.001
2024-11-05 06:43:41,433 - INFO - [diffusion][Epoch 11580] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:41,434 - INFO - [diffusion][Epoch 11581] Epoch 11582/12000
2024-11-05 06:43:45,495 - INFO - [diffusion][Epoch 11581] diffusion training Loss: 0.04562118835747242
2024-11-05 06:43:45,497 - INFO - [diffusion][Epoch 11581] diffusion learning rate: 0.001
2024-11-05 06:43:45,499 - INFO - [diffusion][Epoch 11581] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:45,501 - INFO - [diffusion][Epoch 11582] Epoch 11583/12000
2024-11-05 06:43:49,541 - INFO - [diffusion][Epoch 11582] diffusion training Loss: 0.0471405815333128
2024-11-05 06:43:49,543 - INFO - [diffusion][Epoch 11582] diffusion learning rate: 0.001
2024-11-05 06:43:49,571 - INFO - [diffusion][Epoch 11582] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:49,572 - INFO - [diffusion][Epoch 11583] Epoch 11584/12000
2024-11-05 06:43:53,698 - INFO - [diffusion][Epoch 11583] diffusion training Loss: 0.04617249220609665
2024-11-05 06:43:53,700 - INFO - [diffusion][Epoch 11583] diffusion learning rate: 0.001
2024-11-05 06:43:53,702 - INFO - [diffusion][Epoch 11583] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:53,703 - INFO - [diffusion][Epoch 11584] Epoch 11585/12000
2024-11-05 06:43:57,789 - INFO - [diffusion][Epoch 11584] diffusion training Loss: 0.0456825690343976
2024-11-05 06:43:57,791 - INFO - [diffusion][Epoch 11584] diffusion learning rate: 0.001
2024-11-05 06:43:57,792 - INFO - [diffusion][Epoch 11584] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:43:57,794 - INFO - [diffusion][Epoch 11585] Epoch 11586/12000
2024-11-05 06:44:02,128 - INFO - [diffusion][Epoch 11585] diffusion training Loss: 0.045917436480522156
2024-11-05 06:44:02,130 - INFO - [diffusion][Epoch 11585] diffusion learning rate: 0.001
2024-11-05 06:44:02,132 - INFO - [diffusion][Epoch 11585] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:02,133 - INFO - [diffusion][Epoch 11586] Epoch 11587/12000
2024-11-05 06:44:06,182 - INFO - [diffusion][Epoch 11586] diffusion training Loss: 0.04413938242942095
2024-11-05 06:44:06,184 - INFO - [diffusion][Epoch 11586] diffusion learning rate: 0.001
2024-11-05 06:44:06,212 - INFO - [diffusion][Epoch 11586] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:06,213 - INFO - [diffusion][Epoch 11587] Epoch 11588/12000
2024-11-05 06:44:10,274 - INFO - [diffusion][Epoch 11587] diffusion training Loss: 0.04618045687675476
2024-11-05 06:44:10,276 - INFO - [diffusion][Epoch 11587] diffusion learning rate: 0.001
2024-11-05 06:44:10,278 - INFO - [diffusion][Epoch 11587] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:10,279 - INFO - [diffusion][Epoch 11588] Epoch 11589/12000
2024-11-05 06:44:14,245 - INFO - [diffusion][Epoch 11588] diffusion training Loss: 0.045686443336308
2024-11-05 06:44:14,247 - INFO - [diffusion][Epoch 11588] diffusion learning rate: 0.001
2024-11-05 06:44:14,249 - INFO - [diffusion][Epoch 11588] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:14,250 - INFO - [diffusion][Epoch 11589] Epoch 11590/12000
2024-11-05 06:44:18,316 - INFO - [diffusion][Epoch 11589] diffusion training Loss: 0.04421636834740639
2024-11-05 06:44:18,318 - INFO - [diffusion][Epoch 11589] diffusion learning rate: 0.001
2024-11-05 06:44:18,345 - INFO - [diffusion][Epoch 11589] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:18,346 - INFO - [diffusion][Epoch 11590] Epoch 11591/12000
2024-11-05 06:44:22,440 - INFO - [diffusion][Epoch 11590] diffusion training Loss: 0.04789601359516382
2024-11-05 06:44:22,443 - INFO - [diffusion][Epoch 11590] diffusion learning rate: 0.001
2024-11-05 06:44:22,445 - INFO - [diffusion][Epoch 11590] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:22,446 - INFO - [diffusion][Epoch 11591] Epoch 11592/12000
2024-11-05 06:44:26,467 - INFO - [diffusion][Epoch 11591] diffusion training Loss: 0.048925211653113365
2024-11-05 06:44:26,469 - INFO - [diffusion][Epoch 11591] diffusion learning rate: 0.001
2024-11-05 06:44:26,470 - INFO - [diffusion][Epoch 11591] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:26,472 - INFO - [diffusion][Epoch 11592] Epoch 11593/12000
2024-11-05 06:44:30,563 - INFO - [diffusion][Epoch 11592] diffusion training Loss: 0.04647757671773434
2024-11-05 06:44:30,565 - INFO - [diffusion][Epoch 11592] diffusion learning rate: 0.001
2024-11-05 06:44:30,567 - INFO - [diffusion][Epoch 11592] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:30,568 - INFO - [diffusion][Epoch 11593] Epoch 11594/12000
2024-11-05 06:44:34,659 - INFO - [diffusion][Epoch 11593] diffusion training Loss: 0.04484023991972208
2024-11-05 06:44:34,662 - INFO - [diffusion][Epoch 11593] diffusion learning rate: 0.001
2024-11-05 06:44:34,690 - INFO - [diffusion][Epoch 11593] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:34,692 - INFO - [diffusion][Epoch 11594] Epoch 11595/12000
2024-11-05 06:44:38,778 - INFO - [diffusion][Epoch 11594] diffusion training Loss: 0.054247770458459854
2024-11-05 06:44:38,780 - INFO - [diffusion][Epoch 11594] diffusion learning rate: 0.001
2024-11-05 06:44:38,782 - INFO - [diffusion][Epoch 11594] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:38,784 - INFO - [diffusion][Epoch 11595] Epoch 11596/12000
2024-11-05 06:44:42,815 - INFO - [diffusion][Epoch 11595] diffusion training Loss: 0.04726746119558811
2024-11-05 06:44:42,816 - INFO - [diffusion][Epoch 11595] diffusion learning rate: 0.001
2024-11-05 06:44:42,818 - INFO - [diffusion][Epoch 11595] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:42,820 - INFO - [diffusion][Epoch 11596] Epoch 11597/12000
2024-11-05 06:44:46,898 - INFO - [diffusion][Epoch 11596] diffusion training Loss: 0.048533275723457336
2024-11-05 06:44:46,900 - INFO - [diffusion][Epoch 11596] diffusion learning rate: 0.001
2024-11-05 06:44:46,902 - INFO - [diffusion][Epoch 11596] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:46,903 - INFO - [diffusion][Epoch 11597] Epoch 11598/12000
2024-11-05 06:44:50,968 - INFO - [diffusion][Epoch 11597] diffusion training Loss: 0.042413411661982536
2024-11-05 06:44:50,971 - INFO - [diffusion][Epoch 11597] diffusion learning rate: 0.001
2024-11-05 06:44:50,999 - INFO - [diffusion][Epoch 11597] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:51,000 - INFO - [diffusion][Epoch 11598] Epoch 11599/12000
2024-11-05 06:44:55,106 - INFO - [diffusion][Epoch 11598] diffusion training Loss: 0.04491635970771313
2024-11-05 06:44:55,108 - INFO - [diffusion][Epoch 11598] diffusion learning rate: 0.001
2024-11-05 06:44:55,110 - INFO - [diffusion][Epoch 11598] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:55,111 - INFO - [diffusion][Epoch 11599] Epoch 11600/12000
2024-11-05 06:44:59,222 - INFO - [diffusion][Epoch 11599] diffusion training Loss: 0.05025955382734537
2024-11-05 06:44:59,224 - INFO - [diffusion][Epoch 11599] diffusion learning rate: 0.001
2024-11-05 06:44:59,226 - INFO - [diffusion][Epoch 11599] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:44:59,227 - INFO - [diffusion][Epoch 11600] Epoch 11601/12000
2024-11-05 06:45:03,287 - INFO - [diffusion][Epoch 11600] diffusion training Loss: 0.04851788002997637
2024-11-05 06:45:03,289 - INFO - [diffusion][Epoch 11600] diffusion learning rate: 0.001
2024-11-05 06:45:03,291 - INFO - [diffusion][Epoch 11600] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:03,292 - INFO - [diffusion][Epoch 11601] Epoch 11602/12000
2024-11-05 06:45:07,380 - INFO - [diffusion][Epoch 11601] diffusion training Loss: 0.0446266932412982
2024-11-05 06:45:07,382 - INFO - [diffusion][Epoch 11601] diffusion learning rate: 0.001
2024-11-05 06:45:07,410 - INFO - [diffusion][Epoch 11601] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:07,412 - INFO - [diffusion][Epoch 11602] Epoch 11603/12000
2024-11-05 06:45:11,535 - INFO - [diffusion][Epoch 11602] diffusion training Loss: 0.051122743636369705
2024-11-05 06:45:11,537 - INFO - [diffusion][Epoch 11602] diffusion learning rate: 0.001
2024-11-05 06:45:11,539 - INFO - [diffusion][Epoch 11602] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:11,541 - INFO - [diffusion][Epoch 11603] Epoch 11604/12000
2024-11-05 06:45:15,488 - INFO - [diffusion][Epoch 11603] diffusion training Loss: 0.04924285039305687
2024-11-05 06:45:15,490 - INFO - [diffusion][Epoch 11603] diffusion learning rate: 0.001
2024-11-05 06:45:15,492 - INFO - [diffusion][Epoch 11603] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:15,493 - INFO - [diffusion][Epoch 11604] Epoch 11605/12000
2024-11-05 06:45:19,565 - INFO - [diffusion][Epoch 11604] diffusion training Loss: 0.04941719025373459
2024-11-05 06:45:19,567 - INFO - [diffusion][Epoch 11604] diffusion learning rate: 0.001
2024-11-05 06:45:19,569 - INFO - [diffusion][Epoch 11604] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:19,570 - INFO - [diffusion][Epoch 11605] Epoch 11606/12000
2024-11-05 06:45:23,508 - INFO - [diffusion][Epoch 11605] diffusion training Loss: 0.052524860948324203
2024-11-05 06:45:23,510 - INFO - [diffusion][Epoch 11605] diffusion learning rate: 0.001
2024-11-05 06:45:23,512 - INFO - [diffusion][Epoch 11605] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:23,513 - INFO - [diffusion][Epoch 11606] Epoch 11607/12000
2024-11-05 06:45:27,784 - INFO - [diffusion][Epoch 11606] diffusion training Loss: 0.04388717096298933
2024-11-05 06:45:27,786 - INFO - [diffusion][Epoch 11606] diffusion learning rate: 0.001
2024-11-05 06:45:27,788 - INFO - [diffusion][Epoch 11606] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:27,789 - INFO - [diffusion][Epoch 11607] Epoch 11608/12000
2024-11-05 06:45:31,872 - INFO - [diffusion][Epoch 11607] diffusion training Loss: 0.04331677686423063
2024-11-05 06:45:31,875 - INFO - [diffusion][Epoch 11607] diffusion learning rate: 0.001
2024-11-05 06:45:31,877 - INFO - [diffusion][Epoch 11607] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:31,878 - INFO - [diffusion][Epoch 11608] Epoch 11609/12000
2024-11-05 06:45:35,921 - INFO - [diffusion][Epoch 11608] diffusion training Loss: 0.043863292783498764
2024-11-05 06:45:35,924 - INFO - [diffusion][Epoch 11608] diffusion learning rate: 0.001
2024-11-05 06:45:35,926 - INFO - [diffusion][Epoch 11608] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:35,928 - INFO - [diffusion][Epoch 11609] Epoch 11610/12000
2024-11-05 06:45:39,991 - INFO - [diffusion][Epoch 11609] diffusion training Loss: 0.04151206091046333
2024-11-05 06:45:39,993 - INFO - [diffusion][Epoch 11609] diffusion learning rate: 0.001
2024-11-05 06:45:40,020 - INFO - [diffusion][Epoch 11609] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:40,022 - INFO - [diffusion][Epoch 11610] Epoch 11611/12000
2024-11-05 06:45:44,088 - INFO - [diffusion][Epoch 11610] diffusion training Loss: 0.04704898130148649
2024-11-05 06:45:44,090 - INFO - [diffusion][Epoch 11610] diffusion learning rate: 0.001
2024-11-05 06:45:44,092 - INFO - [diffusion][Epoch 11610] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:44,093 - INFO - [diffusion][Epoch 11611] Epoch 11612/12000
2024-11-05 06:45:48,142 - INFO - [diffusion][Epoch 11611] diffusion training Loss: 0.04627715703099966
2024-11-05 06:45:48,144 - INFO - [diffusion][Epoch 11611] diffusion learning rate: 0.001
2024-11-05 06:45:48,146 - INFO - [diffusion][Epoch 11611] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:48,147 - INFO - [diffusion][Epoch 11612] Epoch 11613/12000
2024-11-05 06:45:52,130 - INFO - [diffusion][Epoch 11612] diffusion training Loss: 0.04709645826369524
2024-11-05 06:45:52,132 - INFO - [diffusion][Epoch 11612] diffusion learning rate: 0.001
2024-11-05 06:45:52,133 - INFO - [diffusion][Epoch 11612] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:52,135 - INFO - [diffusion][Epoch 11613] Epoch 11614/12000
2024-11-05 06:45:56,248 - INFO - [diffusion][Epoch 11613] diffusion training Loss: 0.04905730206519365
2024-11-05 06:45:56,250 - INFO - [diffusion][Epoch 11613] diffusion learning rate: 0.001
2024-11-05 06:45:56,251 - INFO - [diffusion][Epoch 11613] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:45:56,253 - INFO - [diffusion][Epoch 11614] Epoch 11615/12000
2024-11-05 06:46:00,342 - INFO - [diffusion][Epoch 11614] diffusion training Loss: 0.04435976222157478
2024-11-05 06:46:00,344 - INFO - [diffusion][Epoch 11614] diffusion learning rate: 0.001
2024-11-05 06:46:00,346 - INFO - [diffusion][Epoch 11614] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:00,348 - INFO - [diffusion][Epoch 11615] Epoch 11616/12000
2024-11-05 06:46:04,287 - INFO - [diffusion][Epoch 11615] diffusion training Loss: 0.0444973511621356
2024-11-05 06:46:04,289 - INFO - [diffusion][Epoch 11615] diffusion learning rate: 0.001
2024-11-05 06:46:04,290 - INFO - [diffusion][Epoch 11615] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:04,292 - INFO - [diffusion][Epoch 11616] Epoch 11617/12000
2024-11-05 06:46:08,182 - INFO - [diffusion][Epoch 11616] diffusion training Loss: 0.05202340520918369
2024-11-05 06:46:08,184 - INFO - [diffusion][Epoch 11616] diffusion learning rate: 0.001
2024-11-05 06:46:08,186 - INFO - [diffusion][Epoch 11616] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:08,187 - INFO - [diffusion][Epoch 11617] Epoch 11618/12000
2024-11-05 06:46:12,090 - INFO - [diffusion][Epoch 11617] diffusion training Loss: 0.05360374320298433
2024-11-05 06:46:12,092 - INFO - [diffusion][Epoch 11617] diffusion learning rate: 0.001
2024-11-05 06:46:12,093 - INFO - [diffusion][Epoch 11617] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:12,095 - INFO - [diffusion][Epoch 11618] Epoch 11619/12000
2024-11-05 06:46:16,010 - INFO - [diffusion][Epoch 11618] diffusion training Loss: 0.04809839464724064
2024-11-05 06:46:16,013 - INFO - [diffusion][Epoch 11618] diffusion learning rate: 0.001
2024-11-05 06:46:16,016 - INFO - [diffusion][Epoch 11618] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:16,017 - INFO - [diffusion][Epoch 11619] Epoch 11620/12000
2024-11-05 06:46:20,071 - INFO - [diffusion][Epoch 11619] diffusion training Loss: 0.046948863193392754
2024-11-05 06:46:20,073 - INFO - [diffusion][Epoch 11619] diffusion learning rate: 0.001
2024-11-05 06:46:20,075 - INFO - [diffusion][Epoch 11619] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:20,076 - INFO - [diffusion][Epoch 11620] Epoch 11621/12000
2024-11-05 06:46:24,153 - INFO - [diffusion][Epoch 11620] diffusion training Loss: 0.047518943436443806
2024-11-05 06:46:24,155 - INFO - [diffusion][Epoch 11620] diffusion learning rate: 0.001
2024-11-05 06:46:24,156 - INFO - [diffusion][Epoch 11620] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:24,158 - INFO - [diffusion][Epoch 11621] Epoch 11622/12000
2024-11-05 06:46:28,252 - INFO - [diffusion][Epoch 11621] diffusion training Loss: 0.04793127626180649
2024-11-05 06:46:28,254 - INFO - [diffusion][Epoch 11621] diffusion learning rate: 0.001
2024-11-05 06:46:28,255 - INFO - [diffusion][Epoch 11621] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:28,257 - INFO - [diffusion][Epoch 11622] Epoch 11623/12000
2024-11-05 06:46:32,345 - INFO - [diffusion][Epoch 11622] diffusion training Loss: 0.04844085965305567
2024-11-05 06:46:32,348 - INFO - [diffusion][Epoch 11622] diffusion learning rate: 0.001
2024-11-05 06:46:32,349 - INFO - [diffusion][Epoch 11622] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:32,351 - INFO - [diffusion][Epoch 11623] Epoch 11624/12000
2024-11-05 06:46:36,458 - INFO - [diffusion][Epoch 11623] diffusion training Loss: 0.04661916382610798
2024-11-05 06:46:36,462 - INFO - [diffusion][Epoch 11623] diffusion learning rate: 0.001
2024-11-05 06:46:36,464 - INFO - [diffusion][Epoch 11623] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:36,465 - INFO - [diffusion][Epoch 11624] Epoch 11625/12000
2024-11-05 06:46:40,544 - INFO - [diffusion][Epoch 11624] diffusion training Loss: 0.04817082080990076
2024-11-05 06:46:40,546 - INFO - [diffusion][Epoch 11624] diffusion learning rate: 0.001
2024-11-05 06:46:40,548 - INFO - [diffusion][Epoch 11624] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:40,549 - INFO - [diffusion][Epoch 11625] Epoch 11626/12000
2024-11-05 06:46:44,614 - INFO - [diffusion][Epoch 11625] diffusion training Loss: 0.04399644397199154
2024-11-05 06:46:44,616 - INFO - [diffusion][Epoch 11625] diffusion learning rate: 0.001
2024-11-05 06:46:44,618 - INFO - [diffusion][Epoch 11625] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:44,619 - INFO - [diffusion][Epoch 11626] Epoch 11627/12000
2024-11-05 06:46:48,689 - INFO - [diffusion][Epoch 11626] diffusion training Loss: 0.044467559084296227
2024-11-05 06:46:48,691 - INFO - [diffusion][Epoch 11626] diffusion learning rate: 0.001
2024-11-05 06:46:48,693 - INFO - [diffusion][Epoch 11626] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:48,695 - INFO - [diffusion][Epoch 11627] Epoch 11628/12000
2024-11-05 06:46:52,742 - INFO - [diffusion][Epoch 11627] diffusion training Loss: 0.04697942640632391
2024-11-05 06:46:52,743 - INFO - [diffusion][Epoch 11627] diffusion learning rate: 0.001
2024-11-05 06:46:52,745 - INFO - [diffusion][Epoch 11627] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:52,746 - INFO - [diffusion][Epoch 11628] Epoch 11629/12000
2024-11-05 06:46:56,823 - INFO - [diffusion][Epoch 11628] diffusion training Loss: 0.046187655068933964
2024-11-05 06:46:56,825 - INFO - [diffusion][Epoch 11628] diffusion learning rate: 0.001
2024-11-05 06:46:56,827 - INFO - [diffusion][Epoch 11628] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:46:56,828 - INFO - [diffusion][Epoch 11629] Epoch 11630/12000
2024-11-05 06:47:00,885 - INFO - [diffusion][Epoch 11629] diffusion training Loss: 0.04335236828774214
2024-11-05 06:47:00,887 - INFO - [diffusion][Epoch 11629] diffusion learning rate: 0.001
2024-11-05 06:47:00,888 - INFO - [diffusion][Epoch 11629] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:00,890 - INFO - [diffusion][Epoch 11630] Epoch 11631/12000
2024-11-05 06:47:04,977 - INFO - [diffusion][Epoch 11630] diffusion training Loss: 0.04473574925214052
2024-11-05 06:47:04,979 - INFO - [diffusion][Epoch 11630] diffusion learning rate: 0.001
2024-11-05 06:47:04,981 - INFO - [diffusion][Epoch 11630] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:04,982 - INFO - [diffusion][Epoch 11631] Epoch 11632/12000
2024-11-05 06:47:09,079 - INFO - [diffusion][Epoch 11631] diffusion training Loss: 0.04699825495481491
2024-11-05 06:47:09,081 - INFO - [diffusion][Epoch 11631] diffusion learning rate: 0.001
2024-11-05 06:47:09,083 - INFO - [diffusion][Epoch 11631] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:09,084 - INFO - [diffusion][Epoch 11632] Epoch 11633/12000
2024-11-05 06:47:13,163 - INFO - [diffusion][Epoch 11632] diffusion training Loss: 0.048631587997078896
2024-11-05 06:47:13,165 - INFO - [diffusion][Epoch 11632] diffusion learning rate: 0.001
2024-11-05 06:47:13,166 - INFO - [diffusion][Epoch 11632] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:13,168 - INFO - [diffusion][Epoch 11633] Epoch 11634/12000
2024-11-05 06:47:17,277 - INFO - [diffusion][Epoch 11633] diffusion training Loss: 0.047349569387733936
2024-11-05 06:47:17,279 - INFO - [diffusion][Epoch 11633] diffusion learning rate: 0.001
2024-11-05 06:47:17,281 - INFO - [diffusion][Epoch 11633] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:17,282 - INFO - [diffusion][Epoch 11634] Epoch 11635/12000
2024-11-05 06:47:21,331 - INFO - [diffusion][Epoch 11634] diffusion training Loss: 0.04396249260753393
2024-11-05 06:47:21,333 - INFO - [diffusion][Epoch 11634] diffusion learning rate: 0.001
2024-11-05 06:47:21,334 - INFO - [diffusion][Epoch 11634] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:21,336 - INFO - [diffusion][Epoch 11635] Epoch 11636/12000
2024-11-05 06:47:25,390 - INFO - [diffusion][Epoch 11635] diffusion training Loss: 0.04719836078584194
2024-11-05 06:47:25,392 - INFO - [diffusion][Epoch 11635] diffusion learning rate: 0.001
2024-11-05 06:47:25,393 - INFO - [diffusion][Epoch 11635] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:25,395 - INFO - [diffusion][Epoch 11636] Epoch 11637/12000
2024-11-05 06:47:29,512 - INFO - [diffusion][Epoch 11636] diffusion training Loss: 0.04831698723137379
2024-11-05 06:47:29,514 - INFO - [diffusion][Epoch 11636] diffusion learning rate: 0.001
2024-11-05 06:47:29,516 - INFO - [diffusion][Epoch 11636] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:29,517 - INFO - [diffusion][Epoch 11637] Epoch 11638/12000
2024-11-05 06:47:33,567 - INFO - [diffusion][Epoch 11637] diffusion training Loss: 0.04772346932440996
2024-11-05 06:47:33,569 - INFO - [diffusion][Epoch 11637] diffusion learning rate: 0.001
2024-11-05 06:47:33,570 - INFO - [diffusion][Epoch 11637] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:33,572 - INFO - [diffusion][Epoch 11638] Epoch 11639/12000
2024-11-05 06:47:37,678 - INFO - [diffusion][Epoch 11638] diffusion training Loss: 0.04445943422615528
2024-11-05 06:47:37,682 - INFO - [diffusion][Epoch 11638] diffusion learning rate: 0.001
2024-11-05 06:47:37,684 - INFO - [diffusion][Epoch 11638] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:37,685 - INFO - [diffusion][Epoch 11639] Epoch 11640/12000
2024-11-05 06:47:41,741 - INFO - [diffusion][Epoch 11639] diffusion training Loss: 0.04958700016140938
2024-11-05 06:47:41,743 - INFO - [diffusion][Epoch 11639] diffusion learning rate: 0.001
2024-11-05 06:47:41,783 - INFO - [diffusion][Epoch 11639] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:41,784 - INFO - [diffusion][Epoch 11640] Epoch 11641/12000
2024-11-05 06:47:45,877 - INFO - [diffusion][Epoch 11640] diffusion training Loss: 0.041607107035815716
2024-11-05 06:47:45,879 - INFO - [diffusion][Epoch 11640] diffusion learning rate: 0.001
2024-11-05 06:47:45,881 - INFO - [diffusion][Epoch 11640] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:45,882 - INFO - [diffusion][Epoch 11641] Epoch 11642/12000
2024-11-05 06:47:49,987 - INFO - [diffusion][Epoch 11641] diffusion training Loss: 0.04618847742676735
2024-11-05 06:47:49,989 - INFO - [diffusion][Epoch 11641] diffusion learning rate: 0.001
2024-11-05 06:47:49,990 - INFO - [diffusion][Epoch 11641] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:49,992 - INFO - [diffusion][Epoch 11642] Epoch 11643/12000
2024-11-05 06:47:54,092 - INFO - [diffusion][Epoch 11642] diffusion training Loss: 0.04543949197977781
2024-11-05 06:47:54,094 - INFO - [diffusion][Epoch 11642] diffusion learning rate: 0.001
2024-11-05 06:47:54,096 - INFO - [diffusion][Epoch 11642] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:54,097 - INFO - [diffusion][Epoch 11643] Epoch 11644/12000
2024-11-05 06:47:58,228 - INFO - [diffusion][Epoch 11643] diffusion training Loss: 0.04349727928638458
2024-11-05 06:47:58,230 - INFO - [diffusion][Epoch 11643] diffusion learning rate: 0.001
2024-11-05 06:47:58,232 - INFO - [diffusion][Epoch 11643] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:47:58,233 - INFO - [diffusion][Epoch 11644] Epoch 11645/12000
2024-11-05 06:48:02,317 - INFO - [diffusion][Epoch 11644] diffusion training Loss: 0.04783138167113066
2024-11-05 06:48:02,319 - INFO - [diffusion][Epoch 11644] diffusion learning rate: 0.001
2024-11-05 06:48:02,321 - INFO - [diffusion][Epoch 11644] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:02,322 - INFO - [diffusion][Epoch 11645] Epoch 11646/12000
2024-11-05 06:48:06,518 - INFO - [diffusion][Epoch 11645] diffusion training Loss: 0.050538006238639355
2024-11-05 06:48:06,520 - INFO - [diffusion][Epoch 11645] diffusion learning rate: 0.001
2024-11-05 06:48:06,522 - INFO - [diffusion][Epoch 11645] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:06,523 - INFO - [diffusion][Epoch 11646] Epoch 11647/12000
2024-11-05 06:48:10,587 - INFO - [diffusion][Epoch 11646] diffusion training Loss: 0.0514568155631423
2024-11-05 06:48:10,589 - INFO - [diffusion][Epoch 11646] diffusion learning rate: 0.001
2024-11-05 06:48:10,591 - INFO - [diffusion][Epoch 11646] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:10,592 - INFO - [diffusion][Epoch 11647] Epoch 11648/12000
2024-11-05 06:48:14,643 - INFO - [diffusion][Epoch 11647] diffusion training Loss: 0.043921022675931454
2024-11-05 06:48:14,645 - INFO - [diffusion][Epoch 11647] diffusion learning rate: 0.001
2024-11-05 06:48:14,647 - INFO - [diffusion][Epoch 11647] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:14,648 - INFO - [diffusion][Epoch 11648] Epoch 11649/12000
2024-11-05 06:48:18,726 - INFO - [diffusion][Epoch 11648] diffusion training Loss: 0.05114422086626291
2024-11-05 06:48:18,728 - INFO - [diffusion][Epoch 11648] diffusion learning rate: 0.001
2024-11-05 06:48:18,730 - INFO - [diffusion][Epoch 11648] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:18,731 - INFO - [diffusion][Epoch 11649] Epoch 11650/12000
2024-11-05 06:48:22,803 - INFO - [diffusion][Epoch 11649] diffusion training Loss: 0.04790973477065563
2024-11-05 06:48:22,805 - INFO - [diffusion][Epoch 11649] diffusion learning rate: 0.001
2024-11-05 06:48:22,806 - INFO - [diffusion][Epoch 11649] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:22,808 - INFO - [diffusion][Epoch 11650] Epoch 11651/12000
2024-11-05 06:48:26,896 - INFO - [diffusion][Epoch 11650] diffusion training Loss: 0.041607290506362915
2024-11-05 06:48:26,898 - INFO - [diffusion][Epoch 11650] diffusion learning rate: 0.001
2024-11-05 06:48:26,900 - INFO - [diffusion][Epoch 11650] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:26,901 - INFO - [diffusion][Epoch 11651] Epoch 11652/12000
2024-11-05 06:48:30,950 - INFO - [diffusion][Epoch 11651] diffusion training Loss: 0.043939484283328056
2024-11-05 06:48:30,952 - INFO - [diffusion][Epoch 11651] diffusion learning rate: 0.001
2024-11-05 06:48:30,954 - INFO - [diffusion][Epoch 11651] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:30,955 - INFO - [diffusion][Epoch 11652] Epoch 11653/12000
2024-11-05 06:48:35,035 - INFO - [diffusion][Epoch 11652] diffusion training Loss: 0.04635033197700977
2024-11-05 06:48:35,037 - INFO - [diffusion][Epoch 11652] diffusion learning rate: 0.001
2024-11-05 06:48:35,038 - INFO - [diffusion][Epoch 11652] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:35,040 - INFO - [diffusion][Epoch 11653] Epoch 11654/12000
2024-11-05 06:48:38,986 - INFO - [diffusion][Epoch 11653] diffusion training Loss: 0.046149116940796375
2024-11-05 06:48:38,990 - INFO - [diffusion][Epoch 11653] diffusion learning rate: 0.001
2024-11-05 06:48:38,992 - INFO - [diffusion][Epoch 11653] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:38,993 - INFO - [diffusion][Epoch 11654] Epoch 11655/12000
2024-11-05 06:48:43,015 - INFO - [diffusion][Epoch 11654] diffusion training Loss: 0.04693335201591253
2024-11-05 06:48:43,016 - INFO - [diffusion][Epoch 11654] diffusion learning rate: 0.001
2024-11-05 06:48:43,018 - INFO - [diffusion][Epoch 11654] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:43,019 - INFO - [diffusion][Epoch 11655] Epoch 11656/12000
2024-11-05 06:48:47,057 - INFO - [diffusion][Epoch 11655] diffusion training Loss: 0.04588173422962427
2024-11-05 06:48:47,059 - INFO - [diffusion][Epoch 11655] diffusion learning rate: 0.001
2024-11-05 06:48:47,061 - INFO - [diffusion][Epoch 11655] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:47,062 - INFO - [diffusion][Epoch 11656] Epoch 11657/12000
2024-11-05 06:48:51,162 - INFO - [diffusion][Epoch 11656] diffusion training Loss: 0.04488244000822306
2024-11-05 06:48:51,164 - INFO - [diffusion][Epoch 11656] diffusion learning rate: 0.001
2024-11-05 06:48:51,166 - INFO - [diffusion][Epoch 11656] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:51,167 - INFO - [diffusion][Epoch 11657] Epoch 11658/12000
2024-11-05 06:48:55,244 - INFO - [diffusion][Epoch 11657] diffusion training Loss: 0.05095916148275137
2024-11-05 06:48:55,246 - INFO - [diffusion][Epoch 11657] diffusion learning rate: 0.001
2024-11-05 06:48:55,248 - INFO - [diffusion][Epoch 11657] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:55,250 - INFO - [diffusion][Epoch 11658] Epoch 11659/12000
2024-11-05 06:48:59,319 - INFO - [diffusion][Epoch 11658] diffusion training Loss: 0.044369800947606564
2024-11-05 06:48:59,321 - INFO - [diffusion][Epoch 11658] diffusion learning rate: 0.001
2024-11-05 06:48:59,323 - INFO - [diffusion][Epoch 11658] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:48:59,324 - INFO - [diffusion][Epoch 11659] Epoch 11660/12000
2024-11-05 06:49:03,403 - INFO - [diffusion][Epoch 11659] diffusion training Loss: 0.0483263460919261
2024-11-05 06:49:03,405 - INFO - [diffusion][Epoch 11659] diffusion learning rate: 0.001
2024-11-05 06:49:03,406 - INFO - [diffusion][Epoch 11659] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:03,408 - INFO - [diffusion][Epoch 11660] Epoch 11661/12000
2024-11-05 06:49:07,461 - INFO - [diffusion][Epoch 11660] diffusion training Loss: 0.048059978522360325
2024-11-05 06:49:07,463 - INFO - [diffusion][Epoch 11660] diffusion learning rate: 0.001
2024-11-05 06:49:07,465 - INFO - [diffusion][Epoch 11660] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:07,466 - INFO - [diffusion][Epoch 11661] Epoch 11662/12000
2024-11-05 06:49:11,546 - INFO - [diffusion][Epoch 11661] diffusion training Loss: 0.04203057661652565
2024-11-05 06:49:11,548 - INFO - [diffusion][Epoch 11661] diffusion learning rate: 0.001
2024-11-05 06:49:11,551 - INFO - [diffusion][Epoch 11661] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:11,552 - INFO - [diffusion][Epoch 11662] Epoch 11663/12000
2024-11-05 06:49:15,595 - INFO - [diffusion][Epoch 11662] diffusion training Loss: 0.04423299338668585
2024-11-05 06:49:15,597 - INFO - [diffusion][Epoch 11662] diffusion learning rate: 0.001
2024-11-05 06:49:15,599 - INFO - [diffusion][Epoch 11662] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:15,600 - INFO - [diffusion][Epoch 11663] Epoch 11664/12000
2024-11-05 06:49:19,679 - INFO - [diffusion][Epoch 11663] diffusion training Loss: 0.04722537472844124
2024-11-05 06:49:19,681 - INFO - [diffusion][Epoch 11663] diffusion learning rate: 0.001
2024-11-05 06:49:19,683 - INFO - [diffusion][Epoch 11663] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:19,684 - INFO - [diffusion][Epoch 11664] Epoch 11665/12000
2024-11-05 06:49:23,739 - INFO - [diffusion][Epoch 11664] diffusion training Loss: 0.042556329630315304
2024-11-05 06:49:23,741 - INFO - [diffusion][Epoch 11664] diffusion learning rate: 0.001
2024-11-05 06:49:23,742 - INFO - [diffusion][Epoch 11664] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:23,744 - INFO - [diffusion][Epoch 11665] Epoch 11666/12000
2024-11-05 06:49:27,803 - INFO - [diffusion][Epoch 11665] diffusion training Loss: 0.04464075155556202
2024-11-05 06:49:27,805 - INFO - [diffusion][Epoch 11665] diffusion learning rate: 0.001
2024-11-05 06:49:27,807 - INFO - [diffusion][Epoch 11665] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:27,808 - INFO - [diffusion][Epoch 11666] Epoch 11667/12000
2024-11-05 06:49:32,139 - INFO - [diffusion][Epoch 11666] diffusion training Loss: 0.04455460514873266
2024-11-05 06:49:32,141 - INFO - [diffusion][Epoch 11666] diffusion learning rate: 0.001
2024-11-05 06:49:32,143 - INFO - [diffusion][Epoch 11666] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:32,144 - INFO - [diffusion][Epoch 11667] Epoch 11668/12000
2024-11-05 06:49:36,249 - INFO - [diffusion][Epoch 11667] diffusion training Loss: 0.04489358700811863
2024-11-05 06:49:36,251 - INFO - [diffusion][Epoch 11667] diffusion learning rate: 0.001
2024-11-05 06:49:36,253 - INFO - [diffusion][Epoch 11667] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:36,255 - INFO - [diffusion][Epoch 11668] Epoch 11669/12000
2024-11-05 06:49:40,246 - INFO - [diffusion][Epoch 11668] diffusion training Loss: 0.04336208663880825
2024-11-05 06:49:40,249 - INFO - [diffusion][Epoch 11668] diffusion learning rate: 0.001
2024-11-05 06:49:40,251 - INFO - [diffusion][Epoch 11668] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:40,252 - INFO - [diffusion][Epoch 11669] Epoch 11670/12000
2024-11-05 06:49:44,223 - INFO - [diffusion][Epoch 11669] diffusion training Loss: 0.04690198786556721
2024-11-05 06:49:44,225 - INFO - [diffusion][Epoch 11669] diffusion learning rate: 0.001
2024-11-05 06:49:44,226 - INFO - [diffusion][Epoch 11669] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:44,228 - INFO - [diffusion][Epoch 11670] Epoch 11671/12000
2024-11-05 06:49:48,334 - INFO - [diffusion][Epoch 11670] diffusion training Loss: 0.045895044691860676
2024-11-05 06:49:48,336 - INFO - [diffusion][Epoch 11670] diffusion learning rate: 0.001
2024-11-05 06:49:48,338 - INFO - [diffusion][Epoch 11670] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:48,339 - INFO - [diffusion][Epoch 11671] Epoch 11672/12000
2024-11-05 06:49:52,442 - INFO - [diffusion][Epoch 11671] diffusion training Loss: 0.047092504799366
2024-11-05 06:49:52,444 - INFO - [diffusion][Epoch 11671] diffusion learning rate: 0.001
2024-11-05 06:49:52,446 - INFO - [diffusion][Epoch 11671] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:52,447 - INFO - [diffusion][Epoch 11672] Epoch 11673/12000
2024-11-05 06:49:56,492 - INFO - [diffusion][Epoch 11672] diffusion training Loss: 0.041484477929770947
2024-11-05 06:49:56,494 - INFO - [diffusion][Epoch 11672] diffusion learning rate: 0.001
2024-11-05 06:49:56,496 - INFO - [diffusion][Epoch 11672] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:49:56,497 - INFO - [diffusion][Epoch 11673] Epoch 11674/12000
2024-11-05 06:50:00,609 - INFO - [diffusion][Epoch 11673] diffusion training Loss: 0.05132568348199129
2024-11-05 06:50:00,611 - INFO - [diffusion][Epoch 11673] diffusion learning rate: 0.001
2024-11-05 06:50:00,613 - INFO - [diffusion][Epoch 11673] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:00,615 - INFO - [diffusion][Epoch 11674] Epoch 11675/12000
2024-11-05 06:50:04,742 - INFO - [diffusion][Epoch 11674] diffusion training Loss: 0.04288799501955509
2024-11-05 06:50:04,744 - INFO - [diffusion][Epoch 11674] diffusion learning rate: 0.001
2024-11-05 06:50:04,746 - INFO - [diffusion][Epoch 11674] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:04,747 - INFO - [diffusion][Epoch 11675] Epoch 11676/12000
2024-11-05 06:50:08,709 - INFO - [diffusion][Epoch 11675] diffusion training Loss: 0.04851070512086153
2024-11-05 06:50:08,711 - INFO - [diffusion][Epoch 11675] diffusion learning rate: 0.001
2024-11-05 06:50:08,713 - INFO - [diffusion][Epoch 11675] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:08,714 - INFO - [diffusion][Epoch 11676] Epoch 11677/12000
2024-11-05 06:50:12,847 - INFO - [diffusion][Epoch 11676] diffusion training Loss: 0.04927246551960707
2024-11-05 06:50:12,849 - INFO - [diffusion][Epoch 11676] diffusion learning rate: 0.001
2024-11-05 06:50:12,850 - INFO - [diffusion][Epoch 11676] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:12,851 - INFO - [diffusion][Epoch 11677] Epoch 11678/12000
2024-11-05 06:50:16,964 - INFO - [diffusion][Epoch 11677] diffusion training Loss: 0.04361188784241676
2024-11-05 06:50:16,966 - INFO - [diffusion][Epoch 11677] diffusion learning rate: 0.001
2024-11-05 06:50:16,967 - INFO - [diffusion][Epoch 11677] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:16,969 - INFO - [diffusion][Epoch 11678] Epoch 11679/12000
2024-11-05 06:50:21,058 - INFO - [diffusion][Epoch 11678] diffusion training Loss: 0.044841072522103786
2024-11-05 06:50:21,061 - INFO - [diffusion][Epoch 11678] diffusion learning rate: 0.001
2024-11-05 06:50:21,062 - INFO - [diffusion][Epoch 11678] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:21,064 - INFO - [diffusion][Epoch 11679] Epoch 11680/12000
2024-11-05 06:50:25,188 - INFO - [diffusion][Epoch 11679] diffusion training Loss: 0.04629663843661547
2024-11-05 06:50:25,190 - INFO - [diffusion][Epoch 11679] diffusion learning rate: 0.001
2024-11-05 06:50:25,192 - INFO - [diffusion][Epoch 11679] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:25,193 - INFO - [diffusion][Epoch 11680] Epoch 11681/12000
2024-11-05 06:50:29,277 - INFO - [diffusion][Epoch 11680] diffusion training Loss: 0.043948802165687084
2024-11-05 06:50:29,279 - INFO - [diffusion][Epoch 11680] diffusion learning rate: 0.001
2024-11-05 06:50:29,280 - INFO - [diffusion][Epoch 11680] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:29,282 - INFO - [diffusion][Epoch 11681] Epoch 11682/12000
2024-11-05 06:50:33,394 - INFO - [diffusion][Epoch 11681] diffusion training Loss: 0.044430311769247055
2024-11-05 06:50:33,396 - INFO - [diffusion][Epoch 11681] diffusion learning rate: 0.001
2024-11-05 06:50:33,398 - INFO - [diffusion][Epoch 11681] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:33,399 - INFO - [diffusion][Epoch 11682] Epoch 11683/12000
2024-11-05 06:50:37,454 - INFO - [diffusion][Epoch 11682] diffusion training Loss: 0.04339439608156681
2024-11-05 06:50:37,456 - INFO - [diffusion][Epoch 11682] diffusion learning rate: 0.001
2024-11-05 06:50:37,484 - INFO - [diffusion][Epoch 11682] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:37,485 - INFO - [diffusion][Epoch 11683] Epoch 11684/12000
2024-11-05 06:50:41,596 - INFO - [diffusion][Epoch 11683] diffusion training Loss: 0.04536326602101326
2024-11-05 06:50:41,599 - INFO - [diffusion][Epoch 11683] diffusion learning rate: 0.001
2024-11-05 06:50:41,601 - INFO - [diffusion][Epoch 11683] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:41,602 - INFO - [diffusion][Epoch 11684] Epoch 11685/12000
2024-11-05 06:50:45,718 - INFO - [diffusion][Epoch 11684] diffusion training Loss: 0.03753297217190266
2024-11-05 06:50:45,720 - INFO - [diffusion][Epoch 11684] diffusion learning rate: 0.001
2024-11-05 06:50:45,722 - INFO - [diffusion][Epoch 11684] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:45,723 - INFO - [diffusion][Epoch 11685] Epoch 11686/12000
2024-11-05 06:50:49,870 - INFO - [diffusion][Epoch 11685] diffusion training Loss: 0.04277857393026352
2024-11-05 06:50:49,872 - INFO - [diffusion][Epoch 11685] diffusion learning rate: 0.001
2024-11-05 06:50:49,873 - INFO - [diffusion][Epoch 11685] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:49,875 - INFO - [diffusion][Epoch 11686] Epoch 11687/12000
2024-11-05 06:50:53,971 - INFO - [diffusion][Epoch 11686] diffusion training Loss: 0.045152812264859676
2024-11-05 06:50:53,973 - INFO - [diffusion][Epoch 11686] diffusion learning rate: 0.001
2024-11-05 06:50:53,975 - INFO - [diffusion][Epoch 11686] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:53,977 - INFO - [diffusion][Epoch 11687] Epoch 11688/12000
2024-11-05 06:50:58,751 - INFO - [diffusion][Epoch 11687] diffusion training Loss: 0.0462083937600255
2024-11-05 06:50:58,753 - INFO - [diffusion][Epoch 11687] diffusion learning rate: 0.001
2024-11-05 06:50:58,755 - INFO - [diffusion][Epoch 11687] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:50:58,756 - INFO - [diffusion][Epoch 11688] Epoch 11689/12000
2024-11-05 06:51:02,831 - INFO - [diffusion][Epoch 11688] diffusion training Loss: 0.046160075813531876
2024-11-05 06:51:02,942 - INFO - [diffusion][Epoch 11688] diffusion learning rate: 0.001
2024-11-05 06:51:02,945 - INFO - [diffusion][Epoch 11688] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:02,946 - INFO - [diffusion][Epoch 11689] Epoch 11690/12000
2024-11-05 06:51:07,068 - INFO - [diffusion][Epoch 11689] diffusion training Loss: 0.04576517641544342
2024-11-05 06:51:07,070 - INFO - [diffusion][Epoch 11689] diffusion learning rate: 0.001
2024-11-05 06:51:07,072 - INFO - [diffusion][Epoch 11689] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:07,073 - INFO - [diffusion][Epoch 11690] Epoch 11691/12000
2024-11-05 06:51:11,070 - INFO - [diffusion][Epoch 11690] diffusion training Loss: 0.04524217266589403
2024-11-05 06:51:11,072 - INFO - [diffusion][Epoch 11690] diffusion learning rate: 0.001
2024-11-05 06:51:11,074 - INFO - [diffusion][Epoch 11690] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:11,075 - INFO - [diffusion][Epoch 11691] Epoch 11692/12000
2024-11-05 06:51:15,159 - INFO - [diffusion][Epoch 11691] diffusion training Loss: 0.044180325232446194
2024-11-05 06:51:15,161 - INFO - [diffusion][Epoch 11691] diffusion learning rate: 0.001
2024-11-05 06:51:15,163 - INFO - [diffusion][Epoch 11691] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:15,164 - INFO - [diffusion][Epoch 11692] Epoch 11693/12000
2024-11-05 06:51:19,100 - INFO - [diffusion][Epoch 11692] diffusion training Loss: 0.047393590211868286
2024-11-05 06:51:19,102 - INFO - [diffusion][Epoch 11692] diffusion learning rate: 0.001
2024-11-05 06:51:19,104 - INFO - [diffusion][Epoch 11692] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:19,106 - INFO - [diffusion][Epoch 11693] Epoch 11694/12000
2024-11-05 06:51:23,237 - INFO - [diffusion][Epoch 11693] diffusion training Loss: 0.05060420371592045
2024-11-05 06:51:23,239 - INFO - [diffusion][Epoch 11693] diffusion learning rate: 0.001
2024-11-05 06:51:23,241 - INFO - [diffusion][Epoch 11693] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:23,242 - INFO - [diffusion][Epoch 11694] Epoch 11695/12000
2024-11-05 06:51:27,321 - INFO - [diffusion][Epoch 11694] diffusion training Loss: 0.04676484316587448
2024-11-05 06:51:27,323 - INFO - [diffusion][Epoch 11694] diffusion learning rate: 0.001
2024-11-05 06:51:27,324 - INFO - [diffusion][Epoch 11694] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:27,326 - INFO - [diffusion][Epoch 11695] Epoch 11696/12000
2024-11-05 06:51:31,425 - INFO - [diffusion][Epoch 11695] diffusion training Loss: 0.044723061844706535
2024-11-05 06:51:31,427 - INFO - [diffusion][Epoch 11695] diffusion learning rate: 0.001
2024-11-05 06:51:31,429 - INFO - [diffusion][Epoch 11695] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:31,430 - INFO - [diffusion][Epoch 11696] Epoch 11697/12000
2024-11-05 06:51:35,522 - INFO - [diffusion][Epoch 11696] diffusion training Loss: 0.04975567478686571
2024-11-05 06:51:35,523 - INFO - [diffusion][Epoch 11696] diffusion learning rate: 0.001
2024-11-05 06:51:35,525 - INFO - [diffusion][Epoch 11696] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:35,526 - INFO - [diffusion][Epoch 11697] Epoch 11698/12000
2024-11-05 06:51:39,636 - INFO - [diffusion][Epoch 11697] diffusion training Loss: 0.04882470611482859
2024-11-05 06:51:39,639 - INFO - [diffusion][Epoch 11697] diffusion learning rate: 0.001
2024-11-05 06:51:39,640 - INFO - [diffusion][Epoch 11697] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:39,642 - INFO - [diffusion][Epoch 11698] Epoch 11699/12000
2024-11-05 06:51:43,739 - INFO - [diffusion][Epoch 11698] diffusion training Loss: 0.04385297279804945
2024-11-05 06:51:43,742 - INFO - [diffusion][Epoch 11698] diffusion learning rate: 0.001
2024-11-05 06:51:43,744 - INFO - [diffusion][Epoch 11698] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:43,745 - INFO - [diffusion][Epoch 11699] Epoch 11700/12000
2024-11-05 06:51:47,802 - INFO - [diffusion][Epoch 11699] diffusion training Loss: 0.046055179089307785
2024-11-05 06:51:47,804 - INFO - [diffusion][Epoch 11699] diffusion learning rate: 0.001
2024-11-05 06:51:47,806 - INFO - [diffusion][Epoch 11699] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:47,807 - INFO - [diffusion][Epoch 11700] Epoch 11701/12000
2024-11-05 06:51:51,917 - INFO - [diffusion][Epoch 11700] diffusion training Loss: 0.048982586711645126
2024-11-05 06:51:51,918 - INFO - [diffusion][Epoch 11700] diffusion learning rate: 0.001
2024-11-05 06:51:51,920 - INFO - [diffusion][Epoch 11700] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:51,921 - INFO - [diffusion][Epoch 11701] Epoch 11702/12000
2024-11-05 06:51:56,035 - INFO - [diffusion][Epoch 11701] diffusion training Loss: 0.04911512602120638
2024-11-05 06:51:56,037 - INFO - [diffusion][Epoch 11701] diffusion learning rate: 0.001
2024-11-05 06:51:56,039 - INFO - [diffusion][Epoch 11701] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:51:56,040 - INFO - [diffusion][Epoch 11702] Epoch 11703/12000
2024-11-05 06:52:00,134 - INFO - [diffusion][Epoch 11702] diffusion training Loss: 0.04592814389616251
2024-11-05 06:52:00,136 - INFO - [diffusion][Epoch 11702] diffusion learning rate: 0.001
2024-11-05 06:52:00,138 - INFO - [diffusion][Epoch 11702] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:00,139 - INFO - [diffusion][Epoch 11703] Epoch 11704/12000
2024-11-05 06:52:04,213 - INFO - [diffusion][Epoch 11703] diffusion training Loss: 0.04234854876995087
2024-11-05 06:52:04,215 - INFO - [diffusion][Epoch 11703] diffusion learning rate: 0.001
2024-11-05 06:52:04,217 - INFO - [diffusion][Epoch 11703] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:04,218 - INFO - [diffusion][Epoch 11704] Epoch 11705/12000
2024-11-05 06:52:08,330 - INFO - [diffusion][Epoch 11704] diffusion training Loss: 0.042901068925857544
2024-11-05 06:52:08,332 - INFO - [diffusion][Epoch 11704] diffusion learning rate: 0.001
2024-11-05 06:52:08,333 - INFO - [diffusion][Epoch 11704] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:08,335 - INFO - [diffusion][Epoch 11705] Epoch 11706/12000
2024-11-05 06:52:12,496 - INFO - [diffusion][Epoch 11705] diffusion training Loss: 0.0464544165879488
2024-11-05 06:52:12,506 - INFO - [diffusion][Epoch 11705] diffusion learning rate: 0.001
2024-11-05 06:52:12,508 - INFO - [diffusion][Epoch 11705] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:12,509 - INFO - [diffusion][Epoch 11706] Epoch 11707/12000
2024-11-05 06:52:16,875 - INFO - [diffusion][Epoch 11706] diffusion training Loss: 0.04740781057626009
2024-11-05 06:52:16,877 - INFO - [diffusion][Epoch 11706] diffusion learning rate: 0.001
2024-11-05 06:52:16,878 - INFO - [diffusion][Epoch 11706] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:16,880 - INFO - [diffusion][Epoch 11707] Epoch 11708/12000
2024-11-05 06:52:21,047 - INFO - [diffusion][Epoch 11707] diffusion training Loss: 0.044703682884573936
2024-11-05 06:52:21,049 - INFO - [diffusion][Epoch 11707] diffusion learning rate: 0.001
2024-11-05 06:52:21,051 - INFO - [diffusion][Epoch 11707] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:21,052 - INFO - [diffusion][Epoch 11708] Epoch 11709/12000
2024-11-05 06:52:25,102 - INFO - [diffusion][Epoch 11708] diffusion training Loss: 0.049457388930022717
2024-11-05 06:52:25,104 - INFO - [diffusion][Epoch 11708] diffusion learning rate: 0.001
2024-11-05 06:52:25,106 - INFO - [diffusion][Epoch 11708] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:25,107 - INFO - [diffusion][Epoch 11709] Epoch 11710/12000
2024-11-05 06:52:29,211 - INFO - [diffusion][Epoch 11709] diffusion training Loss: 0.04723542649298906
2024-11-05 06:52:29,213 - INFO - [diffusion][Epoch 11709] diffusion learning rate: 0.001
2024-11-05 06:52:29,214 - INFO - [diffusion][Epoch 11709] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:29,216 - INFO - [diffusion][Epoch 11710] Epoch 11711/12000
2024-11-05 06:52:33,318 - INFO - [diffusion][Epoch 11710] diffusion training Loss: 0.05214732885360718
2024-11-05 06:52:33,320 - INFO - [diffusion][Epoch 11710] diffusion learning rate: 0.001
2024-11-05 06:52:33,322 - INFO - [diffusion][Epoch 11710] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:33,323 - INFO - [diffusion][Epoch 11711] Epoch 11712/12000
2024-11-05 06:52:37,448 - INFO - [diffusion][Epoch 11711] diffusion training Loss: 0.05045644752681255
2024-11-05 06:52:37,450 - INFO - [diffusion][Epoch 11711] diffusion learning rate: 0.001
2024-11-05 06:52:37,451 - INFO - [diffusion][Epoch 11711] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:37,453 - INFO - [diffusion][Epoch 11712] Epoch 11713/12000
2024-11-05 06:52:41,586 - INFO - [diffusion][Epoch 11712] diffusion training Loss: 0.0483015077188611
2024-11-05 06:52:41,588 - INFO - [diffusion][Epoch 11712] diffusion learning rate: 0.001
2024-11-05 06:52:41,589 - INFO - [diffusion][Epoch 11712] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:41,591 - INFO - [diffusion][Epoch 11713] Epoch 11714/12000
2024-11-05 06:52:45,673 - INFO - [diffusion][Epoch 11713] diffusion training Loss: 0.04825528431683779
2024-11-05 06:52:45,677 - INFO - [diffusion][Epoch 11713] diffusion learning rate: 0.001
2024-11-05 06:52:45,678 - INFO - [diffusion][Epoch 11713] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:45,680 - INFO - [diffusion][Epoch 11714] Epoch 11715/12000
2024-11-05 06:52:49,802 - INFO - [diffusion][Epoch 11714] diffusion training Loss: 0.04671428073197603
2024-11-05 06:52:49,804 - INFO - [diffusion][Epoch 11714] diffusion learning rate: 0.001
2024-11-05 06:52:49,806 - INFO - [diffusion][Epoch 11714] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:49,807 - INFO - [diffusion][Epoch 11715] Epoch 11716/12000
2024-11-05 06:52:53,915 - INFO - [diffusion][Epoch 11715] diffusion training Loss: 0.04933709278702736
2024-11-05 06:52:53,917 - INFO - [diffusion][Epoch 11715] diffusion learning rate: 0.001
2024-11-05 06:52:53,919 - INFO - [diffusion][Epoch 11715] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:53,920 - INFO - [diffusion][Epoch 11716] Epoch 11717/12000
2024-11-05 06:52:58,065 - INFO - [diffusion][Epoch 11716] diffusion training Loss: 0.04861160181462765
2024-11-05 06:52:58,067 - INFO - [diffusion][Epoch 11716] diffusion learning rate: 0.001
2024-11-05 06:52:58,068 - INFO - [diffusion][Epoch 11716] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:52:58,070 - INFO - [diffusion][Epoch 11717] Epoch 11718/12000
2024-11-05 06:53:02,040 - INFO - [diffusion][Epoch 11717] diffusion training Loss: 0.050727042369544506
2024-11-05 06:53:02,042 - INFO - [diffusion][Epoch 11717] diffusion learning rate: 0.001
2024-11-05 06:53:02,044 - INFO - [diffusion][Epoch 11717] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:02,045 - INFO - [diffusion][Epoch 11718] Epoch 11719/12000
2024-11-05 06:53:06,203 - INFO - [diffusion][Epoch 11718] diffusion training Loss: 0.04362183064222336
2024-11-05 06:53:06,205 - INFO - [diffusion][Epoch 11718] diffusion learning rate: 0.001
2024-11-05 06:53:06,207 - INFO - [diffusion][Epoch 11718] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:06,208 - INFO - [diffusion][Epoch 11719] Epoch 11720/12000
2024-11-05 06:53:10,325 - INFO - [diffusion][Epoch 11719] diffusion training Loss: 0.05185681767761707
2024-11-05 06:53:10,327 - INFO - [diffusion][Epoch 11719] diffusion learning rate: 0.001
2024-11-05 06:53:10,329 - INFO - [diffusion][Epoch 11719] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:10,330 - INFO - [diffusion][Epoch 11720] Epoch 11721/12000
2024-11-05 06:53:14,420 - INFO - [diffusion][Epoch 11720] diffusion training Loss: 0.043968950398266315
2024-11-05 06:53:14,422 - INFO - [diffusion][Epoch 11720] diffusion learning rate: 0.001
2024-11-05 06:53:14,424 - INFO - [diffusion][Epoch 11720] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:14,425 - INFO - [diffusion][Epoch 11721] Epoch 11722/12000
2024-11-05 06:53:18,541 - INFO - [diffusion][Epoch 11721] diffusion training Loss: 0.04616272635757923
2024-11-05 06:53:18,542 - INFO - [diffusion][Epoch 11721] diffusion learning rate: 0.001
2024-11-05 06:53:18,544 - INFO - [diffusion][Epoch 11721] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:18,545 - INFO - [diffusion][Epoch 11722] Epoch 11723/12000
2024-11-05 06:53:22,613 - INFO - [diffusion][Epoch 11722] diffusion training Loss: 0.05480425152927637
2024-11-05 06:53:22,615 - INFO - [diffusion][Epoch 11722] diffusion learning rate: 0.001
2024-11-05 06:53:22,617 - INFO - [diffusion][Epoch 11722] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:22,619 - INFO - [diffusion][Epoch 11723] Epoch 11724/12000
2024-11-05 06:53:26,758 - INFO - [diffusion][Epoch 11723] diffusion training Loss: 0.045882112346589565
2024-11-05 06:53:26,759 - INFO - [diffusion][Epoch 11723] diffusion learning rate: 0.001
2024-11-05 06:53:26,761 - INFO - [diffusion][Epoch 11723] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:26,762 - INFO - [diffusion][Epoch 11724] Epoch 11725/12000
2024-11-05 06:53:30,881 - INFO - [diffusion][Epoch 11724] diffusion training Loss: 0.047735605388879776
2024-11-05 06:53:30,883 - INFO - [diffusion][Epoch 11724] diffusion learning rate: 0.001
2024-11-05 06:53:30,885 - INFO - [diffusion][Epoch 11724] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:30,886 - INFO - [diffusion][Epoch 11725] Epoch 11726/12000
2024-11-05 06:53:34,965 - INFO - [diffusion][Epoch 11725] diffusion training Loss: 0.042619043961167336
2024-11-05 06:53:34,967 - INFO - [diffusion][Epoch 11725] diffusion learning rate: 0.001
2024-11-05 06:53:34,969 - INFO - [diffusion][Epoch 11725] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:34,970 - INFO - [diffusion][Epoch 11726] Epoch 11727/12000
2024-11-05 06:53:39,123 - INFO - [diffusion][Epoch 11726] diffusion training Loss: 0.04591235425323248
2024-11-05 06:53:39,125 - INFO - [diffusion][Epoch 11726] diffusion learning rate: 0.001
2024-11-05 06:53:39,127 - INFO - [diffusion][Epoch 11726] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:39,128 - INFO - [diffusion][Epoch 11727] Epoch 11728/12000
2024-11-05 06:53:43,208 - INFO - [diffusion][Epoch 11727] diffusion training Loss: 0.047248294577002525
2024-11-05 06:53:43,210 - INFO - [diffusion][Epoch 11727] diffusion learning rate: 0.001
2024-11-05 06:53:43,212 - INFO - [diffusion][Epoch 11727] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:43,213 - INFO - [diffusion][Epoch 11728] Epoch 11729/12000
2024-11-05 06:53:47,292 - INFO - [diffusion][Epoch 11728] diffusion training Loss: 0.04628785979002714
2024-11-05 06:53:47,296 - INFO - [diffusion][Epoch 11728] diffusion learning rate: 0.001
2024-11-05 06:53:47,297 - INFO - [diffusion][Epoch 11728] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:47,299 - INFO - [diffusion][Epoch 11729] Epoch 11730/12000
2024-11-05 06:53:51,377 - INFO - [diffusion][Epoch 11729] diffusion training Loss: 0.04290719889104366
2024-11-05 06:53:51,379 - INFO - [diffusion][Epoch 11729] diffusion learning rate: 0.001
2024-11-05 06:53:51,381 - INFO - [diffusion][Epoch 11729] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:51,382 - INFO - [diffusion][Epoch 11730] Epoch 11731/12000
2024-11-05 06:53:55,463 - INFO - [diffusion][Epoch 11730] diffusion training Loss: 0.04605184309184551
2024-11-05 06:53:55,466 - INFO - [diffusion][Epoch 11730] diffusion learning rate: 0.001
2024-11-05 06:53:55,485 - INFO - [diffusion][Epoch 11730] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:55,487 - INFO - [diffusion][Epoch 11731] Epoch 11732/12000
2024-11-05 06:53:59,581 - INFO - [diffusion][Epoch 11731] diffusion training Loss: 0.048255947418510914
2024-11-05 06:53:59,583 - INFO - [diffusion][Epoch 11731] diffusion learning rate: 0.001
2024-11-05 06:53:59,585 - INFO - [diffusion][Epoch 11731] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:53:59,586 - INFO - [diffusion][Epoch 11732] Epoch 11733/12000
2024-11-05 06:54:03,644 - INFO - [diffusion][Epoch 11732] diffusion training Loss: 0.04330570995807648
2024-11-05 06:54:03,646 - INFO - [diffusion][Epoch 11732] diffusion learning rate: 0.001
2024-11-05 06:54:03,647 - INFO - [diffusion][Epoch 11732] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:03,649 - INFO - [diffusion][Epoch 11733] Epoch 11734/12000
2024-11-05 06:54:07,770 - INFO - [diffusion][Epoch 11733] diffusion training Loss: 0.04744424670934677
2024-11-05 06:54:07,771 - INFO - [diffusion][Epoch 11733] diffusion learning rate: 0.001
2024-11-05 06:54:07,773 - INFO - [diffusion][Epoch 11733] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:07,774 - INFO - [diffusion][Epoch 11734] Epoch 11735/12000
2024-11-05 06:54:11,885 - INFO - [diffusion][Epoch 11734] diffusion training Loss: 0.04888515546917915
2024-11-05 06:54:11,887 - INFO - [diffusion][Epoch 11734] diffusion learning rate: 0.001
2024-11-05 06:54:11,888 - INFO - [diffusion][Epoch 11734] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:11,890 - INFO - [diffusion][Epoch 11735] Epoch 11736/12000
2024-11-05 06:54:15,922 - INFO - [diffusion][Epoch 11735] diffusion training Loss: 0.04343275725841522
2024-11-05 06:54:15,924 - INFO - [diffusion][Epoch 11735] diffusion learning rate: 0.001
2024-11-05 06:54:15,926 - INFO - [diffusion][Epoch 11735] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:15,927 - INFO - [diffusion][Epoch 11736] Epoch 11737/12000
2024-11-05 06:54:19,971 - INFO - [diffusion][Epoch 11736] diffusion training Loss: 0.04540703818202019
2024-11-05 06:54:19,973 - INFO - [diffusion][Epoch 11736] diffusion learning rate: 0.001
2024-11-05 06:54:19,975 - INFO - [diffusion][Epoch 11736] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:19,976 - INFO - [diffusion][Epoch 11737] Epoch 11738/12000
2024-11-05 06:54:24,080 - INFO - [diffusion][Epoch 11737] diffusion training Loss: 0.03831698279827833
2024-11-05 06:54:24,082 - INFO - [diffusion][Epoch 11737] diffusion learning rate: 0.001
2024-11-05 06:54:24,083 - INFO - [diffusion][Epoch 11737] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:24,085 - INFO - [diffusion][Epoch 11738] Epoch 11739/12000
2024-11-05 06:54:28,185 - INFO - [diffusion][Epoch 11738] diffusion training Loss: 0.0436320761218667
2024-11-05 06:54:28,187 - INFO - [diffusion][Epoch 11738] diffusion learning rate: 0.001
2024-11-05 06:54:28,189 - INFO - [diffusion][Epoch 11738] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:28,190 - INFO - [diffusion][Epoch 11739] Epoch 11740/12000
2024-11-05 06:54:32,257 - INFO - [diffusion][Epoch 11739] diffusion training Loss: 0.0475979121401906
2024-11-05 06:54:32,259 - INFO - [diffusion][Epoch 11739] diffusion learning rate: 0.001
2024-11-05 06:54:32,261 - INFO - [diffusion][Epoch 11739] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:32,262 - INFO - [diffusion][Epoch 11740] Epoch 11741/12000
2024-11-05 06:54:36,365 - INFO - [diffusion][Epoch 11740] diffusion training Loss: 0.04305675532668829
2024-11-05 06:54:36,367 - INFO - [diffusion][Epoch 11740] diffusion learning rate: 0.001
2024-11-05 06:54:36,369 - INFO - [diffusion][Epoch 11740] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:36,370 - INFO - [diffusion][Epoch 11741] Epoch 11742/12000
2024-11-05 06:54:40,486 - INFO - [diffusion][Epoch 11741] diffusion training Loss: 0.04635719768702984
2024-11-05 06:54:40,488 - INFO - [diffusion][Epoch 11741] diffusion learning rate: 0.001
2024-11-05 06:54:40,489 - INFO - [diffusion][Epoch 11741] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:40,491 - INFO - [diffusion][Epoch 11742] Epoch 11743/12000
2024-11-05 06:54:44,535 - INFO - [diffusion][Epoch 11742] diffusion training Loss: 0.04866161569952965
2024-11-05 06:54:44,537 - INFO - [diffusion][Epoch 11742] diffusion learning rate: 0.001
2024-11-05 06:54:44,539 - INFO - [diffusion][Epoch 11742] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:44,540 - INFO - [diffusion][Epoch 11743] Epoch 11744/12000
2024-11-05 06:54:48,648 - INFO - [diffusion][Epoch 11743] diffusion training Loss: 0.04269715491682291
2024-11-05 06:54:48,651 - INFO - [diffusion][Epoch 11743] diffusion learning rate: 0.001
2024-11-05 06:54:48,653 - INFO - [diffusion][Epoch 11743] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:48,655 - INFO - [diffusion][Epoch 11744] Epoch 11745/12000
2024-11-05 06:54:52,773 - INFO - [diffusion][Epoch 11744] diffusion training Loss: 0.050003768876194954
2024-11-05 06:54:52,775 - INFO - [diffusion][Epoch 11744] diffusion learning rate: 0.001
2024-11-05 06:54:52,777 - INFO - [diffusion][Epoch 11744] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:52,778 - INFO - [diffusion][Epoch 11745] Epoch 11746/12000
2024-11-05 06:54:56,911 - INFO - [diffusion][Epoch 11745] diffusion training Loss: 0.04470617324113846
2024-11-05 06:54:56,913 - INFO - [diffusion][Epoch 11745] diffusion learning rate: 0.001
2024-11-05 06:54:56,915 - INFO - [diffusion][Epoch 11745] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:54:56,916 - INFO - [diffusion][Epoch 11746] Epoch 11747/12000
2024-11-05 06:55:01,197 - INFO - [diffusion][Epoch 11746] diffusion training Loss: 0.04689801670610905
2024-11-05 06:55:01,199 - INFO - [diffusion][Epoch 11746] diffusion learning rate: 0.001
2024-11-05 06:55:01,201 - INFO - [diffusion][Epoch 11746] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:01,203 - INFO - [diffusion][Epoch 11747] Epoch 11748/12000
2024-11-05 06:55:05,337 - INFO - [diffusion][Epoch 11747] diffusion training Loss: 0.04764982406049967
2024-11-05 06:55:05,339 - INFO - [diffusion][Epoch 11747] diffusion learning rate: 0.001
2024-11-05 06:55:05,341 - INFO - [diffusion][Epoch 11747] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:05,343 - INFO - [diffusion][Epoch 11748] Epoch 11749/12000
2024-11-05 06:55:09,420 - INFO - [diffusion][Epoch 11748] diffusion training Loss: 0.04599534347653389
2024-11-05 06:55:09,422 - INFO - [diffusion][Epoch 11748] diffusion learning rate: 0.001
2024-11-05 06:55:09,424 - INFO - [diffusion][Epoch 11748] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:09,426 - INFO - [diffusion][Epoch 11749] Epoch 11750/12000
2024-11-05 06:55:13,458 - INFO - [diffusion][Epoch 11749] diffusion training Loss: 0.04485719557851553
2024-11-05 06:55:13,460 - INFO - [diffusion][Epoch 11749] diffusion learning rate: 0.001
2024-11-05 06:55:13,462 - INFO - [diffusion][Epoch 11749] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:13,463 - INFO - [diffusion][Epoch 11750] Epoch 11751/12000
2024-11-05 06:55:17,545 - INFO - [diffusion][Epoch 11750] diffusion training Loss: 0.04562030080705881
2024-11-05 06:55:17,547 - INFO - [diffusion][Epoch 11750] diffusion learning rate: 0.001
2024-11-05 06:55:17,549 - INFO - [diffusion][Epoch 11750] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:17,550 - INFO - [diffusion][Epoch 11751] Epoch 11752/12000
2024-11-05 06:55:21,659 - INFO - [diffusion][Epoch 11751] diffusion training Loss: 0.04459182173013687
2024-11-05 06:55:21,661 - INFO - [diffusion][Epoch 11751] diffusion learning rate: 0.001
2024-11-05 06:55:21,688 - INFO - [diffusion][Epoch 11751] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:21,691 - INFO - [diffusion][Epoch 11752] Epoch 11753/12000
2024-11-05 06:55:25,809 - INFO - [diffusion][Epoch 11752] diffusion training Loss: 0.04472716525197029
2024-11-05 06:55:25,811 - INFO - [diffusion][Epoch 11752] diffusion learning rate: 0.001
2024-11-05 06:55:25,812 - INFO - [diffusion][Epoch 11752] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:25,814 - INFO - [diffusion][Epoch 11753] Epoch 11754/12000
2024-11-05 06:55:29,900 - INFO - [diffusion][Epoch 11753] diffusion training Loss: 0.052685754373669624
2024-11-05 06:55:29,901 - INFO - [diffusion][Epoch 11753] diffusion learning rate: 0.001
2024-11-05 06:55:29,903 - INFO - [diffusion][Epoch 11753] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:29,904 - INFO - [diffusion][Epoch 11754] Epoch 11755/12000
2024-11-05 06:55:33,988 - INFO - [diffusion][Epoch 11754] diffusion training Loss: 0.04657841473817825
2024-11-05 06:55:33,990 - INFO - [diffusion][Epoch 11754] diffusion learning rate: 0.001
2024-11-05 06:55:33,991 - INFO - [diffusion][Epoch 11754] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:33,993 - INFO - [diffusion][Epoch 11755] Epoch 11756/12000
2024-11-05 06:55:38,095 - INFO - [diffusion][Epoch 11755] diffusion training Loss: 0.04557288717478514
2024-11-05 06:55:38,097 - INFO - [diffusion][Epoch 11755] diffusion learning rate: 0.001
2024-11-05 06:55:38,098 - INFO - [diffusion][Epoch 11755] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:38,100 - INFO - [diffusion][Epoch 11756] Epoch 11757/12000
2024-11-05 06:55:42,178 - INFO - [diffusion][Epoch 11756] diffusion training Loss: 0.04493840038776398
2024-11-05 06:55:42,180 - INFO - [diffusion][Epoch 11756] diffusion learning rate: 0.001
2024-11-05 06:55:42,182 - INFO - [diffusion][Epoch 11756] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:42,183 - INFO - [diffusion][Epoch 11757] Epoch 11758/12000
2024-11-05 06:55:46,272 - INFO - [diffusion][Epoch 11757] diffusion training Loss: 0.04718608222901821
2024-11-05 06:55:46,274 - INFO - [diffusion][Epoch 11757] diffusion learning rate: 0.001
2024-11-05 06:55:46,276 - INFO - [diffusion][Epoch 11757] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:46,277 - INFO - [diffusion][Epoch 11758] Epoch 11759/12000
2024-11-05 06:55:50,276 - INFO - [diffusion][Epoch 11758] diffusion training Loss: 0.04488630034029484
2024-11-05 06:55:50,279 - INFO - [diffusion][Epoch 11758] diffusion learning rate: 0.001
2024-11-05 06:55:50,281 - INFO - [diffusion][Epoch 11758] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:50,282 - INFO - [diffusion][Epoch 11759] Epoch 11760/12000
2024-11-05 06:55:54,217 - INFO - [diffusion][Epoch 11759] diffusion training Loss: 0.04578122869133949
2024-11-05 06:55:54,218 - INFO - [diffusion][Epoch 11759] diffusion learning rate: 0.001
2024-11-05 06:55:54,220 - INFO - [diffusion][Epoch 11759] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:54,221 - INFO - [diffusion][Epoch 11760] Epoch 11761/12000
2024-11-05 06:55:58,371 - INFO - [diffusion][Epoch 11760] diffusion training Loss: 0.0440628444775939
2024-11-05 06:55:58,373 - INFO - [diffusion][Epoch 11760] diffusion learning rate: 0.001
2024-11-05 06:55:58,375 - INFO - [diffusion][Epoch 11760] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:55:58,376 - INFO - [diffusion][Epoch 11761] Epoch 11762/12000
2024-11-05 06:56:02,434 - INFO - [diffusion][Epoch 11761] diffusion training Loss: 0.046564098447561264
2024-11-05 06:56:02,436 - INFO - [diffusion][Epoch 11761] diffusion learning rate: 0.001
2024-11-05 06:56:02,438 - INFO - [diffusion][Epoch 11761] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:02,439 - INFO - [diffusion][Epoch 11762] Epoch 11763/12000
2024-11-05 06:56:06,528 - INFO - [diffusion][Epoch 11762] diffusion training Loss: 0.046658385545015335
2024-11-05 06:56:06,530 - INFO - [diffusion][Epoch 11762] diffusion learning rate: 0.001
2024-11-05 06:56:06,532 - INFO - [diffusion][Epoch 11762] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:06,533 - INFO - [diffusion][Epoch 11763] Epoch 11764/12000
2024-11-05 06:56:10,503 - INFO - [diffusion][Epoch 11763] diffusion training Loss: 0.04590366594493389
2024-11-05 06:56:10,505 - INFO - [diffusion][Epoch 11763] diffusion learning rate: 0.001
2024-11-05 06:56:10,507 - INFO - [diffusion][Epoch 11763] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:10,508 - INFO - [diffusion][Epoch 11764] Epoch 11765/12000
2024-11-05 06:56:14,614 - INFO - [diffusion][Epoch 11764] diffusion training Loss: 0.043407575227320194
2024-11-05 06:56:14,616 - INFO - [diffusion][Epoch 11764] diffusion learning rate: 0.001
2024-11-05 06:56:14,618 - INFO - [diffusion][Epoch 11764] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:14,619 - INFO - [diffusion][Epoch 11765] Epoch 11766/12000
2024-11-05 06:56:18,679 - INFO - [diffusion][Epoch 11765] diffusion training Loss: 0.04482692666351795
2024-11-05 06:56:18,681 - INFO - [diffusion][Epoch 11765] diffusion learning rate: 0.001
2024-11-05 06:56:18,683 - INFO - [diffusion][Epoch 11765] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:18,684 - INFO - [diffusion][Epoch 11766] Epoch 11767/12000
2024-11-05 06:56:22,875 - INFO - [diffusion][Epoch 11766] diffusion training Loss: 0.04534433130174875
2024-11-05 06:56:22,877 - INFO - [diffusion][Epoch 11766] diffusion learning rate: 0.001
2024-11-05 06:56:22,879 - INFO - [diffusion][Epoch 11766] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:22,881 - INFO - [diffusion][Epoch 11767] Epoch 11768/12000
2024-11-05 06:56:27,549 - INFO - [diffusion][Epoch 11767] diffusion training Loss: 0.0470282007008791
2024-11-05 06:56:27,551 - INFO - [diffusion][Epoch 11767] diffusion learning rate: 0.001
2024-11-05 06:56:27,553 - INFO - [diffusion][Epoch 11767] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:27,554 - INFO - [diffusion][Epoch 11768] Epoch 11769/12000
2024-11-05 06:56:31,668 - INFO - [diffusion][Epoch 11768] diffusion training Loss: 0.04599105753004551
2024-11-05 06:56:31,670 - INFO - [diffusion][Epoch 11768] diffusion learning rate: 0.001
2024-11-05 06:56:31,671 - INFO - [diffusion][Epoch 11768] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:31,673 - INFO - [diffusion][Epoch 11769] Epoch 11770/12000
2024-11-05 06:56:35,761 - INFO - [diffusion][Epoch 11769] diffusion training Loss: 0.0444285674020648
2024-11-05 06:56:35,763 - INFO - [diffusion][Epoch 11769] diffusion learning rate: 0.001
2024-11-05 06:56:35,764 - INFO - [diffusion][Epoch 11769] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:35,766 - INFO - [diffusion][Epoch 11770] Epoch 11771/12000
2024-11-05 06:56:39,826 - INFO - [diffusion][Epoch 11770] diffusion training Loss: 0.04481877200305462
2024-11-05 06:56:39,828 - INFO - [diffusion][Epoch 11770] diffusion learning rate: 0.001
2024-11-05 06:56:39,830 - INFO - [diffusion][Epoch 11770] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:39,831 - INFO - [diffusion][Epoch 11771] Epoch 11772/12000
2024-11-05 06:56:43,879 - INFO - [diffusion][Epoch 11771] diffusion training Loss: 0.04748709965497255
2024-11-05 06:56:43,881 - INFO - [diffusion][Epoch 11771] diffusion learning rate: 0.001
2024-11-05 06:56:43,883 - INFO - [diffusion][Epoch 11771] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:43,884 - INFO - [diffusion][Epoch 11772] Epoch 11773/12000
2024-11-05 06:56:47,933 - INFO - [diffusion][Epoch 11772] diffusion training Loss: 0.047185818664729595
2024-11-05 06:56:47,935 - INFO - [diffusion][Epoch 11772] diffusion learning rate: 0.001
2024-11-05 06:56:47,936 - INFO - [diffusion][Epoch 11772] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:47,937 - INFO - [diffusion][Epoch 11773] Epoch 11774/12000
2024-11-05 06:56:52,043 - INFO - [diffusion][Epoch 11773] diffusion training Loss: 0.04629764333367348
2024-11-05 06:56:52,046 - INFO - [diffusion][Epoch 11773] diffusion learning rate: 0.001
2024-11-05 06:56:52,048 - INFO - [diffusion][Epoch 11773] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:52,049 - INFO - [diffusion][Epoch 11774] Epoch 11775/12000
2024-11-05 06:56:56,173 - INFO - [diffusion][Epoch 11774] diffusion training Loss: 0.04748806729912758
2024-11-05 06:56:56,175 - INFO - [diffusion][Epoch 11774] diffusion learning rate: 0.001
2024-11-05 06:56:56,177 - INFO - [diffusion][Epoch 11774] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:56:56,178 - INFO - [diffusion][Epoch 11775] Epoch 11776/12000
2024-11-05 06:57:00,279 - INFO - [diffusion][Epoch 11775] diffusion training Loss: 0.051527366042137146
2024-11-05 06:57:00,281 - INFO - [diffusion][Epoch 11775] diffusion learning rate: 0.001
2024-11-05 06:57:00,282 - INFO - [diffusion][Epoch 11775] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:00,284 - INFO - [diffusion][Epoch 11776] Epoch 11777/12000
2024-11-05 06:57:04,425 - INFO - [diffusion][Epoch 11776] diffusion training Loss: 0.04924026411026716
2024-11-05 06:57:04,427 - INFO - [diffusion][Epoch 11776] diffusion learning rate: 0.001
2024-11-05 06:57:04,429 - INFO - [diffusion][Epoch 11776] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:04,430 - INFO - [diffusion][Epoch 11777] Epoch 11778/12000
2024-11-05 06:57:08,552 - INFO - [diffusion][Epoch 11777] diffusion training Loss: 0.04408549424260855
2024-11-05 06:57:08,554 - INFO - [diffusion][Epoch 11777] diffusion learning rate: 0.001
2024-11-05 06:57:08,581 - INFO - [diffusion][Epoch 11777] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:08,583 - INFO - [diffusion][Epoch 11778] Epoch 11779/12000
2024-11-05 06:57:12,514 - INFO - [diffusion][Epoch 11778] diffusion training Loss: 0.04538376536220312
2024-11-05 06:57:12,524 - INFO - [diffusion][Epoch 11778] diffusion learning rate: 0.001
2024-11-05 06:57:12,526 - INFO - [diffusion][Epoch 11778] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:12,527 - INFO - [diffusion][Epoch 11779] Epoch 11780/12000
2024-11-05 06:57:16,329 - INFO - [diffusion][Epoch 11779] diffusion training Loss: 0.047439551912248135
2024-11-05 06:57:16,331 - INFO - [diffusion][Epoch 11779] diffusion learning rate: 0.001
2024-11-05 06:57:16,332 - INFO - [diffusion][Epoch 11779] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:16,334 - INFO - [diffusion][Epoch 11780] Epoch 11781/12000
2024-11-05 06:57:20,429 - INFO - [diffusion][Epoch 11780] diffusion training Loss: 0.046391877345740795
2024-11-05 06:57:20,431 - INFO - [diffusion][Epoch 11780] diffusion learning rate: 0.001
2024-11-05 06:57:20,433 - INFO - [diffusion][Epoch 11780] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:20,434 - INFO - [diffusion][Epoch 11781] Epoch 11782/12000
2024-11-05 06:57:24,551 - INFO - [diffusion][Epoch 11781] diffusion training Loss: 0.045512658543884754
2024-11-05 06:57:24,553 - INFO - [diffusion][Epoch 11781] diffusion learning rate: 0.001
2024-11-05 06:57:24,555 - INFO - [diffusion][Epoch 11781] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:24,556 - INFO - [diffusion][Epoch 11782] Epoch 11783/12000
2024-11-05 06:57:28,565 - INFO - [diffusion][Epoch 11782] diffusion training Loss: 0.04798587504774332
2024-11-05 06:57:28,567 - INFO - [diffusion][Epoch 11782] diffusion learning rate: 0.001
2024-11-05 06:57:28,569 - INFO - [diffusion][Epoch 11782] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:28,570 - INFO - [diffusion][Epoch 11783] Epoch 11784/12000
2024-11-05 06:57:32,676 - INFO - [diffusion][Epoch 11783] diffusion training Loss: 0.045423874631524086
2024-11-05 06:57:32,679 - INFO - [diffusion][Epoch 11783] diffusion learning rate: 0.001
2024-11-05 06:57:32,681 - INFO - [diffusion][Epoch 11783] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:32,682 - INFO - [diffusion][Epoch 11784] Epoch 11785/12000
2024-11-05 06:57:36,693 - INFO - [diffusion][Epoch 11784] diffusion training Loss: 0.043699163012206554
2024-11-05 06:57:36,695 - INFO - [diffusion][Epoch 11784] diffusion learning rate: 0.001
2024-11-05 06:57:36,697 - INFO - [diffusion][Epoch 11784] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:36,698 - INFO - [diffusion][Epoch 11785] Epoch 11786/12000
2024-11-05 06:57:40,848 - INFO - [diffusion][Epoch 11785] diffusion training Loss: 0.04484414774924517
2024-11-05 06:57:40,850 - INFO - [diffusion][Epoch 11785] diffusion learning rate: 0.001
2024-11-05 06:57:40,852 - INFO - [diffusion][Epoch 11785] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:40,853 - INFO - [diffusion][Epoch 11786] Epoch 11787/12000
2024-11-05 06:57:44,888 - INFO - [diffusion][Epoch 11786] diffusion training Loss: 0.04820626322180033
2024-11-05 06:57:44,890 - INFO - [diffusion][Epoch 11786] diffusion learning rate: 0.001
2024-11-05 06:57:44,891 - INFO - [diffusion][Epoch 11786] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:44,893 - INFO - [diffusion][Epoch 11787] Epoch 11788/12000
2024-11-05 06:57:48,996 - INFO - [diffusion][Epoch 11787] diffusion training Loss: 0.04912644065916538
2024-11-05 06:57:48,998 - INFO - [diffusion][Epoch 11787] diffusion learning rate: 0.001
2024-11-05 06:57:48,999 - INFO - [diffusion][Epoch 11787] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:49,001 - INFO - [diffusion][Epoch 11788] Epoch 11789/12000
2024-11-05 06:57:53,212 - INFO - [diffusion][Epoch 11788] diffusion training Loss: 0.04400296788662672
2024-11-05 06:57:53,215 - INFO - [diffusion][Epoch 11788] diffusion learning rate: 0.001
2024-11-05 06:57:53,217 - INFO - [diffusion][Epoch 11788] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:53,218 - INFO - [diffusion][Epoch 11789] Epoch 11790/12000
2024-11-05 06:57:57,336 - INFO - [diffusion][Epoch 11789] diffusion training Loss: 0.044678389094769955
2024-11-05 06:57:57,338 - INFO - [diffusion][Epoch 11789] diffusion learning rate: 0.001
2024-11-05 06:57:57,340 - INFO - [diffusion][Epoch 11789] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:57:57,341 - INFO - [diffusion][Epoch 11790] Epoch 11791/12000
2024-11-05 06:58:01,436 - INFO - [diffusion][Epoch 11790] diffusion training Loss: 0.04942404106259346
2024-11-05 06:58:01,438 - INFO - [diffusion][Epoch 11790] diffusion learning rate: 0.001
2024-11-05 06:58:01,463 - INFO - [diffusion][Epoch 11790] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:01,465 - INFO - [diffusion][Epoch 11791] Epoch 11792/12000
2024-11-05 06:58:05,535 - INFO - [diffusion][Epoch 11791] diffusion training Loss: 0.048157899640500546
2024-11-05 06:58:05,537 - INFO - [diffusion][Epoch 11791] diffusion learning rate: 0.001
2024-11-05 06:58:05,539 - INFO - [diffusion][Epoch 11791] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:05,540 - INFO - [diffusion][Epoch 11792] Epoch 11793/12000
2024-11-05 06:58:09,663 - INFO - [diffusion][Epoch 11792] diffusion training Loss: 0.04600890073925257
2024-11-05 06:58:09,665 - INFO - [diffusion][Epoch 11792] diffusion learning rate: 0.001
2024-11-05 06:58:09,667 - INFO - [diffusion][Epoch 11792] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:09,668 - INFO - [diffusion][Epoch 11793] Epoch 11794/12000
2024-11-05 06:58:13,741 - INFO - [diffusion][Epoch 11793] diffusion training Loss: 0.043822151608765125
2024-11-05 06:58:13,743 - INFO - [diffusion][Epoch 11793] diffusion learning rate: 0.001
2024-11-05 06:58:13,744 - INFO - [diffusion][Epoch 11793] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:13,746 - INFO - [diffusion][Epoch 11794] Epoch 11795/12000
2024-11-05 06:58:17,797 - INFO - [diffusion][Epoch 11794] diffusion training Loss: 0.05271314177662134
2024-11-05 06:58:17,799 - INFO - [diffusion][Epoch 11794] diffusion learning rate: 0.001
2024-11-05 06:58:17,801 - INFO - [diffusion][Epoch 11794] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:17,803 - INFO - [diffusion][Epoch 11795] Epoch 11796/12000
2024-11-05 06:58:21,908 - INFO - [diffusion][Epoch 11795] diffusion training Loss: 0.04575136676430702
2024-11-05 06:58:21,910 - INFO - [diffusion][Epoch 11795] diffusion learning rate: 0.001
2024-11-05 06:58:21,911 - INFO - [diffusion][Epoch 11795] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:21,913 - INFO - [diffusion][Epoch 11796] Epoch 11797/12000
2024-11-05 06:58:26,033 - INFO - [diffusion][Epoch 11796] diffusion training Loss: 0.04933458939194679
2024-11-05 06:58:26,035 - INFO - [diffusion][Epoch 11796] diffusion learning rate: 0.001
2024-11-05 06:58:26,037 - INFO - [diffusion][Epoch 11796] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:26,038 - INFO - [diffusion][Epoch 11797] Epoch 11798/12000
2024-11-05 06:58:30,147 - INFO - [diffusion][Epoch 11797] diffusion training Loss: 0.046893796883523464
2024-11-05 06:58:30,149 - INFO - [diffusion][Epoch 11797] diffusion learning rate: 0.001
2024-11-05 06:58:30,176 - INFO - [diffusion][Epoch 11797] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:30,178 - INFO - [diffusion][Epoch 11798] Epoch 11799/12000
2024-11-05 06:58:34,288 - INFO - [diffusion][Epoch 11798] diffusion training Loss: 0.04737937077879906
2024-11-05 06:58:34,290 - INFO - [diffusion][Epoch 11798] diffusion learning rate: 0.001
2024-11-05 06:58:34,292 - INFO - [diffusion][Epoch 11798] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:34,293 - INFO - [diffusion][Epoch 11799] Epoch 11800/12000
2024-11-05 06:58:38,218 - INFO - [diffusion][Epoch 11799] diffusion training Loss: 0.04593473765999079
2024-11-05 06:58:38,220 - INFO - [diffusion][Epoch 11799] diffusion learning rate: 0.001
2024-11-05 06:58:38,222 - INFO - [diffusion][Epoch 11799] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:38,223 - INFO - [diffusion][Epoch 11800] Epoch 11801/12000
2024-11-05 06:58:42,326 - INFO - [diffusion][Epoch 11800] diffusion training Loss: 0.04906973149627447
2024-11-05 06:58:42,328 - INFO - [diffusion][Epoch 11800] diffusion learning rate: 0.001
2024-11-05 06:58:42,330 - INFO - [diffusion][Epoch 11800] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:42,331 - INFO - [diffusion][Epoch 11801] Epoch 11802/12000
2024-11-05 06:58:46,402 - INFO - [diffusion][Epoch 11801] diffusion training Loss: 0.05065268650650978
2024-11-05 06:58:46,404 - INFO - [diffusion][Epoch 11801] diffusion learning rate: 0.001
2024-11-05 06:58:46,405 - INFO - [diffusion][Epoch 11801] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:46,406 - INFO - [diffusion][Epoch 11802] Epoch 11803/12000
2024-11-05 06:58:50,496 - INFO - [diffusion][Epoch 11802] diffusion training Loss: 0.046228173188865185
2024-11-05 06:58:50,498 - INFO - [diffusion][Epoch 11802] diffusion learning rate: 0.001
2024-11-05 06:58:50,500 - INFO - [diffusion][Epoch 11802] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:50,501 - INFO - [diffusion][Epoch 11803] Epoch 11804/12000
2024-11-05 06:58:54,600 - INFO - [diffusion][Epoch 11803] diffusion training Loss: 0.04391891788691282
2024-11-05 06:58:54,604 - INFO - [diffusion][Epoch 11803] diffusion learning rate: 0.001
2024-11-05 06:58:54,606 - INFO - [diffusion][Epoch 11803] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:54,607 - INFO - [diffusion][Epoch 11804] Epoch 11805/12000
2024-11-05 06:58:58,704 - INFO - [diffusion][Epoch 11804] diffusion training Loss: 0.04916757345199585
2024-11-05 06:58:58,706 - INFO - [diffusion][Epoch 11804] diffusion learning rate: 0.001
2024-11-05 06:58:58,708 - INFO - [diffusion][Epoch 11804] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:58:58,709 - INFO - [diffusion][Epoch 11805] Epoch 11806/12000
2024-11-05 06:59:02,798 - INFO - [diffusion][Epoch 11805] diffusion training Loss: 0.04671279713511467
2024-11-05 06:59:02,800 - INFO - [diffusion][Epoch 11805] diffusion learning rate: 0.001
2024-11-05 06:59:02,802 - INFO - [diffusion][Epoch 11805] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:02,803 - INFO - [diffusion][Epoch 11806] Epoch 11807/12000
2024-11-05 06:59:06,890 - INFO - [diffusion][Epoch 11806] diffusion training Loss: 0.044251288287341595
2024-11-05 06:59:06,892 - INFO - [diffusion][Epoch 11806] diffusion learning rate: 0.001
2024-11-05 06:59:06,894 - INFO - [diffusion][Epoch 11806] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:06,895 - INFO - [diffusion][Epoch 11807] Epoch 11808/12000
2024-11-05 06:59:10,994 - INFO - [diffusion][Epoch 11807] diffusion training Loss: 0.04493761342018843
2024-11-05 06:59:10,996 - INFO - [diffusion][Epoch 11807] diffusion learning rate: 0.001
2024-11-05 06:59:10,998 - INFO - [diffusion][Epoch 11807] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:10,999 - INFO - [diffusion][Epoch 11808] Epoch 11809/12000
2024-11-05 06:59:15,051 - INFO - [diffusion][Epoch 11808] diffusion training Loss: 0.04901022370904684
2024-11-05 06:59:15,053 - INFO - [diffusion][Epoch 11808] diffusion learning rate: 0.001
2024-11-05 06:59:15,055 - INFO - [diffusion][Epoch 11808] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:15,056 - INFO - [diffusion][Epoch 11809] Epoch 11810/12000
2024-11-05 06:59:19,443 - INFO - [diffusion][Epoch 11809] diffusion training Loss: 0.04214037302881479
2024-11-05 06:59:19,445 - INFO - [diffusion][Epoch 11809] diffusion learning rate: 0.001
2024-11-05 06:59:19,447 - INFO - [diffusion][Epoch 11809] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:19,448 - INFO - [diffusion][Epoch 11810] Epoch 11811/12000
2024-11-05 06:59:23,346 - INFO - [diffusion][Epoch 11810] diffusion training Loss: 0.046430413611233234
2024-11-05 06:59:23,348 - INFO - [diffusion][Epoch 11810] diffusion learning rate: 0.001
2024-11-05 06:59:23,350 - INFO - [diffusion][Epoch 11810] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:23,351 - INFO - [diffusion][Epoch 11811] Epoch 11812/12000
2024-11-05 06:59:27,405 - INFO - [diffusion][Epoch 11811] diffusion training Loss: 0.04955264646559954
2024-11-05 06:59:27,407 - INFO - [diffusion][Epoch 11811] diffusion learning rate: 0.001
2024-11-05 06:59:27,409 - INFO - [diffusion][Epoch 11811] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:27,410 - INFO - [diffusion][Epoch 11812] Epoch 11813/12000
2024-11-05 06:59:31,502 - INFO - [diffusion][Epoch 11812] diffusion training Loss: 0.04594178590923548
2024-11-05 06:59:31,504 - INFO - [diffusion][Epoch 11812] diffusion learning rate: 0.001
2024-11-05 06:59:31,506 - INFO - [diffusion][Epoch 11812] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:31,507 - INFO - [diffusion][Epoch 11813] Epoch 11814/12000
2024-11-05 06:59:35,597 - INFO - [diffusion][Epoch 11813] diffusion training Loss: 0.046614328399300575
2024-11-05 06:59:35,599 - INFO - [diffusion][Epoch 11813] diffusion learning rate: 0.001
2024-11-05 06:59:35,601 - INFO - [diffusion][Epoch 11813] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:35,602 - INFO - [diffusion][Epoch 11814] Epoch 11815/12000
2024-11-05 06:59:39,672 - INFO - [diffusion][Epoch 11814] diffusion training Loss: 0.04468240775167942
2024-11-05 06:59:39,673 - INFO - [diffusion][Epoch 11814] diffusion learning rate: 0.001
2024-11-05 06:59:39,675 - INFO - [diffusion][Epoch 11814] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:39,676 - INFO - [diffusion][Epoch 11815] Epoch 11816/12000
2024-11-05 06:59:43,735 - INFO - [diffusion][Epoch 11815] diffusion training Loss: 0.046524662524461746
2024-11-05 06:59:43,737 - INFO - [diffusion][Epoch 11815] diffusion learning rate: 0.001
2024-11-05 06:59:43,739 - INFO - [diffusion][Epoch 11815] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:43,740 - INFO - [diffusion][Epoch 11816] Epoch 11817/12000
2024-11-05 06:59:47,820 - INFO - [diffusion][Epoch 11816] diffusion training Loss: 0.04852796997874975
2024-11-05 06:59:47,822 - INFO - [diffusion][Epoch 11816] diffusion learning rate: 0.001
2024-11-05 06:59:47,824 - INFO - [diffusion][Epoch 11816] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:47,825 - INFO - [diffusion][Epoch 11817] Epoch 11818/12000
2024-11-05 06:59:51,907 - INFO - [diffusion][Epoch 11817] diffusion training Loss: 0.04248286597430706
2024-11-05 06:59:51,909 - INFO - [diffusion][Epoch 11817] diffusion learning rate: 0.001
2024-11-05 06:59:51,910 - INFO - [diffusion][Epoch 11817] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:51,911 - INFO - [diffusion][Epoch 11818] Epoch 11819/12000
2024-11-05 06:59:56,046 - INFO - [diffusion][Epoch 11818] diffusion training Loss: 0.04940760415047407
2024-11-05 06:59:56,050 - INFO - [diffusion][Epoch 11818] diffusion learning rate: 0.001
2024-11-05 06:59:56,052 - INFO - [diffusion][Epoch 11818] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 06:59:56,053 - INFO - [diffusion][Epoch 11819] Epoch 11820/12000
2024-11-05 07:00:00,147 - INFO - [diffusion][Epoch 11819] diffusion training Loss: 0.04709108453243971
2024-11-05 07:00:00,149 - INFO - [diffusion][Epoch 11819] diffusion learning rate: 0.001
2024-11-05 07:00:00,150 - INFO - [diffusion][Epoch 11819] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:00,152 - INFO - [diffusion][Epoch 11820] Epoch 11821/12000
2024-11-05 07:00:04,257 - INFO - [diffusion][Epoch 11820] diffusion training Loss: 0.04696392174810171
2024-11-05 07:00:04,259 - INFO - [diffusion][Epoch 11820] diffusion learning rate: 0.001
2024-11-05 07:00:04,261 - INFO - [diffusion][Epoch 11820] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:04,262 - INFO - [diffusion][Epoch 11821] Epoch 11822/12000
2024-11-05 07:00:08,359 - INFO - [diffusion][Epoch 11821] diffusion training Loss: 0.045154410414397717
2024-11-05 07:00:08,361 - INFO - [diffusion][Epoch 11821] diffusion learning rate: 0.001
2024-11-05 07:00:08,363 - INFO - [diffusion][Epoch 11821] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:08,364 - INFO - [diffusion][Epoch 11822] Epoch 11823/12000
2024-11-05 07:00:12,480 - INFO - [diffusion][Epoch 11822] diffusion training Loss: 0.04648932069540024
2024-11-05 07:00:12,699 - INFO - [diffusion][Epoch 11822] diffusion learning rate: 0.001
2024-11-05 07:00:12,701 - INFO - [diffusion][Epoch 11822] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:12,703 - INFO - [diffusion][Epoch 11823] Epoch 11824/12000
2024-11-05 07:00:16,856 - INFO - [diffusion][Epoch 11823] diffusion training Loss: 0.04701193701475859
2024-11-05 07:00:16,858 - INFO - [diffusion][Epoch 11823] diffusion learning rate: 0.001
2024-11-05 07:00:16,860 - INFO - [diffusion][Epoch 11823] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:16,861 - INFO - [diffusion][Epoch 11824] Epoch 11825/12000
2024-11-05 07:00:20,917 - INFO - [diffusion][Epoch 11824] diffusion training Loss: 0.04827153589576483
2024-11-05 07:00:20,919 - INFO - [diffusion][Epoch 11824] diffusion learning rate: 0.001
2024-11-05 07:00:20,921 - INFO - [diffusion][Epoch 11824] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:20,922 - INFO - [diffusion][Epoch 11825] Epoch 11826/12000
2024-11-05 07:00:25,003 - INFO - [diffusion][Epoch 11825] diffusion training Loss: 0.046463774517178535
2024-11-05 07:00:25,005 - INFO - [diffusion][Epoch 11825] diffusion learning rate: 0.001
2024-11-05 07:00:25,006 - INFO - [diffusion][Epoch 11825] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:25,008 - INFO - [diffusion][Epoch 11826] Epoch 11827/12000
2024-11-05 07:00:29,076 - INFO - [diffusion][Epoch 11826] diffusion training Loss: 0.04688684921711683
2024-11-05 07:00:29,078 - INFO - [diffusion][Epoch 11826] diffusion learning rate: 0.001
2024-11-05 07:00:29,080 - INFO - [diffusion][Epoch 11826] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:29,081 - INFO - [diffusion][Epoch 11827] Epoch 11828/12000
2024-11-05 07:00:33,152 - INFO - [diffusion][Epoch 11827] diffusion training Loss: 0.0471424600109458
2024-11-05 07:00:33,154 - INFO - [diffusion][Epoch 11827] diffusion learning rate: 0.001
2024-11-05 07:00:33,156 - INFO - [diffusion][Epoch 11827] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:33,157 - INFO - [diffusion][Epoch 11828] Epoch 11829/12000
2024-11-05 07:00:37,245 - INFO - [diffusion][Epoch 11828] diffusion training Loss: 0.04359457828104496
2024-11-05 07:00:37,247 - INFO - [diffusion][Epoch 11828] diffusion learning rate: 0.001
2024-11-05 07:00:37,249 - INFO - [diffusion][Epoch 11828] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:37,250 - INFO - [diffusion][Epoch 11829] Epoch 11830/12000
2024-11-05 07:00:41,357 - INFO - [diffusion][Epoch 11829] diffusion training Loss: 0.04472428094595671
2024-11-05 07:00:41,359 - INFO - [diffusion][Epoch 11829] diffusion learning rate: 0.001
2024-11-05 07:00:41,361 - INFO - [diffusion][Epoch 11829] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:41,362 - INFO - [diffusion][Epoch 11830] Epoch 11831/12000
2024-11-05 07:00:45,543 - INFO - [diffusion][Epoch 11830] diffusion training Loss: 0.04549167677760124
2024-11-05 07:00:45,546 - INFO - [diffusion][Epoch 11830] diffusion learning rate: 0.001
2024-11-05 07:00:45,547 - INFO - [diffusion][Epoch 11830] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:45,549 - INFO - [diffusion][Epoch 11831] Epoch 11832/12000
2024-11-05 07:00:49,665 - INFO - [diffusion][Epoch 11831] diffusion training Loss: 0.04323784913867712
2024-11-05 07:00:49,667 - INFO - [diffusion][Epoch 11831] diffusion learning rate: 0.001
2024-11-05 07:00:49,669 - INFO - [diffusion][Epoch 11831] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:49,670 - INFO - [diffusion][Epoch 11832] Epoch 11833/12000
2024-11-05 07:00:53,755 - INFO - [diffusion][Epoch 11832] diffusion training Loss: 0.046093035489320755
2024-11-05 07:00:53,757 - INFO - [diffusion][Epoch 11832] diffusion learning rate: 0.001
2024-11-05 07:00:53,759 - INFO - [diffusion][Epoch 11832] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:53,760 - INFO - [diffusion][Epoch 11833] Epoch 11834/12000
2024-11-05 07:00:57,842 - INFO - [diffusion][Epoch 11833] diffusion training Loss: 0.04548748303204775
2024-11-05 07:00:57,845 - INFO - [diffusion][Epoch 11833] diffusion learning rate: 0.001
2024-11-05 07:00:57,847 - INFO - [diffusion][Epoch 11833] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:00:57,850 - INFO - [diffusion][Epoch 11834] Epoch 11835/12000
2024-11-05 07:01:01,908 - INFO - [diffusion][Epoch 11834] diffusion training Loss: 0.04940157663077116
2024-11-05 07:01:01,910 - INFO - [diffusion][Epoch 11834] diffusion learning rate: 0.001
2024-11-05 07:01:01,911 - INFO - [diffusion][Epoch 11834] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:01,912 - INFO - [diffusion][Epoch 11835] Epoch 11836/12000
2024-11-05 07:01:06,012 - INFO - [diffusion][Epoch 11835] diffusion training Loss: 0.04395992308855057
2024-11-05 07:01:06,014 - INFO - [diffusion][Epoch 11835] diffusion learning rate: 0.001
2024-11-05 07:01:06,016 - INFO - [diffusion][Epoch 11835] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:06,018 - INFO - [diffusion][Epoch 11836] Epoch 11837/12000
2024-11-05 07:01:10,126 - INFO - [diffusion][Epoch 11836] diffusion training Loss: 0.04597174096852541
2024-11-05 07:01:10,128 - INFO - [diffusion][Epoch 11836] diffusion learning rate: 0.001
2024-11-05 07:01:10,130 - INFO - [diffusion][Epoch 11836] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:10,131 - INFO - [diffusion][Epoch 11837] Epoch 11838/12000
2024-11-05 07:01:14,233 - INFO - [diffusion][Epoch 11837] diffusion training Loss: 0.04567212611436844
2024-11-05 07:01:14,235 - INFO - [diffusion][Epoch 11837] diffusion learning rate: 0.001
2024-11-05 07:01:14,237 - INFO - [diffusion][Epoch 11837] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:14,239 - INFO - [diffusion][Epoch 11838] Epoch 11839/12000
2024-11-05 07:01:18,294 - INFO - [diffusion][Epoch 11838] diffusion training Loss: 0.04495274368673563
2024-11-05 07:01:18,296 - INFO - [diffusion][Epoch 11838] diffusion learning rate: 0.001
2024-11-05 07:01:18,298 - INFO - [diffusion][Epoch 11838] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:18,299 - INFO - [diffusion][Epoch 11839] Epoch 11840/12000
2024-11-05 07:01:22,402 - INFO - [diffusion][Epoch 11839] diffusion training Loss: 0.0499632703140378
2024-11-05 07:01:22,404 - INFO - [diffusion][Epoch 11839] diffusion learning rate: 0.001
2024-11-05 07:01:22,406 - INFO - [diffusion][Epoch 11839] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:22,408 - INFO - [diffusion][Epoch 11840] Epoch 11841/12000
2024-11-05 07:01:26,494 - INFO - [diffusion][Epoch 11840] diffusion training Loss: 0.04725584574043751
2024-11-05 07:01:26,496 - INFO - [diffusion][Epoch 11840] diffusion learning rate: 0.001
2024-11-05 07:01:26,498 - INFO - [diffusion][Epoch 11840] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:26,500 - INFO - [diffusion][Epoch 11841] Epoch 11842/12000
2024-11-05 07:01:30,555 - INFO - [diffusion][Epoch 11841] diffusion training Loss: 0.04062951263040304
2024-11-05 07:01:30,558 - INFO - [diffusion][Epoch 11841] diffusion learning rate: 0.001
2024-11-05 07:01:30,559 - INFO - [diffusion][Epoch 11841] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:30,560 - INFO - [diffusion][Epoch 11842] Epoch 11843/12000
2024-11-05 07:01:34,651 - INFO - [diffusion][Epoch 11842] diffusion training Loss: 0.04656975623220205
2024-11-05 07:01:34,653 - INFO - [diffusion][Epoch 11842] diffusion learning rate: 0.001
2024-11-05 07:01:34,655 - INFO - [diffusion][Epoch 11842] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:34,656 - INFO - [diffusion][Epoch 11843] Epoch 11844/12000
2024-11-05 07:01:38,621 - INFO - [diffusion][Epoch 11843] diffusion training Loss: 0.0493726646527648
2024-11-05 07:01:38,623 - INFO - [diffusion][Epoch 11843] diffusion learning rate: 0.001
2024-11-05 07:01:38,625 - INFO - [diffusion][Epoch 11843] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:38,626 - INFO - [diffusion][Epoch 11844] Epoch 11845/12000
2024-11-05 07:01:42,744 - INFO - [diffusion][Epoch 11844] diffusion training Loss: 0.04427995253354311
2024-11-05 07:01:42,746 - INFO - [diffusion][Epoch 11844] diffusion learning rate: 0.001
2024-11-05 07:01:42,747 - INFO - [diffusion][Epoch 11844] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:42,749 - INFO - [diffusion][Epoch 11845] Epoch 11846/12000
2024-11-05 07:01:46,836 - INFO - [diffusion][Epoch 11845] diffusion training Loss: 0.04603544808924198
2024-11-05 07:01:46,838 - INFO - [diffusion][Epoch 11845] diffusion learning rate: 0.001
2024-11-05 07:01:46,840 - INFO - [diffusion][Epoch 11845] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:46,841 - INFO - [diffusion][Epoch 11846] Epoch 11847/12000
2024-11-05 07:01:50,950 - INFO - [diffusion][Epoch 11846] diffusion training Loss: 0.04721330665051937
2024-11-05 07:01:50,952 - INFO - [diffusion][Epoch 11846] diffusion learning rate: 0.001
2024-11-05 07:01:50,954 - INFO - [diffusion][Epoch 11846] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:50,955 - INFO - [diffusion][Epoch 11847] Epoch 11848/12000
2024-11-05 07:01:55,087 - INFO - [diffusion][Epoch 11847] diffusion training Loss: 0.046620914712548256
2024-11-05 07:01:55,089 - INFO - [diffusion][Epoch 11847] diffusion learning rate: 0.001
2024-11-05 07:01:55,091 - INFO - [diffusion][Epoch 11847] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:55,092 - INFO - [diffusion][Epoch 11848] Epoch 11849/12000
2024-11-05 07:01:59,244 - INFO - [diffusion][Epoch 11848] diffusion training Loss: 0.04453208204358816
2024-11-05 07:01:59,247 - INFO - [diffusion][Epoch 11848] diffusion learning rate: 0.001
2024-11-05 07:01:59,249 - INFO - [diffusion][Epoch 11848] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:01:59,250 - INFO - [diffusion][Epoch 11849] Epoch 11850/12000
2024-11-05 07:02:03,353 - INFO - [diffusion][Epoch 11849] diffusion training Loss: 0.050906757824122906
2024-11-05 07:02:03,355 - INFO - [diffusion][Epoch 11849] diffusion learning rate: 0.001
2024-11-05 07:02:03,357 - INFO - [diffusion][Epoch 11849] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:03,358 - INFO - [diffusion][Epoch 11850] Epoch 11851/12000
2024-11-05 07:02:07,471 - INFO - [diffusion][Epoch 11850] diffusion training Loss: 0.04661442432552576
2024-11-05 07:02:07,473 - INFO - [diffusion][Epoch 11850] diffusion learning rate: 0.001
2024-11-05 07:02:07,475 - INFO - [diffusion][Epoch 11850] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:07,476 - INFO - [diffusion][Epoch 11851] Epoch 11852/12000
2024-11-05 07:02:11,668 - INFO - [diffusion][Epoch 11851] diffusion training Loss: 0.04633411858230829
2024-11-05 07:02:11,670 - INFO - [diffusion][Epoch 11851] diffusion learning rate: 0.001
2024-11-05 07:02:11,672 - INFO - [diffusion][Epoch 11851] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:11,673 - INFO - [diffusion][Epoch 11852] Epoch 11853/12000
2024-11-05 07:02:15,799 - INFO - [diffusion][Epoch 11852] diffusion training Loss: 0.0494763208553195
2024-11-05 07:02:15,944 - INFO - [diffusion][Epoch 11852] diffusion learning rate: 0.001
2024-11-05 07:02:16,069 - INFO - [diffusion][Epoch 11852] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:16,127 - INFO - [diffusion][Epoch 11853] Epoch 11854/12000
2024-11-05 07:02:20,210 - INFO - [diffusion][Epoch 11853] diffusion training Loss: 0.04938652366399765
2024-11-05 07:02:20,212 - INFO - [diffusion][Epoch 11853] diffusion learning rate: 0.001
2024-11-05 07:02:20,214 - INFO - [diffusion][Epoch 11853] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:20,215 - INFO - [diffusion][Epoch 11854] Epoch 11855/12000
2024-11-05 07:02:24,286 - INFO - [diffusion][Epoch 11854] diffusion training Loss: 0.04974237643182278
2024-11-05 07:02:24,288 - INFO - [diffusion][Epoch 11854] diffusion learning rate: 0.001
2024-11-05 07:02:24,290 - INFO - [diffusion][Epoch 11854] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:24,291 - INFO - [diffusion][Epoch 11855] Epoch 11856/12000
2024-11-05 07:02:28,264 - INFO - [diffusion][Epoch 11855] diffusion training Loss: 0.047649004496634007
2024-11-05 07:02:28,266 - INFO - [diffusion][Epoch 11855] diffusion learning rate: 0.001
2024-11-05 07:02:28,328 - INFO - [diffusion][Epoch 11855] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:28,329 - INFO - [diffusion][Epoch 11856] Epoch 11857/12000
2024-11-05 07:02:32,432 - INFO - [diffusion][Epoch 11856] diffusion training Loss: 0.04954450950026512
2024-11-05 07:02:32,434 - INFO - [diffusion][Epoch 11856] diffusion learning rate: 0.001
2024-11-05 07:02:32,435 - INFO - [diffusion][Epoch 11856] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:32,437 - INFO - [diffusion][Epoch 11857] Epoch 11858/12000
2024-11-05 07:02:36,534 - INFO - [diffusion][Epoch 11857] diffusion training Loss: 0.04887879453599453
2024-11-05 07:02:36,536 - INFO - [diffusion][Epoch 11857] diffusion learning rate: 0.001
2024-11-05 07:02:36,538 - INFO - [diffusion][Epoch 11857] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:36,539 - INFO - [diffusion][Epoch 11858] Epoch 11859/12000
2024-11-05 07:02:40,659 - INFO - [diffusion][Epoch 11858] diffusion training Loss: 0.041455721482634544
2024-11-05 07:02:40,661 - INFO - [diffusion][Epoch 11858] diffusion learning rate: 0.001
2024-11-05 07:02:40,663 - INFO - [diffusion][Epoch 11858] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:40,664 - INFO - [diffusion][Epoch 11859] Epoch 11860/12000
2024-11-05 07:02:44,753 - INFO - [diffusion][Epoch 11859] diffusion training Loss: 0.05122525431215763
2024-11-05 07:02:44,755 - INFO - [diffusion][Epoch 11859] diffusion learning rate: 0.001
2024-11-05 07:02:44,758 - INFO - [diffusion][Epoch 11859] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:44,759 - INFO - [diffusion][Epoch 11860] Epoch 11861/12000
2024-11-05 07:02:48,834 - INFO - [diffusion][Epoch 11860] diffusion training Loss: 0.05136328004300594
2024-11-05 07:02:48,835 - INFO - [diffusion][Epoch 11860] diffusion learning rate: 0.001
2024-11-05 07:02:48,837 - INFO - [diffusion][Epoch 11860] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:48,838 - INFO - [diffusion][Epoch 11861] Epoch 11862/12000
2024-11-05 07:02:52,990 - INFO - [diffusion][Epoch 11861] diffusion training Loss: 0.04734431393444538
2024-11-05 07:02:52,992 - INFO - [diffusion][Epoch 11861] diffusion learning rate: 0.001
2024-11-05 07:02:52,994 - INFO - [diffusion][Epoch 11861] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:52,995 - INFO - [diffusion][Epoch 11862] Epoch 11863/12000
2024-11-05 07:02:57,063 - INFO - [diffusion][Epoch 11862] diffusion training Loss: 0.04287920705974102
2024-11-05 07:02:57,065 - INFO - [diffusion][Epoch 11862] diffusion learning rate: 0.001
2024-11-05 07:02:57,066 - INFO - [diffusion][Epoch 11862] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:02:57,068 - INFO - [diffusion][Epoch 11863] Epoch 11864/12000
2024-11-05 07:03:01,146 - INFO - [diffusion][Epoch 11863] diffusion training Loss: 0.04612819477915764
2024-11-05 07:03:01,149 - INFO - [diffusion][Epoch 11863] diffusion learning rate: 0.001
2024-11-05 07:03:01,151 - INFO - [diffusion][Epoch 11863] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:01,152 - INFO - [diffusion][Epoch 11864] Epoch 11865/12000
2024-11-05 07:03:05,265 - INFO - [diffusion][Epoch 11864] diffusion training Loss: 0.04534505307674408
2024-11-05 07:03:05,267 - INFO - [diffusion][Epoch 11864] diffusion learning rate: 0.001
2024-11-05 07:03:05,269 - INFO - [diffusion][Epoch 11864] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:05,270 - INFO - [diffusion][Epoch 11865] Epoch 11866/12000
2024-11-05 07:03:09,383 - INFO - [diffusion][Epoch 11865] diffusion training Loss: 0.05220839474350214
2024-11-05 07:03:09,385 - INFO - [diffusion][Epoch 11865] diffusion learning rate: 0.001
2024-11-05 07:03:09,386 - INFO - [diffusion][Epoch 11865] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:09,388 - INFO - [diffusion][Epoch 11866] Epoch 11867/12000
2024-11-05 07:03:13,487 - INFO - [diffusion][Epoch 11866] diffusion training Loss: 0.042903803288936615
2024-11-05 07:03:13,489 - INFO - [diffusion][Epoch 11866] diffusion learning rate: 0.001
2024-11-05 07:03:13,491 - INFO - [diffusion][Epoch 11866] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:13,492 - INFO - [diffusion][Epoch 11867] Epoch 11868/12000
2024-11-05 07:03:17,617 - INFO - [diffusion][Epoch 11867] diffusion training Loss: 0.04932702984660864
2024-11-05 07:03:17,619 - INFO - [diffusion][Epoch 11867] diffusion learning rate: 0.001
2024-11-05 07:03:17,646 - INFO - [diffusion][Epoch 11867] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:17,647 - INFO - [diffusion][Epoch 11868] Epoch 11869/12000
2024-11-05 07:03:21,744 - INFO - [diffusion][Epoch 11868] diffusion training Loss: 0.046309380792081356
2024-11-05 07:03:21,746 - INFO - [diffusion][Epoch 11868] diffusion learning rate: 0.001
2024-11-05 07:03:21,748 - INFO - [diffusion][Epoch 11868] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:21,749 - INFO - [diffusion][Epoch 11869] Epoch 11870/12000
2024-11-05 07:03:25,836 - INFO - [diffusion][Epoch 11869] diffusion training Loss: 0.04528009984642267
2024-11-05 07:03:25,839 - INFO - [diffusion][Epoch 11869] diffusion learning rate: 0.001
2024-11-05 07:03:25,840 - INFO - [diffusion][Epoch 11869] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:25,842 - INFO - [diffusion][Epoch 11870] Epoch 11871/12000
2024-11-05 07:03:30,155 - INFO - [diffusion][Epoch 11870] diffusion training Loss: 0.04156416282057762
2024-11-05 07:03:30,157 - INFO - [diffusion][Epoch 11870] diffusion learning rate: 0.001
2024-11-05 07:03:30,184 - INFO - [diffusion][Epoch 11870] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:30,186 - INFO - [diffusion][Epoch 11871] Epoch 11872/12000
2024-11-05 07:03:34,295 - INFO - [diffusion][Epoch 11871] diffusion training Loss: 0.045132665894925594
2024-11-05 07:03:34,296 - INFO - [diffusion][Epoch 11871] diffusion learning rate: 0.001
2024-11-05 07:03:34,298 - INFO - [diffusion][Epoch 11871] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:34,299 - INFO - [diffusion][Epoch 11872] Epoch 11873/12000
2024-11-05 07:03:38,376 - INFO - [diffusion][Epoch 11872] diffusion training Loss: 0.05084336828440428
2024-11-05 07:03:38,378 - INFO - [diffusion][Epoch 11872] diffusion learning rate: 0.001
2024-11-05 07:03:38,380 - INFO - [diffusion][Epoch 11872] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:38,381 - INFO - [diffusion][Epoch 11873] Epoch 11874/12000
2024-11-05 07:03:42,489 - INFO - [diffusion][Epoch 11873] diffusion training Loss: 0.047083187848329544
2024-11-05 07:03:42,491 - INFO - [diffusion][Epoch 11873] diffusion learning rate: 0.001
2024-11-05 07:03:42,493 - INFO - [diffusion][Epoch 11873] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:42,494 - INFO - [diffusion][Epoch 11874] Epoch 11875/12000
2024-11-05 07:03:46,602 - INFO - [diffusion][Epoch 11874] diffusion training Loss: 0.04933209531009197
2024-11-05 07:03:46,604 - INFO - [diffusion][Epoch 11874] diffusion learning rate: 0.001
2024-11-05 07:03:46,606 - INFO - [diffusion][Epoch 11874] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:46,607 - INFO - [diffusion][Epoch 11875] Epoch 11876/12000
2024-11-05 07:03:50,480 - INFO - [diffusion][Epoch 11875] diffusion training Loss: 0.04893691744655371
2024-11-05 07:03:50,482 - INFO - [diffusion][Epoch 11875] diffusion learning rate: 0.001
2024-11-05 07:03:50,484 - INFO - [diffusion][Epoch 11875] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:50,486 - INFO - [diffusion][Epoch 11876] Epoch 11877/12000
2024-11-05 07:03:54,599 - INFO - [diffusion][Epoch 11876] diffusion training Loss: 0.0452639851719141
2024-11-05 07:03:54,601 - INFO - [diffusion][Epoch 11876] diffusion learning rate: 0.001
2024-11-05 07:03:54,603 - INFO - [diffusion][Epoch 11876] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:54,604 - INFO - [diffusion][Epoch 11877] Epoch 11878/12000
2024-11-05 07:03:58,707 - INFO - [diffusion][Epoch 11877] diffusion training Loss: 0.0474282605573535
2024-11-05 07:03:58,709 - INFO - [diffusion][Epoch 11877] diffusion learning rate: 0.001
2024-11-05 07:03:58,711 - INFO - [diffusion][Epoch 11877] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:03:58,712 - INFO - [diffusion][Epoch 11878] Epoch 11879/12000
2024-11-05 07:04:02,806 - INFO - [diffusion][Epoch 11878] diffusion training Loss: 0.050483972765505314
2024-11-05 07:04:02,809 - INFO - [diffusion][Epoch 11878] diffusion learning rate: 0.001
2024-11-05 07:04:02,811 - INFO - [diffusion][Epoch 11878] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:02,812 - INFO - [diffusion][Epoch 11879] Epoch 11880/12000
2024-11-05 07:04:06,920 - INFO - [diffusion][Epoch 11879] diffusion training Loss: 0.04434135556221008
2024-11-05 07:04:06,922 - INFO - [diffusion][Epoch 11879] diffusion learning rate: 0.001
2024-11-05 07:04:06,924 - INFO - [diffusion][Epoch 11879] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:06,925 - INFO - [diffusion][Epoch 11880] Epoch 11881/12000
2024-11-05 07:04:10,983 - INFO - [diffusion][Epoch 11880] diffusion training Loss: 0.04139312542974949
2024-11-05 07:04:10,985 - INFO - [diffusion][Epoch 11880] diffusion learning rate: 0.001
2024-11-05 07:04:10,987 - INFO - [diffusion][Epoch 11880] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:10,988 - INFO - [diffusion][Epoch 11881] Epoch 11882/12000
2024-11-05 07:04:15,061 - INFO - [diffusion][Epoch 11881] diffusion training Loss: 0.04917153250426054
2024-11-05 07:04:15,063 - INFO - [diffusion][Epoch 11881] diffusion learning rate: 0.001
2024-11-05 07:04:15,065 - INFO - [diffusion][Epoch 11881] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:15,066 - INFO - [diffusion][Epoch 11882] Epoch 11883/12000
2024-11-05 07:04:19,137 - INFO - [diffusion][Epoch 11882] diffusion training Loss: 0.050167216919362545
2024-11-05 07:04:19,139 - INFO - [diffusion][Epoch 11882] diffusion learning rate: 0.001
2024-11-05 07:04:19,141 - INFO - [diffusion][Epoch 11882] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:19,142 - INFO - [diffusion][Epoch 11883] Epoch 11884/12000
2024-11-05 07:04:23,211 - INFO - [diffusion][Epoch 11883] diffusion training Loss: 0.04953193757683039
2024-11-05 07:04:23,213 - INFO - [diffusion][Epoch 11883] diffusion learning rate: 0.001
2024-11-05 07:04:23,215 - INFO - [diffusion][Epoch 11883] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:23,216 - INFO - [diffusion][Epoch 11884] Epoch 11885/12000
2024-11-05 07:04:27,263 - INFO - [diffusion][Epoch 11884] diffusion training Loss: 0.044089389964938164
2024-11-05 07:04:27,265 - INFO - [diffusion][Epoch 11884] diffusion learning rate: 0.001
2024-11-05 07:04:27,267 - INFO - [diffusion][Epoch 11884] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:27,268 - INFO - [diffusion][Epoch 11885] Epoch 11886/12000
2024-11-05 07:04:31,357 - INFO - [diffusion][Epoch 11885] diffusion training Loss: 0.05199760943651199
2024-11-05 07:04:31,359 - INFO - [diffusion][Epoch 11885] diffusion learning rate: 0.001
2024-11-05 07:04:31,361 - INFO - [diffusion][Epoch 11885] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:31,362 - INFO - [diffusion][Epoch 11886] Epoch 11887/12000
2024-11-05 07:04:35,448 - INFO - [diffusion][Epoch 11886] diffusion training Loss: 0.04352597799152136
2024-11-05 07:04:35,450 - INFO - [diffusion][Epoch 11886] diffusion learning rate: 0.001
2024-11-05 07:04:35,452 - INFO - [diffusion][Epoch 11886] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:35,453 - INFO - [diffusion][Epoch 11887] Epoch 11888/12000
2024-11-05 07:04:39,537 - INFO - [diffusion][Epoch 11887] diffusion training Loss: 0.04230494890362024
2024-11-05 07:04:39,539 - INFO - [diffusion][Epoch 11887] diffusion learning rate: 0.001
2024-11-05 07:04:39,541 - INFO - [diffusion][Epoch 11887] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:39,542 - INFO - [diffusion][Epoch 11888] Epoch 11889/12000
2024-11-05 07:04:43,590 - INFO - [diffusion][Epoch 11888] diffusion training Loss: 0.0502177607268095
2024-11-05 07:04:43,592 - INFO - [diffusion][Epoch 11888] diffusion learning rate: 0.001
2024-11-05 07:04:43,594 - INFO - [diffusion][Epoch 11888] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:43,595 - INFO - [diffusion][Epoch 11889] Epoch 11890/12000
2024-11-05 07:04:47,696 - INFO - [diffusion][Epoch 11889] diffusion training Loss: 0.0376506969332695
2024-11-05 07:04:47,698 - INFO - [diffusion][Epoch 11889] diffusion learning rate: 0.001
2024-11-05 07:04:47,700 - INFO - [diffusion][Epoch 11889] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:47,701 - INFO - [diffusion][Epoch 11890] Epoch 11891/12000
2024-11-05 07:04:52,023 - INFO - [diffusion][Epoch 11890] diffusion training Loss: 0.041719356551766396
2024-11-05 07:04:52,025 - INFO - [diffusion][Epoch 11890] diffusion learning rate: 0.001
2024-11-05 07:04:52,027 - INFO - [diffusion][Epoch 11890] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:52,028 - INFO - [diffusion][Epoch 11891] Epoch 11892/12000
2024-11-05 07:04:56,081 - INFO - [diffusion][Epoch 11891] diffusion training Loss: 0.043731958605349064
2024-11-05 07:04:56,083 - INFO - [diffusion][Epoch 11891] diffusion learning rate: 0.001
2024-11-05 07:04:56,084 - INFO - [diffusion][Epoch 11891] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:04:56,086 - INFO - [diffusion][Epoch 11892] Epoch 11893/12000
2024-11-05 07:05:00,194 - INFO - [diffusion][Epoch 11892] diffusion training Loss: 0.048379371874034405
2024-11-05 07:05:00,196 - INFO - [diffusion][Epoch 11892] diffusion learning rate: 0.001
2024-11-05 07:05:00,197 - INFO - [diffusion][Epoch 11892] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:00,199 - INFO - [diffusion][Epoch 11893] Epoch 11894/12000
2024-11-05 07:05:04,308 - INFO - [diffusion][Epoch 11893] diffusion training Loss: 0.04163189232349396
2024-11-05 07:05:04,312 - INFO - [diffusion][Epoch 11893] diffusion learning rate: 0.001
2024-11-05 07:05:04,313 - INFO - [diffusion][Epoch 11893] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:04,315 - INFO - [diffusion][Epoch 11894] Epoch 11895/12000
2024-11-05 07:05:08,418 - INFO - [diffusion][Epoch 11894] diffusion training Loss: 0.042647398076951504
2024-11-05 07:05:08,420 - INFO - [diffusion][Epoch 11894] diffusion learning rate: 0.001
2024-11-05 07:05:08,422 - INFO - [diffusion][Epoch 11894] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:08,423 - INFO - [diffusion][Epoch 11895] Epoch 11896/12000
2024-11-05 07:05:12,515 - INFO - [diffusion][Epoch 11895] diffusion training Loss: 0.04766503442078829
2024-11-05 07:05:12,517 - INFO - [diffusion][Epoch 11895] diffusion learning rate: 0.001
2024-11-05 07:05:12,519 - INFO - [diffusion][Epoch 11895] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:12,520 - INFO - [diffusion][Epoch 11896] Epoch 11897/12000
2024-11-05 07:05:16,637 - INFO - [diffusion][Epoch 11896] diffusion training Loss: 0.04621514678001404
2024-11-05 07:05:16,639 - INFO - [diffusion][Epoch 11896] diffusion learning rate: 0.001
2024-11-05 07:05:16,640 - INFO - [diffusion][Epoch 11896] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:16,642 - INFO - [diffusion][Epoch 11897] Epoch 11898/12000
2024-11-05 07:05:20,760 - INFO - [diffusion][Epoch 11897] diffusion training Loss: 0.04275811556726694
2024-11-05 07:05:20,762 - INFO - [diffusion][Epoch 11897] diffusion learning rate: 0.001
2024-11-05 07:05:20,764 - INFO - [diffusion][Epoch 11897] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:20,765 - INFO - [diffusion][Epoch 11898] Epoch 11899/12000
2024-11-05 07:05:24,832 - INFO - [diffusion][Epoch 11898] diffusion training Loss: 0.048188178800046444
2024-11-05 07:05:24,834 - INFO - [diffusion][Epoch 11898] diffusion learning rate: 0.001
2024-11-05 07:05:24,861 - INFO - [diffusion][Epoch 11898] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:24,862 - INFO - [diffusion][Epoch 11899] Epoch 11900/12000
2024-11-05 07:05:28,928 - INFO - [diffusion][Epoch 11899] diffusion training Loss: 0.045226785354316235
2024-11-05 07:05:28,930 - INFO - [diffusion][Epoch 11899] diffusion learning rate: 0.001
2024-11-05 07:05:28,932 - INFO - [diffusion][Epoch 11899] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:28,934 - INFO - [diffusion][Epoch 11900] Epoch 11901/12000
2024-11-05 07:05:32,974 - INFO - [diffusion][Epoch 11900] diffusion training Loss: 0.0459479708224535
2024-11-05 07:05:32,976 - INFO - [diffusion][Epoch 11900] diffusion learning rate: 0.001
2024-11-05 07:05:32,977 - INFO - [diffusion][Epoch 11900] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:32,979 - INFO - [diffusion][Epoch 11901] Epoch 11902/12000
2024-11-05 07:05:37,062 - INFO - [diffusion][Epoch 11901] diffusion training Loss: 0.04921423923224211
2024-11-05 07:05:37,064 - INFO - [diffusion][Epoch 11901] diffusion learning rate: 0.001
2024-11-05 07:05:37,066 - INFO - [diffusion][Epoch 11901] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:37,067 - INFO - [diffusion][Epoch 11902] Epoch 11903/12000
2024-11-05 07:05:41,165 - INFO - [diffusion][Epoch 11902] diffusion training Loss: 0.049895480275154114
2024-11-05 07:05:41,167 - INFO - [diffusion][Epoch 11902] diffusion learning rate: 0.001
2024-11-05 07:05:41,169 - INFO - [diffusion][Epoch 11902] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:41,170 - INFO - [diffusion][Epoch 11903] Epoch 11904/12000
2024-11-05 07:05:45,292 - INFO - [diffusion][Epoch 11903] diffusion training Loss: 0.04005568567663431
2024-11-05 07:05:45,294 - INFO - [diffusion][Epoch 11903] diffusion learning rate: 0.001
2024-11-05 07:05:45,295 - INFO - [diffusion][Epoch 11903] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:45,297 - INFO - [diffusion][Epoch 11904] Epoch 11905/12000
2024-11-05 07:05:49,402 - INFO - [diffusion][Epoch 11904] diffusion training Loss: 0.04732361435890198
2024-11-05 07:05:49,404 - INFO - [diffusion][Epoch 11904] diffusion learning rate: 0.001
2024-11-05 07:05:49,406 - INFO - [diffusion][Epoch 11904] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:49,407 - INFO - [diffusion][Epoch 11905] Epoch 11906/12000
2024-11-05 07:05:53,526 - INFO - [diffusion][Epoch 11905] diffusion training Loss: 0.04826534539461136
2024-11-05 07:05:53,527 - INFO - [diffusion][Epoch 11905] diffusion learning rate: 0.001
2024-11-05 07:05:53,529 - INFO - [diffusion][Epoch 11905] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:53,530 - INFO - [diffusion][Epoch 11906] Epoch 11907/12000
2024-11-05 07:05:57,658 - INFO - [diffusion][Epoch 11906] diffusion training Loss: 0.04573757387697697
2024-11-05 07:05:57,661 - INFO - [diffusion][Epoch 11906] diffusion learning rate: 0.001
2024-11-05 07:05:57,662 - INFO - [diffusion][Epoch 11906] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:05:57,664 - INFO - [diffusion][Epoch 11907] Epoch 11908/12000
2024-11-05 07:06:01,774 - INFO - [diffusion][Epoch 11907] diffusion training Loss: 0.04692348279058933
2024-11-05 07:06:01,776 - INFO - [diffusion][Epoch 11907] diffusion learning rate: 0.001
2024-11-05 07:06:01,778 - INFO - [diffusion][Epoch 11907] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:01,779 - INFO - [diffusion][Epoch 11908] Epoch 11909/12000
2024-11-05 07:06:05,903 - INFO - [diffusion][Epoch 11908] diffusion training Loss: 0.0456458805128932
2024-11-05 07:06:05,907 - INFO - [diffusion][Epoch 11908] diffusion learning rate: 0.001
2024-11-05 07:06:05,908 - INFO - [diffusion][Epoch 11908] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:05,910 - INFO - [diffusion][Epoch 11909] Epoch 11910/12000
2024-11-05 07:06:09,964 - INFO - [diffusion][Epoch 11909] diffusion training Loss: 0.04847972560673952
2024-11-05 07:06:09,966 - INFO - [diffusion][Epoch 11909] diffusion learning rate: 0.001
2024-11-05 07:06:09,968 - INFO - [diffusion][Epoch 11909] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:09,969 - INFO - [diffusion][Epoch 11910] Epoch 11911/12000
2024-11-05 07:06:14,038 - INFO - [diffusion][Epoch 11910] diffusion training Loss: 0.04751507006585598
2024-11-05 07:06:14,040 - INFO - [diffusion][Epoch 11910] diffusion learning rate: 0.001
2024-11-05 07:06:14,042 - INFO - [diffusion][Epoch 11910] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:14,043 - INFO - [diffusion][Epoch 11911] Epoch 11912/12000
2024-11-05 07:06:18,152 - INFO - [diffusion][Epoch 11911] diffusion training Loss: 0.04096417874097824
2024-11-05 07:06:18,154 - INFO - [diffusion][Epoch 11911] diffusion learning rate: 0.001
2024-11-05 07:06:18,156 - INFO - [diffusion][Epoch 11911] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:18,159 - INFO - [diffusion][Epoch 11912] Epoch 11913/12000
2024-11-05 07:06:22,960 - INFO - [diffusion][Epoch 11912] diffusion training Loss: 0.04989190772175789
2024-11-05 07:06:22,962 - INFO - [diffusion][Epoch 11912] diffusion learning rate: 0.001
2024-11-05 07:06:22,963 - INFO - [diffusion][Epoch 11912] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:22,965 - INFO - [diffusion][Epoch 11913] Epoch 11914/12000
2024-11-05 07:06:27,031 - INFO - [diffusion][Epoch 11913] diffusion training Loss: 0.04199713282287121
2024-11-05 07:06:27,032 - INFO - [diffusion][Epoch 11913] diffusion learning rate: 0.001
2024-11-05 07:06:27,034 - INFO - [diffusion][Epoch 11913] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:27,036 - INFO - [diffusion][Epoch 11914] Epoch 11915/12000
2024-11-05 07:06:31,100 - INFO - [diffusion][Epoch 11914] diffusion training Loss: 0.04530672263354063
2024-11-05 07:06:31,102 - INFO - [diffusion][Epoch 11914] diffusion learning rate: 0.001
2024-11-05 07:06:31,104 - INFO - [diffusion][Epoch 11914] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:31,105 - INFO - [diffusion][Epoch 11915] Epoch 11916/12000
2024-11-05 07:06:35,192 - INFO - [diffusion][Epoch 11915] diffusion training Loss: 0.04276949632912874
2024-11-05 07:06:35,194 - INFO - [diffusion][Epoch 11915] diffusion learning rate: 0.001
2024-11-05 07:06:35,196 - INFO - [diffusion][Epoch 11915] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:35,197 - INFO - [diffusion][Epoch 11916] Epoch 11917/12000
2024-11-05 07:06:39,298 - INFO - [diffusion][Epoch 11916] diffusion training Loss: 0.04576726630330086
2024-11-05 07:06:39,300 - INFO - [diffusion][Epoch 11916] diffusion learning rate: 0.001
2024-11-05 07:06:39,301 - INFO - [diffusion][Epoch 11916] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:39,303 - INFO - [diffusion][Epoch 11917] Epoch 11918/12000
2024-11-05 07:06:43,351 - INFO - [diffusion][Epoch 11917] diffusion training Loss: 0.04365486837923527
2024-11-05 07:06:43,353 - INFO - [diffusion][Epoch 11917] diffusion learning rate: 0.001
2024-11-05 07:06:43,355 - INFO - [diffusion][Epoch 11917] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:43,356 - INFO - [diffusion][Epoch 11918] Epoch 11919/12000
2024-11-05 07:06:47,450 - INFO - [diffusion][Epoch 11918] diffusion training Loss: 0.05425608344376087
2024-11-05 07:06:47,452 - INFO - [diffusion][Epoch 11918] diffusion learning rate: 0.001
2024-11-05 07:06:47,454 - INFO - [diffusion][Epoch 11918] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:47,455 - INFO - [diffusion][Epoch 11919] Epoch 11920/12000
2024-11-05 07:06:51,514 - INFO - [diffusion][Epoch 11919] diffusion training Loss: 0.045220544561743736
2024-11-05 07:06:51,516 - INFO - [diffusion][Epoch 11919] diffusion learning rate: 0.001
2024-11-05 07:06:51,517 - INFO - [diffusion][Epoch 11919] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:51,518 - INFO - [diffusion][Epoch 11920] Epoch 11921/12000
2024-11-05 07:06:55,599 - INFO - [diffusion][Epoch 11920] diffusion training Loss: 0.04404343385249376
2024-11-05 07:06:55,601 - INFO - [diffusion][Epoch 11920] diffusion learning rate: 0.001
2024-11-05 07:06:55,603 - INFO - [diffusion][Epoch 11920] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:55,604 - INFO - [diffusion][Epoch 11921] Epoch 11922/12000
2024-11-05 07:06:59,703 - INFO - [diffusion][Epoch 11921] diffusion training Loss: 0.048412113450467587
2024-11-05 07:06:59,705 - INFO - [diffusion][Epoch 11921] diffusion learning rate: 0.001
2024-11-05 07:06:59,707 - INFO - [diffusion][Epoch 11921] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:06:59,708 - INFO - [diffusion][Epoch 11922] Epoch 11923/12000
2024-11-05 07:07:03,713 - INFO - [diffusion][Epoch 11922] diffusion training Loss: 0.04786393977701664
2024-11-05 07:07:03,715 - INFO - [diffusion][Epoch 11922] diffusion learning rate: 0.001
2024-11-05 07:07:03,717 - INFO - [diffusion][Epoch 11922] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:03,718 - INFO - [diffusion][Epoch 11923] Epoch 11924/12000
2024-11-05 07:07:07,775 - INFO - [diffusion][Epoch 11923] diffusion training Loss: 0.04557548277080059
2024-11-05 07:07:07,779 - INFO - [diffusion][Epoch 11923] diffusion learning rate: 0.001
2024-11-05 07:07:07,781 - INFO - [diffusion][Epoch 11923] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:07,782 - INFO - [diffusion][Epoch 11924] Epoch 11925/12000
2024-11-05 07:07:11,865 - INFO - [diffusion][Epoch 11924] diffusion training Loss: 0.04885206185281277
2024-11-05 07:07:11,867 - INFO - [diffusion][Epoch 11924] diffusion learning rate: 0.001
2024-11-05 07:07:11,869 - INFO - [diffusion][Epoch 11924] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:11,870 - INFO - [diffusion][Epoch 11925] Epoch 11926/12000
2024-11-05 07:07:15,959 - INFO - [diffusion][Epoch 11925] diffusion training Loss: 0.04846199881285429
2024-11-05 07:07:15,961 - INFO - [diffusion][Epoch 11925] diffusion learning rate: 0.001
2024-11-05 07:07:15,963 - INFO - [diffusion][Epoch 11925] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:15,964 - INFO - [diffusion][Epoch 11926] Epoch 11927/12000
2024-11-05 07:07:20,046 - INFO - [diffusion][Epoch 11926] diffusion training Loss: 0.048493255861103535
2024-11-05 07:07:20,048 - INFO - [diffusion][Epoch 11926] diffusion learning rate: 0.001
2024-11-05 07:07:20,049 - INFO - [diffusion][Epoch 11926] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:20,051 - INFO - [diffusion][Epoch 11927] Epoch 11928/12000
2024-11-05 07:07:24,161 - INFO - [diffusion][Epoch 11927] diffusion training Loss: 0.04815886542201042
2024-11-05 07:07:24,163 - INFO - [diffusion][Epoch 11927] diffusion learning rate: 0.001
2024-11-05 07:07:24,165 - INFO - [diffusion][Epoch 11927] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:24,167 - INFO - [diffusion][Epoch 11928] Epoch 11929/12000
2024-11-05 07:07:28,238 - INFO - [diffusion][Epoch 11928] diffusion training Loss: 0.04586567543447018
2024-11-05 07:07:28,240 - INFO - [diffusion][Epoch 11928] diffusion learning rate: 0.001
2024-11-05 07:07:28,243 - INFO - [diffusion][Epoch 11928] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:28,245 - INFO - [diffusion][Epoch 11929] Epoch 11930/12000
2024-11-05 07:07:32,359 - INFO - [diffusion][Epoch 11929] diffusion training Loss: 0.04223868437111378
2024-11-05 07:07:32,361 - INFO - [diffusion][Epoch 11929] diffusion learning rate: 0.001
2024-11-05 07:07:32,363 - INFO - [diffusion][Epoch 11929] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:32,364 - INFO - [diffusion][Epoch 11930] Epoch 11931/12000
2024-11-05 07:07:36,424 - INFO - [diffusion][Epoch 11930] diffusion training Loss: 0.050263818353414536
2024-11-05 07:07:36,425 - INFO - [diffusion][Epoch 11930] diffusion learning rate: 0.001
2024-11-05 07:07:36,428 - INFO - [diffusion][Epoch 11930] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:36,429 - INFO - [diffusion][Epoch 11931] Epoch 11932/12000
2024-11-05 07:07:40,749 - INFO - [diffusion][Epoch 11931] diffusion training Loss: 0.04519757069647312
2024-11-05 07:07:40,751 - INFO - [diffusion][Epoch 11931] diffusion learning rate: 0.001
2024-11-05 07:07:40,752 - INFO - [diffusion][Epoch 11931] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:40,754 - INFO - [diffusion][Epoch 11932] Epoch 11933/12000
2024-11-05 07:07:44,919 - INFO - [diffusion][Epoch 11932] diffusion training Loss: 0.046856981702148914
2024-11-05 07:07:44,921 - INFO - [diffusion][Epoch 11932] diffusion learning rate: 0.001
2024-11-05 07:07:44,923 - INFO - [diffusion][Epoch 11932] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:44,924 - INFO - [diffusion][Epoch 11933] Epoch 11934/12000
2024-11-05 07:07:49,054 - INFO - [diffusion][Epoch 11933] diffusion training Loss: 0.04921423923224211
2024-11-05 07:07:49,056 - INFO - [diffusion][Epoch 11933] diffusion learning rate: 0.001
2024-11-05 07:07:49,058 - INFO - [diffusion][Epoch 11933] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:49,059 - INFO - [diffusion][Epoch 11934] Epoch 11935/12000
2024-11-05 07:07:53,013 - INFO - [diffusion][Epoch 11934] diffusion training Loss: 0.04791843518614769
2024-11-05 07:07:53,016 - INFO - [diffusion][Epoch 11934] diffusion learning rate: 0.001
2024-11-05 07:07:53,019 - INFO - [diffusion][Epoch 11934] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:53,020 - INFO - [diffusion][Epoch 11935] Epoch 11936/12000
2024-11-05 07:07:57,123 - INFO - [diffusion][Epoch 11935] diffusion training Loss: 0.05212181340903044
2024-11-05 07:07:57,125 - INFO - [diffusion][Epoch 11935] diffusion learning rate: 0.001
2024-11-05 07:07:57,127 - INFO - [diffusion][Epoch 11935] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:07:57,128 - INFO - [diffusion][Epoch 11936] Epoch 11937/12000
2024-11-05 07:08:01,202 - INFO - [diffusion][Epoch 11936] diffusion training Loss: 0.04483627248555422
2024-11-05 07:08:01,204 - INFO - [diffusion][Epoch 11936] diffusion learning rate: 0.001
2024-11-05 07:08:01,205 - INFO - [diffusion][Epoch 11936] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:01,207 - INFO - [diffusion][Epoch 11937] Epoch 11938/12000
2024-11-05 07:08:05,307 - INFO - [diffusion][Epoch 11937] diffusion training Loss: 0.04548416659235954
2024-11-05 07:08:05,309 - INFO - [diffusion][Epoch 11937] diffusion learning rate: 0.001
2024-11-05 07:08:05,311 - INFO - [diffusion][Epoch 11937] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:05,312 - INFO - [diffusion][Epoch 11938] Epoch 11939/12000
2024-11-05 07:08:09,368 - INFO - [diffusion][Epoch 11938] diffusion training Loss: 0.04779566451907158
2024-11-05 07:08:09,372 - INFO - [diffusion][Epoch 11938] diffusion learning rate: 0.001
2024-11-05 07:08:09,373 - INFO - [diffusion][Epoch 11938] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:09,375 - INFO - [diffusion][Epoch 11939] Epoch 11940/12000
2024-11-05 07:08:13,448 - INFO - [diffusion][Epoch 11939] diffusion training Loss: 0.04543572198599577
2024-11-05 07:08:13,450 - INFO - [diffusion][Epoch 11939] diffusion learning rate: 0.001
2024-11-05 07:08:13,451 - INFO - [diffusion][Epoch 11939] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:13,453 - INFO - [diffusion][Epoch 11940] Epoch 11941/12000
2024-11-05 07:08:17,594 - INFO - [diffusion][Epoch 11940] diffusion training Loss: 0.04568747337907553
2024-11-05 07:08:17,596 - INFO - [diffusion][Epoch 11940] diffusion learning rate: 0.001
2024-11-05 07:08:17,598 - INFO - [diffusion][Epoch 11940] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:17,599 - INFO - [diffusion][Epoch 11941] Epoch 11942/12000
2024-11-05 07:08:21,677 - INFO - [diffusion][Epoch 11941] diffusion training Loss: 0.04509232845157385
2024-11-05 07:08:21,679 - INFO - [diffusion][Epoch 11941] diffusion learning rate: 0.001
2024-11-05 07:08:21,680 - INFO - [diffusion][Epoch 11941] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:21,682 - INFO - [diffusion][Epoch 11942] Epoch 11943/12000
2024-11-05 07:08:25,767 - INFO - [diffusion][Epoch 11942] diffusion training Loss: 0.04454388562589884
2024-11-05 07:08:25,769 - INFO - [diffusion][Epoch 11942] diffusion learning rate: 0.001
2024-11-05 07:08:25,771 - INFO - [diffusion][Epoch 11942] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:25,772 - INFO - [diffusion][Epoch 11943] Epoch 11944/12000
2024-11-05 07:08:29,885 - INFO - [diffusion][Epoch 11943] diffusion training Loss: 0.042800349183380604
2024-11-05 07:08:29,887 - INFO - [diffusion][Epoch 11943] diffusion learning rate: 0.001
2024-11-05 07:08:29,889 - INFO - [diffusion][Epoch 11943] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:29,890 - INFO - [diffusion][Epoch 11944] Epoch 11945/12000
2024-11-05 07:08:33,994 - INFO - [diffusion][Epoch 11944] diffusion training Loss: 0.051746588200330734
2024-11-05 07:08:33,996 - INFO - [diffusion][Epoch 11944] diffusion learning rate: 0.001
2024-11-05 07:08:33,998 - INFO - [diffusion][Epoch 11944] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:33,999 - INFO - [diffusion][Epoch 11945] Epoch 11946/12000
2024-11-05 07:08:38,065 - INFO - [diffusion][Epoch 11945] diffusion training Loss: 0.048349302262067795
2024-11-05 07:08:38,067 - INFO - [diffusion][Epoch 11945] diffusion learning rate: 0.001
2024-11-05 07:08:38,069 - INFO - [diffusion][Epoch 11945] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:38,070 - INFO - [diffusion][Epoch 11946] Epoch 11947/12000
2024-11-05 07:08:42,146 - INFO - [diffusion][Epoch 11946] diffusion training Loss: 0.04670282546430826
2024-11-05 07:08:42,148 - INFO - [diffusion][Epoch 11946] diffusion learning rate: 0.001
2024-11-05 07:08:42,150 - INFO - [diffusion][Epoch 11946] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:42,151 - INFO - [diffusion][Epoch 11947] Epoch 11948/12000
2024-11-05 07:08:46,252 - INFO - [diffusion][Epoch 11947] diffusion training Loss: 0.045806096866726875
2024-11-05 07:08:46,254 - INFO - [diffusion][Epoch 11947] diffusion learning rate: 0.001
2024-11-05 07:08:46,256 - INFO - [diffusion][Epoch 11947] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:46,257 - INFO - [diffusion][Epoch 11948] Epoch 11949/12000
2024-11-05 07:08:50,313 - INFO - [diffusion][Epoch 11948] diffusion training Loss: 0.041090684942901134
2024-11-05 07:08:50,315 - INFO - [diffusion][Epoch 11948] diffusion learning rate: 0.001
2024-11-05 07:08:50,317 - INFO - [diffusion][Epoch 11948] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:50,318 - INFO - [diffusion][Epoch 11949] Epoch 11950/12000
2024-11-05 07:08:54,418 - INFO - [diffusion][Epoch 11949] diffusion training Loss: 0.04750187508761883
2024-11-05 07:08:54,420 - INFO - [diffusion][Epoch 11949] diffusion learning rate: 0.001
2024-11-05 07:08:54,421 - INFO - [diffusion][Epoch 11949] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:54,423 - INFO - [diffusion][Epoch 11950] Epoch 11951/12000
2024-11-05 07:08:58,487 - INFO - [diffusion][Epoch 11950] diffusion training Loss: 0.046198888681828976
2024-11-05 07:08:58,489 - INFO - [diffusion][Epoch 11950] diffusion learning rate: 0.001
2024-11-05 07:08:58,491 - INFO - [diffusion][Epoch 11950] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:08:58,492 - INFO - [diffusion][Epoch 11951] Epoch 11952/12000
2024-11-05 07:09:02,546 - INFO - [diffusion][Epoch 11951] diffusion training Loss: 0.042439923621714115
2024-11-05 07:09:02,548 - INFO - [diffusion][Epoch 11951] diffusion learning rate: 0.001
2024-11-05 07:09:02,550 - INFO - [diffusion][Epoch 11951] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:02,551 - INFO - [diffusion][Epoch 11952] Epoch 11953/12000
2024-11-05 07:09:07,161 - INFO - [diffusion][Epoch 11952] diffusion training Loss: 0.04325955081731081
2024-11-05 07:09:07,163 - INFO - [diffusion][Epoch 11952] diffusion learning rate: 0.001
2024-11-05 07:09:07,165 - INFO - [diffusion][Epoch 11952] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:07,166 - INFO - [diffusion][Epoch 11953] Epoch 11954/12000
2024-11-05 07:09:11,237 - INFO - [diffusion][Epoch 11953] diffusion training Loss: 0.04735658597201109
2024-11-05 07:09:11,241 - INFO - [diffusion][Epoch 11953] diffusion learning rate: 0.001
2024-11-05 07:09:11,242 - INFO - [diffusion][Epoch 11953] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:11,244 - INFO - [diffusion][Epoch 11954] Epoch 11955/12000
2024-11-05 07:09:15,305 - INFO - [diffusion][Epoch 11954] diffusion training Loss: 0.048387191258370876
2024-11-05 07:09:15,307 - INFO - [diffusion][Epoch 11954] diffusion learning rate: 0.001
2024-11-05 07:09:15,309 - INFO - [diffusion][Epoch 11954] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:15,310 - INFO - [diffusion][Epoch 11955] Epoch 11956/12000
2024-11-05 07:09:19,417 - INFO - [diffusion][Epoch 11955] diffusion training Loss: 0.04364059120416641
2024-11-05 07:09:19,419 - INFO - [diffusion][Epoch 11955] diffusion learning rate: 0.001
2024-11-05 07:09:19,421 - INFO - [diffusion][Epoch 11955] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:19,422 - INFO - [diffusion][Epoch 11956] Epoch 11957/12000
2024-11-05 07:09:23,497 - INFO - [diffusion][Epoch 11956] diffusion training Loss: 0.0414751498028636
2024-11-05 07:09:23,498 - INFO - [diffusion][Epoch 11956] diffusion learning rate: 0.001
2024-11-05 07:09:23,500 - INFO - [diffusion][Epoch 11956] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:23,502 - INFO - [diffusion][Epoch 11957] Epoch 11958/12000
2024-11-05 07:09:27,634 - INFO - [diffusion][Epoch 11957] diffusion training Loss: 0.039967507123947144
2024-11-05 07:09:27,636 - INFO - [diffusion][Epoch 11957] diffusion learning rate: 0.001
2024-11-05 07:09:27,638 - INFO - [diffusion][Epoch 11957] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:27,639 - INFO - [diffusion][Epoch 11958] Epoch 11959/12000
2024-11-05 07:09:31,739 - INFO - [diffusion][Epoch 11958] diffusion training Loss: 0.045066241174936295
2024-11-05 07:09:31,741 - INFO - [diffusion][Epoch 11958] diffusion learning rate: 0.001
2024-11-05 07:09:31,743 - INFO - [diffusion][Epoch 11958] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:31,744 - INFO - [diffusion][Epoch 11959] Epoch 11960/12000
2024-11-05 07:09:35,821 - INFO - [diffusion][Epoch 11959] diffusion training Loss: 0.05160502623766661
2024-11-05 07:09:35,823 - INFO - [diffusion][Epoch 11959] diffusion learning rate: 0.001
2024-11-05 07:09:35,825 - INFO - [diffusion][Epoch 11959] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:35,826 - INFO - [diffusion][Epoch 11960] Epoch 11961/12000
2024-11-05 07:09:39,950 - INFO - [diffusion][Epoch 11960] diffusion training Loss: 0.047254965640604496
2024-11-05 07:09:39,952 - INFO - [diffusion][Epoch 11960] diffusion learning rate: 0.001
2024-11-05 07:09:39,954 - INFO - [diffusion][Epoch 11960] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:39,955 - INFO - [diffusion][Epoch 11961] Epoch 11962/12000
2024-11-05 07:09:43,823 - INFO - [diffusion][Epoch 11961] diffusion training Loss: 0.04486420936882496
2024-11-05 07:09:43,825 - INFO - [diffusion][Epoch 11961] diffusion learning rate: 0.001
2024-11-05 07:09:43,827 - INFO - [diffusion][Epoch 11961] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:43,828 - INFO - [diffusion][Epoch 11962] Epoch 11963/12000
2024-11-05 07:09:47,875 - INFO - [diffusion][Epoch 11962] diffusion training Loss: 0.04517014231532812
2024-11-05 07:09:47,876 - INFO - [diffusion][Epoch 11962] diffusion learning rate: 0.001
2024-11-05 07:09:47,878 - INFO - [diffusion][Epoch 11962] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:47,880 - INFO - [diffusion][Epoch 11963] Epoch 11964/12000
2024-11-05 07:09:51,990 - INFO - [diffusion][Epoch 11963] diffusion training Loss: 0.04463600181043148
2024-11-05 07:09:51,992 - INFO - [diffusion][Epoch 11963] diffusion learning rate: 0.001
2024-11-05 07:09:51,993 - INFO - [diffusion][Epoch 11963] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:51,995 - INFO - [diffusion][Epoch 11964] Epoch 11965/12000
2024-11-05 07:09:56,090 - INFO - [diffusion][Epoch 11964] diffusion training Loss: 0.04823356959968805
2024-11-05 07:09:56,092 - INFO - [diffusion][Epoch 11964] diffusion learning rate: 0.001
2024-11-05 07:09:56,094 - INFO - [diffusion][Epoch 11964] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:09:56,095 - INFO - [diffusion][Epoch 11965] Epoch 11966/12000
2024-11-05 07:10:00,241 - INFO - [diffusion][Epoch 11965] diffusion training Loss: 0.04978412762284279
2024-11-05 07:10:00,243 - INFO - [diffusion][Epoch 11965] diffusion learning rate: 0.001
2024-11-05 07:10:00,245 - INFO - [diffusion][Epoch 11965] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:00,247 - INFO - [diffusion][Epoch 11966] Epoch 11967/12000
2024-11-05 07:10:04,235 - INFO - [diffusion][Epoch 11966] diffusion training Loss: 0.04899549577385187
2024-11-05 07:10:04,237 - INFO - [diffusion][Epoch 11966] diffusion learning rate: 0.001
2024-11-05 07:10:04,239 - INFO - [diffusion][Epoch 11966] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:04,240 - INFO - [diffusion][Epoch 11967] Epoch 11968/12000
2024-11-05 07:10:08,305 - INFO - [diffusion][Epoch 11967] diffusion training Loss: 0.04228078946471214
2024-11-05 07:10:08,307 - INFO - [diffusion][Epoch 11967] diffusion learning rate: 0.001
2024-11-05 07:10:08,308 - INFO - [diffusion][Epoch 11967] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:08,310 - INFO - [diffusion][Epoch 11968] Epoch 11969/12000
2024-11-05 07:10:12,412 - INFO - [diffusion][Epoch 11968] diffusion training Loss: 0.04409819841384888
2024-11-05 07:10:12,415 - INFO - [diffusion][Epoch 11968] diffusion learning rate: 0.001
2024-11-05 07:10:12,417 - INFO - [diffusion][Epoch 11968] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:12,418 - INFO - [diffusion][Epoch 11969] Epoch 11970/12000
2024-11-05 07:10:16,510 - INFO - [diffusion][Epoch 11969] diffusion training Loss: 0.04453540313988924
2024-11-05 07:10:16,512 - INFO - [diffusion][Epoch 11969] diffusion learning rate: 0.001
2024-11-05 07:10:16,514 - INFO - [diffusion][Epoch 11969] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:16,515 - INFO - [diffusion][Epoch 11970] Epoch 11971/12000
2024-11-05 07:10:20,603 - INFO - [diffusion][Epoch 11970] diffusion training Loss: 0.04599849320948124
2024-11-05 07:10:20,605 - INFO - [diffusion][Epoch 11970] diffusion learning rate: 0.001
2024-11-05 07:10:20,607 - INFO - [diffusion][Epoch 11970] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:20,608 - INFO - [diffusion][Epoch 11971] Epoch 11972/12000
2024-11-05 07:10:24,729 - INFO - [diffusion][Epoch 11971] diffusion training Loss: 0.04867713060230017
2024-11-05 07:10:24,731 - INFO - [diffusion][Epoch 11971] diffusion learning rate: 0.001
2024-11-05 07:10:24,733 - INFO - [diffusion][Epoch 11971] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:24,734 - INFO - [diffusion][Epoch 11972] Epoch 11973/12000
2024-11-05 07:10:28,813 - INFO - [diffusion][Epoch 11972] diffusion training Loss: 0.04737736750394106
2024-11-05 07:10:28,815 - INFO - [diffusion][Epoch 11972] diffusion learning rate: 0.001
2024-11-05 07:10:28,817 - INFO - [diffusion][Epoch 11972] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:28,818 - INFO - [diffusion][Epoch 11973] Epoch 11974/12000
2024-11-05 07:10:32,973 - INFO - [diffusion][Epoch 11973] diffusion training Loss: 0.04621375910937786
2024-11-05 07:10:32,975 - INFO - [diffusion][Epoch 11973] diffusion learning rate: 0.001
2024-11-05 07:10:32,977 - INFO - [diffusion][Epoch 11973] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:32,978 - INFO - [diffusion][Epoch 11974] Epoch 11975/12000
2024-11-05 07:10:37,064 - INFO - [diffusion][Epoch 11974] diffusion training Loss: 0.04415156040340662
2024-11-05 07:10:37,066 - INFO - [diffusion][Epoch 11974] diffusion learning rate: 0.001
2024-11-05 07:10:37,100 - INFO - [diffusion][Epoch 11974] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:37,101 - INFO - [diffusion][Epoch 11975] Epoch 11976/12000
2024-11-05 07:10:41,183 - INFO - [diffusion][Epoch 11975] diffusion training Loss: 0.0456506060436368
2024-11-05 07:10:41,185 - INFO - [diffusion][Epoch 11975] diffusion learning rate: 0.001
2024-11-05 07:10:41,187 - INFO - [diffusion][Epoch 11975] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:41,188 - INFO - [diffusion][Epoch 11976] Epoch 11977/12000
2024-11-05 07:10:45,293 - INFO - [diffusion][Epoch 11976] diffusion training Loss: 0.041004281491041183
2024-11-05 07:10:45,295 - INFO - [diffusion][Epoch 11976] diffusion learning rate: 0.001
2024-11-05 07:10:45,297 - INFO - [diffusion][Epoch 11976] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:45,298 - INFO - [diffusion][Epoch 11977] Epoch 11978/12000
2024-11-05 07:10:49,423 - INFO - [diffusion][Epoch 11977] diffusion training Loss: 0.047579484060406685
2024-11-05 07:10:49,425 - INFO - [diffusion][Epoch 11977] diffusion learning rate: 0.001
2024-11-05 07:10:49,427 - INFO - [diffusion][Epoch 11977] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:49,428 - INFO - [diffusion][Epoch 11978] Epoch 11979/12000
2024-11-05 07:10:53,552 - INFO - [diffusion][Epoch 11978] diffusion training Loss: 0.041070131585001945
2024-11-05 07:10:53,554 - INFO - [diffusion][Epoch 11978] diffusion learning rate: 0.001
2024-11-05 07:10:53,556 - INFO - [diffusion][Epoch 11978] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:53,557 - INFO - [diffusion][Epoch 11979] Epoch 11980/12000
2024-11-05 07:10:57,646 - INFO - [diffusion][Epoch 11979] diffusion training Loss: 0.0438900263980031
2024-11-05 07:10:57,648 - INFO - [diffusion][Epoch 11979] diffusion learning rate: 0.001
2024-11-05 07:10:57,650 - INFO - [diffusion][Epoch 11979] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:10:57,651 - INFO - [diffusion][Epoch 11980] Epoch 11981/12000
2024-11-05 07:11:01,773 - INFO - [diffusion][Epoch 11980] diffusion training Loss: 0.0469847097992897
2024-11-05 07:11:01,775 - INFO - [diffusion][Epoch 11980] diffusion learning rate: 0.001
2024-11-05 07:11:01,776 - INFO - [diffusion][Epoch 11980] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:01,778 - INFO - [diffusion][Epoch 11981] Epoch 11982/12000
2024-11-05 07:11:05,867 - INFO - [diffusion][Epoch 11981] diffusion training Loss: 0.04406648222357035
2024-11-05 07:11:05,869 - INFO - [diffusion][Epoch 11981] diffusion learning rate: 0.001
2024-11-05 07:11:05,870 - INFO - [diffusion][Epoch 11981] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:05,871 - INFO - [diffusion][Epoch 11982] Epoch 11983/12000
2024-11-05 07:11:10,006 - INFO - [diffusion][Epoch 11982] diffusion training Loss: 0.05082145147025585
2024-11-05 07:11:10,008 - INFO - [diffusion][Epoch 11982] diffusion learning rate: 0.001
2024-11-05 07:11:10,010 - INFO - [diffusion][Epoch 11982] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:10,011 - INFO - [diffusion][Epoch 11983] Epoch 11984/12000
2024-11-05 07:11:14,108 - INFO - [diffusion][Epoch 11983] diffusion training Loss: 0.04762678686529398
2024-11-05 07:11:14,111 - INFO - [diffusion][Epoch 11983] diffusion learning rate: 0.001
2024-11-05 07:11:14,113 - INFO - [diffusion][Epoch 11983] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:14,115 - INFO - [diffusion][Epoch 11984] Epoch 11985/12000
2024-11-05 07:11:18,171 - INFO - [diffusion][Epoch 11984] diffusion training Loss: 0.046524171717464924
2024-11-05 07:11:18,173 - INFO - [diffusion][Epoch 11984] diffusion learning rate: 0.001
2024-11-05 07:11:18,174 - INFO - [diffusion][Epoch 11984] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:18,176 - INFO - [diffusion][Epoch 11985] Epoch 11986/12000
2024-11-05 07:11:22,234 - INFO - [diffusion][Epoch 11985] diffusion training Loss: 0.046076360158622265
2024-11-05 07:11:22,236 - INFO - [diffusion][Epoch 11985] diffusion learning rate: 0.001
2024-11-05 07:11:22,238 - INFO - [diffusion][Epoch 11985] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:22,239 - INFO - [diffusion][Epoch 11986] Epoch 11987/12000
2024-11-05 07:11:26,302 - INFO - [diffusion][Epoch 11986] diffusion training Loss: 0.04398663341999054
2024-11-05 07:11:26,304 - INFO - [diffusion][Epoch 11986] diffusion learning rate: 0.001
2024-11-05 07:11:26,306 - INFO - [diffusion][Epoch 11986] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:26,307 - INFO - [diffusion][Epoch 11987] Epoch 11988/12000
2024-11-05 07:11:30,391 - INFO - [diffusion][Epoch 11987] diffusion training Loss: 0.048236310482025146
2024-11-05 07:11:30,393 - INFO - [diffusion][Epoch 11987] diffusion learning rate: 0.001
2024-11-05 07:11:30,395 - INFO - [diffusion][Epoch 11987] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:30,396 - INFO - [diffusion][Epoch 11988] Epoch 11989/12000
2024-11-05 07:11:34,504 - INFO - [diffusion][Epoch 11988] diffusion training Loss: 0.04357329197227955
2024-11-05 07:11:34,506 - INFO - [diffusion][Epoch 11988] diffusion learning rate: 0.001
2024-11-05 07:11:34,508 - INFO - [diffusion][Epoch 11988] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:34,509 - INFO - [diffusion][Epoch 11989] Epoch 11990/12000
2024-11-05 07:11:38,596 - INFO - [diffusion][Epoch 11989] diffusion training Loss: 0.04440537095069885
2024-11-05 07:11:38,598 - INFO - [diffusion][Epoch 11989] diffusion learning rate: 0.001
2024-11-05 07:11:38,600 - INFO - [diffusion][Epoch 11989] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:11:38,601 - INFO - [diffusion][Epoch 11990] Epoch 11991/12000
2024-11-05 07:11:42,593 - INFO - [diffusion][Epoch 11990] diffusion training Loss: 0.04496757034212351
2024-11-05 07:11:42,595 - INFO - [diffusion][Epoch 11990] diffusion learning rate: 0.001
[INFO|configuration_utils.py:728] 2024-11-05 07:11:57,012 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:11:57,014 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:11:57,062 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:11:57,063 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:11:57,068 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:11:57,068 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:11:57,068 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:11:57,068 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:11:57,068 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:11:57,068 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:11:57,069 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:11:57,070 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:11:57,238 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:11:58,320 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:11:58,321 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:11:59,053 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:11:59,055 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:11:59,057 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:11:59,058 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:11:59,058 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
[INFO|training_args.py:1902] 2024-11-05 07:12:03,575 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:03,575 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:03,576 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:06,447 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:06,448 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:06,495 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:06,496 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:06,498 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:06,498 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:06,498 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:06,498 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:06,498 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:06,498 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:06,498 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:06,499 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:06,633 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:07,496 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:07,496 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:08,160 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:08,161 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:08,163 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:08,163 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:12:08,163 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]
{'eval_loss': 0.20987382531166077, 'eval_accuracy': 0.9346330275229358, 'eval_runtime': 4.4571, 'eval_samples_per_second': 195.645, 'eval_steps_per_second': 0.224}
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:04.45
  eval_samples            =        872
  eval_samples_per_second =    195.645
  eval_steps_per_second   =      0.224
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.84it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:09,180 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:09,180 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:09,181 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:12,230 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:12,231 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:12,503 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:12,504 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:12,506 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:12,506 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:12,506 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:12,506 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:12,506 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:12,506 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:12,507 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:12,507 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:12,646 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:13,511 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:13,511 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:14,213 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:14,214 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:14,216 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:14,216 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:12:14,216 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:00.95
  eval_samples            =        872
  eval_samples_per_second =    910.724
  eval_steps_per_second   =      1.044
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.76it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:14,618 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:14,618 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:14,619 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:17,673 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:17,674 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:17,724 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:17,725 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:17,730 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:17,731 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:17,731 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:17,731 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:17,731 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:17,731 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:17,732 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:17,733 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:17,873 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:18,763 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:18,763 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:19,684 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:19,687 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:19,690 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:19,690 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:12:19,690 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.36
  eval_samples            =        277
  eval_samples_per_second =    768.835
  eval_steps_per_second   =      2.776
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.84it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:20,135 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:20,135 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:20,136 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:23,014 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:23,015 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:23,198 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:23,199 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:23,201 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:23,201 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:23,201 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:23,201 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:23,202 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:23,202 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:23,202 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:23,203 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:23,334 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:24,246 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:24,246 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:24,944 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:24,946 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:24,948 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:24,948 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:12:24,948 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.41
  eval_samples            =        277
  eval_samples_per_second =    669.651
  eval_steps_per_second   =      2.418
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.72it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:25,475 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:25,475 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:25,476 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:28,498 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:28,499 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:28,557 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:28,558 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:28,560 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:28,560 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:28,560 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:28,560 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:28,560 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:28,560 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:28,561 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:28,562 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:28,722 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:29,624 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:29,624 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:30,513 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:30,515 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:30,518 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:30,518 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:12:30,518 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.49
  eval_samples            =        408
  eval_samples_per_second =    823.048
  eval_steps_per_second   =      2.017
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.94it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:31,020 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:31,021 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:31,021 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:34,330 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:34,331 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:34,463 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:34,464 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:34,466 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:34,467 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:34,467 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:34,467 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:34,467 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:34,467 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:34,467 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:34,468 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:34,593 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:35,517 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:35,518 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:36,180 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:36,182 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:36,185 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:36,185 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:12:36,185 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.46
  eval_samples            =        408
  eval_samples_per_second =    882.696
  eval_steps_per_second   =      2.163
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],

  0% 0/1 [00:00<?, ?it/s]    num_rows: 1043
})]

100% 1/1 [00:00<00:00, 36.12it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:37,648 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:37,649 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:37,649 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:40,418 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:40,419 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:40,534 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:40,535 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:40,537 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:40,537 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:40,537 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:40,537 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:40,537 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:40,537 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:40,538 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:40,539 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:40,662 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:41,532 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:41,532 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:42,426 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:42,428 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:42,430 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:42,430 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:12:42,430 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.43
  eval_samples              =       1043
  eval_samples_per_second   =    729.153
  eval_steps_per_second     =      0.699
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.64it/s]
[INFO|training_args.py:1902] 2024-11-05 07:12:43,538 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:43,539 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:43,540 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:46,409 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:46,411 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:46,473 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:46,474 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:46,476 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:46,476 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:46,476 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:46,476 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:46,476 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:46,476 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:46,476 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:46,477 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:46,591 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:47,437 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:47,438 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:48,146 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:48,149 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:48,153 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:48,153 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:12:48,153 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =      0.472
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.07
  eval_samples              =       1043
  eval_samples_per_second   =    973.067
  eval_steps_per_second     =      0.933
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.01s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
[INFO|training_args.py:1902] 2024-11-05 07:12:54,145 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:12:54,146 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:12:54,147 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:12:57,403 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:57,404 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:12:57,459 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:57,460 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:57,464 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:57,464 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:57,464 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:57,464 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:57,464 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:12:57,464 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:12:57,465 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:12:57,466 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:12:57,605 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:12:58,825 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:12:58,825 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:12:59,589 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:12:59,592 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:12:59,595 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:12:59,595 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:12:59,595 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.92
  eval_samples            =       5463
  eval_samples_per_second =    921.634
  eval_steps_per_second   =      0.506
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.03s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
[INFO|training_args.py:1902] 2024-11-05 07:13:05,615 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:05,616 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:05,616 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:13:08,576 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:08,578 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:13:08,626 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:08,627 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:08,629 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:08,629 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:08,629 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:08,629 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:08,629 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:08,629 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:13:08,630 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:08,631 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:13:08,763 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:13:09,695 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:13:09,695 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:13:10,371 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:13:10,373 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:13:10,375 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:13:10,375 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:13:10,375 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.95
  eval_samples            =       5463
  eval_samples_per_second =    917.121
  eval_steps_per_second   =      0.504
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 26.92it/s]
[INFO|training_args.py:1902] 2024-11-05 07:13:11,981 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:11,981 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:11,982 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:13:15,900 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:15,902 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:13:15,957 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:15,959 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:15,962 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:15,962 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:15,962 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:15,963 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:15,963 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:15,963 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:13:15,964 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:15,966 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:13:16,081 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:13:16,946 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:13:16,946 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:13:17,691 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:13:17,693 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:13:17,696 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:13:17,697 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:13:17,697 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6028
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.57
  eval_samples            =       1500
  eval_samples_per_second =    954.461
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.636
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 14.15it/s]
2024-11-05 07:13:19,742 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:13:19,744 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models rte: 0.6895306859205776
2024-11-05 07:13:19,745 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:13:19,746 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:13:19,747 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:13:19,748 - INFO - [diffusion][Epoch 11990] DIFF reconstruction auto_encoder_models stsb: 0.8798513204785041
2024-11-05 07:13:19,749 - INFO - [diffusion][Epoch 11990] average DIFF reconstruction auto_encoder_models accuracy: 0.8104353732190258
2024-11-05 07:13:19,750 - INFO - [diffusion][Epoch 11990] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6895306859205776), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798513204785041), ('stsb', 0.8798512870693244)]
2024-11-05 07:13:19,752 - INFO - [diffusion][Epoch 11990] ---------------------------------
2024-11-05 07:13:19,941 - INFO - [diffusion][Epoch 11990] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:13:19,942 - INFO - [diffusion][Epoch 11991] Epoch 11992/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:02.02
  eval_samples            =       1500
  eval_samples_per_second =    742.283
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.495
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:13:24,113 - INFO - [diffusion][Epoch 11991] diffusion training Loss: 0.04956433363258839
2024-11-05 07:13:24,114 - INFO - [diffusion][Epoch 11991] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:13:35,918 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:35,920 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:35,921 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:13:38,675 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:38,677 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:13:38,729 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:38,730 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:38,732 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:38,732 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:38,732 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:38,732 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:38,732 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:38,732 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:13:38,733 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:38,734 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:13:38,892 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:13:39,822 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:13:39,822 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:13:40,486 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:13:40,489 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:13:40,492 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:13:40,492 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:13:40,492 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.46it/s]
[INFO|training_args.py:1902] 2024-11-05 07:13:41,621 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:41,621 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:41,622 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:13:44,484 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:44,485 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:13:44,535 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:44,536 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:44,538 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:44,538 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:44,538 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:44,538 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:44,538 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:44,538 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:13:44,538 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:44,539 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:13:44,661 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:13:45,556 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:13:45,556 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:13:46,196 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:13:46,197 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:13:46,199 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:13:46,199 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:13:46,199 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    814.624
  eval_steps_per_second   =      0.934
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.14it/s]
[INFO|training_args.py:1902] 2024-11-05 07:13:47,170 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:47,170 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:47,171 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:13:49,930 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:49,931 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:13:49,982 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:49,983 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:49,985 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:49,985 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:49,985 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:49,985 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:49,985 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:49,985 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:13:49,986 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:49,987 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:13:50,115 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:13:51,046 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:13:51,046 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:13:51,727 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:13:51,729 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:13:51,731 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:13:51,731 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:13:51,731 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:00.91
  eval_samples            =        872
  eval_samples_per_second =    955.251
  eval_steps_per_second   =      1.095
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 48.68it/s]
[INFO|training_args.py:1902] 2024-11-05 07:13:52,585 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:52,586 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:52,586 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:13:55,431 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:55,432 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:13:55,485 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:55,486 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:55,488 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:55,488 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:55,488 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:55,488 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:55,488 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:13:55,488 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:13:55,489 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:13:55,490 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:13:55,628 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:13:56,545 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:13:56,545 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:13:57,191 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:13:57,193 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:13:57,196 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:13:57,196 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:13:57,196 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.82
  eval_samples            =        277
  eval_samples_per_second =    336.635
  eval_steps_per_second   =      1.215
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.27it/s]
[INFO|training_args.py:1902] 2024-11-05 07:13:57,667 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:13:57,668 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:13:57,669 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:00,397 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:00,398 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:00,446 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:00,447 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:00,451 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:00,451 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:00,451 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:00,451 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:00,451 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:00,451 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:00,452 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:00,453 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:00,576 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:01,498 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:01,498 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:02,532 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:02,535 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:02,537 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:02,538 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:14:02,538 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.42
  eval_samples            =        277
  eval_samples_per_second =    659.449
  eval_steps_per_second   =      2.381
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.38it/s]
[INFO|training_args.py:1902] 2024-11-05 07:14:03,141 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:03,141 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:03,142 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:06,671 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:06,672 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:06,761 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:06,762 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:06,764 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:06,764 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:06,764 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:06,764 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:06,764 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:06,764 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:06,764 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:06,765 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:06,921 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:07,827 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:07,832 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:08,456 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:08,458 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:08,461 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:08,461 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:14:08,461 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.56
  eval_samples            =        408
  eval_samples_per_second =    723.402
  eval_steps_per_second   =      1.773
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408

  0% 0/1 [00:00<?, ?it/s]})]

100% 1/1 [00:00<00:00, 32.94it/s]
[INFO|training_args.py:1902] 2024-11-05 07:14:09,031 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:09,032 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:09,033 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:11,949 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:11,951 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:12,002 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:12,003 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:12,005 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:12,005 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:12,005 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:12,005 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:12,005 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:12,005 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:12,005 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:12,006 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:12,127 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:13,394 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:13,395 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:14,016 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:14,018 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:14,020 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:14,020 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:14:14,020 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.53
  eval_samples            =        408
  eval_samples_per_second =    768.511
  eval_steps_per_second   =      1.884
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.17it/s]
[INFO|training_args.py:1902] 2024-11-05 07:14:15,312 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:15,313 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:15,314 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:18,214 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:18,215 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:18,261 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:18,262 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:18,264 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:18,264 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:18,264 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:18,264 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:18,264 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:18,264 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:18,265 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:18,268 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:18,415 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:19,323 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:19,323 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:19,888 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:19,891 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:19,894 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:19,894 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:14:19,894 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.24
  eval_samples              =       1043
  eval_samples_per_second   =    835.322
  eval_steps_per_second     =      0.801
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 22.06it/s]
[INFO|training_args.py:1902] 2024-11-05 07:14:21,147 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:21,147 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:21,148 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:24,627 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:24,628 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:24,674 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:24,675 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:24,676 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:24,676 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:24,677 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:24,677 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:24,677 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:24,677 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:24,677 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:24,678 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:24,799 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:25,728 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:25,728 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:26,344 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:26,346 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:26,348 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:26,348 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:14:26,348 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    864.776
  eval_steps_per_second     =      0.829
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.41it/s]
100% 3/3 [00:02<00:00,  1.45it/s]
100% 3/3 [00:02<00:00,  1.41it/s]
[INFO|training_args.py:1902] 2024-11-05 07:14:31,032 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:31,032 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:31,033 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:33,950 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:33,951 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:34,005 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:34,006 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:34,008 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:34,008 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:34,008 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:34,008 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:34,008 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:34,008 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:34,009 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:34,010 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:34,149 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:35,084 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:35,085 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:35,928 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:35,930 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:35,932 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:35,932 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:14:35,932 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:04.61
  eval_samples            =       5463
  eval_samples_per_second =     1182.7
  eval_steps_per_second   =      0.649
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.06s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
[INFO|training_args.py:1902] 2024-11-05 07:14:42,011 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:42,012 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:42,012 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:44,859 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:44,860 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:44,925 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:44,927 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:44,928 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:44,928 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:44,929 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:44,929 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:44,929 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:44,929 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:44,929 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:44,930 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:45,049 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:46,009 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:46,009 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:46,629 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:46,631 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:46,633 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:46,633 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:14:46,633 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.02
  eval_samples            =       5463
  eval_samples_per_second =     907.19
  eval_steps_per_second   =      0.498
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.37it/s]
[INFO|training_args.py:1902] 2024-11-05 07:14:48,363 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:14:48,363 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:14:48,364 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:14:51,110 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:51,111 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:14:51,172 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:51,172 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:51,174 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:51,174 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:51,174 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:51,175 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:51,175 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:14:51,175 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:14:51,175 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:14:51,176 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:14:51,298 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:14:52,205 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:14:52,205 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:14:53,057 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:14:53,059 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:14:53,062 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:14:53,062 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:14:53,062 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.69
  eval_samples            =       1500
  eval_samples_per_second =    886.468
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.591
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.54it/s]
2024-11-05 07:14:54,717 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:14:54,719 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:14:54,720 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:14:54,721 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:14:54,722 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:14:54,723 - INFO - [diffusion][Epoch 11991] DIFF reconstruction auto_encoder_models stsb: 0.879852891010193
2024-11-05 07:14:54,724 - INFO - [diffusion][Epoch 11991] average DIFF reconstruction auto_encoder_models accuracy: 0.8107361175569247
2024-11-05 07:14:54,725 - INFO - [diffusion][Epoch 11991] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.879852891010193), ('stsb', 0.8798485402891746)]
2024-11-05 07:14:54,726 - INFO - [diffusion][Epoch 11991] ---------------------------------
2024-11-05 07:14:54,937 - INFO - [diffusion][Epoch 11991] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:14:54,938 - INFO - [diffusion][Epoch 11992] Epoch 11993/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.62
  eval_samples            =       1500
  eval_samples_per_second =    920.817
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.614
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:14:59,151 - INFO - [diffusion][Epoch 11992] diffusion training Loss: 0.04096189700067043
2024-11-05 07:14:59,153 - INFO - [diffusion][Epoch 11992] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:15:10,946 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:10,947 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:10,949 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:14,422 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:14,423 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:14,474 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:14,475 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:14,479 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:14,479 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:14,479 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:14,479 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:14,479 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:14,479 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:14,479 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:14,480 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:14,616 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:15,566 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:15,566 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:16,129 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:16,132 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:16,135 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:16,135 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:15:16,135 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.21it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:17,254 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:17,254 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:17,255 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:20,917 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:20,918 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:20,968 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:20,969 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:20,971 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:20,971 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:20,971 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:20,971 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:20,971 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:20,971 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:20,972 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:20,973 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:21,110 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:22,035 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:22,035 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:22,643 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:22,645 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:22,648 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:22,648 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:15:22,648 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.05
  eval_samples            =        872
  eval_samples_per_second =    825.472
  eval_steps_per_second   =      0.947
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.38it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:23,749 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:23,750 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:23,750 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:26,716 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:26,717 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:26,764 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:26,765 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:26,767 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:26,767 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:26,767 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:26,767 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:26,767 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:26,767 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:26,767 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:26,768 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:26,899 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:27,834 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:27,835 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:28,553 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:28,555 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:28,557 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:28,557 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:15:28,557 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.04
  eval_samples            =        872
  eval_samples_per_second =    836.359
  eval_steps_per_second   =      0.959
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.30it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:29,337 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:29,338 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:29,339 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:32,174 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:32,175 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:32,223 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:32,224 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:32,226 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:32,226 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:32,226 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:32,226 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:32,226 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:32,226 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:32,227 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:32,227 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:32,350 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:33,279 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:33,280 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:33,897 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:33,899 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:33,901 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:33,901 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:15:33,901 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.74
  eval_samples            =        277
  eval_samples_per_second =    372.236
  eval_steps_per_second   =      1.344
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.70it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:34,296 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:34,297 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:34,297 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:37,447 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:37,448 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:37,496 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:37,497 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:37,502 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:37,502 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:37,502 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:37,502 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:37,502 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:37,502 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:37,503 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:37,504 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:37,624 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:38,558 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:38,559 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:39,234 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:39,236 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:39,239 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:39,239 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:15:39,239 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.36
  eval_samples            =        277
  eval_samples_per_second =    761.709
  eval_steps_per_second   =       2.75
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.44it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:39,815 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:39,815 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:39,816 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:42,629 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:42,630 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:42,707 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:42,709 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:42,711 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:42,711 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:42,711 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:42,711 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:42,711 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:42,711 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:42,711 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:42,712 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:42,834 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:43,750 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:43,750 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:44,418 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:44,420 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:44,423 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:44,423 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:15:44,423 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.54
  eval_samples            =        408
  eval_samples_per_second =     753.88
  eval_steps_per_second   =      1.848
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.65it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:45,360 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:45,360 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:45,361 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:48,596 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:48,597 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:48,656 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:48,657 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:48,659 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:48,659 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:48,659 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:48,659 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:48,659 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:48,659 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:48,659 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:48,660 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:48,777 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:49,750 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:49,751 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:50,415 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:50,417 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:50,419 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:50,420 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:15:50,420 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3451
  eval_runtime            = 0:00:00.90
  eval_samples            =        408
  eval_samples_per_second =    451.177
  eval_steps_per_second   =      1.106
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.05it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:51,717 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:51,718 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:51,719 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:15:55,089 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:55,090 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:15:55,157 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:55,158 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:55,160 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:55,160 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:55,160 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:55,160 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:55,160 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:15:55,160 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:15:55,161 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:15:55,162 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:15:55,280 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:15:56,207 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:15:56,207 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:15:56,853 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:15:56,855 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:15:56,858 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:15:56,858 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:15:56,858 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.25
  eval_samples              =       1043
  eval_samples_per_second   =    828.122
  eval_steps_per_second     =      0.794
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.73it/s]
[INFO|training_args.py:1902] 2024-11-05 07:15:58,144 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:15:58,145 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:15:58,146 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:16:01,154 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:01,155 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:16:01,208 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:01,209 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:01,211 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:01,211 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:01,211 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:01,211 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:01,211 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:01,211 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:16:01,211 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:01,212 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:16:01,330 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:16:02,247 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:16:02,247 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:16:02,961 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:16:02,963 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:16:02,965 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:16:02,966 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:16:02,966 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.25
  eval_samples              =       1043
  eval_samples_per_second   =    833.682
  eval_steps_per_second     =      0.799
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

 67% 2/3 [00:02<00:01,  1.04s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.12s/it]
[INFO|training_args.py:1902] 2024-11-05 07:16:09,366 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:16:09,367 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:16:09,369 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:16:12,703 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:12,704 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:16:12,761 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:12,762 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:12,765 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:12,765 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:12,765 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:12,765 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:12,765 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:12,765 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:16:12,765 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:12,766 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:16:13,044 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:16:13,965 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:16:13,966 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:16:14,550 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:16:14,553 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:16:14,555 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:16:14,555 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:16:14,555 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.33
  eval_samples            =       5463
  eval_samples_per_second =    862.684
  eval_steps_per_second   =      0.474
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.06it/s]
100% 3/3 [00:03<00:00,  1.04s/it]
100% 3/3 [00:03<00:00,  1.04s/it]
[INFO|training_args.py:1902] 2024-11-05 07:16:20,434 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:16:20,435 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:16:20,435 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:16:23,199 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:23,200 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:16:23,260 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:23,261 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:23,265 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:23,265 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:23,265 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:23,265 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:23,265 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:23,265 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:16:23,265 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:23,266 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:16:23,387 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:16:24,308 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:16:24,309 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:16:24,898 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:16:24,901 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:16:24,903 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:16:24,903 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:16:24,903 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.82
  eval_samples            =       5463
  eval_samples_per_second =    937.638
  eval_steps_per_second   =      0.515
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.41it/s]
[INFO|training_args.py:1902] 2024-11-05 07:16:26,884 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:16:26,884 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:16:26,885 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:16:29,892 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:29,893 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:16:29,943 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:29,944 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:29,945 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:29,945 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:29,945 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:29,945 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:29,945 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:29,946 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:16:29,946 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:29,947 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:16:30,037 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:16:30,889 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:16:30,889 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:16:31,528 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:16:31,530 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:16:31,531 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:16:31,531 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:16:31,531 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.94
  eval_samples            =       1500
  eval_samples_per_second =    772.073
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.515
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.44it/s]
2024-11-05 07:16:33,065 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:16:33,067 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models rte: 0.6895306859205776
2024-11-05 07:16:33,068 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:16:33,070 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:16:33,071 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:16:33,072 - INFO - [diffusion][Epoch 11992] DIFF reconstruction auto_encoder_models stsb: 0.8798534700413526
2024-11-05 07:16:33,073 - INFO - [diffusion][Epoch 11992] average DIFF reconstruction auto_encoder_models accuracy: 0.8104354051015935
2024-11-05 07:16:33,074 - INFO - [diffusion][Epoch 11992] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6895306859205776), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798534700413526), ('stsb', 0.8798495200972904)]
2024-11-05 07:16:33,076 - INFO - [diffusion][Epoch 11992] ---------------------------------
2024-11-05 07:16:33,281 - INFO - [diffusion][Epoch 11992] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:16:33,287 - INFO - [diffusion][Epoch 11993] Epoch 11994/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.50
  eval_samples            =       1500
  eval_samples_per_second =    993.812
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.663
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:16:37,559 - INFO - [diffusion][Epoch 11993] diffusion training Loss: 0.034429197665303946
2024-11-05 07:16:37,561 - INFO - [diffusion][Epoch 11993] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:16:49,646 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:16:49,649 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:16:49,652 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:16:52,431 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:52,432 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:16:52,490 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:52,491 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:52,494 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:52,494 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:52,494 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:52,494 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:52,494 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:52,494 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:16:52,494 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:52,495 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:16:52,623 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:16:53,562 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:16:53,562 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:16:54,080 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:16:54,082 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:16:54,085 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:16:54,085 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:16:54,085 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.91it/s]
[INFO|training_args.py:1902] 2024-11-05 07:16:55,262 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:16:55,262 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:16:55,263 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:16:58,171 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:58,172 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:16:58,228 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:58,229 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:58,231 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:58,231 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:58,231 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:58,231 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:58,231 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:16:58,231 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:16:58,232 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:16:58,233 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:16:58,347 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:16:59,259 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:16:59,259 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:16:59,895 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:16:59,896 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:16:59,899 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:16:59,899 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:16:59,899 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.11
  eval_samples            =        872
  eval_samples_per_second =    779.045
  eval_steps_per_second   =      0.893
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.60it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:00,969 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:00,969 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:00,970 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:03,728 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:03,730 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:03,794 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:03,795 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:03,797 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:03,797 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:03,797 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:03,797 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:03,797 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:03,797 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:03,798 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:03,799 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:04,253 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:05,168 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:05,169 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:05,783 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:05,785 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:05,787 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:05,788 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:17:05,788 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.01
  eval_samples            =        872
  eval_samples_per_second =    859.234
  eval_steps_per_second   =      0.985
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 51.48it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:06,232 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:06,232 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:06,233 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:10,080 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:10,081 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:10,130 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:10,131 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:10,132 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:10,132 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:10,133 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:10,133 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:10,133 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:10,133 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:10,133 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:10,134 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:10,267 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:11,180 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:11,181 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:12,388 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:12,391 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:12,393 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:12,393 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:17:12,393 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    679.605
  eval_steps_per_second   =      2.453
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.51it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:12,848 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:12,848 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:12,849 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:15,745 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:15,746 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:15,793 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:15,794 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:15,796 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:15,796 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:15,796 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:15,796 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:15,796 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:15,796 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:15,796 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:15,797 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:15,925 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:16,850 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:16,850 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:17,516 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:17,518 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:17,521 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:17,521 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:17:17,521 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.42
  eval_samples            =        277
  eval_samples_per_second =    657.533
  eval_steps_per_second   =      2.374
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.38it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:18,117 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:18,118 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:18,119 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:21,687 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:21,688 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:21,796 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:21,797 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:21,799 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:21,799 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:21,799 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:21,799 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:21,799 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:21,799 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:21,800 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:21,801 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:21,918 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:23,146 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:23,146 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:23,781 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:23,783 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:23,785 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:23,785 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:17:23,785 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.56
  eval_samples            =        408
  eval_samples_per_second =    724.144
  eval_steps_per_second   =      1.775
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.26it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:24,353 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:24,354 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:24,354 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:27,464 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:27,465 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:27,512 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:27,513 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:27,516 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:27,516 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:27,516 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:27,516 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:27,516 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:27,516 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:27,517 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:27,518 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:27,633 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:28,537 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:28,537 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:29,148 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:29,150 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:29,152 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:29,152 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:17:29,152 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.53
  eval_samples            =        408
  eval_samples_per_second =    764.183
  eval_steps_per_second   =      1.873
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.41it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:30,362 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:30,362 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:30,363 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:33,297 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:33,298 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:33,344 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:33,345 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:33,347 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:33,347 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:33,347 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:33,347 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:33,347 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:33,347 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:33,347 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:33,348 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:33,462 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:34,359 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:34,360 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:34,954 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:34,957 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:34,959 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:34,959 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:17:34,959 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.17
  eval_samples              =       1043
  eval_samples_per_second   =     887.28
  eval_steps_per_second     =      0.851
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.36it/s]
[INFO|training_args.py:1902] 2024-11-05 07:17:36,205 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:36,205 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:36,206 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:38,779 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:38,780 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:38,826 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:38,827 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:38,829 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:38,829 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:38,829 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:38,829 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:38,829 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:38,829 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:38,830 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:38,831 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:38,960 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:39,847 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:39,847 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:40,460 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:40,462 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:40,464 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:40,464 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:17:40,464 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.21
  eval_samples              =       1043
  eval_samples_per_second   =    855.807
  eval_steps_per_second     =      0.821
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.03s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
[INFO|training_args.py:1902] 2024-11-05 07:17:46,616 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:46,617 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:46,617 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:17:50,857 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:50,858 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:17:50,940 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:50,942 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:50,944 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:50,944 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:50,944 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:50,944 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:50,944 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:17:50,944 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:17:50,944 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:17:50,945 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:17:51,079 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:17:51,985 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:17:51,985 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:17:52,534 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:17:52,536 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:17:52,538 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:17:52,538 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:17:52,538 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.09
  eval_samples            =       5463
  eval_samples_per_second =    896.263
  eval_steps_per_second   =      0.492
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.08it/s]
100% 3/3 [00:03<00:00,  1.04s/it]
100% 3/3 [00:03<00:00,  1.03s/it]
[INFO|training_args.py:1902] 2024-11-05 07:17:57,866 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:17:57,867 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:17:57,867 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:00,919 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:00,920 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:00,984 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:00,985 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:00,987 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:00,987 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:00,987 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:00,987 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:00,987 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:00,987 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:00,987 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:00,988 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:01,106 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:02,030 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:02,031 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:02,685 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:02,687 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:02,689 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:02,690 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:18:02,690 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.26
  eval_samples            =       5463
  eval_samples_per_second =   1037.232
  eval_steps_per_second   =       0.57
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.93it/s]
[INFO|training_args.py:1902] 2024-11-05 07:18:04,659 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:04,660 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:04,660 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:07,412 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:07,413 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:07,469 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:07,470 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:07,472 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:07,472 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:07,472 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:07,472 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:07,472 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:07,472 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:07,472 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:07,473 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:07,587 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:08,492 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:08,492 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:09,120 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:09,122 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:09,125 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:09,125 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:18:09,125 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.93
  eval_samples            =       1500
  eval_samples_per_second =    773.468
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.516
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.12it/s]
2024-11-05 07:18:10,804 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:18:10,806 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models rte: 0.6895306859205776
2024-11-05 07:18:10,807 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:18:10,808 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:18:10,809 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:18:10,810 - INFO - [diffusion][Epoch 11993] DIFF reconstruction auto_encoder_models stsb: 0.8798509052388105
2024-11-05 07:18:10,811 - INFO - [diffusion][Epoch 11993] average DIFF reconstruction auto_encoder_models accuracy: 0.8104352378055442
2024-11-05 07:18:10,812 - INFO - [diffusion][Epoch 11993] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6895306859205776), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798509052388105), ('stsb', 0.8798500773472385)]
2024-11-05 07:18:10,813 - INFO - [diffusion][Epoch 11993] ---------------------------------
2024-11-05 07:18:11,023 - INFO - [diffusion][Epoch 11993] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:18:11,024 - INFO - [diffusion][Epoch 11994] Epoch 11995/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.65
  eval_samples            =       1500
  eval_samples_per_second =    905.798
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.604
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:18:15,066 - INFO - [diffusion][Epoch 11994] diffusion training Loss: 0.02984006702899933
2024-11-05 07:18:15,068 - INFO - [diffusion][Epoch 11994] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:18:27,557 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:27,558 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:27,559 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:30,770 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:30,772 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:30,831 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:30,832 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:30,836 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:30,836 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:30,836 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:30,837 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:30,837 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:30,837 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:30,838 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:30,840 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:30,946 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:31,859 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:31,859 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:32,482 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:32,485 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:32,487 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:32,487 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:18:32,487 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.84it/s]
[INFO|training_args.py:1902] 2024-11-05 07:18:33,571 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:33,571 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:33,572 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:36,502 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:36,503 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:36,554 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:36,555 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:36,557 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:36,557 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:36,557 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:36,557 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:36,557 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:36,557 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:36,558 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:36,558 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:36,687 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:37,589 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:37,589 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:38,179 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:38,181 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:38,183 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:38,183 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:18:38,183 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.02
  eval_samples            =        872
  eval_samples_per_second =    847.087
  eval_steps_per_second   =      0.971
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.81it/s]
[INFO|training_args.py:1902] 2024-11-05 07:18:39,560 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:39,560 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:39,561 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:42,464 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:42,465 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:42,512 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:42,513 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:42,515 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:42,515 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:42,515 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:42,515 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:42,515 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:42,515 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:42,516 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:42,517 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:42,630 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:43,553 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:43,553 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:44,283 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:44,285 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:44,287 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:44,287 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:18:44,287 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.32
  eval_samples            =        872
  eval_samples_per_second =    658.562
  eval_steps_per_second   =      0.755
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.72it/s]
[INFO|training_args.py:1902] 2024-11-05 07:18:44,744 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:44,744 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:44,745 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:47,408 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:47,409 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:47,457 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:47,458 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:47,460 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:47,460 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:47,460 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:47,460 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:47,460 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:47,460 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:47,461 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:47,462 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:47,581 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:48,538 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:48,539 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:49,347 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:49,349 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:49,351 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:49,351 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:18:49,351 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.42
  eval_samples            =        277
  eval_samples_per_second =    650.851
  eval_steps_per_second   =       2.35
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.53it/s]
[INFO|training_args.py:1902] 2024-11-05 07:18:49,823 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:49,823 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:49,824 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:53,094 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:53,095 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:53,145 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:53,146 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:53,148 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:53,148 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:53,148 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:53,148 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:53,148 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:53,148 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:53,148 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:53,149 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:53,269 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:54,179 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:54,179 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:18:54,808 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:18:54,810 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:18:54,813 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:18:54,813 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:18:54,813 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =    629.941
  eval_steps_per_second   =      2.274
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.12it/s]
[INFO|training_args.py:1902] 2024-11-05 07:18:55,392 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:18:55,393 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:18:55,393 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:18:58,453 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:58,456 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:18:58,529 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:58,530 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:58,532 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:58,532 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:58,532 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:58,532 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:58,532 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:18:58,532 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:18:58,533 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:18:58,534 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:18:58,647 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:18:59,548 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:18:59,548 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:00,112 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:00,114 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:00,116 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:00,116 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:19:00,116 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.54
  eval_samples            =        408
  eval_samples_per_second =    742.984
  eval_steps_per_second   =      1.821
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.36it/s]
[INFO|training_args.py:1902] 2024-11-05 07:19:00,709 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:19:00,709 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:19:00,710 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:19:03,841 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:03,842 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:19:03,901 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:03,902 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:03,904 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:03,904 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:03,904 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:03,904 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:03,904 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:03,904 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:19:03,905 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:03,906 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:19:04,043 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:19:04,978 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:19:04,978 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:05,615 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:05,616 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:05,619 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:05,619 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:19:05,619 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.56
  eval_samples            =        408
  eval_samples_per_second =    726.778
  eval_steps_per_second   =      1.781
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 33.95it/s]
[INFO|training_args.py:1902] 2024-11-05 07:19:06,853 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:19:06,853 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:19:06,854 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:19:09,845 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:09,846 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:19:09,896 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:09,897 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:09,899 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:09,899 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:09,899 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:09,899 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:09,899 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:09,899 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:19:09,899 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:09,900 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:19:10,024 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:19:10,978 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:19:10,978 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:11,551 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:11,553 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:11,555 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:11,555 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:19:11,556 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.20
  eval_samples              =       1043
  eval_samples_per_second   =    867.787
  eval_steps_per_second     =      0.832
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.72it/s]
[INFO|training_args.py:1902] 2024-11-05 07:19:12,855 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:19:12,856 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:19:12,857 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:19:15,684 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:15,685 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:19:15,733 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:15,734 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:15,736 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:15,736 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:15,736 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:15,736 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:15,736 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:15,736 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:19:15,736 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:15,737 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:19:15,870 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:19:16,759 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:19:16,760 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:17,485 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:17,487 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:17,490 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:17,490 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:19:17,490 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.26
  eval_samples              =       1043
  eval_samples_per_second   =    826.259
  eval_steps_per_second     =      0.792
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.07s/it]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
[INFO|training_args.py:1902] 2024-11-05 07:19:23,574 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:19:23,575 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:19:23,576 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:19:26,425 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:26,426 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:19:26,474 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:26,474 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:26,476 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:26,476 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:26,476 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:26,476 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:26,476 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:26,476 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:19:26,477 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:26,478 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:19:26,605 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:19:27,541 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:19:27,541 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:28,140 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:28,142 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:28,145 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:28,145 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:19:28,145 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.00
  eval_samples            =       5463
  eval_samples_per_second =    909.421
  eval_steps_per_second   =      0.499
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.05s/it]
100% 3/3 [00:03<00:00,  1.25s/it]
100% 3/3 [00:03<00:00,  1.22s/it]
[INFO|training_args.py:1902] 2024-11-05 07:19:34,630 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:19:34,630 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:19:34,631 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:19:37,980 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:37,981 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:19:38,050 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:38,051 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:38,054 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:38,054 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:38,054 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:38,054 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:38,054 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:38,054 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:19:38,055 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:38,056 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:19:38,179 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:19:39,111 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:19:39,112 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:39,850 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:39,852 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:39,854 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:39,854 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:19:39,854 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.42
  eval_samples            =       5463
  eval_samples_per_second =    850.509
  eval_steps_per_second   =      0.467
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 28.48it/s]
[INFO|training_args.py:1902] 2024-11-05 07:19:41,606 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:19:41,606 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:19:41,606 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:19:44,792 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:44,793 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:19:44,838 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:44,839 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:44,841 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:44,841 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:44,841 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:44,841 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:44,841 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:19:44,841 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:19:44,842 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:19:44,843 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:19:44,984 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:19:45,886 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:19:45,887 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:19:46,527 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:19:46,529 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:19:46,532 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:19:46,532 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:19:46,532 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.71
  eval_samples            =       1500
  eval_samples_per_second =     874.88
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.583
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.70it/s]
2024-11-05 07:19:48,229 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:19:48,230 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:19:48,231 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:19:48,232 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:19:48,234 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:19:48,235 - INFO - [diffusion][Epoch 11994] DIFF reconstruction auto_encoder_models stsb: 0.8798503835459673
2024-11-05 07:19:48,236 - INFO - [diffusion][Epoch 11994] average DIFF reconstruction auto_encoder_models accuracy: 0.81103687523261
2024-11-05 07:19:48,237 - INFO - [diffusion][Epoch 11994] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6931407942238267), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798503835459673), ('stsb', 0.879850031558375)]
2024-11-05 07:19:48,238 - INFO - [diffusion][Epoch 11994] ---------------------------------
2024-11-05 07:19:48,443 - INFO - [diffusion][Epoch 11994] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:19:48,445 - INFO - [diffusion][Epoch 11995] Epoch 11996/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.67
  eval_samples            =       1500
  eval_samples_per_second =    898.029
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.599
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:19:52,746 - INFO - [diffusion][Epoch 11995] diffusion training Loss: 0.02684192731976509
2024-11-05 07:19:52,748 - INFO - [diffusion][Epoch 11995] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:20:04,379 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:04,381 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:04,382 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:07,076 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:07,077 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:07,139 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:07,140 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:07,142 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:07,142 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:07,142 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:07,142 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:07,142 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:07,142 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:07,143 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:07,144 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:07,288 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:08,148 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:08,148 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:08,674 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:08,676 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:08,679 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:08,679 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:20:08,679 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.81it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:09,810 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:09,810 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:09,811 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:13,123 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:13,124 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:13,172 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:13,173 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:13,175 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:13,175 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:13,175 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:13,175 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:13,175 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:13,175 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:13,176 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:13,177 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:13,299 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:14,238 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:14,238 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:14,837 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:14,839 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:14,842 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:14,842 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:20:14,842 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    812.254
  eval_steps_per_second   =      0.931
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.40it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:15,902 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:15,902 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:15,903 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:18,573 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:18,574 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:18,621 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:18,622 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:18,624 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:18,624 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:18,624 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:18,624 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:18,624 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:18,624 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:18,624 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:18,625 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:18,759 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:19,672 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:19,672 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:20,307 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:20,309 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:20,311 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:20,311 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:20:20,311 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.00
  eval_samples            =        872
  eval_samples_per_second =    867.653
  eval_steps_per_second   =      0.995
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 47.29it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:20,773 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:20,773 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:20,774 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:23,675 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:23,676 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:23,729 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:23,730 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:23,732 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:23,732 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:23,732 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:23,732 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:23,732 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:23,732 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:23,732 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:23,733 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:23,868 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:24,778 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:24,778 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:25,452 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:25,454 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:25,455 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:25,455 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:20:25,455 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.42
  eval_samples            =        277
  eval_samples_per_second =    648.692
  eval_steps_per_second   =      2.342
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 49.15it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:25,882 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:25,882 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:25,883 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:28,514 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:28,515 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:28,570 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:28,571 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:28,573 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:28,573 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:28,573 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:28,573 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:28,573 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:28,573 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:28,573 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:28,574 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:28,697 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:29,594 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:29,595 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:30,437 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:30,439 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:30,442 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:30,442 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:20:30,442 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.39
  eval_samples            =        277
  eval_samples_per_second =    708.595
  eval_steps_per_second   =      2.558
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.65it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:31,012 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:31,013 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:31,014 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:34,221 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:34,223 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:34,277 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:34,278 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:34,280 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:34,280 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:34,280 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:34,280 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:34,280 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:34,280 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:34,280 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:34,281 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:34,405 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:35,308 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:35,309 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:35,861 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:35,863 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:35,866 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:35,866 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:20:35,866 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.53
  eval_samples            =        408
  eval_samples_per_second =    759.492
  eval_steps_per_second   =      1.861
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.21it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:36,463 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:36,466 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:36,466 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:39,271 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:39,272 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:39,329 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:39,330 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:39,334 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:39,334 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:39,334 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:39,334 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:39,334 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:39,334 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:39,334 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:39,335 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:39,456 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:40,373 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:40,373 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:41,043 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:41,045 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:41,048 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:41,048 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:20:41,048 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.56
  eval_samples            =        408
  eval_samples_per_second =    721.421
  eval_steps_per_second   =      1.768
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.11it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:42,278 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:42,279 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:42,280 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:45,284 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:45,285 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:45,338 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:45,339 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:45,341 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:45,341 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:45,341 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:45,341 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:45,341 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:45,341 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:45,342 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:45,343 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:45,463 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:46,389 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:46,389 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:47,207 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:47,209 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:47,211 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:47,211 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:20:47,211 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.19
  eval_samples              =       1043
  eval_samples_per_second   =     870.61
  eval_steps_per_second     =      0.835
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 34.11it/s]
[INFO|training_args.py:1902] 2024-11-05 07:20:48,478 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:48,479 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:48,480 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:20:51,691 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:51,692 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:20:51,742 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:51,743 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:51,745 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:51,745 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:51,745 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:51,745 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:51,745 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:20:51,745 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:20:51,745 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:20:51,746 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:20:51,869 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:20:52,788 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:20:52,788 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:20:53,413 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:20:53,415 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:20:53,418 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:20:53,418 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:20:53,418 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.22
  eval_samples              =       1043
  eval_samples_per_second   =    849.984
  eval_steps_per_second     =      0.815
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.03s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
[INFO|training_args.py:1902] 2024-11-05 07:20:59,396 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:20:59,396 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:20:59,397 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:02,106 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:02,107 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:02,157 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:02,158 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:02,160 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:02,160 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:02,160 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:02,160 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:02,160 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:02,160 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:02,161 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:02,162 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:02,285 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:03,187 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:03,187 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:21:03,989 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:21:03,991 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:21:03,993 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:21:03,993 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:21:03,993 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.92
  eval_samples            =       5463
  eval_samples_per_second =    922.002
  eval_steps_per_second   =      0.506
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.08it/s]
100% 3/3 [00:03<00:00,  1.14s/it]
100% 3/3 [00:03<00:00,  1.12s/it]
[INFO|training_args.py:1902] 2024-11-05 07:21:10,022 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:21:10,022 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:21:10,023 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:12,775 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:12,777 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:12,826 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:12,827 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:12,830 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:12,830 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:12,830 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:12,830 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:12,830 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:12,830 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:12,831 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:12,832 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:12,973 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:13,852 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:13,853 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:21:14,421 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:21:14,423 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:21:14,426 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:21:14,426 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:21:14,426 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.97
  eval_samples            =       5463
  eval_samples_per_second =    914.934
  eval_steps_per_second   =      0.502
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.33it/s]
[INFO|training_args.py:1902] 2024-11-05 07:21:16,015 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:21:16,015 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:21:16,016 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:18,854 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:18,855 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:18,956 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:18,957 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:18,960 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:18,960 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:18,960 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:18,960 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:18,960 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:18,960 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:18,960 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:18,961 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:19,105 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:20,010 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:20,010 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:21:20,584 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:21:20,586 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:21:20,588 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:21:20,588 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:21:20,588 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.55
  eval_samples            =       1500
  eval_samples_per_second =     962.63
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.642
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 30.92it/s]
2024-11-05 07:21:22,293 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:21:22,295 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:21:22,296 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:21:22,297 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:21:22,298 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:21:22,299 - INFO - [diffusion][Epoch 11995] DIFF reconstruction auto_encoder_models stsb: 0.8798503325824358
2024-11-05 07:21:22,300 - INFO - [diffusion][Epoch 11995] average DIFF reconstruction auto_encoder_models accuracy: 0.8110367652875875
2024-11-05 07:21:22,301 - INFO - [diffusion][Epoch 11995] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6931407942238267), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.879848763181636), ('stsb', 0.8798503325824358)]
2024-11-05 07:21:22,302 - INFO - [diffusion][Epoch 11995] ---------------------------------
2024-11-05 07:21:22,514 - INFO - [diffusion][Epoch 11995] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:21:22,515 - INFO - [diffusion][Epoch 11996] Epoch 11997/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.67
  eval_samples            =       1500
  eval_samples_per_second =    894.204
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.596
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:21:26,740 - INFO - [diffusion][Epoch 11996] diffusion training Loss: 0.026064009871333838
2024-11-05 07:21:26,742 - INFO - [diffusion][Epoch 11996] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:21:38,690 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:21:38,691 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:21:38,692 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:41,320 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:41,321 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:41,379 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:41,380 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:41,384 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:41,384 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:41,384 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:41,384 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:41,384 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:41,384 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:41,385 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:41,386 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:41,512 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:42,454 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:42,454 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:21:43,073 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:21:43,075 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:21:43,078 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:21:43,078 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:21:43,078 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.62it/s]
[INFO|training_args.py:1902] 2024-11-05 07:21:44,484 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:21:44,484 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:21:44,485 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:47,469 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:47,471 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:47,527 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:47,528 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:47,530 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:47,530 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:47,530 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:47,530 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:47,530 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:47,530 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:47,530 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:47,531 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:47,649 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:48,602 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:48,603 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:21:49,408 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:21:49,411 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:21:49,413 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:21:49,413 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:21:49,413 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.34
  eval_samples            =        872
  eval_samples_per_second =    649.731
  eval_steps_per_second   =      0.745
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.58it/s]
[INFO|training_args.py:1902] 2024-11-05 07:21:50,466 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:21:50,467 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:21:50,468 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:53,076 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:53,077 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:53,151 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:53,152 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:53,154 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:53,154 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:53,154 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:53,154 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:53,154 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:53,154 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:53,155 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:53,156 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:53,289 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:54,200 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:54,201 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:21:54,881 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:21:54,885 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:21:54,888 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:21:54,888 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:21:54,888 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:00.99
  eval_samples            =        872
  eval_samples_per_second =    880.007
  eval_steps_per_second   =      1.009
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 44.00it/s]
[INFO|training_args.py:1902] 2024-11-05 07:21:55,357 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:21:55,357 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:21:55,358 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:21:58,793 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:58,794 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:21:58,844 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:58,845 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:58,847 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:58,847 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:58,847 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:58,847 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:58,847 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:21:58,847 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:21:58,847 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:21:58,848 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:21:58,970 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:21:59,888 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:21:59,889 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:00,526 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:00,528 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:00,530 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:00,531 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:22:00,531 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =     634.88
  eval_steps_per_second   =      2.292
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.16it/s]
[INFO|training_args.py:1902] 2024-11-05 07:22:00,972 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:00,973 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:00,973 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:04,020 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:04,021 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:04,067 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:04,068 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:04,070 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:04,070 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:04,070 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:04,070 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:04,070 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:04,070 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:04,071 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:04,072 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:04,180 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:05,078 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:05,079 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:05,698 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:05,700 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:05,703 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:05,703 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:22:05,703 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    679.685
  eval_steps_per_second   =      2.454
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.37it/s]
[INFO|training_args.py:1902] 2024-11-05 07:22:06,566 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:06,567 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:06,568 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:09,570 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:09,571 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:09,629 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:09,630 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:09,632 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:09,633 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:09,633 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:09,633 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:09,633 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:09,633 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:09,633 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:09,635 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:09,754 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:10,692 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:10,692 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:11,307 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:11,309 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:11,311 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:11,312 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:22:11,312 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.82
  eval_samples            =        408
  eval_samples_per_second =    494.795
  eval_steps_per_second   =      1.213
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.07it/s]
[INFO|training_args.py:1902] 2024-11-05 07:22:11,886 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:11,887 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:11,887 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:15,811 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:15,812 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:15,869 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:15,870 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:15,872 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:15,872 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:15,872 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:15,872 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:15,872 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:15,872 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:15,873 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:15,874 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:15,999 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:16,888 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:16,888 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:17,611 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:17,612 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:17,614 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:17,614 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:22:17,614 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3451
  eval_runtime            = 0:00:00.53
  eval_samples            =        408
  eval_samples_per_second =    756.769
  eval_steps_per_second   =      1.855
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 37.29it/s]
[INFO|training_args.py:1902] 2024-11-05 07:22:18,700 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:18,701 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:18,702 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:21,413 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:21,414 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:21,477 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:21,478 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:21,480 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:21,480 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:21,480 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:21,480 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:21,480 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:21,480 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:21,480 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:21,481 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:21,601 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:22,486 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:22,486 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:23,097 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:23,099 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:23,102 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:23,102 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:22:23,102 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.05
  eval_samples              =       1043
  eval_samples_per_second   =    993.064
  eval_steps_per_second     =      0.952
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 45.38it/s]
[INFO|training_args.py:1902] 2024-11-05 07:22:24,621 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:24,621 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:24,622 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:27,356 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:27,357 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:27,408 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:27,409 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:27,411 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:27,411 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:27,411 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:27,411 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:27,411 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:27,411 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:27,412 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:27,413 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:27,532 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:28,453 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:28,453 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:29,056 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:29,059 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:29,061 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:29,061 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:22:29,061 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.48
  eval_samples              =       1043
  eval_samples_per_second   =     700.11
  eval_steps_per_second     =      0.671
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.00it/s]
100% 3/3 [00:03<00:00,  1.11s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
[INFO|training_args.py:1902] 2024-11-05 07:22:34,986 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:34,987 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:34,988 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:37,976 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:37,977 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:38,032 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:38,033 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:38,035 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:38,035 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:38,035 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:38,035 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:38,035 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:38,035 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:38,036 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:38,037 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:38,155 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:39,056 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:39,057 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:39,729 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:39,731 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:39,733 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:39,733 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:22:39,733 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.85
  eval_samples            =       5463
  eval_samples_per_second =    932.593
  eval_steps_per_second   =      0.512
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:01<00:00,  1.03it/s]
100% 3/3 [00:03<00:00,  1.18s/it]
100% 3/3 [00:03<00:00,  1.15s/it]
[INFO|training_args.py:1902] 2024-11-05 07:22:45,760 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:45,760 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:45,761 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:48,917 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:48,918 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:49,008 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:49,009 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:49,013 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:49,013 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:49,013 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:49,013 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:49,013 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:49,013 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:49,013 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:49,014 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:49,159 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:50,094 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:50,095 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:50,666 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:50,668 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:50,670 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:50,670 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:22:50,670 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.96
  eval_samples            =       5463
  eval_samples_per_second =    915.535
  eval_steps_per_second   =      0.503
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.02it/s]
[INFO|training_args.py:1902] 2024-11-05 07:22:52,291 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:22:52,291 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:22:52,292 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:22:55,261 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:55,262 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:22:55,304 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:55,305 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:55,307 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:55,307 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:55,307 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:55,307 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:55,307 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:22:55,307 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:22:55,308 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:22:55,309 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:22:55,425 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:22:56,352 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:22:56,352 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:22:57,070 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:22:57,072 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:22:57,074 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:22:57,074 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:22:57,074 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.58
  eval_samples            =       1500
  eval_samples_per_second =    945.598
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =       0.63
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 25.06it/s]
2024-11-05 07:22:58,801 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:22:58,803 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:22:58,804 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:22:58,805 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:22:58,806 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:22:58,808 - INFO - [diffusion][Epoch 11996] DIFF reconstruction auto_encoder_models stsb: 0.8798502195172502
2024-11-05 07:22:58,809 - INFO - [diffusion][Epoch 11996] average DIFF reconstruction auto_encoder_models accuracy: 0.8107358937081046
2024-11-05 07:22:58,810 - INFO - [diffusion][Epoch 11996] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798502195172502), ('stsb', 0.879848525596276)]
2024-11-05 07:22:58,811 - INFO - [diffusion][Epoch 11996] ---------------------------------
2024-11-05 07:22:59,023 - INFO - [diffusion][Epoch 11996] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:22:59,024 - INFO - [diffusion][Epoch 11997] Epoch 11998/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.69
  eval_samples            =       1500
  eval_samples_per_second =    883.491
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.589
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:23:03,218 - INFO - [diffusion][Epoch 11997] diffusion training Loss: 0.02668474381789565
2024-11-05 07:23:03,220 - INFO - [diffusion][Epoch 11997] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:23:15,271 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:15,273 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:15,275 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:18,153 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:18,154 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:18,214 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:18,215 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:18,217 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:18,217 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:18,217 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:18,217 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:18,217 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:18,217 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:18,217 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:18,218 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:18,354 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:19,253 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:19,253 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:19,885 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:19,887 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:19,889 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:19,889 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:23:19,889 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.55it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:21,240 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:21,240 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:21,241 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:24,249 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:24,250 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:24,303 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:24,304 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:24,305 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:24,306 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:24,306 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:24,306 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:24,306 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:24,306 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:24,306 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:24,307 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:24,423 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:25,198 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:25,198 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:25,789 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:25,791 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:25,793 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:25,793 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:23:25,793 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.29
  eval_samples            =        872
  eval_samples_per_second =    673.923
  eval_steps_per_second   =      0.773
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.75it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:26,815 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:26,816 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:26,817 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:29,683 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:29,684 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:29,731 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:29,732 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:29,734 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:29,734 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:29,734 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:29,734 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:29,734 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:29,734 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:29,735 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:29,736 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:29,852 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:30,769 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:30,769 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:31,462 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:31,464 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:31,466 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:31,467 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:23:31,467 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:00.96
  eval_samples            =        872
  eval_samples_per_second =    901.064
  eval_steps_per_second   =      1.033
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 43.33it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:31,886 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:31,887 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:31,887 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:34,948 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:34,950 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:35,003 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:35,003 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:35,005 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:35,005 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:35,005 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:35,006 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:35,006 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:35,006 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:35,006 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:35,007 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:35,118 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:36,058 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:36,058 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:36,683 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:36,685 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:36,688 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:36,688 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:23:36,688 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.38
  eval_samples            =        277
  eval_samples_per_second =    715.071
  eval_steps_per_second   =      2.581
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 54.07it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:37,098 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:37,098 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:37,099 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:40,123 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:40,124 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:40,173 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:40,174 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:40,175 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:40,176 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:40,176 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:40,176 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:40,176 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:40,176 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:40,176 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:40,177 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:40,298 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:41,198 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:41,198 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:41,806 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:41,808 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:41,811 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:41,811 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:23:41,811 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.38
  eval_samples            =        277
  eval_samples_per_second =    723.221
  eval_steps_per_second   =      2.611
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.95it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:42,652 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:42,652 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:42,653 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:45,347 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:45,348 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:45,398 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:45,399 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:45,401 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:45,401 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:45,401 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:45,401 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:45,401 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:45,401 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:45,402 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:45,403 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:45,519 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:46,439 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:46,439 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:47,030 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:47,032 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:47,035 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:47,035 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:23:47,035 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.80
  eval_samples            =        408
  eval_samples_per_second =     504.93
  eval_steps_per_second   =      1.238
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.09it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:47,595 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:47,595 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:47,596 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:50,716 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:50,717 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:50,773 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:50,774 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:50,777 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:50,777 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:50,777 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:50,777 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:50,777 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:50,777 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:50,778 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:50,779 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:50,895 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:51,814 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:51,814 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:52,428 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:52,430 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:52,432 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:52,432 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:23:52,432 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3451
  eval_runtime            = 0:00:00.52
  eval_samples            =        408
  eval_samples_per_second =     772.58
  eval_steps_per_second   =      1.894
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.58it/s]
[INFO|training_args.py:1902] 2024-11-05 07:23:53,625 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:23:53,626 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:23:53,626 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:23:57,382 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:57,383 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:23:57,446 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:57,447 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:57,449 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:57,449 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:57,449 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:57,449 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:57,449 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:23:57,449 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:23:57,450 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:23:57,451 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:23:57,569 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:23:58,398 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:23:58,398 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:23:58,936 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:23:58,938 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:23:58,941 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:23:58,941 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:23:58,941 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.16
  eval_samples              =       1043
  eval_samples_per_second   =    898.571
  eval_steps_per_second     =      0.862
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 36.52it/s]
[INFO|training_args.py:1902] 2024-11-05 07:24:00,085 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:24:00,085 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:24:00,086 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:24:33,932 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:33,933 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:24:34,021 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:34,022 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:34,024 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:34,024 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:34,024 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:34,024 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:34,024 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:34,024 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:24:34,025 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:34,026 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:24:34,153 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:24:35,059 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:24:35,060 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:24:35,657 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:24:35,659 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:24:35,662 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:24:35,662 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:24:35,662 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.10
  eval_samples              =       1043
  eval_samples_per_second   =    939.859
  eval_steps_per_second     =      0.901
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.05s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
100% 3/3 [00:03<00:00,  1.13s/it]
[INFO|training_args.py:1902] 2024-11-05 07:24:41,909 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:24:41,909 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:24:41,910 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:24:44,644 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:44,645 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:24:44,698 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:44,699 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:44,701 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:44,701 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:44,702 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:44,702 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:44,702 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:44,702 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:24:44,702 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:44,703 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:24:44,840 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:24:45,752 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:24:45,752 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:24:46,269 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:24:46,271 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:24:46,274 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:24:46,274 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:24:46,274 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.18
  eval_samples            =       5463
  eval_samples_per_second =     882.86
  eval_steps_per_second   =      0.485
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.14s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
100% 3/3 [00:03<00:00,  1.17s/it]
[INFO|training_args.py:1902] 2024-11-05 07:24:52,308 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:24:52,308 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:24:52,309 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:24:55,067 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:55,069 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:24:55,123 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:55,124 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:55,127 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:55,127 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:55,127 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:55,127 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:55,127 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:24:55,127 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:24:55,128 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:24:55,129 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:24:55,251 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:24:56,184 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:24:56,184 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:24:56,744 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:24:56,745 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:24:56,747 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:24:56,747 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:24:56,747 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.98
  eval_samples            =       5463
  eval_samples_per_second =    913.398
  eval_steps_per_second   =      0.502
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 31.50it/s]
[INFO|training_args.py:1902] 2024-11-05 07:24:58,312 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:24:58,312 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:24:58,313 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:01,293 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:01,294 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:01,341 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:01,342 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:01,344 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:01,344 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:01,344 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:01,344 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:01,344 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:01,344 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:01,344 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:01,345 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:01,469 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:02,403 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:02,404 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:03,130 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:03,132 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:03,135 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:03,135 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:25:03,135 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =      0.603
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.53
  eval_samples            =       1500
  eval_samples_per_second =    979.968
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.653
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.92it/s]
2024-11-05 07:25:04,838 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:25:04,840 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:25:04,841 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:25:04,842 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:25:04,843 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:25:04,844 - INFO - [diffusion][Epoch 11997] DIFF reconstruction auto_encoder_models stsb: 0.8798471640873637
2024-11-05 07:25:04,845 - INFO - [diffusion][Epoch 11997] average DIFF reconstruction auto_encoder_models accuracy: 0.810735454687436
2024-11-05 07:25:04,846 - INFO - [diffusion][Epoch 11997] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798471640873637), ('stsb', 0.8798463127781401)]
2024-11-05 07:25:04,847 - INFO - [diffusion][Epoch 11997] ---------------------------------
2024-11-05 07:25:05,061 - INFO - [diffusion][Epoch 11997] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:25:05,062 - INFO - [diffusion][Epoch 11998] Epoch 11999/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =      0.603
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.67
  eval_samples            =       1500
  eval_samples_per_second =    893.486
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.596
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:25:09,270 - INFO - [diffusion][Epoch 11998] diffusion training Loss: 0.027879029978066683
2024-11-05 07:25:09,272 - INFO - [diffusion][Epoch 11998] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:25:21,384 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:21,385 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:21,386 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:24,584 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:24,585 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:24,671 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:24,672 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:24,674 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:24,674 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:24,674 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:24,674 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:24,674 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:24,674 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:24,674 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:24,675 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:24,821 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:25,755 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:25,755 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:26,326 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:26,328 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:26,332 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:26,332 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:25:26,332 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  3.53it/s]
[INFO|training_args.py:1902] 2024-11-05 07:25:27,708 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:27,708 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:27,709 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:30,708 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:30,709 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:30,755 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:30,756 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:30,758 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:30,758 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:30,758 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:30,758 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:30,758 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:30,758 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:30,759 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:30,759 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:30,896 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:31,822 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:31,822 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:32,640 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:32,643 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:32,645 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:32,645 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:25:32,645 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.31
  eval_samples            =        872
  eval_samples_per_second =    661.866
  eval_steps_per_second   =      0.759
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.56it/s]
[INFO|training_args.py:1902] 2024-11-05 07:25:33,671 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:33,671 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:33,672 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:36,464 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:36,465 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:36,513 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:36,514 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:36,516 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:36,516 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:36,516 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:36,516 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:36,516 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:36,516 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:36,516 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:36,517 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:36,635 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:37,546 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:37,547 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:38,123 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:38,125 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:38,128 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:38,128 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:25:38,128 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:00.96
  eval_samples            =        872
  eval_samples_per_second =    904.757
  eval_steps_per_second   =      1.038
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.87it/s]
[INFO|training_args.py:1902] 2024-11-05 07:25:38,594 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:38,594 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:38,595 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:41,539 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:41,541 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:41,586 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:41,587 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:41,589 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:41,589 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:41,589 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:41,589 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:41,589 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:41,589 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:41,590 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:41,591 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:41,710 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:42,630 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:42,630 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:43,244 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:43,247 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:43,249 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:43,249 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:25:43,249 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.43
  eval_samples            =        277
  eval_samples_per_second =     638.59
  eval_steps_per_second   =      2.305
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 42.72it/s]
[INFO|training_args.py:1902] 2024-11-05 07:25:43,704 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:43,704 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:43,705 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:46,456 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:46,457 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:46,499 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:46,500 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:46,502 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:46,502 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:46,502 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:46,502 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:46,502 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:46,502 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:46,503 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:46,504 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:46,631 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:47,544 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:47,544 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:48,334 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:48,336 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:48,338 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:48,338 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:25:48,338 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.42
  eval_samples            =        277
  eval_samples_per_second =    655.039
  eval_steps_per_second   =      2.365
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.45it/s]
[INFO|training_args.py:1902] 2024-11-05 07:25:48,904 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:48,904 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:48,905 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:52,653 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:52,654 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:52,704 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:52,705 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:52,706 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:52,706 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:52,706 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:52,706 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:52,706 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:52,706 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:52,707 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:52,708 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:52,830 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:53,750 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:53,753 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:54,377 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:54,379 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:54,382 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:54,382 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:25:54,382 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3453
  eval_runtime            = 0:00:00.53
  eval_samples            =        408
  eval_samples_per_second =    767.166
  eval_steps_per_second   =       1.88
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 39.48it/s]
[INFO|training_args.py:1902] 2024-11-05 07:25:54,938 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:25:54,939 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:25:54,939 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:25:57,855 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:57,856 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:25:57,941 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:57,942 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:57,945 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:57,945 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:57,945 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:57,945 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:57,945 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:25:57,945 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:25:57,946 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:25:57,947 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:25:58,072 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:25:59,018 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:25:59,018 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:25:59,789 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:25:59,792 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:25:59,796 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:25:59,796 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:25:59,796 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3451
  eval_runtime            = 0:00:00.52
  eval_samples            =        408
  eval_samples_per_second =    780.443
  eval_steps_per_second   =      1.913
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.35it/s]
[INFO|training_args.py:1902] 2024-11-05 07:26:00,930 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:26:00,930 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:26:00,931 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:26:03,943 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:03,945 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:26:03,995 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:03,996 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:03,998 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:03,998 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:03,998 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:03,998 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:03,998 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:03,998 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:26:03,998 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:03,999 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:26:04,140 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:26:05,040 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:26:05,040 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:26:05,646 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:26:05,647 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:26:05,650 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:26:05,651 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:26:05,651 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.10
  eval_samples              =       1043
  eval_samples_per_second   =    946.013
  eval_steps_per_second     =      0.907
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.45it/s]
[INFO|training_args.py:1902] 2024-11-05 07:26:07,004 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:26:07,004 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:26:07,005 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:26:10,100 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:10,101 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:26:10,165 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:10,166 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:10,168 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:10,168 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:10,168 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:10,168 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:10,168 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:10,168 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:26:10,169 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:10,170 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:26:10,287 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:26:11,214 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:26:11,214 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:26:11,898 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:26:11,900 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:26:11,903 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:26:11,903 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:26:11,903 >>   Batch size = 2048
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.31
  eval_samples              =       1043
  eval_samples_per_second   =    791.604
  eval_steps_per_second     =      0.759
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.04s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
100% 3/3 [00:03<00:00,  1.09s/it]
[INFO|training_args.py:1902] 2024-11-05 07:26:17,930 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:26:17,931 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:26:17,933 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:26:21,919 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:21,920 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:26:21,975 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:21,976 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:21,978 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:21,978 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:21,978 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:21,978 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:21,978 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:21,978 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:26:21,979 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:21,980 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:26:22,103 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:26:23,036 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:26:23,036 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:26:23,730 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:26:23,732 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:26:23,735 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:26:23,735 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:26:23,735 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.95
  eval_samples            =       5463
  eval_samples_per_second =     916.67
  eval_steps_per_second   =      0.503
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.08s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
100% 3/3 [00:03<00:00,  1.16s/it]
[INFO|training_args.py:1902] 2024-11-05 07:26:29,603 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:26:29,603 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:26:29,604 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:26:32,898 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:32,899 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:26:32,950 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:32,951 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:32,953 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:32,953 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:32,953 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:32,953 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:32,953 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:32,953 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:26:32,953 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:32,954 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:26:33,095 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:26:34,031 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:26:34,031 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:26:34,651 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:26:34,654 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:26:34,656 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:26:34,656 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:26:34,656 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.80
  eval_samples            =       5463
  eval_samples_per_second =    940.889
  eval_steps_per_second   =      0.517
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 29.42it/s]
[INFO|training_args.py:1902] 2024-11-05 07:26:36,294 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:26:36,294 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:26:36,295 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:26:39,030 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:39,031 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:26:39,082 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:39,083 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:39,085 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:39,085 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:39,085 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:39,085 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:39,085 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:26:39,085 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:26:39,085 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:26:39,086 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:26:39,232 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:26:40,126 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:26:40,127 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:26:40,792 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:26:40,794 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:26:40,796 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:26:40,796 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:26:40,796 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.60
  eval_samples            =       1500
  eval_samples_per_second =    937.276
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.625
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 23.59it/s]
2024-11-05 07:26:42,492 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:26:42,494 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:26:42,496 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:26:42,497 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:26:42,498 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:26:42,499 - INFO - [diffusion][Epoch 11998] DIFF reconstruction auto_encoder_models stsb: 0.8798503985961831
2024-11-05 07:26:42,500 - INFO - [diffusion][Epoch 11998] average DIFF reconstruction auto_encoder_models accuracy: 0.8107358885413612
2024-11-05 07:26:42,501 - INFO - [diffusion][Epoch 11998] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798503985961831), ('stsb', 0.8798482845164236)]
2024-11-05 07:26:42,502 - INFO - [diffusion][Epoch 11998] ---------------------------------
2024-11-05 07:26:42,707 - INFO - [diffusion][Epoch 11998] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:26:42,708 - INFO - [diffusion][Epoch 11999] Epoch 12000/12000
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8798
  eval_runtime            = 0:00:01.66
  eval_samples            =       1500
  eval_samples_per_second =    898.897
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.599
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False
  warnings.warn(f"input's size at dim={feature_dim} does not match num_features. "
2024-11-05 07:26:46,996 - INFO - [diffusion][Epoch 11999] diffusion training Loss: 0.031563971657305956
2024-11-05 07:26:46,999 - INFO - [diffusion][Epoch 11999] diffusion learning rate: 0.001
[INFO|training_args.py:1902] 2024-11-05 07:26:59,139 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:26:59,141 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:26:59,143 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:02,650 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:02,651 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:02,706 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:02,707 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:02,711 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:02,711 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:02,711 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:02,711 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:02,711 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:02,711 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:02,711 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:02,712 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:02,842 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:03,776 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:03,777 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:04,564 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:04,566 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:04,569 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:04,569 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:27:04,569 >>   Batch size = 2048
The tensors are equal.
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 40.08it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:05,955 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:05,955 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:05,956 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:08,854 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:08,855 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:08,913 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:08,914 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:08,916 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:08,916 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:08,916 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:08,916 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:08,916 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:08,916 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:08,916 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:08,917 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:09,028 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:09,976 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:09,976 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:10,651 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:10,653 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:10,655 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:10,655 >>   Num examples = 872
[INFO|trainer.py:3389] 2024-11-05 07:27:10,656 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.32
  eval_samples            =        872
  eval_samples_per_second =    655.904
  eval_steps_per_second   =      0.752
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 872
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 38.81it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:11,797 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:11,798 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:11,799 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:15,715 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:15,716 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:15,767 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:15,767 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:15,769 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:15,769 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:15,769 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:15,769 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:15,769 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:15,769 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:15,770 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:15,771 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:15,906 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:16,789 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:16,789 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:17,383 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:17,384 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:17,386 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:17,386 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:27:17,386 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9346
  eval_loss               =     0.2099
  eval_runtime            = 0:00:01.07
  eval_samples            =        872
  eval_samples_per_second =    808.724
  eval_steps_per_second   =      0.927
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.08it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:17,794 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:17,794 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:17,795 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:20,939 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:20,940 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:20,990 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:20,991 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:20,993 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:20,993 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:20,993 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:20,993 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:20,993 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:20,993 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:20,993 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:20,994 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:21,115 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:22,010 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:22,010 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:22,619 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:22,621 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:22,623 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:22,623 >>   Num examples = 277
[INFO|trainer.py:3389] 2024-11-05 07:27:22,623 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6895
  eval_loss               =     0.5929
  eval_runtime            = 0:00:00.37
  eval_samples            =        277
  eval_samples_per_second =     737.86
  eval_steps_per_second   =      2.664
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 277
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 46.58it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:23,056 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:23,056 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:23,057 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:26,047 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:26,048 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:26,142 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:26,143 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:26,145 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:26,145 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:26,145 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:26,145 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:26,145 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:26,145 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:26,145 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:26,146 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:26,272 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:27,181 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:27,181 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:27,854 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:27,856 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:27,858 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:27,858 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:27:27,858 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.6931
  eval_loss               =     0.5928
  eval_runtime            = 0:00:00.40
  eval_samples            =        277
  eval_samples_per_second =    690.606
  eval_steps_per_second   =      2.493
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 41.76it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:28,635 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:28,635 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:28,636 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:31,115 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:31,116 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:31,165 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:31,166 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:31,168 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:31,168 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:31,168 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:31,168 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:31,168 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:31,168 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:31,169 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:31,170 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:31,293 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:32,209 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:32,210 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:32,885 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:32,887 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:32,890 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:32,890 >>   Num examples = 408
[INFO|trainer.py:3389] 2024-11-05 07:27:32,890 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.74
  eval_samples            =        408
  eval_samples_per_second =    549.339
  eval_steps_per_second   =      1.346
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 408
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 32.20it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:33,508 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:33,509 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:33,510 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:36,624 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:36,625 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:36,677 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:36,678 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:36,680 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:36,680 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:36,680 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:36,680 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:36,680 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:36,680 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:36,681 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:36,682 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:36,801 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:37,741 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:37,741 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:38,347 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:38,349 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:38,351 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:38,351 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:27:38,352 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.8578
  eval_combined_score     =     0.8762
  eval_f1                 =     0.8945
  eval_loss               =     0.3452
  eval_runtime            = 0:00:00.58
  eval_samples            =        408
  eval_samples_per_second =    700.302
  eval_steps_per_second   =      1.716
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 35.55it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:39,141 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:39,142 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:39,143 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:41,877 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:41,878 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:41,932 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:41,933 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:41,935 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:41,935 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:41,935 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:41,935 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:41,935 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:41,935 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:41,935 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:41,936 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:42,063 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:43,015 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:43,020 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:43,683 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:43,686 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:43,688 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:43,688 >>   Num examples = 1043
[INFO|trainer.py:3389] 2024-11-05 07:27:43,688 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:00.75
  eval_samples              =       1043
  eval_samples_per_second   =   1376.652
  eval_steps_per_second     =       1.32
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1043
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00,  3.62it/s]
[INFO|training_args.py:1902] 2024-11-05 07:27:44,842 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:44,842 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:44,843 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:47,674 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:47,675 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:47,725 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:47,726 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:47,728 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:47,728 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:47,728 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:47,728 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:47,728 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:47,728 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:47,729 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:47,729 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:47,846 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:49,057 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:49,057 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:27:49,662 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:27:49,665 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:27:49,667 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:27:49,667 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:27:49,667 >>   Batch size = 2048
/home/jin509/anaconda3/envs/pdiff/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
***** eval metrics *****
  eval_loss                 =     0.4721
  eval_matthews_correlation =     0.5397
  eval_runtime              = 0:00:01.12
  eval_samples              =       1043
  eval_samples_per_second   =    930.359
  eval_steps_per_second     =      0.892
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.03s/it]
100% 3/3 [00:03<00:00,  1.10s/it]
100% 3/3 [00:03<00:00,  1.11s/it]
[INFO|training_args.py:1902] 2024-11-05 07:27:55,671 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:27:55,672 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:27:55,673 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:27:58,665 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:58,666 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:27:58,780 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:58,781 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:58,783 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:58,783 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:58,783 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:58,783 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:58,783 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:27:58,783 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:27:58,783 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:27:58,784 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:27:58,911 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:27:59,810 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:27:59,810 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:28:00,368 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:28:00,370 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence, question. If idx, sentence, question are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:28:00,373 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:28:00,373 >>   Num examples = 5463
[INFO|trainer.py:3389] 2024-11-05 07:28:00,373 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:05.93
  eval_samples            =       5463
  eval_samples_per_second =    920.432
  eval_steps_per_second   =      0.505
trainable params: 0 || all params: 124,942,082 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['question', 'sentence', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 5463
})]

  0% 0/3 [00:00<?, ?it/s]
 67% 2/3 [00:02<00:01,  1.01s/it]
100% 3/3 [00:03<00:00,  1.23s/it]
100% 3/3 [00:03<00:00,  1.21s/it]
[INFO|training_args.py:1902] 2024-11-05 07:28:06,708 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:28:06,708 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:28:06,709 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:28:10,371 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:28:10,372 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:28:10,546 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:28:10,547 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:10,551 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:10,551 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:10,551 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:10,551 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:10,551 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:10,551 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:28:10,552 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:28:10,553 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:28:10,682 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:28:11,603 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:28:11,604 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:28:12,224 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:28:12,226 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:28:12,229 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:28:12,229 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:28:12,229 >>   Batch size = 2048
***** eval metrics *****
  eval_accuracy           =     0.9244
  eval_loss               =     0.1968
  eval_runtime            = 0:00:06.27
  eval_samples            =       5463
  eval_samples_per_second =    871.074
  eval_steps_per_second   =      0.478
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 27.50it/s]
[INFO|training_args.py:1902] 2024-11-05 07:28:13,844 >> PyTorch: setting up devices
[INFO|training_args.py:1611] 2024-11-05 07:28:13,844 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[WARNING|integration_utils.py:82] 2024-11-05 07:28:13,845 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|configuration_utils.py:728] 2024-11-05 07:28:16,790 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:28:16,791 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:728] 2024-11-05 07:28:16,836 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:28:16,837 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:16,839 >> loading file vocab.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:16,839 >> loading file merges.txt from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:16,839 >> loading file tokenizer.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:16,839 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:16,839 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-05 07:28:16,839 >> loading file tokenizer_config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-11-05 07:28:16,840 >> loading configuration file config.json from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:791] 2024-11-05 07:28:16,841 >> Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3257] 2024-11-05 07:28:16,973 >> loading weights file model.safetensors from cache at /data3/user/jin509/hf_cache/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:3982] 2024-11-05 07:28:17,868 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3994] 2024-11-05 07:28:17,870 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:602] 2024-11-05 07:28:18,686 >> Using auto half precision backend
[INFO|trainer.py:766] 2024-11-05 07:28:18,688 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModel.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `PeftModel.forward`,  you can safely ignore this message.
[INFO|trainer.py:3384] 2024-11-05 07:28:18,691 >> ***** Running Evaluation *****
[INFO|trainer.py:3386] 2024-11-05 07:28:18,691 >>   Num examples = 1500
[INFO|trainer.py:3389] 2024-11-05 07:28:18,691 >>   Batch size = 2048
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.57
  eval_samples            =       1500
  eval_samples_per_second =    950.847
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.634
trainable params: 0 || all params: 124,941,313 || trainable%: 0.0
eval_datasets: [Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'],
    num_rows: 1500
})]

  0% 0/1 [00:00<?, ?it/s]
100% 1/1 [00:00<00:00, 24.96it/s]
2024-11-05 07:28:20,468 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models sst2: 0.9346330275229358
2024-11-05 07:28:20,470 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models rte: 0.6931407942238267
2024-11-05 07:28:20,472 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models mrpc: 0.8945454545454546
2024-11-05 07:28:20,474 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models cola: 0.5396512550123742
2024-11-05 07:28:20,475 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models qnli: 0.924400512538898
2024-11-05 07:28:20,476 - INFO - [diffusion][Epoch 11999] DIFF reconstruction auto_encoder_models stsb: 0.8798529911237275
2024-11-05 07:28:20,477 - INFO - [diffusion][Epoch 11999] average DIFF reconstruction auto_encoder_models accuracy: 0.8107363703098125
2024-11-05 07:28:20,478 - INFO - [diffusion][Epoch 11999] Diffusion reconstruction accuracy:[('sst2', 0.9346330275229358), ('sst2', 0.9346330275229358), ('rte', 0.6895306859205776), ('rte', 0.6931407942238267), ('mrpc', 0.8945454545454546), ('mrpc', 0.8945454545454546), ('cola', 0.5396512550123742), ('cola', 0.5396512550123742), ('qnli', 0.924400512538898), ('qnli', 0.924400512538898), ('stsb', 0.8798529911237275), ('stsb', 0.8798514732102936)]
2024-11-05 07:28:20,479 - INFO - [diffusion][Epoch 11999] ---------------------------------
2024-11-05 07:28:20,693 - INFO - [diffusion][Epoch 11999] /data3/user/jin509/lora-pdiff/dataset/roberta-base/lora_r_4/para_dataset_sst2_rte_mrpc_cola_qnli_stsb/layer_11_10_9_8_7_6_5_4_3_2_1_0/log/cond_clip_ddpm_training_log.txt
2024-11-05 07:28:20,701 - INFO - [diffusion][Epoch 11999] Training complete
***** eval metrics *****
  eval_combined_score     =     0.8822
  eval_loss               =     0.6029
  eval_pearson            =     0.8799
  eval_runtime            = 0:00:01.74
  eval_samples            =       1500
  eval_samples_per_second =    858.403
  eval_spearmanr          =     0.8846
  eval_steps_per_second   =      0.572
=====================================
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb: | 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: diffusion learning rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    diffusion train_loss █▇█▇█▅▅▇▄▃▅▅▅▅▅▅▆▄▅▄▄▄▄▄▂▃▅▄▂▃▁▂▄▄▂▃▄▂▂▂
wandb: 
wandb: Run summary:
wandb: diffusion learning rate 0.001
wandb:    diffusion train_loss 0.03156
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/jin509/para_diff/lora-cond-p-diff/wandb/offline-run-20241105_002035-2dh5lzu2
wandb: Find logs at: ./wandb/offline-run-20241105_002035-2dh5lzu2/logs
